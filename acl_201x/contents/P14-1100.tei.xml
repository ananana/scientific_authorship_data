<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spectral Unsupervised Parsing with Additive Tree Metrics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
							<email>apparikh@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<email>scohen@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<email>epxing@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spectral Unsupervised Parsing with Additive Tree Metrics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1062" to="1072"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a spectral approach for un-supervised constituent parsing that comes with theoretical guarantees on latent structure recovery. Our approach is grammar-less-we directly learn the bracketing structure of a given sentence without using a grammar model. The main algorithm is based on lifting the concept of additive tree metrics for structure learning of latent trees in the phylogenetic and machine learning communities to the case where the tree structure varies across examples. Although finding the &quot;minimal&quot; latent tree is NP-hard in general, for the case of pro-jective trees we find that it can be found using bilexical parsing algorithms. Empirically , our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Solutions to the problem of grammar induction have been long sought after since the early days of computational linguistics and are interesting both from cognitive and engineering perspectives. Cog- nitively, it is more plausible to assume that chil- dren obtain only terminal strings of parse trees and not the actual parse trees. This means the unsu- pervised setting is a better model for studying lan- guage acquisition. From the engineering perspec- tive, training data for unsupervised parsing exists in abundance (i.e. sentences and part-of-speech tags), and is much cheaper than the syntactically annotated data required for supervised training.</p><p>Most existing solutions treat the problem of un- supervised parsing by assuming a generative pro- cess over parse trees e.g. probabilistic context free grammars <ref type="bibr" target="#b25">(Jelinek et al., 1992)</ref>, and the con- stituent context model ( <ref type="bibr" target="#b26">Klein and Manning, 2002</ref>). Learning then reduces to finding a set of parame- ters that are estimated by identifying a local max- imum of an objective function such as the likeli- hood ( <ref type="bibr" target="#b26">Klein and Manning, 2002</ref>) or a variant of it ( <ref type="bibr" target="#b35">Smith and Eisner, 2005;</ref><ref type="bibr" target="#b8">Cohen and Smith, 2009;</ref><ref type="bibr" target="#b21">Headden et al., 2009;</ref><ref type="bibr" target="#b38">Spitkovsky et al., 2010b;</ref><ref type="bibr" target="#b15">Gillenwater et al., 2010;</ref><ref type="bibr" target="#b17">Golland et al., 2012)</ref>. Un- fortunately, finding the global maximum for these objective functions is usually intractable <ref type="bibr" target="#b9">(Cohen and Smith, 2012</ref>) which often leads to severe lo- cal optima problems (but see <ref type="bibr" target="#b18">Gormley and Eisner, 2013</ref>). Thus, strong experimental results are often achieved by initialization techniques ( <ref type="bibr" target="#b26">Klein and Manning, 2002;</ref><ref type="bibr" target="#b16">Gimpel and Smith, 2012)</ref>, incre- mental dataset use ( <ref type="bibr" target="#b37">Spitkovsky et al., 2010a</ref>) and other specialized techniques to avoid local optima such as count transforms <ref type="bibr" target="#b39">(Spitkovsky et al., 2013)</ref>. These approaches, while empirically promising, generally lack theoretical justification.</p><p>On the other hand, recently proposed spectral methods approach the problem via restriction of the PCFG model ( <ref type="bibr" target="#b23">Hsu et al., 2012</ref>) or matrix com- pletion ( <ref type="bibr" target="#b2">Bailly et al., 2013</ref>). These novel perspec- tives offer strong theoretical guarantees but are not designed to achieve competitive empirical results.</p><p>In this paper, we suggest a different approach, to provide a first step to bridging this theory- experiment gap. More specifically, we approach unsupervised constituent parsing from the per- spective of structure learning as opposed to pa- rameter learning. We associate each sentence with an undirected latent tree graphical model, which is a tree consisting of both observed variables (corre- sponding to the words in the sentence) and an ad- ditional set of latent variables that are unobserved in the data. This undirected latent tree is then di- rected via a direction mapping to give the final constituent parse.</p><p>In our framework, parsing reduces to finding the best latent structure for a given sentence. How- ever, due to the presence of latent variables, struc- ture learning of latent trees is substantially more complicated than in observed models. As before, one solution would be local search heuristics.</p><p>Intuitively, however, latent tree models en- code low rank dependencies among the observed variables permitting the development of "spec-tral" methods that can lead to provably correct solutions. In particular we leverage the con- cept of additive tree metrics <ref type="bibr" target="#b4">(Buneman, 1971;</ref><ref type="bibr" target="#b5">Buneman, 1974</ref>) in phylogenetics and machine learning that can create a special distance met- ric among the observed variables as a function of the underlying spectral dependencies <ref type="bibr" target="#b6">(Choi et al., 2011;</ref><ref type="bibr" target="#b24">Ishteva et al., 2012)</ref>. Additive tree met- rics can be leveraged by "meta-algorithms" such as neighbor-joining ( <ref type="bibr" target="#b33">Saitou and Nei, 1987)</ref> and recursive grouping <ref type="bibr" target="#b6">(Choi et al., 2011</ref>) to provide consistent learning algorithms for latent trees.</p><p>Moreover, we show that it is desirable to learn the "minimal" latent tree based on the tree metric ("minimum evolution" in phylogenetics). While this criterion is in general NP-hard <ref type="bibr" target="#b11">(Desper and Gascuel, 2005</ref>), for projective trees we find that a bilexical parsing algorithm can be used to find an exact solution efficiently <ref type="bibr" target="#b13">(Eisner and Satta, 1999)</ref>.</p><p>Unlike in phylogenetics and graphical models, where a single latent tree is constructed for all the data, in our case, each part of speech sequence is associated with its own parse tree. This leads to a severe data sparsity problem even for moderately long sentences. To handle this issue, we present a strategy that is inspired by ideas from kernel smoothing in the statistics community ( <ref type="bibr" target="#b41">Zhou et al., 2010;</ref><ref type="bibr" target="#b28">Kolar et al., 2010b;</ref><ref type="bibr" target="#b27">Kolar et al., 2010a)</ref>. This allows principled sharing of samples from different but similar underlying distributions.</p><p>We provide theoretical guarantees on the re- covery of the correct underlying latent tree and characterize the associated sample complexity un- der our technique. Empirically we evaluate our method on data in English, German and Chi- nese. Our algorithm performs favorably to <ref type="bibr" target="#b26">Klein and Manning's (2002)</ref> constituent-context model (CCM), without the need for careful initialization.</p><p>In addition, we also analyze CCM's sensitivity to initialization, and compare our results to Seginer's algorithm <ref type="bibr" target="#b34">(Seginer, 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Setting and Model</head><p>In this section, we detail the learning setting and a conditional tree model we learn the structure for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning Setting</head><p>Let w = (w 1 , ..., w ) be a vector of words corre- sponding to a sentence of length . Each w i is rep- resented by a vector in R p for p ∈ N. The vector is an embedding of the word in some space, cho- sen from a fixed dictionary that maps word types to R p . In addition, let x = (x 1 , ..., x ) be the as- sociated vector of part-of-speech (POS) tags (i.e. x i is the POS tag of w i ).</p><p>In our learning algorithm, we assume that ex- amples of the form (w (i) , x (i) ) for i ∈ [N ] = {1, . . . , N } are given, and the goal is to predict a bracketing parse tree for each of these examples. The word embeddings are used during the learn- ing process, but the final decoder that the learning algorithm outputs maps a POS tag sequence x to a parse tree. While ideally we would want to use the word information in decoding as well, much of the syntax of a sentence is determined by the POS tags, and relatively high level of accuracy can be achieved by learning, for example, a supervised parser from POS tag sequences.</p><p>Just like our decoder, our model assumes that the bracketing of a given sentence is a function of its POS tags. The POS tags are generated from some distribution, followed by a determin- istic generation of the bracketing parse tree. Then, latent states are generated for each bracket, and finally, the latent states at the yield of the bracket- ing parse tree generate the words of the sentence (in the form of embeddings). The latent states are represented by vectors z ∈ R m where m &lt; p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Intuition</head><p>For intuition, consider the simple tag sequence x = (VBD, DT, NN). Two candidate constituent parse structures are shown in <ref type="figure" target="#fig_0">Figure 2</ref> and the cor- rect one is boxed in green (the other in red). Re- call that our training data contains word phrases that have the tag sequence x e.g. w (1) = (hit, the, ball), w (2) = (ate, an, apple).</p><p>Intuitively, the words in the above phrases ex- hibit dependencies that can reveal the parse struc- ture. The determiner (w 2 ) and the direct object (w 3 ) are correlated in that the choice of deter- miner depends on the plurality of w 3 . However, the choice of verb (w 1 ) is mostly independent of the determiner. We could thus conclude that w 2 and w 3 should be closer in the parse tree than w 1</p><p>The bear ate the fish í µí±¤ 1 , í µí±¤ 2 , í µí±¤ 3 , í µí±¤ 4 , í µí±¤ 5 , í µí± § 1 , í µí± § 2 , í µí± § 3 í µí² = (í µí°·í µí±, í µí±í µí±, í µí±í µí°µí µí°·, í µí°·í µí±, í µí±í µí±) í µí±¢(í µí²) ((DT NN) (VBD (DT NN))) <ref type="bibr">w1</ref> w2 w3 z3 z1 w4 w5 z2 w1 w2 w3 z3 z1 w4 w5 z2 <ref type="figure">Figure 1</ref>: Example for the tag sequence <ref type="bibr">(DT, NN, VBD, DT, NN)</ref> showing the overview of our approach. We first learn a undi- rected latent tree for the se- quence (left). We then ap- ply a direction mapping h dir to direct the latent tree (center). This can then easily be con- verted into a bracketing (right). and w 2 , giving us the correct structure. Informally, the latent state z corresponding to the (w 2 , w 3 ) bracket would store information about the plural- ity of z, the key to the dependence between w 2 and w 3 . It would then be reasonable to assume that w 2 and w 3 are independent given z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">A Conditional Latent Tree Model</head><p>Following this intuition, we propose to model the distribution over the latent bracketing states and words for each tag sequence x as a latent tree graphical model, which encodes conditional inde- pendences among the words given the latent states.</p><p>Let V := {w 1 , ..., w , z 1 , ..., z H }, with w i rep- resenting the word embeddings, and z i represent- ing the latent states of the bracketings. Then, ac- cording to our base model it holds that:</p><formula xml:id="formula_0">p(w, z|x) = H i=1 p(z i |π x (z i ), θ(x)) × (x) i=1 p(w i |π x (w i ), θ(x)) (1)</formula><p>where π x (·) returns the parent node index of the argument in the latent tree corresponding to tag sequence x. 1 If z is the root, then π x (z) = ∅. All the w i are assumed to be leaves while all the z i are internal (i.e. non-leaf) nodes. The param- eters θ(x) control the conditional probability ta- bles. We do not commit to a certain parametric family, but see more about the assumptions we make about θ in §3.2. The parameter space is de- noted Θ. The model assumes a factorization ac- cording to a latent-variable tree. The latent vari- ables can incorporate various linguistic properties, such as head information, valence of dependency being generated, and so on. This information is expected to be learned automatically from data.</p><p>Our generative model deterministically maps a POS sequence to a bracketing via an undirected latent-variable tree. The orientation of the tree is determined by a direction mapping h dir (u), which is fixed during learning and decoding. This means our decoder first identifies (given a POS sequence) an undirected tree, and then orients it by applying h dir on the resulting tree (see below).</p><p>Define U to be the set of undirected latent trees where all internal nodes have degree exactly 3 (i.e. they correspond to binary bracketing), and in addi- tion h dir (u) for any u ∈ U is projective (explained in the h dir section). In addition, let T be the set of binary bracketings. The complete generative model that we follow is then:</p><p>• Generate a tag sequence x = (x 1 , . . . , x )</p><p>• Decide on u(x) ∈ U, the undirected latent tree that x maps to.</p><p>• Set t ∈ T by computing t = h dir (u).</p><p>• Set θ ∈ Θ by computing θ = θ(x).</p><p>• Generate a tuple v = (w 1 , . . . , w , z 1 , ..., z H ) where w i ∈ R p , z j ∈ R m according to Eq. 1.</p><p>See <ref type="figure">Figure 1</ref> (left) for an example.</p><p>The Direction Mapping h dir . Generating a bracketing via an undirected tree enables us to build on existing methods for structure learning of latent-tree graphical models <ref type="bibr" target="#b6">(Choi et al., 2011;</ref>). Our learning algorithm focuses on recovering the undirected tree based for the generative model that was described above. This undirected tree is converted into a directed tree by applying h dir . The mapping h dir works in three steps:</p><formula xml:id="formula_1">• It first chooses a top bracket ([1, R − 1], [R, ])</formula><p>where R is the mid-point of the bracket and is the length of the sentence.</p><p>• It marks the edge e i,j that splits the tree accord- ing to the top bracket as the "root edge" (marked in red in <ref type="figure">Figure 1</ref>(center)) • It then creates t from u by directing the tree out- ward from e i,j as shown in <ref type="figure">Figure 1</ref> The resulting t is a binary bracketing parse tree. As implied by the above definition of h dir , se- lecting which edge is the root can be interpreted as determining the top bracket of the constituent parse. For example, in <ref type="figure">Figure 1</ref>, the top bracket is ( <ref type="bibr">[1,</ref><ref type="bibr">2]</ref>, <ref type="bibr">[3,</ref><ref type="bibr">5]</ref>) = ( <ref type="bibr">[DT, NN]</ref>, <ref type="bibr">[VBD, DT, NN]</ref>). Note that the "root" edge e z 1 ,z 2 partitions the leaves into precisely this bracketing. As indicated in the above section, we restrict the set of undirected trees to be those such that after applying h dir the resulting t is projective i.e. there are no crossing brackets. In §4.1, we discuss an effective heuristic to find the top bracket without supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spectral Learning Algorithm based on Additive Tree Metrics</head><p>Our goal is to recover t ∈ T for tag sequence x using the data</p><formula xml:id="formula_2">D = [(w (i) , x (i) )] N i=1</formula><p>. To get an in- tuition about the algorithm, consider a partition of the set of examples</p><formula xml:id="formula_3">D into D(x) = {(w (i) , x (i) ) ∈ D|x (i) = x}, i.</formula><p>e. each section in the partition has an identical sequence of part of speech tags. As- sume for this section |D(x)| is large (we address the data sparsity issue in §3.4).</p><p>We can then proceed by learning how to map a POS sequence x to a tree t ∈ T (through u ∈ U) by focusing only on examples in D(x).</p><p>Directly attempting to maximize the likelihood unfortunately results in an intractable optimiza- tion problem and greedy heuristics are often em- ployed ( <ref type="bibr" target="#b19">Harmeling and Williams, 2011</ref>). Instead we propose a method that is provably consistent and returns a tree that can be mapped to a bracket- ing using h dir .</p><p>If all the variables were observed, then the Chow-Liu algorithm <ref type="bibr" target="#b7">(Chow and Liu, 1968)</ref> could be used to find the most likely tree structure u ∈ U. The Chow-Liu algorithm essentially computes the distances among all pairs of variables (the neg- ative of the mutual information) and then finds the minimum cost tree. However, the fact that the z i are latent variables makes this strategy substan- tially more complicated. In particular, it becomes challenging to compute the distances among pairs of latent variables. What is needed is a "special" distance function that allows us to reverse engineer the distances among the latent variables given the distances among the observed variables. This is the key idea behind additive tree metrics that are the basis of our approach.</p><p>In the following sections, we describe the key steps to our method. §3.1 and §3.2 largely describe existing background on additive tree metrics and latent tree structure learning, while §3.3 and §3.4 discuss novel aspects that are unique to our prob- lem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Additive Tree Metrics</head><p>Let u(x) be the true undirected tree of sentence x and assume the nodes V to be indexed by [M ] = {1, . . . , M } such that M = |V| = H + . Fur- thermore, let v ∈ V refer to a node in the undi- rected tree (either observed or latent). We assume the existence of a distance function that allows us to compute distances between pairs of nodes. For example, as we see in §3.2 we will define the dis- tance d(i, j) to be a function of the covariance ma-</p><formula xml:id="formula_4">trix E[v i v j |u(x), θ(x)].</formula><p>Thus if v i and v j are both observed variables, the distance can be directly computed from the data.</p><p>Moreover, the metrics we construct are such that they are tree additive, defined below: <ref type="bibr" target="#b14">Erdõs et al., 1999</ref>) for the undirected tree u(x) if it is a distance metric, 2 and furthermore, ∀i, j ∈ [M ] the following relation holds:</p><formula xml:id="formula_5">Definition 1 A function d u(x) : [M ]×[M ] → R is an additive tree metric (</formula><formula xml:id="formula_6">d u(x) (i, j) = (a,b)∈path u(x) (i,j) d u(x) (a, b) (2)</formula><p>where path u(x) (i, j) is the set of all the edges in the (undirected) path from i to j in the tree u(x).</p><p>As we describe below, given the tree structure, the additive tree metric property allows us to com- pute "backwards" the distances among the latent variables as a function of the distances among the observed variables.</p><p>Define D to be the M × M distance matrix among the M variables, i.e.</p><formula xml:id="formula_7">D ij = d u(x) (i, j). Let D W W , D ZW (equal to D W Z )</formula><p>, and D ZZ indi- cate the word-word, latent-word and latent-latent sub-blocks of D respectively. In addition, since u(x) is assumed to be known from context, we denote d u(x) (i, j) just by d <ref type="figure">(i, j)</ref>.</p><p>Given the fact that the distance between a pair of nodes is a function of the random variables they represent (according to the true model), only D W W can be empirically estimated from data. However, if the underlying tree structure is known, then Definition 1 can be leveraged to compute D ZZ and D ZW as we show below.   . Let A denote the set of nodes that are closer to a than i and similarly let B denote the set of nodes that are closer to b than i. Let A * and B * denote all the leaves (word nodes) in A and B respectively. Then using path additivity (Definition 1), it can be shown that for any a * ∈ A * , b * ∈ B * it holds that:</p><formula xml:id="formula_8">d(i, j) = 1 2 (d(j, a * ) + d(j, b * ) − d(a * , b * ))<label>(3)</label></formula><p>Note that the right-hand side only depends on distances between observed random variables. . Let A denote the set of nodes closer to a than i, and analogously for B, G, and H. Let A * , B * , G * , and H * refer to the leaves in A, B, G, and H respectively. Then for any a * ∈ A * , b * ∈ B * , g * ∈ G * , and h * ∈ H * it can be shown that:</p><formula xml:id="formula_9">d(i, j) = 1 4 d(a * , g * ) + d(a * , h * ) + d(b * , g * ) +d(b * , h * ) − 2d(a * , b * ) − 2d(g * , h * )<label>(4)</label></formula><p>Empirically, one can obtain a more robust em- pirical estimate d(i, j) by averaging over all valid choices of a * , b * in Eq. 3 and all valid choices of a * , b * , g * , h * in Eq. 4 <ref type="bibr" target="#b11">(Desper and Gascuel, 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Constructing a Spectral Additive Metric</head><p>In constructing our distance metric, we begin with the following assumption on the distribution in Eq. 1 (analogous to the assumptions made in Anandkumar et al., 2011).</p><formula xml:id="formula_10">Assumption 1 (Linear, Rank m, Means) E[z i |π x (z i ), x] = A (z i |z πx(z i ) ,x) π x (z i ) ∀i ∈ [H]</formula><p>where</p><formula xml:id="formula_11">A (z i |πx(z i ),x) ∈ R m×m has rank m. E[w i |π x (w i ), x] = C (w i |πx(w i ),x) π x (w i ) ∀i ∈ [(x)]</formula><p>where C (w i |πx(w i ),x) ∈ R p×m has rank m.</p><formula xml:id="formula_12">Also assume that E[z i z i |x] has rank m ∀i ∈ [H].</formula><p>Note that the matrices A and C are a direct function of θ(x), but we do not specify a model family for θ(x). The only restriction is in the form of the above assumption. If w i and z i were dis- crete, represented as binary vectors, the above as- sumption would correspond to requiring all con- ditional probability tables in the latent tree to have rank m. Assumption 1 allows for the w i to be high dimensional features, as long as the expectation requirement above is satisfied. Similar assump- tions are made with spectral parameter learning methods e.g. Furthermore, Assumption 1 makes it explicit that regardless of the size of p, the relationships among the variables in the latent tree are restricted to be of rank m, and are thus low rank since p &gt; m. To leverage this low rank structure, we propose using the following additive metric, a normalized variant of that in Anandkumar et al. (2011): We can then show that this metric is additive:</p><formula xml:id="formula_13">d spectral (i, j) = − log Λ m (Σ x (i, j)) + 1 2 log Λ m (Σ x (i, i)) + 1 2 log Λ m (Σ x (j, j))<label>(</label></formula><p>Lemma 1 If Assumption 1 holds then, d spectral is an additive tree metric (Definition 1).</p><p>A proof is in the supplementary for completeness. From here, we use d to denote d spectral , since that is the metric we use for our learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Recovering the Minimal Projective Latent Tree</head><p>It has been shown <ref type="bibr" target="#b32">(Rzhetsky and Nei, 1993</ref>) that for any additive tree metric, u(x) can be recovered by solving arg min u∈U c(u) for c(u):</p><formula xml:id="formula_14">c(u) = (i,j)∈Eu d(i, j).<label>(6)</label></formula><p>where E u is the set of pairs of nodes which are adjacent to each other in u and d(i, j) is computed using Eq. 3 and Eq. 4. Note that the metric d we use in defining c(u) is based on the expectations from the true distri- bution. In practice, the true distribution is un- known, and therefore we use an approximation for the distance metricˆdmetricˆ metricˆd. As we discussed in §3.1 all elements of the distance matrix are functions of observable quantities if the underlying tree u is known. However, only the word-word sub-block D W W can be directly estimated from the data without knowledge of the tree structure.</p><p>This subtlety makes solving the minimization problem in Eq. 6 NP-hard <ref type="bibr" target="#b11">(Desper and Gascuel, 2005</ref>) if u is allowed to be an arbitrary undirected tree. However, if we restrict u to be in U, as we do in the above, then maximizingˆcmaximizingˆ maximizingˆc(u) over U can be solved using the bilexical parsing algorithm from <ref type="bibr" target="#b13">Eisner and Satta (1999)</ref>. This is because the com- putation of the other sub-blocks of the distance matrix only depend on the partitions of the nodes shown in <ref type="figure" target="#fig_4">Figure 3</ref> into A, B, G, and H, and not on the entire tree structure. Therefore, the procedure to find a bracketing for a given POS tag x is to first estimate the dis- tance matrix sub-block D W W from raw text data (see §3.4), and then solve the optimization prob- lem arg min u∈Uˆcu∈Uˆ u∈Uˆc(u) using a variant of the Eisner- Satta algorithm wherê c(u) is identical to c(u) in Eq. 6, with d replaced withˆdwithˆ withˆd.</p><p>Summary. We first defined a generative model that describes how a sentence, its sequence of POS tags, and its bracketing is generated ( §2.3). First an undirected u ∈ U is generated (only as a func- tion of the POS tags), and then u is mapped to a bracketing using a direction mapping h dir . We then showed that we can define a distance met- ric between nodes in the undirected tree, such that minimizing it leads to a recovery of u. This dis- tance metric can be computed based only on the text, without needing to identify the latent infor- mation ( §3.2). If the true distance metric is known, Algorithm 1 The learning algorithm for find- ing the latent structure from a set of examples</p><formula xml:id="formula_15">(w (i) , x (i) ), i ∈ [N ].</formula><p>Inputs: Set of examples (w (i) ,</p><formula xml:id="formula_16">x (i) ) for i ∈ [N ], a kernel K γ (j, k, j , k |x, x ), an integer m Data structures: For each i ∈ [N ], j, k ∈ (x (i) ) there is a (uncentered) covariance matrix Σ x (i) (j, k) ∈ R p×p , and a distancê d spectral (j, k). Algorithm: (Covariance estimation) ∀i ∈ [N ], j, k ∈ (x (i) ) • Let C j ,k |i = w (i ) j (w (i ) k ) , k j,k,j ,k ,i,i = K γ (j, k, j , k |x (i) , x (i ) ) and i = (x (i ) ),</formula><p>and estimate each p × p covariance matrix as:</p><formula xml:id="formula_17">Σ x (j, k) = N i =1 i j =1 i k =1 k j,k,j ,k ,i,i C j ,k |i N i =1 i j =1 i k =1 k j,k,j ,k ,i,i • Computê d spectral (j, k) ∀j, k ∈ (x (i) ) using Eq. 5. (Uncover structure) ∀i ∈ [N ]</formula><p>• FindûFindˆFindû (i) = arg min u∈Uˆcu∈Uˆ u∈Uˆc(u), and for the ith example, return the structure h dir (ˆ u (i) ).</p><p>with respect to the true distribution that generates the words in a sentence, then u can be fully recov- ered by optimizing the cost function c(u). How- ever, in practice the distance metric must be esti- mated from data, as discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Estimation of d from Sparse Data</head><p>We now address the data sparsity problem, in par- ticular that D(x) can be very small, and therefore estimating d for each POS sequence separately can be problematic. <ref type="bibr">3</ref> In order to estimate d from data, we need to es- timate the covariance matrices Σ x (i, j) (for i, j ∈ {1, . . . , (x)}) from Eq. 5.</p><p>To give some motivation to our solu- tion, consider estimating the covariance matrix Σ x (1, 2) for the tag sequence x = (DT 1 , NN 2 , VBD 3 , DT 4 , NN 5 ). D(x) may be insufficient for an accurate empirical es-timate. However, consider another sequence x = (RB 1 , DT 2 , NN 3 , VBD 4 , DT 5 , ADJ 6 , NN 7 ). Although x and x are not identical, it is likely that Σ x (2, 3) is similar to Σ x (1, 2) because the determiner and the noun appear in similar syn- tactic context. Σ x (5, 7) also may be somewhat similar, but Σ x (2, 7) should not be very similar to Σ x (1, 2) because the noun and the determiner appear in a different syntactic context.</p><p>The observation that the covariance matrices depend on local syntactic context is the main driv- ing force behind our solution. The local syntactic context acts as an "anchor," which enhances or re- places a word index in a sentence with local syn- tactic context. More formally, an anchor is a func- tion G that maps a word index j and a sequence of POS tags x to a local context G(j, x). The anchor we use is G(j, x) = (j, x j ). Then, the covariance matrices Σ x are estimated using kernel smooth- ing ( <ref type="bibr" target="#b20">Hastie et al., 2009)</ref>, where the smoother tests similarity between the different anchors G(j, x).</p><p>The full learning algorithm is given in <ref type="figure">Figure 1</ref>. The first step in the algorithm is to estimate the covariance matrix block Σ x (i) (j, k) for each train- ing example x (i) and each pair of preterminal po- sitions (j, k) in x (i) . Instead of computing this block by computing the empirical covariance ma- trix for positions (j, k) in the data D(x), the al- gorithm uses all of the pairs (j , k ) from all of N training examples. It averages the empirical covariance matrices from these contexts using a kernel weight, which gives a similarity measure for the position (j, k) in x (i) and (j , k ) in an- other example x (i ) . γ is the kernel "bandwidth", a user-specified parameter that controls how in- clusive the kernel will be with respect to exam- ples in D (see § 4.1 for a concrete example). Note that the learning algorithm is such that it ensures that</p><formula xml:id="formula_18">Σ x (i) (j, k) = Σ x (i ) (j , k ) if G(j, x (i) ) = G(j , x (i ) ) and G(k, x (i) ) = G(k , x (i ) ).</formula><p>Once the empirical estimates for the covariance matrices are obtained, a variant of the Eisner-Satta algorithm is used, as mentioned in §3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Theoretical Guarantees</head><p>Our main theoretical guarantee is that Algorithm 1 will recover the correct tree u ∈ U with high prob- ability, if the given top bracket is correct and if we obtain enough examples (w (i) , x (i) ) from the model in §2. We give the theorem statement be- low. The constants lurking in the O-notation and the full proof are in the supplementary.</p><p>Denote σ x (j, k) (r) as the r th singu- lar value of Σ x (j, k).</p><p>Let σ * (x) := min j,k∈(x) min σ x (j, k) (m) .</p><p>Theorem 1 Definê u as the estimated tree for tag sequence x and u(x) as the correct tree. Let</p><formula xml:id="formula_19">(x) := min u ∈U :u =u(x) (c(u(x)) − c(u ))/(8|(x)|)</formula><p>Assume that</p><formula xml:id="formula_20">N ≥ O   m 2 log p 2 (x) 2 δ min(σ * (x) 2 (x) 2 , σ * (x) 2 )ν x (γ) 2   Then with probability 1 − δ, ˆ u = u(x).</formula><p>where ν x (γ), defined in the supplementary, is a function of the underlying distribution over the tag sequences x and the kernel bandwidth γ. Thus, the sample complexity of our approach depends on the dimensionality of the latent and observed states (m and p), the underlying singu- lar values of the cross-covariance matrices (σ * (x)) and the difference in the cost of the true tree com- pared to the cost of the incorrect trees ((x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We report results on three different languages: En- glish, German, and Chinese. For English we use the Penn treebank ( <ref type="bibr" target="#b29">Marcus et al., 1993)</ref>, with sec- tions 2-21 for training and section 23 for final testing. For German and Chinese we use the Ne- gra treebank and the Chinese treebank respectively and the first 80% of the sentences are used for training and the last 20% for testing. All punc- tuation from the data is removed. <ref type="bibr">4</ref> We primarily compare our method to the constituent-context model (CCM) of <ref type="bibr" target="#b26">Klein and Manning (2002)</ref>. We also compare our method to the algorithm of Seginer (2007).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Top bracket heuristic Our algorithm requires the top bracket in order to direct the latent tree. In practice, we employ the following heuristic to find the bracket using the following three steps:</p><p>• If there exists a comma/semicolon/colon at in- dex i that has at least a verb before i and both a noun followed by a verb after i, then return • Otherwise find the first non-participle verb (say at index j) and return</p><formula xml:id="formula_21">([0, i − 1], [i, (x)])</formula><formula xml:id="formula_22">([0, j − 1], [j, (x)]). • If no verb exists, return ([0, 1], [1, (x)]).</formula><p>Word embeddings As mentioned earlier, each w i can be an arbitrary feature vector. For all lan- guages we use Brown clustering <ref type="bibr" target="#b3">(Brown et al., 1992)</ref> to construct a log(C) + C feature vector where the first log(C) elements indicate which mergable cluster the word belongs to, and the last C elements indicate the cluster identity. For En- glish, more sophisticated word embeddings are easily obtainable, and we experiment with neural word embeddings <ref type="bibr" target="#b40">Turian et al. (2010)</ref> of length 50. We also explored two types of CCA embed- dings: OSCCA and TSCCA, given in <ref type="bibr" target="#b12">Dhillon et al. (2012)</ref>. The OSCCA embeddings behaved bet- ter, so we only report its results.</p><p>Choice of kernel For our experiments, we use the kernel</p><formula xml:id="formula_23">K γ (j, k, j , k |x, x ) = max 0, 1 − κ(j, k, j , k |x, x ) γ</formula><p>where γ denotes the user-specified bandwidth,</p><formula xml:id="formula_24">and κ(j, k, j , k |x, x ) = |j − k| − |j − k | |j − k| + |j − k | if x(j) = x(j ) and x(k ) = x(k), and sign(j − k) = sign(j − k ) (and ∞ otherwise).</formula><p>The kernel is non-zero if and only if the tags at position j and k in x are identical to the ones in position j and k in x , and if the direction be- tween j and k is identical to the one between j and k . Note that the kernel is not binary, as op- posed to the theoretical kernel in the supplemen- tary material. Our experiments show that using a non-zero value different than 1 that is a function of the distance between j and k compared to the distance between j and k does better in practice.</p><p>Choice of data For CCM, we found that if the full dataset (all sentence lengths) is used in train- ing, then performance degrades when evaluating on sentences of length ≤ 10. We therefore restrict the data used with CCM to sentences of length ≤ , where is the maximal sentence length being evaluated. This does not happen with our algo- rithm, which manages to leverage lexical informa- tion whenever more data is available. We therefore use the full data for our method for all lengths.</p><p>We also experimented with the original POS tags and the universal POS tags of <ref type="bibr" target="#b31">Petrov et al. (2011)</ref>. Here, we found out that our method does better with the universal part of speech tags. For CCM, we also experimented with the origi- nal parts of speech, universal tags (CCM-U), the cross-product of the original parts of speech with the Brown clusters (CCM-OB), and the cross- product of the universal tags with the Brown clus- ters (CCM-UB). The results in <ref type="table">Table 1</ref> indicate that the vanilla setting is the best for CCM.</p><p>Thus, for all results, we use universal tags for our method and the original POS tags for CCM. We believe that our approach substitutes the need for fine-grained POS tags with the lexical informa- tion. CCM, on the other hand, is fully unlexical- ized.</p><p>Parameter Selection Our method requires two parameters, the latent dimension m and the band- width γ. CCM also has two parameters, the num- ber of extra constituent/distituent counts used for smoothing. For both methods we chose the best parameters for sentences of length ≤ 10 on the English Penn Treebank (training) and used this set for all other experiments. This resulted in m = 7, γ = 0.4 for our method and 2, 8 for CCM's extra constituent/distituent counts respec- tively. We also tried letting CCM choose differ- ent hyperparameters for different sentence lengths based on dev-set likelihood, but this gave worse results than holding them fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Test I: Accuracy <ref type="table" target="#tab_2">Table 2</ref> summarizes our re- sults. CCM is used with the initializer proposed in <ref type="bibr" target="#b26">Klein and Manning (2002)</ref>. <ref type="bibr">5</ref>    scribed in § 4.1. NN-O, CC-O, and BC-O indicate that the oracle (i.e. true top bracket) was used for h dir . For our method, test set results can be ob- tained by using Algorithm 1 (except the distances are computed using the training data). For English, while CCM behaves better for short sentences ( ≤ 10), our algorithm is more robust with longer sentences. This is especially noticeable for length ≤ 40, where CCM breaks down and our algorithm is more stable. We find that the neural embeddings modestly outperform the CCA and Brown cluster embeddings.</p><p>The results for German are similar, except CCM breaks down earlier at sentences of ≤ 30. For Chinese, our method substantially outperforms CCM for all lengths. Note that CCM performs very poorly, obtaining only around 20% accu- racy even for sentences of ≤ 20. We didn't have neural embeddings for German and Chinese (which worked best for English) and thus only used Brown cluster embeddings.</p><p>For English, the disparity between NN-O (ora- cle top bracket) and NN (heuristic top bracket) is rather low suggesting that our top bracket heuris- tic is rather effective. However, for German and Chinese note that the "BC-O" performs substan- tially better, suggesting that if we had a better top bracket heuristic our performance would increase.</p><p>Test II: Sensitivity to initialization The EM al- gorithm with the CCM requires very careful ini- tialization, which is described in <ref type="bibr" target="#b26">Klein and Manning (2002)</ref>. If, on the other hand, random ini- tialization is used, the variance of the performance of the CCM varies greatly. <ref type="figure" target="#fig_8">Figure 4</ref> shows a his- togram of the performance level for sentences of length ≤ 10 for different random initializers. As one can see, for some restarts, CCM obtains ac- curacies lower than 30% due to local optima. Our method does not suffer from local optima and thus does not require careful initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test III: Comparison to Seginer's algorithm</head><p>Our approach is not directly comparable to Seginer's because he uses punctuation, while we use POS tags. Using Seginer's parser we were able to get results on the training sets. On English: 75.2% ( ≤ 10), 64.2% ( ≤ 20), 56.7% ( ≤ 40). On German: 57.8% ( ≤ 10), 45.0% <ref type="bibr">( ≤ 20)</ref>, and 39.9% ( ≤ 40). On Chinese: 56.6% ( ≤ 10), 45.1% <ref type="bibr">( ≤ 20)</ref>, and 38.9% ( ≤ 40).</p><p>Thus, while Seginer's method performs better on English, our approach performs 2-3 points bet- ter on German, and both methods give similar per- formance on Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We described a spectral approach for unsu- pervised constituent parsing that comes with theoretical guarantees on latent structure recovery. Empirically, our algorithm performs favorably to the CCM of <ref type="bibr" target="#b26">Klein and Manning (2002)</ref> without the need for careful initialization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Candidate constituent parses for x = (VBD, DT, NN) (left-correct, right-incorrect)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(center)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two types of edges in general undirected latent trees. (a) leaf edge, (b) internal edge</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Case 2 (internal edge, figure 3(b)) Both i and j are internal nodes. In this case, i has exactly two other neighbors a ∈ [M ] and b ∈ [M ], and similarly, j has exactly other two neighbors g ∈ [M ] and h ∈ [M ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Hsu et al. (2009), Bailly et al. (2009), Parikh et al. (2011), and Cohen et al. (2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>5) where Λ m (A) denotes the product of the top m singular values of A and Σ x (i, j) := E[v i v j |x], i.e. the uncentered cross-covariance matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histogram showing performance of CCM across 100 random restarts for sentences of length ≤ 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>F1 bracketing measure for the test sets and train sets in three languages. NN, CC, and BC indicate the performance of 
our method for neural embeddings, CCA embeddings, and Brown clustering respectively, using the heuristic for h dir described 
in  § 4.1. NN-O, CC-O, and BC-O indicate that the oracle (i.e. true top bracket) was used for h dir . 

</table></figure>

			<note place="foot" n="1"> At this point, π refers to an arbitrary direction of the undirected tree u(x).</note>

			<note place="foot" n="2"> This means that it satisfies d(i, j) = 0 if and only if i = j, the triangle inequality and is also symmetric.</note>

			<note place="foot" n="3"> This data sparsity problem is quite severe-for example, the Penn treebank (Marcus et al., 1993) has a total number of 43,498 sentences, with 42,246 unique POS tag sequences, averaging |D(x)| to be 1.04.</note>

			<note place="foot" n="4"> We make brief use of punctuation for our top bracket heuristic detailed below before removing it.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1107.1283</idno>
		<title level="m">Spectral methods for learning multivariate latent tree structure</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Grammatical inference as a principal component analysis problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ralaivola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised spectral learning of WCFG as low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The recovery of trees from measures of dissimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">P</forename><surname>Buneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics in the archaeological and historical sciences</title>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A note on the metric properties of trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="50" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning latent tree graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1771" to="1812" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Willsky</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximating Discrete Probability Distributions With Dependence Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory, IT</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="462" to="467" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empirical risk minimization for probabilistic grammars: Sample complexity and hardness of learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="479" to="526" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The minimum evolution distance-based approach to phylogenetic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gascuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of evolution and phylogeny</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two step cca: A new spectral method for estimating vector models of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rodu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient parsing for bilexical context-free grammars and head automaton grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A few logs suffice to build (almost) all trees: Part ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdõs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Warnow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="118" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparsity in dependency grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Concavity and initialization for unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A feature-rich constituent context model for grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonconvex global optimization for latent-variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Greedy learning of binary latent trees. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1087" to="1097" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Series in Statistics</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving unsupervised dependency parsing with richer contexts and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">P</forename><surname>Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLT</title>
		<meeting>COLT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.3137</idno>
		<title level="m">Identifiability and unmixing of latent parse trees</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unfolding latent tree structures using 4th order tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ishteva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.1258</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Basic methods of probabilistic context free grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A generative constituent-context model for improved grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On sparse nonparametric conditional covariance selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating time-varying networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="123" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A spectral algorithm for latent tree graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno>ArXiv:1104.2086</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Theoretical foundation of the minimum-evolution method of phylogenetic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rzhetsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Biology and Evolution</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1073" to="1095" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The neighbor-joining method: a new method for reconstructing phylogenetic trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saitou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular biology and evolution</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="406" to="425" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast unsupervised incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Seginer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kernel embeddings of latent tree graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">From baby steps to leapfrog: how less is more in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Breaking out of local optima with count transforms and model recombination: A study in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-A</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Time varying undirected graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="295" to="319" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
