<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Characters to Words to in Between: Do We Capture Morphology?</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Vania</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
						</author>
						<title level="a" type="main">From Characters to Words to in Between: Do We Capture Morphology?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2016" to="2027"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1184</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results extend previous findings that character representations are effective across typologies, and we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most others. But we also find room for improvement: none of the character-level models match the predictive accuracy of a model with access to true morphological analyses , even when learned from an order of magnitude more data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Continuous representations of words learned by neural networks are central to many NLP tasks ( <ref type="bibr" target="#b5">Cho et al., 2014;</ref><ref type="bibr" target="#b4">Chen and Manning, 2014;</ref>). However, directly mapping a fi- nite set of word types to a continuous representa- tion has well-known limitations. First, it makes a closed vocabulary assumption, enabling only generic out-of-vocabulary handling. Second, it cannot exploit systematic functional relationships in learning. For example, cat and cats stand in the same relationship as dog and dogs. While this re- lationship might be discovered for these specific frequent words, it does not help us learn that the same relationship also holds for the much rarer words sloth and sloths.</p><p>These functional relationships reflect the fact that words are composed from smaller units of meaning, or morphemes. For instance, cats con- sists of two morphemes, cat and -s, with the latter shared by the words dogs and tarsiers. Modeling this effect is crucial for languages with rich mor- phology, where vocabulary sizes are larger, many more words are rare, and many more such func- tional relationships exist. Hence, some models produce word representations as a function of sub- word units obtained from morphological segmen- tation or analysis ( <ref type="bibr" target="#b19">Luong et al., 2013;</ref><ref type="bibr" target="#b3">Botha and Blunsom, 2014;</ref><ref type="bibr" target="#b6">Cotterell and Schütze, 2015)</ref>. A downside of these models is that they depend on morphological segmenters or analyzers.</p><p>Morphemes typically have similar orthographic representations across words. For example, the morpheme -s is realized as -es in finches. Since this variation is limited, the general relationship between morphology and orthography can be ex- ploited by composing the representations of char- acters ( <ref type="bibr" target="#b15">Kim et al., 2016)</ref>, char- acter n-grams ( <ref type="bibr" target="#b28">Sperr et al., 2013;</ref><ref type="bibr" target="#b30">Wieting et al., 2016;</ref><ref type="bibr" target="#b2">Bojanowski et al., 2016;</ref><ref type="bibr" target="#b3">Botha and Blunsom, 2014</ref>), bytes <ref type="bibr" target="#b21">(Plank et al., 2016;</ref><ref type="bibr" target="#b9">Gillick et al., 2016)</ref>, or combinations thereof <ref type="bibr" target="#b25">(Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b22">Qiu et al., 2014</ref>). These mod- els are compact, can represent rare and unknown words, and do not require morphological analyz- ers. They raise a provocative question: Does NLP benefit from models of morphology, or can they be replaced entirely by models of characters?</p><p>The relative merits of word, subword. and character-level models are not fully understood because each new model has been compared on different tasks and datasets, and often compared against word-level models. A number of questions remain open:</p><p>1. How do representations based on morphemes compare with those based on characters?</p><p>2. What is the best way to compose subword representations?</p><p>3. Do character-level models capture morphol- ogy in terms of predictive utility?</p><p>4. How do different representations interact with languages of different morphological ty- pologies?</p><p>The last question is raised by Bender (2013): languages are typologically diverse, and the behavior of a model on one language may not generalize to others. Character-level mod- els implicitly assume concatenative morphology, but many widely-spoken languages feature non- concatenative morphology, and it is unclear how such models will behave on these languages.</p><p>To answer these questions, we performed a sys- tematic comparison across different models for the simple and ubiquitous task of language model- ing. We present experiments that vary (1) the type of subword unit; (2) the composition function; and (3) morphological typology. To understand the extent to which character-level models capture true morphological regularities, we present ora- cle experiments using human morphological an- notations instead of automatic morphological seg- ments. Our results show that:</p><p>1. For most languages, character-level represen- tations outperform the standard word repre- sentations. Most interestingly, a previously unstudied combination of character trigrams composed with bi-LSTMs performs best on the majority of languages.</p><p>2. Bi-LSTMs and CNNs are more effective composition functions than addition.</p><p>3. Character-level models learn functional re- lationships between orthographically similar words, but don't (yet) match the predictive accuracy of models with access to true mor- phological analyses.</p><p>4. Character-level models are effective across a range of morphological typologies, but or- thography influences their effectiveness. word tries morphemes try+s morphs tri+es morph. analysis try+VB+3rd+SG+Pres <ref type="table">Table 1</ref>: The morphemes, morphs, and morpho- logical analysis of tries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Morphological Typology</head><p>A morpheme is the smallest unit of meaning in a word. Some morphemes express core meaning (roots), while others express one or more depen- dent features of the core meaning, such as per- son, gender, or aspect. A morphological analysis identifies the lemma and features of a word. A morph is the surface realization of a morpheme <ref type="bibr">(Morley, 2000</ref>), which may vary from word to word. These distinctions are shown in <ref type="table">Table 1</ref>.</p><p>Morphological typology classifies languages based on the processes by which morphemes are composed to form words. While most languages will exhibit a variety of such processes, for any given language, some processes are much more frequent than others, and we will broadly identify our experimental languages with these processes.</p><p>When morphemes are combined sequentially, the morphology is concatenative. However, morphemes can also be composed by non- concatenative processes.</p><p>We consider four broad categories of both concatenative and non- concatenative processes in our experiments.</p><p>Fusional languages realize multiple features in a single concatenated morpheme. For exam- ple, English verbs can express number, person, and tense in a single morpheme:</p><formula xml:id="formula_0">wanted (English) want + ed want + VB+1st+SG+Past</formula><p>Agglutinative languages assign one feature per morpheme. Morphemes are concatenated to form a word and the morpheme boundaries are clear. For example <ref type="bibr" target="#b11">(Haspelmath, 2010)</ref>:</p><p>okursam (Turkish) oku+r+sa+m "read"+AOR+COND+1SG Root and Pattern Morphology forms words by inserting consonants and vowels of dependent morphemes into a consonantal root based on a given pattern. For example, the Arabic root ktb ("write") produces <ref type="bibr" target="#b24">(Roark and Sproat, 2007)</ref>:</p><p>katab "wrote" (Arabic)</p><p>takaatab "wrote to each other" (Arabic) Reduplication is a process where a word form is produced by repeating part or all of the root to express new features. For example:</p><p>anak "child" (Indonesian) anak-anak "children" <ref type="bibr">(Indonesian)</ref> buah "fruit" (Indonesian) buah-buahan "various fruits" (Indonesian)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Representation Models</head><p>We compare ten different models, varying sub- word units and composition functions that have commonly been used in recent work, but evalu- ated on various different tasks <ref type="table" target="#tab_0">(Table 2)</ref>. Given word w, we compute its representation w as:</p><formula xml:id="formula_1">w = f (W s , σ(w))<label>(1)</label></formula><p>where σ is a deterministic function that returns a sequence of subword units; W s is a parameter ma- trix of representations for the vocabulary of sub- word units; and f is a composition function which takes σ(w) and W s as input and returns w. All of the representations that we consider take this form, varying only in f and σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Subword Units</head><p>We consider four variants of σ in Equation 1, each returning a different type of subword unit: character, character trigram, or one of two types of morph. Morphs are obtained from Morfes- sor ( <ref type="bibr" target="#b27">Smit et al., 2014</ref>) or a word segmentation based on Byte Pair Encoding (BPE; <ref type="bibr" target="#b8">Gage (1994)</ref>), which has been shown to be effective for handling rare words in neural machine translation <ref type="bibr" target="#b26">(Sennrich et al., 2016)</ref>. BPE works by iteratively replac- ing frequent pairs of characters with a single un- used character. For Morfessor, we use default parameters while for BPE we set the number of merge operations to 10,000. 1 When we segment into character trigrams, we consider all trigrams in the word, including those covering notional begin- ning and end of word characters, as in <ref type="bibr" target="#b28">Sperr et al. (2013)</ref>. Example output of σ is shown in <ref type="table" target="#tab_1">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Composition Functions</head><p>We use three variants of f in Eq. 1. The first con- structs the representation w of word w by adding 1 BPE takes a single parameter: the number of merge op- erations. We tried different parameter values (1k, 10k, 100k) and manually examined the resulting segmentation on the En- glish dataset. Qualitatively, 10k gave the most plausible seg- mentation and we used this setting across all languages. the representations of its subwords s 1 , . . . , s n = σ(w), where the representation of s i is vector s i .</p><formula xml:id="formula_2">w = n i=1 s i (2)</formula><p>The only subword unit that we don't compose by addition is characters, since this will produce the same representation for many different words. Our second composition function is a bidi- rectional long-short-term memory (bi-LSTM), which we adapt based on its use in the character- level model of  and its widespread use in NLP generally. Given s i and the previous LSTM hidden state h i−1 , an LSTM (Hochreiter and Schmidhuber, 1997) computes the following outputs for the subword at position i:</p><formula xml:id="formula_3">h i = LST M (s i , h i−1 ) (3) ˆ s i+1 = g(V T · h i )<label>(4)</label></formula><p>wherê s i+1 is the predicted target subword, g is the softmax function and V is a weight matrix.</p><p>A bi-LSTM ( <ref type="bibr" target="#b10">Graves et al., 2005</ref>) combines the final state of an LSTM over the input sequence with one over the reversed input sequence. Given the hidden state produced from the final input of the forward LSTM, h f w n and the hidden state pro- duced from the final input of the backward LSTM h bw 0 , we compute the word representation as:</p><formula xml:id="formula_4">w t = W f · h f w n + W b · h bw 0 + b (5)</formula><p>where W f , W b , and b are parameter matrices and h f w n and h bw 0 are forward and backward LSTM states, respectively.</p><p>The third composition function is a convolu- tional neural network (CNN) with highway lay- ers, as in <ref type="bibr" target="#b15">Kim et al. (2016)</ref>. Let c 1 , . . . , c k be the sequence of characters of word w. The character embedding matrix is C ∈ R d×k , where the i-th column corresponds to the embeddings of c i . We first apply a narrow convolution between C and a filter F ∈ R d×n of width n to obtain a feature map f ∈ R k−n+1 . In particular, the computation of the j-th element of f is defined as</p><formula xml:id="formula_5">f [j] = tanh(C[ * , j : j + n − 1], F + b) (6)</formula><p>where A, B = Tr(AB T ) is the Frobenius in- ner product and b is a bias. The CNN model ap- plies filters of varying width, representing features Models Subword Unit(s) Composition Function Sperr et al. <ref type="formula" target="#formula_1">(2013)</ref> words, character n-grams addition <ref type="bibr" target="#b19">Luong et al. (2013)</ref> morphs (Morfessor) recursive NN <ref type="bibr" target="#b3">Botha and Blunsom (2014)</ref> words, morphs (Morfessor) addition <ref type="bibr" target="#b22">Qiu et al. (2014)</ref> words, morphs (Morfessor) addition Santos and Zadrozny (2014) words, characters CNN <ref type="bibr" target="#b6">Cotterell and Schütze (2015)</ref> words, morphological analyses addition Sennrich et al. <ref type="formula" target="#formula_1">(2016)</ref> morphs (BPE) none <ref type="bibr" target="#b15">Kim et al. (2016)</ref> characters CNN Ling et al. <ref type="formula" target="#formula_1">(2015)</ref> characters bi-LSTM <ref type="bibr" target="#b30">Wieting et al. (2016)</ref> character n-grams addition <ref type="bibr" target="#b2">Bojanowski et al. (2016)</ref> character n-grams addition <ref type="bibr" target="#b29">Vylomova et al. (2016)</ref> characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho <ref type="formula" target="#formula_1">(2016)</ref> words, characters bi-LSTM <ref type="bibr" target="#b23">Rei et al. (2016)</ref> words, characters bi-LSTM <ref type="bibr" target="#b17">Lee et al. (2016)</ref> characters CNN <ref type="bibr" target="#b14">Kann and Schütze (2016)</ref> characters, morphological analyses none <ref type="bibr" target="#b12">Heigold et al. (2017)</ref> words, characters bi-LSTM, CNN  of character n-grams. We then calculate the max- over-time of each feature map.</p><formula xml:id="formula_6">y j = max j f [j]<label>(7)</label></formula><p>and concatenate them to derive the word represen- tation w t = [y 1 , . . . , y m ], where m is the number of filters applied. Highway layers allow some di- mensions of w t to be carried or transformed. Since it can learn character n-grams directly, we only use the CNN with character input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Language Model</head><p>We use language models (LM) because they are simple and fundamental to many NLP applica- tions. Given a sequence of text s = w 1 , . . . , w T , our LM computes the probability of s as: where y t = w t if w t is in the output vocabulary and y t = UNK otherwise. Our language model is an LSTM variant of recurrent neural network language (RNN) LM ( <ref type="bibr">Mikolov et al., 2010)</ref>. At time step t, it receives input w t and predicts y t+1 . Using Eq. 1, it first computes representation w t of w t . Given this rep- resentation and previous state h t−1 , it produces a new state h t and predicts y t+1 :</p><formula xml:id="formula_7">P (w 1 , . . . , w T ) =</formula><formula xml:id="formula_8">h t = LST M (w t , h t−1 )<label>(9)</label></formula><formula xml:id="formula_9">ˆ y t+1 = g(V T · h t )<label>(10)</label></formula><p>where g is a softmax function over the vocabulary yielding the probability in Equation 8. Note that this design means that we can predict only words  from a finite output vocabulary, so our models dif- fer only in their representation of context words. This design makes it possible to compare language models using perplexity, since they have the same event space, though open vocabulary word predic- tion is an interesting direction for future work.</p><p>The complete architecture of our system is shown in <ref type="figure">Figure 1</ref>, showing segmentation function σ and composition function f from Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We perform experiments on ten languages ( <ref type="table">Ta- ble</ref>  To ensure that we compared models and not im- plementations, we reimplemented all models in a single framework using Tensorflow ( <ref type="bibr">Abadi et al., 2015)</ref>. <ref type="bibr">3</ref> We use a common setup for all experi- ments based on that of , <ref type="bibr" target="#b15">Kim et al. (2016)</ref>, and <ref type="bibr">Miyamoto and Cho (2016)</ref>. In preliminary experiments, we confirmed that our models produced similar patterns of perplexities for the reimplemented word and character LSTM <ref type="bibr">2</ref> The Arabic and Hebrew dataset are unvocalized. Japanese mixes Kanji, Katakana, Hiragana, and Latin charac- ters (for foreign words). Hence, a Japanese character can cor- respond to a character, syllable, or word. The preprocessed dataset is already word-segmented. <ref type="bibr">3</ref> Our implementation of these models can be found at https://github.com/claravania/subword-lstm-lm models of </p><note type="other">. Even following de- tailed discussion with Ling (p.c.), we were unable to reproduce their perplexities exactly-our En- glish reimplementation gives lower perplexities; our Turkish higher-but we do reproduce their general result that character bi-LSTMs outperform word models. We suspect that different prepro- cessing and the stochastic learning explains dif- ferences in perplexities. Our final model with bi- LSTMs composition follows Miyamoto and Cho (2016) as it gives us the same perplexity results for our preliminary experiments on the Penn Tree- bank dataset (Marcus et al., 1993), preprocessed by Mikolov et al. (2010).</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training and Evaluation</head><p>Our LSTM-LM uses two hidden layers with 200 hidden units and representation vectors for words, characters, and morphs all have dimension 200. All parameters are initialized uniformly at random from -0.1 to 0.1, trained by stochastic gradient de- scent with mini-batch size of 32, time steps of 20, for 50 epochs. To avoid overfitting, we ap- ply dropout with probability 0.5 on the input-to- hidden layer and all of the LSTM cells (includ- ing those in the bi-LSTM, if used). For all models which do not use bi-LSTM composition, we start with a learning rate of 1.0 and decrease it by half if the validation perplexity does not decrease by 0.1 after 3 epochs. For models with bi-LSTMs com- position, we use a constant learning rate of 0.2 and stop training when validation perplexity does not improve after 3 epochs. For the character CNN model, we use the same settings as the small model of <ref type="bibr" target="#b15">Kim et al. (2016)</ref>.</p><p>To make our results comparable to , for each language we limit the output vo- cabulary to the most frequent 5,000 training words plus an unknown word token. To learn to predict unknown words, we follow : in training, words that occur only once are stochasti- cally replaced with the unknown token with prob- ability 0.5. To evaluate the models, we compute perplexity on the test data.  Fusional languages. For these languages, character trigrams composed with bi-LSTMs outperformed all other models, particularly for Czech and Russian (up to 20%), which is unsur- prising since both are morphologically richer than English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head><p>Agglutinative languages. We observe differ- ent results for each language. For Finnish, char- acter trigrams composed with bi-LSTMs achieves the best perplexity. Surprisingly, for Turkish char- acter trigrams composed via addition is best and addition also performs quite well for other rep- resentations, potentially useful since the addition function is simpler and faster than bi-LSTMs. We suspect that this is due to the fact that Turk- ish morphemes are reasonably short, hence well- approximated by character trigrams. For Japanese, we improvements from character models are more modest than in other languages.</p><p>Root and Pattern. For these languages, char- acter trigrams composed with bi-LSTMs also achieve the best perplexity.</p><p>We had won- dered whether CNNs would be more effective for root-and-pattern morphology, but since these data are unvocalized, it is more likely that non- concatenative effects are minimized, though we do still find morphological variants with consonan- tal inflections that behave more like concatenation. For example, maktab (root:ktb) is written as mktb. We suspect this makes character trigrams quite ef- fective since they match the tri-consonantal root patterns among words which share the same root.</p><p>Reduplication. For Indonesian, BPE morphs composed with bi-LSTMs model obtain the best perplexity. For Malay, the character CNN out- performs other models. However, these improve- ments are small compared to other languages. This likely reflects that Indonesian and Malay are only moderately inflected, where inflection in- volves both concatenative and non-concatenative processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effects of Morphological Analysis</head><p>In the experiments above, we used unsupervised morphological segmentation as a proxy for mor- phological analysis <ref type="table" target="#tab_1">(Table 3)</ref>. However, as dis- cussed in Section 2, this is quite approximate, so it is natural to wonder what would happen if we had the true morphological analysis. If character- level models are powerful enough to capture the effects of morphology, then they should have the predictive accuracy of a model with access to this analysis. To find out, we conducted an oracle experiment using the human-annotated morpho- logical analyses provided in the UD datasets for Czech and Russian, the only languages in our set for which these analyses were available. In these experiments we treat the lemma and each morpho- logical feature as a subword unit.</p><p>The results <ref type="table">(Table 6)</ref> show that bi-LSTM com- position of these representations outperforms all Languages Addition bi-LSTM Czech 51.8 30.07 Russian 41.82 26.44 <ref type="table">Table 6</ref>: Perplexity results using hand-annotated morphological analyses (cf. <ref type="table" target="#tab_5">Table 5</ref>).</p><p>other models for both languages. These results demonstrate that neither character representations nor unsupervised segmentation is a perfect re- placement for manual morphological analysis, at least in terms of predictive accuracy. In light of character-level results, they imply that current un- supervised morphological analyzers are poor sub- stitutes for real morphological analysis. However, we can obtain much more unanno- tated than annotated data, and we might guess that the character-level models would outperform those based on morphological analyses if trained on larger data. To test this, we ran experiments that varied the training data size on three represen- tation models: word, character-trigram bi-LSTM, and character CNN. Since we want to see how much training data is needed to reach perplexity obtained using annotated data, we use the same output vocabulary derived from the original train- ing. While this makes it possible to compare per- plexities across models, it is unfavorable to the models trained on larger data, which may focus on other words. This is a limitation of our experimen- tal setup, but does allow us to draw some tentative conclusions. As shown in <ref type="table">Table 7</ref>, a character- level model trained on an order of magnitude more data still does not match the predictive accuracy of a model with access to morphological analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Automatic Morphological Analysis</head><p>The oracle experiments show promising results if we have annotated data. But these annotations are expensive, so we also investigated the use of auto- matic morphological analysis. We obtained analy- ses for Arabic with the MADAMIRA ( <ref type="bibr" target="#b20">Pasha et al., 2014</ref>). <ref type="bibr">4</ref> As in the experiment using annotations, we treated each morphological feature as a sub- word unit. The resulting perplexities of 71.94 and 42.85 for addition and bi-LSTMs, respectively, are worse than those obtained with character trigrams (39.87), though it approaches the best perplexities.   <ref type="table">Table 7</ref>: Perplexity results on the Czech develop- ment data, varying training data size. Perplexity using ~1M tokens annotated data is 28.83.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Targeted Perplexity Results</head><p>A difficulty in interpreting the results of <ref type="table" target="#tab_5">Table 5</ref> with respect to specific morphological processes is that perplexity is measured for all words. But these processes do not apply to all words, so it may be that the effects of specific morphological processes are washed out. To get a clearer picture, we measured perplexity for only specific subsets of words in our test data: specifically, given tar- get word w i , we measure perplexity of word w i+1 .</p><p>In other words, we analyze the perplexities when the inflected words of interest are in the most re- cent history, exploiting the recency bias of our LSTM-LM. This is the perplexity most likely to be strongly affected by different representations, since we do not vary representations of the pre- dicted word itself. We look at several cases: nouns and verbs in Czech and Russian, where word classes can be identified from annotations, and reduplication in Indonesian, which we can identify mostly auto- matically. For each analysis, we also distinguish between frequent cases, where the inflected word occurs more than ten times in the training data, and rare cases, where it occurs fewer than ten times. We compare only bi-LSTM models.</p><p>For Czech and Russian, we again use the UD annotation to identify words of interest. The re- sults ( <ref type="table">Table 8</ref>), show that manual morphologi- cal analysis uniformly outperforms other subword models, with an especially strong effect for Czech nouns, suggesting that other models do not cap- ture useful predictive properties of a morpholog- ical analysis. We do however note that character trigrams achieve low perplexities in most cases, similar to overall results <ref type="table" target="#tab_5">(Table 5</ref>). We also ob- serve that the subword models are more effective for rare words.  <ref type="table">Table 8</ref>: Average perplexities of words that occur after nouns and verbs. Frequent words occur more than ten times in the training data; rare words oc- cur fewer times than this. The best perplexity is in bold while the second best is underlined.</p><p>For Indonesian, we exploit the fact that the hy- phen symbol '-' typically separates the first and second occurrence of a reduplicated morpheme, as in the examples of Section 2. We use the presence of word tokens containing hyphens to estimate the percentage of those exhibiting reduplication. As shown in <ref type="table" target="#tab_9">Table 9</ref>, the numbers are quite low. <ref type="table" target="#tab_10">Table 10</ref> shows results for reduplication. In contrast with the overall results, the BPE bi-LSTM model has the worst perplexities, while character bi-LSTM has the best, suggesting that these mod- els are more effective for reduplication.</p><p>Looking more closely at BPE segmentation of reduplicated words, we found that only 6 of 252 reduplicated words have a correct word segmenta- tion, with the reduplicated morpheme often com- bining differently with the notional start-of-word or hyphen character. One the other hand BPE cor- rectly learns 8 out of 9 Indonesian prefixes and 4 out of 7 Indonesian suffixes. 5 This analysis sup- ports our intuition that the improvement from BPE might come from its modeling of concatenative morphology. <ref type="table">Table 11</ref> presents nearest neighbors under co- sine similarity for in-vocabulary, rare, and out-of- <ref type="bibr">5</ref> We use Indonesian affixes listed in <ref type="bibr" target="#b16">Larasati et al. (2011)</ref> Language type-level (%) token-level (%) Indonesian 1.10 2.60 Malay 1.29 2.89  vocabulary (OOV) words. <ref type="bibr">6</ref> For frequent words, standard word embeddings are clearly superior for lexical meaning. Character and morph representa- tions tend to find words that are orthographically similar, suggesting that they are better at model- ing dependent than root morphemes. The same pattern holds for rare and OOV words. We sus- pect that the subword models outperform words on language modeling because they exploit affixes to signal word class. We also noticed similar pat- terns in Japanese. We analyze reduplication by querying redupli- cated words to find their nearest neighbors using the BPE bi-LSTM model. If the model were sensi- tive to reduplication, we would expect to see mor- phological variants of the query word among its nearest neighbors. However, from Table 12, this is not so. With the partially reduplicated query berlembah-lembah, we do not find the lemma lem- bah.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a systematic comparison of word representation models with different levels of mor- phological awareness, across languages with dif- ferent morphological typologies. Our results con- firm previous findings that character-level models are effective for many languages, but these mod- els do not match the predictive accuracy of model with explicit knowledge of morphology, even af- ter we increase the training data size by ten times. Moreover, our qualitative analysis suggests that they learn orthographic similarity of affixes, and lose the meaning of root morphemes.</p><p>Although morphological analyses are available  <ref type="table">Rare Words  OOV words  man  including  relatively  unconditional  hydroplane  uploading  foodism   word   person  like  extremely  nazi  molybdenum  - - anyone  featuring  making  fairly  your  - - children  include  very  joints  imperial  - - men  includes  quite  supreme  intervene  - -  BPE  ii  called  newly  unintentional  emphasize  upbeat  vigilantism   LSTM  hill  involve  never  ungenerous  heartbeat  uprising  pyrethrum  text  like  essentially  unanimous  hybridized  handling  pausanias  netherlands  creating  least  unpalatable  unplatable</ref>   <ref type="table">Table 11</ref>: Nearest neighbours of semantically and syntactically similar words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Top nearest neighbours kota-kota wilayah-wilayah (areas), pulau-pulau (islands), negara-negara (countries), (cities) bahasa-bahasa (languages), koloni-koloni (colonies) berlembah-lembah berargumentasi (argue), bercakap-cakap (converse), berkemauan (will), (have many valleys) berimplikasi (imply), berketebalan (have a thickness) <ref type="table" target="#tab_0">Table 12</ref>: Nearest neighbours of Indonesian reduplicated words in the BPE bi-LSTM model. in limited quantities, our results suggest that there might be utility in semi-supervised learning from partially annotated data. Across languages with different typologies, our experiments show that the subword unit models are most effective on agglu- tinative languages. However, these results do not generalize to all languages, since factors such as morphology and orthography affect the utility of these representations. We plan to explore these ef- fects in future work. <ref type="bibr">Computational Linguistics, Sofia, Bulgaria, pages 104-113</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>P</head><label></label><figDesc>Figure 1: Our LSTM-LM architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>#tokens</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 : Summary of previous work on representing words through compositions of subword units.</head><label>2</label><figDesc></figDesc><table>Unit 
Output of σ(wants) 
Morfessorˆwant Morfessorˆwant, s$ 
BPEˆw BPEˆw, ants$ 
char-trigramˆwatrigramˆwa, wan, ant, nts ts$ 
characterˆ, characterˆ, w, a, n, t, s, $ 
analysis 
want+VB, +3rd, +SG, +Pres 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 : Input representations for wants.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 : Statistics of our datasets.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 presents</head><label>5</label><figDesc>our main results. In six of ten languages, character-trigram representations com- posed with bi-LSTMs achieve the lowest perplex- ities. As far as we know, this particular model has not been tested before, though it is similar</figDesc><table>Language 

word 
character 
char trigrams 
BPE 
Morfessor 
%imp 

bi-lstm 

CNN 
add 

bi-lstm 

add bi-lstm 
add bi-lstm 
Czech 
41.46 34.25 36.60 
42.73 
33.59 
49.96 33.74 
47.74 36.87 18.98 
English 
46.40 43.53 44.67 
45.41 
42.97 
47.51 43.30 
49.72 49.72 
7.39 
Russian 
34.93 28.44 29.47 
35.15 
27.72 
40.10 28.52 
39.60 31.31 20.64 
Finnish 
24.21 20.05 20.29 
24.89 
18.62 
26.77 19.08 
27.79 22.45 23.09 
Japanese 
98.14 98.14 91.63 101.99 101.09 126.53 96.80 111.97 99.23 
6.63 
Turkish 
66.97 54.46 55.07 
50.07 
54.23 
59.49 57.32 
62.20 62.70 25.24 
Arabic 
48.20 42.02 43.17 
50.85 
39.87 
50.85 42.79 
52.88 45.46 17.28 
Hebrew 
38.23 31.63 33.19 
39.67 
30.40 
44.15 32.91 
44.94 34.28 20.48 
Indonesian 46.07 45.47 46.60 
58.51 
45.96 
59.17 43.37 
59.33 44.86 
5.86 
Malay 
54.67 53.01 50.56 
68.51 
50.74 
68.99 51.21 
68.20 52.50 
7.52 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Language model perplexities on test. The best model for each language is highlighted in bold and the improvement of this model over the word-level model is shown in the final column. to (but more general than) the model of Sperr et al. (2013). We can see that the performance of character, character trigrams, and BPE are very competitive. Composition by bi-LSTMs or CNN is more effective than addition, except for Turk- ish. We also observe that BPE always outperforms Morfessor, even for the agglutinative languages. We now turn to a more detailed analysis by mor- phological typology.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 9 : Percentage of full reduplication on the type and token level.</head><label>9</label><figDesc></figDesc><table>Model 
all 
frequent 
rare 
word 
101.71 
91.71 156.98 
characters 
99.21 
91.35 137.42 
BPE 
117.2 
108.86 156.81 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>Average perplexities of words that occur 
after reduplicated words in the test set. 

</table></figure>

			<note place="foot" n="4"> We only experimented with Arabic since MADAMIRA disambiguates words in contexts; most other analyzers we found did not do this, and would require additional work to add disambiguation.</note>

			<note place="foot" n="6"> https://radimrehurek.com/gensim/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Clara Vania is supported by the Indonesian En-dowment Fund for Education (LPDP), the Cen-tre for Doctoral Training in Data Science, funded by the UK EPSRC (grant EP/L016427/1), and the University of Edinburgh. We thank Sameer Bansal, Toms Bergmanis, Marco Damonte, Fed-erico Fancellu, Sorcha Gilroy, Sharon Gold-water, Frank Keller, Mirella Lapata, Felicia Liu, Jonathan Mallinson, Joana Ribeiro, Naomi Saphra, Ida Szubert, and the anonymous reviewers for helpful discussion of this work and comments on previous drafts of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-3520" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning. Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR abs/1607.04606</idno>
		<ptr target="http://arxiv.org/abs/1607.04606" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compositional Morphology for Word Representations and Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v32/botha14.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1082" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Morphological word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1287" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A new algorithm for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C Users J</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilingual language processing from bytes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Brunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1155" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Artificial Neural Networks: Formal Models and Their Applications-Volume Part II</title>
		<meeting>the 15th International Conference on Artificial Neural Networks: Formal Models and Their Applications-Volume Part II<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
	<note>ICANN&apos;05</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding Morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Haspelmath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Understanding Language Series. Arnold, London</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An extensive empirical evaluation of character-based morphological tagging for 14 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guenter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E17-1048" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
	<note>Long Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="doi">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/W16-2010</idno>
		<ptr target="https://doi.org/10.18653/v1/W16-2010" />
		<title level="m">Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, Association for Computational Linguistics, chapter MED: The LMU System for the SIGMORPHON 2016 Shared Task on Morphological Reinflection</title>
		<meeting>the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology, Association for Computational Linguistics, chapter MED: The LMU System for the SIGMORPHON 2016 Shared Task on Morphological Reinflection</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="62" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the 2016 Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Indonesian Morphology Tool (MorphInd): Towards an Indonesian Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladislav</forename><surname>Septina Dian Larasati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kuboň</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeman</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-642-23138-48</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-23138-48" />
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="119" to="129" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fully character-level neural machine translation without explicit segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<idno>CoRR abs/1610.03017</idno>
		<ptr target="http://arxiv.org/abs/1610.03017" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1176" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11234/1-1548" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Association for University in Prague</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning. Association for University in Prague</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arfath</forename><surname>Pasha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Al-Badrashiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Eskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Pooleery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14). European Language Resources Association (ELRA)</title>
		<editor>Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard</editor>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14). European Language Resources Association (ELRA)<address><addrLine>Joseph Mariani, Asuncion Moreno, Jan Odijk; Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="14" to="1479" />
		</imprint>
	</monogr>
	<note>ACL Anthology Identifier</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="http://anthology.aclweb.org/P16-2067" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Co-learning of word representations and morpheme representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C14-1015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attending to characters in neural sequence labeling models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamal</forename><surname>Crichton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1030" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Computational Approach to Morphology and Syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning character-level representations for partof-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dos</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/santos14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning. PMLR, Bejing, China</title>
		<editor>Eric P. Xing and Tony Jebara</editor>
		<meeting>the 31st International Conference on Machine Learning. PMLR, Bejing, China</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Morfessor 2.0: Toolkit for statistical morphological segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Smit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Virpioja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig-Arne</forename><surname>Grönroos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Kurimo</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E14-2006" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Letter n-gram-based input encoding for continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Sperr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-3204" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality. Association for Computational Linguistics</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality. Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Word representation models for morphologically rich languages in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<idno>CoRR abs/1606.04217</idno>
		<ptr target="http://arxiv.org/abs/1606.04217" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1157" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1504" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
