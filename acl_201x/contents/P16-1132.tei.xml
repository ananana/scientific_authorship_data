<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Search-Based Dynamic Reranking Model for Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
							<email>zhouh@nlp.nju.edu.cn, yue zhang@sutd.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">†State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">‡Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Nanjing Normal University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Search-Based Dynamic Reranking Model for Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1393" to="1402"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel reranking method to extend a deterministic neural dependency parser. Different to conventional k-best reranking, the proposed model integrates search and learning by utilizing a dynamic action revising process, using the rerank-ing model to guide modification for the base outputs and to rerank the candidates. The dynamic reranking model achieves an absolute 1.78% accuracy improvement over the deterministic baseline parser on PTB, which is the highest improvement by neural rerankers in the literature.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network models have recently been ex- ploited for dependency parsing. <ref type="bibr" target="#b2">Chen and Manning (2014)</ref> built a seminal model by replacing the SVM classifier at the transition-based Malt- Parser ( <ref type="bibr" target="#b23">Nivre et al., 2007</ref>) with a feed-forward neural network, achieving significantly higher ac- curacies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demon- strates strong potentials for neural network models in transition-based dependency parsing.</p><p>Subsequent work aimed to improve the model of <ref type="bibr" target="#b2">Chen and Manning (2014)</ref> in two main direc- tions. First, global optimization learning and beam search inference have been exploited to reduce er- ror propagation <ref type="bibr">(Weiss et al., 2015;</ref><ref type="bibr" target="#b3">Zhou et al., 2015)</ref>. Second, recurrent neural network models have been used to extend the range of neural fea- tures beyond a local window ( . These methods give ac- curacies that are competitive to the best results in the literature.  Another direction to extend a baseline parser is reranking <ref type="bibr" target="#b4">(Collins and Koo, 2000;</ref><ref type="bibr" target="#b1">Charniak and Johnson, 2005;</ref><ref type="bibr" target="#b16">Huang, 2008)</ref>. Recently, neural network models have been used to con- stituent ( <ref type="bibr">Socher et al., 2013;</ref><ref type="bibr" target="#b19">Le et al., 2013)</ref> and dependency ( <ref type="bibr" target="#b18">Le and Zuidema, 2014;</ref><ref type="bibr" target="#b3">Zhu et al., 2015</ref>) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>John loves Mary</head><p>Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transition- based neural parser and neural reranker, which have rather weak feature locality. In addition, k- best lists from the baseline parser are not necessar- ily the best candidates for a reranker. Our prelim- inary results show that reranking candidates can be constructed by modifying unconfident actions in the baseline parser output, and letting the base- line parser re-decode the sentence from the mod- ified action. In particular, revising two incorrect actions of the baseline parser yields oracle with 97.79% UAS, which increases to 99.74% by revis- ing five actions. Accordingly, we design a novel search-based dynamic reranking algorithm by re- vising baseline parser outputs.</p><p>For example, the sentence: "John loves Mary", the baseline parser generates a base tree ( <ref type="figure" target="#fig_1">Figure  1a</ref>) using 5 shift-reduce actions <ref type="figure" target="#fig_1">(Figure 1d</ref>) of Sec- tion 2. The gold parse tree can be obtained by a 2-step action revising process: As shown in <ref type="figure" target="#fig_1">Figure 1d</ref>, we first revise the least confident action S of the base tree, running the baseline parser again from the revised action to obtain tree 1. This corrects the John loves de- pendency arc. Then we obtain the gold parsing tree (tree 2) by further revising the least confident action in tree 1 on the second action sequence.</p><p>Rather than relying on the baseline model scores alone for deciding the action to re- vise (static search), we build a neural network model to guide which actions to revise, as well as to rerank the output trees (dynamic search).</p><p>The resulting model integrates search and learn- ing, yielding the minimum amount of candidates for the best accuracies. Given the extensively fast speed of the baseline parser, the reranker can be executed with high efficiency.</p><p>Our dynamic search reranker has two main ad- vantages over the static one: the first is train- ing diversity, the dynamic reranker searches over more different structurally diverse candidate trees, which allows the reranker to distinguish candi- dates more easily; the second is reranking oracle, with the guidance of the reranking model, the dy- namic reranker has a better reranking oracle com- pared to the static reranker.</p><p>On WSJ, our dynamic reranker achieved 94.08% and 93.61% UAS on the development and test sets, respectively, at a speed of 16.1 sentences per second. It yields a 0.44% accuracy improve- ment (+1.78%) from the same number of candi-</p><formula xml:id="formula_0">… … … … … x h o act o label W 1 W2 W 3</formula><p>Output <ref type="figure">Figure 2</ref>: Hierarchical neural parsing model.</p><p>dates, compared to a static reranker (+1.34%), ob- taining the largest accuracy improvement among related neural rerankers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline Dependency Parser</head><p>Transition-based dependency parsers scan an in- put sentence from left to right, performing a se- quence of transition actions to predict its parse tree <ref type="bibr">(Nivre, 2008)</ref>. We employ the arc-standard system ( <ref type="bibr" target="#b23">Nivre et al., 2007)</ref>, which maintains partially-constructed outputs using a stack, and orders the incoming words in the sentence in a queue. Parsing starts with an empty stack and a queue consisting of the whole input sentence. At each step, a transition action is taken to consume the input and construct the output. Formally, a parsing state is denoted as j, S, L, where S is a stack of subtrees [. . . s 2 , s 1 , s 0 ], j is the head of the queue (i.e. [ q 0 = w j , q 1 = w j+1 · · · ]), and L is a set of dependency arcs that has been built. At each step, the parser chooses one of the following actions:</p><p>• SHIFT (S): move the front word w j from the queue onto the stacks.</p><p>• LEFT-l (L): add an arc with label l between the top two trees on the stack (s 1 ← s 0 ), and remove s 1 from the stack.</p><p>• RIGHT-l (R): add an arc with label l between the top two trees on the stack (s 1 → s 0 ), and remove s 0 from the stack. Given the sentence "John loves Mary", the gold standard action sequence is S, S, L, S, R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model</head><p>Chen and Manning (2014) proposed a determinis- tic neural dependency parser, which rely on dense embeddings to predict the optimal actions at each step. We propose a variation of <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>, which splits the output layer into two hi- erarchical layers: the action layer and dependency label layer. The hierarchical parser determines a action in two steps, first deciding the action type, and then the dependency label <ref type="figure">(Figure 2)</ref>.</p><p>At each step of deterministic parsing, the neural model extracts n atomic features from the parsing state. We adopt the feature templates of <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>. Every atomic feature is repre- sented by a feature embedding e i ∈ R d , An input layer is used to concatenate the n feature embed- dings into a vector x = [e 1 ; e 2 . . . e n ], where x ∈ R d·n . Then x is mapped to a d h -dimensional hid- den layer h by a mapping matrix W 1 ∈ R d h ×d·n and a cube activation function for feature combi- nation:</p><formula xml:id="formula_1">h = (W 1 x + b 1 ) 3 (1)</formula><p>Our method is different from Chen and Man- ning (2014) in the output layer. Given the hidden layer h, the action type output layer o act and the label output layer o label (a i ) of the action type a i are computed as</p><formula xml:id="formula_2">o act = W 2 h (2) o label (a i ) = W i 3 h ,<label>(3)</label></formula><p>Where W 2 ∈ R da×d h is the mapping matrix from the hidden layer to the action layer, and d a is the number of action types. W i 3 ∈ R d label ×d h is the mapping matrix from the hidden layer to the cor- responding label layer, d label is the number of de- pendency labels.</p><p>The probability of a labeled action y i,j given its history Acts and input x is computed as:</p><formula xml:id="formula_3">p(y i,j | x, Acts) = p(a i | x, Acts) × p(l j | x, Acts, a i ) (4) where p(a i | x, Acts) = e o i act da k=1 e o k act (5) p(l j | x, Acts, a i ) = e o j label (a i ) d label k=1 e o k label (a i )</formula><p>, <ref type="formula">(6)</ref> Here a i is the i th action in the action layer, and l j is the j th label in the label layer for a i . In training, we use the cross-entropy loss to maximum the probability of training data A:</p><formula xml:id="formula_4">L(θ) = − y i,j ∈A log p(y i,j | x, Acts)<label>(7)</label></formula><p>Experiments show that our hierarchical neural parser is both faster and slightly accurate than the original neural parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reranking Scorer</head><p>We adopt the recursive convolutional neural net- work (RCNN) of Zhu et al. <ref type="formula" target="#formula_13">(2015)</ref> for scoring full trees. Given a dependency subtree rooted at h, c i (0 &lt; i ≤ L) is the i th child of h. The de- pendency arc (h, c i ) is represented by:</p><formula xml:id="formula_5">z i = tanh(W (h,c i ) p i ) ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_6">p i = w h ⊕ x c i ⊕ d (h,c i )<label>(9)</label></formula><p>Here p i ∈ R n is the concatenation of head word embedding w h , child phrase representation x c i and the distance embeddings </p><formula xml:id="formula_7">d (h,c i ) . W (h,c i ) ∈ R</formula><formula xml:id="formula_8">Z h = [z 1 , z 2 , . . . , z L ]<label>(10)</label></formula><formula xml:id="formula_9">x h j = max i Z h j,i , 0 &lt; j &lt; m (11)</formula><p>The subtree with the head h is scored by:</p><formula xml:id="formula_10">score(h) = L i=1 v h,c i z i<label>(12)</label></formula><p>Here, v h,c i is the score vector, which is a vector of parameters that need to be trained. The score of the whole dependency tree y is computed as:</p><formula xml:id="formula_11">s t (x, y, Θ) = w∈y score(w),<label>(13)</label></formula><p>where w is the node in tree y and Θ denotes the set of parameters in the network.  <ref type="table">Table 2</ref>: Average action probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Revising</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Properties of the Baseline Parser</head><p>To demonstrate the above three properties, we give some preliminary results for the baseline. To parse the 1,695 sentences in Section 22 of WSJ, our baseline parser needs to perform 78,227 shift- reduce actions. During the process, if we correct every encountered incorrectly determined action and let the baseline parser re-decode the sentence from the point, we need to revise 2,052 actions, av- eraging 1.2 actions per sentence. In other words, the baseline parser can parse the 1,695 sentences correctly with 2,052 action being revised. Note that the revise operation is required to change the action type (i.e. S, L). After revising the action type, the optimal dependency label will be chosen for parsing by the hierarchical baseline parser. We only modify the action type in the re- vising process. Thus the modified trees are always structurally different instead of only with different dependency labels compared to the original one, which guarantees structured diversity.</p><p>Revising Efficiency It can be seen from <ref type="table">Table 1</ref> that revising one incorrect action results in 3.5% accuracy improvement. We obtain a 99.74% UAS after a maximum 5 depth revising. Although we only revise the action type, the LAS goes up with the UAS. The property of revising efficiency sug- gests that high quality tree candidates can be found with a small number of changes.</p><p>Probability Diversity Actions with lower prob- abilities are more likely to be incorrect. We com- pute the average probabilities of gold and incor- rect actions in parsing the section 22 of WSJ (Ta- ble 2), finding that most gold actions have very high probabilities. The average probabilities of the gold actions is much higher than that of the incorrectly predicted ones, indicating that revising actions with lower probabilities can lead to better trees.</p><p>Search Efficiency The fast speed of the baseline parser allows the reranker to search a large num- ber of tree candidates efficiently. With the graph stack trick ( <ref type="bibr" target="#b11">Goldberg et al., 2013)</ref>, the reranker only needs to perform partial parsing to obtain new trees. This enables a fast reranker in theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Search Strategy</head><p>Given an output sequence of actions by the base- line parser, we revise the action with the lowest probability margin, and start a new branch by tak- ing a new action at this point. The probability mar- gin of an action a is computed as: p(a max ) − p(a), where a max is the action taken by the baseline, which has the highest model probability. a is taken instead of a max for this branch, and the baseline parser is executed deterministically until parsing finishes, thus yielding a new dependency tree. We require that the action type must change in the revision and the most probable dependency label among all for the revised action type will be used.</p><p>Multiple strategies can be used to search for the revised reranking process. For example, one intuitive strategy is best-first, which modifies the action with the lowest probability margin among all sequences of actions constructed so far. Start- ing from the original output of the baseline parser, modifying the action with the lowest probability margin results in a new tree. According to the best-first strategy, the action with the lowest prob- ability margin in the two outputs will be revised next to yield the third output. The search repeats until k candidates are obtained, which are used as candidates for reranking.</p><p>The best-first strategy, however, does not con- sider the quality of the output, which is like a greedy process. A better candidate ( with higher F1 score) is more likely to take us to the gold tree. With the best-first strategy, we revise one tree at each time. If the selected tree is not the optimal one, the revised tree will be less likely the gold one. Revising a worse output is less likely to gen- erate the gold parse tree compared with revising a relatively better output. Our preliminary experi-ments confirms this intuition. As a result, we take a beam search strategy, which uses a beam to hold b outputs to modify.</p><p>For each tree in beam search, most f actions with the lowest probability margin are modified, leading to b × f new trees. Here, b is the beam size, f is the revising factor. From these trees, the b best are put to the beam for the next step. Search starts with the beam containing only the original base parse, and repeats for l steps, where l is called the revising depth. The best tree will be selected from all the trees constructed. The search process for example in <ref type="figure" target="#fig_1">Figure 1</ref> is illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>, in which b = 1, f = 3 and l = 2.</p><p>At each iteration, the b best candidates can be decided by the baseline parser score alone, which is the product of the probability of each action. We call this the static search reranking. As mentioned in the introduction, the baseline model score might not be the optimal criteria to select candidates for reranking, since they may not reflect the best or- acle or diversity. We introduce a dynamic search strategy instead, using the reranking model to cal- culate heuristic scores for guiding the search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Search-Based Dynamic Reranking</head><p>Doppa et al. <ref type="formula" target="#formula_2">(2013)</ref> propose that structured- prediction by learning guide search should main- tain two different scoring functions, a heuristic function for guiding search and a cost function for obtaining the best output. Following <ref type="bibr" target="#b8">Doppa et al. (2013)</ref>, we use the RCNN in Section 3 to yield two different scores, namely a heuristic score s t (x, y, Θ h ) to guide the search of revising, and a cost score s t (x, y, Θ c ) to select the best tree out- put.</p><p>Denote b(i) as the beam at i-th step of search, k-best candidates in the beam of i + 1 step is:</p><formula xml:id="formula_12">b(i + 1) = arg K c∈c(i) (s t (x, c, Θ h ) + s b (x, c)), (14)</formula><p>where c(i) denotes the set of newly constructed trees by revising trees in b(i), s b (x, c) is the base- line model score and arg K leaves the k best can- didate trees to the next beam. Finally, the output tree y i of reranking is selected from all searched trees C in the revising process</p><formula xml:id="formula_13">y i = arg max c∈C (s t (x, c, Θ c ) + s b (x, c))<label>(15)</label></formula><p>Interpolated Reranker In testing, we also adopt the popular mixture reranking strat- egy ( <ref type="bibr" target="#b14">Hayashi et al., 2013;</ref><ref type="bibr" target="#b17">Le and Mikolov, 2014</ref>),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Training Algorithm for the Search-Based Dynamic Reranking.</head><p>Input: Sentence x, Gold Trees y Output:</p><formula xml:id="formula_14">Θ h , Θc for iter ← 1 to N do D h = []; D k = [];</formula><p>foreach (x, y) ∈ (x, y) do bestHScoreT = null; bestCScoreT = null; bestUAST = null; initTree = BASELINEPARSE(x); b1 = [initTree]; b2 = []; for d ← 1 to depth do foreach t ∈ b1 do revisedActs = SEEK (t); revisedTrees = REVISE (t, revisedActs); bestK = SORT (revisedTrees, Θ h ) b2.ADD (bestK); bestHScoreT = MAXSCORE (bestHScoreT, revisedTrees, Θ h ); bestCScoreT = MAXSCORE (bestCScoreT, revisedTrees, Θc);</p><formula xml:id="formula_15">bestUAST = MAXUAS (bestUAST, revisedTrees, y) b1 = b2; b2 = []; D h .ADD (x, bestUAST, bestTScoreT); Dc.ADD (x, y, bestCScoreT); UPDATE(D h , Θ h ); UPDATE(Dc, Θc);</formula><p>which obtains better reranking performance by a linear combination of the reranking score and the baseline model score.</p><formula xml:id="formula_16">y i = arg max y∈τ (x i ) (β(s t (x i , y, Θ c ) + s t (x, y, Θ h )) + (1 − β)s b (x i , y))<label>(16)</label></formula><p>Here y i is the final output tree for a sentence x i ; τ (x i ) returns all the trees candidates of the dy- namic reranking; β ∈[0, 1] is a hyper-parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>As k-best neural rerankers ( <ref type="bibr">Socher et al., 2013;</ref><ref type="bibr" target="#b3">Zhu et al., 2015)</ref>, we use the max-margin cri- terion to train our model in a stage-wise man- ner ( <ref type="bibr" target="#b8">Doppa et al., 2013)</ref>. Given training data</p><formula xml:id="formula_17">D c = (x i , y i , ˆ y i ) N i=1</formula><p>, where x i is the sentence, ˆ y i is the output tree with highest cost score and y i is the corresponding gold tree, the final training objec- tive is to minimize the loss function J(Θ c ), plus a </p><formula xml:id="formula_18">S S S S S S L L L L S S L S S L L S S R R R S S S S S L R ... beam(0) L Action Candidates 0.1 R 0.1 R 1.0 R 0.0 6.3 5.2 4.9 S S L S S L L beam(1) S S L S R</formula><formula xml:id="formula_19">J(Θ c ) = 1 |D c | (x i ,y i , ˆ y i )∈Dc r i (Θ c ) + λ 2 ||Θ c || (17) r i (Θ c ) = max(0, s t (x i , ˆ y i , Θ c ) + ∆(y i , ˆ y i ) − s t (x i , y i , Θ c ))<label>(18)</label></formula><p>Here, Θ c is the model, s t (x i , y i , Θ c ) is the cost reranking score for y i .</p><formula xml:id="formula_20">∆(y i , ˆ y i ) = d∈ˆyd∈ˆ d∈ˆy i κ1{d / ∈ y i }<label>(19)</label></formula><p>∆(y i , ˆ y i ) is the structured margin loss between y i andˆyandˆ andˆy i , measured by counting the number of incor- rect dependency arcs in the tree <ref type="bibr" target="#b12">(Goodman, 1998;</ref><ref type="bibr" target="#b3">Zhu et al., 2015)</ref>.</p><p>Given training data</p><formula xml:id="formula_21">D h = (x i , y i , ˆ y i ) N i=1</formula><p>for the heuristic score model, the training objective is to minimize the loss between the tree with the best UAS y i and the tree with the best heuristic rerank- ing scorê y i .</p><formula xml:id="formula_22">J(Θ h ) = 1 |D h | (x i ,y i ,ˆ y i )∈D h r i (Θ h ) + λ 2 ||Θ h || (20) r i (Θ h ) = max(0, s t (x i , ˆ y i , Θ h )) − s t (x i , y i , Θ h )<label>(21)</label></formula><p>The detailed training algorithm is given by Al- gorithm 1. AdaGrad ( <ref type="bibr" target="#b9">Duchi et al., 2011</ref>) updating with subgradient ( <ref type="bibr">Ratliff et al., 2007)</ref> and mini- batch is adopted for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Set-up</head><p>Our experiments are performed using the English Penn Treebank (PTB; <ref type="bibr" target="#b20">Marcus et al., (1993)</ref>). We follow the standard splits of PTB3, using sections 2-21 for training, section 22 for development and section 23 for final testing. Following prior work on reranking, we use Penn2Malt 1 to convert con- stituent trees to dependency trees. Ten-fold POS jackknifing is used in the training of the baseline parser. We use the POS-tagger of Collins <ref type="formula">(2002)</ref> to assign POS automatically. Because our reranking model is a dynamic reranking model, which gen- erates training instances during search, we train 10 baseline parsing models on the 10-fold jackknifing data, and load the baseline parser model dynami- cally for reranking training .</p><p>We follow <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>, using the set of pre-trained word embeddings with a dictio- nary size of 13,000 2 from Collobert et al. (2011). The word embeddings were trained on the entire English Wikipedia, which contains about 631 mil- lion words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyper-parameters</head><p>There are two different networks in our system, namely a hierarchical feed-forward neural net- work for the baseline parsing and a recursive con- volution network for dynamic reranking. The hyper-parameters of the hierarchical parser are set as described by <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>, with the embedding size d = 50, the hidden layer size d h = 300, the regularization parameter λ = 10 −8 , the initial learning rate of Adagrad α = 0.01 and the batch size b = 100,000. We set the hyper- parameters of the RCNN as follows: word embed- ding size d w rnn = 25, distance embedding size d d rnn = 25, initial learning rate of Adagrad α rnn = 0.1, regularization parameter λ rnn = 10 −4 , margin loss discount κ = 0.1 and revising factor f = 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Hierarchical Neural Parser</head><p>Shown in <ref type="table" target="#tab_1">Table 3</ref>, the proposed hierarchical base parser is 1.3 times faster, and obtains a slight ac- curacy improvement (   <ref type="table">Table 4</ref>: Accuracies of the revising reranker with different beam sizes on the development set.</p><p>speed gain is that smaller output layer leads to less computation of mapping from the hidden layer to the output layer in neural networks <ref type="bibr" target="#b22">(Morin and Bengio, 2005;</ref><ref type="bibr" target="#b21">Mnih and Hinton, 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Development Tests</head><p>For the beam search dynamic reranking model, the selection of beam size b and revising depth l affect the accuracy and efficiency of the reranker. We tune the values on the development set.</p><p>Beam Size A proper beam size balances effi- ciency and accuracy in the search process. The reranking accuracies with different beam sizes are listed in <ref type="table">Table 4</ref>. Here, the oracle is the best UAS among searched trees during reranking. K is the number of searched candidate trees in testing. The UAS and parsing oracle both go up with increas- ing the beam size. Reranking with beam size = 4 gives the best development performance. We set the final beam size as 4 in the next experiments.</p><p>Revising Depth As shown in <ref type="table" target="#tab_4">Table 5</ref>, with re- vising depth increasing from 1 to 3, the reranker obtains better parsing oracle. The depth of 3 gives the best UAS 93.81% on the development set. The parsing oracle stops improving with deeper revised search. This may because in the fourth search step, the high quality trees begin to fall out the beam, resulting in worse output candi- dates, which make the revising step yield less ora- cle gains. We set the search depth as 3 in the next experiments.</p><p>Integrating Search and Learning Shown in <ref type="table" target="#tab_5">Table 6</ref>, the dynamic and static rerankers both achieve significant accuracy improvements over the baseline parser. The dynamic reranker gives   much better improvement, although the oracle of dynamic reranker is only 0.2% higher than the static one. This demostrates the benefit of diver- sity. The candidates are always the same for static search, but the dynamic reranker searches more diverse tree candidates in different iterations of training.</p><p>To further explore the impact of training diver- sity to dynamic reranking, we also compare the dynamic search reranker of training and testing with different revising depth. In <ref type="table" target="#tab_7">Table 7</ref>, origin is the results by training and testing with the same depth d. Results of ts is obtained by training with d = 3, and testing with a smaller d. For example, a reranker with training d = 3 and testing d = 2 achieves better performance than with training d = 2 and testing d = 2. The testing oracle of the for- mer reranker is lower than the later, yet the former learns more from the training instance, obtaining better parsing accuracies. This again indicates that training diversity is very important besides the or- acle accuracy.</p><p>Interpolated Reranker Finally, we mix the baseline model score and the reranking score by following <ref type="bibr" target="#b14">Hayashi et al. (2013)</ref> and <ref type="bibr" target="#b3">Zhu et al. (2015)</ref>, and the mixture parameter β is optimized by searching with the step size of 0.005. With the mixture reranking trick, the dynamic reranker ob- tains an accuracy of 94.08% <ref type="table" target="#tab_8">(Table 8)</ref>, with an im- provement of 0.28% on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Final Results</head><p>Comparison with Dependency Rerankers In <ref type="table" target="#tab_10">Table 9</ref>, we compare the search-based dynamic rerankers with a list of dependency rerankers. The reranking models of <ref type="bibr" target="#b14">Hayashi et al. (2013)</ref> and <ref type="bibr" target="#b13">Hayashi et al. (2011)</ref> are forest reranking mod- els. <ref type="bibr" target="#b18">Le and Zuidema (2014)</ref> and <ref type="bibr" target="#b3">Zhu et al. (2015)</ref> are neural k-best reranking models. Our dynamic   reranking model achieves the highest accuracy im- provement over the baseline parser on both the de- velopment and test sets. We obtain the best perfor- mance on the development set. <ref type="bibr" target="#b3">Zhu et al. (2015)</ref> achieved higher accuracy on the test set, but they adopted a better baseline parser than ours, which could not be used in our dynamic reranker because it is not fast enough and will make our reranker slow in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing with Neural Dependency Parsers</head><p>We also compare parsing accuracies and speeds with a number of neural network dependency parsers.  proposed a dependency parser with stack LSTM; <ref type="bibr" target="#b3">Zhou et al. (2015)</ref> ap- plied the beam search for structured dependency parsing. Both achieved significant accuracy im- provements over the deterministic neural parser of <ref type="bibr" target="#b2">Chen and Manning (2014)</ref>. Our dynamic search reranker obtains a 93.61% UAS on the test set, which is higher than most of the neural parsers ex- cept <ref type="bibr">Weiss et al. (2015)</ref>, who employ a structured prediction model upon the neural greedy baseline, achieving very high parsing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Results on Stanford dependencies</head><p>We also evaluate the proposed static and dynamic rerankers on Staford dependency treebank. The main results are consistent with CoNLL depen- dency treebank with the dynamic reranker achiev- ing a 0.41% accuracy improvement upon the static reranker on test data. But the parsing accuracy on Stanford dependency is not the state-of-the-art. We speculate that there may be two reasons. First, the baseline parsing accuracy on Stanford depen- dencies is lower than CoNLL. Second, all the hyper-parameters are tuned on the CoNLL data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Neural Networks Reranking A line of work has been proposed to explore reranking using neu- ral networks. <ref type="bibr">Socher et al. (2013)</ref> first proposed a neural reranker using a recursive neural net- work for constituent parsing. <ref type="bibr" target="#b18">Le and Zuidema (2014)</ref> extended the neural reranker to dependency parsing using a inside-outside recursive neural network (IORNN), which can process trees both bottom-up and top-down. <ref type="bibr" target="#b3">Zhu et al. (2015)</ref> pro- posed a RCNN method, which solved the prob- lem of modeling k-ary parsing tree in dependency parsing. The neural rerankers are capable of cap- turing global syntax features across the tree. In contrast, the most non-local neural parser with LSTM ( ) cannot exploit global features. Different to previous neural rerankers, our work in this paper contributes on integrat- ing search and learning for reranking, instead of proposing a new neural model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Forest</head><p>Reranking Forest reranking <ref type="bibr" target="#b16">(Huang, 2008;</ref><ref type="bibr" target="#b14">Hayashi et al., 2013</ref>) offers a different way to extend the coverage of reranking candidates, with computing the reranking score in the trees forests by decomposing non-local features with cube-pruning ( <ref type="bibr" target="#b15">Huang and Chiang, 2005</ref>). In con- trast, the neural reranking score encodes the whole dependency tree, which cannot be decomposed for forest reranking efficiently and accurately. <ref type="bibr" target="#b8">Doppa et al. (2013)</ref> proposed a structured prediction model with HC-Search strat- egy and imitation learning, which is closely re- lated to our work in spirit. They used the complete space search ( <ref type="bibr" target="#b7">Doppa et al., 2012</ref>) for sequence la- beling tasks, and the whole search process halts after a specific time bound. Different from them, we propose a dynamic parsing reranking model based on the action revising process, which is a multi-step process by revising the least confident   actions from the base output and the search stops in a given revising depth. The dynamic rerank- ing model concentrates on extending the train- ing diversity and testing oracle for parsing rerank- ing, which is built on the transition-based parsing framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HC-Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed a search-based dy- namic reranking model using a hierarchical neu- ral base parser and a recursive convolutional neu- ral score model. The dynamic model is the first reranker integrating search and learning for de- pendency parsing. It achieves significant accuracy improvement (+1.78%) upon the baseline deter- ministic parser. With the dynamic search process, our reranker obtains a 0.44% accuracy improve- ment upon the static reranker. The code of this pa- per can be downloaded from http://github. com/zhouh/dynamic-reranker.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(d) 2-step action revising process for sentence "John loves Mary". Numbers before actions are the probabilities for that action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>m×n is a linear composition matrix, which de- pends on the POS tags of h and c i . The sub- tree phrase representation x h are computed using a max-pooling function on rows, over the matrix of arc representations Z h .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The beam search revising process of the example in Figure 1 with b = 1, f = 3 and l = 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 )</head><label>3</label><figDesc></figDesc><table>upon the parser of 
Chen and Manning (2014). The reason for the </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Performance comparison between the hi- erarchical and original neural parsers. Speed: sen- tences per second.</head><label>3</label><figDesc></figDesc><table>Beam Size 
1 
2 
4 
8 
UAS 
93.38 93.45 93.81 93.51 
Oracle 
96.95 97.29 97.80 97.81 
K 
22.57 37.16 
65.8 
118.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : Accuracies of the revised reranker with different revising depths on development set.</head><label>5</label><figDesc></figDesc><table>Search Type 
UAS 
+UAS Oracle 
Dynamic 
93.81 +1.53 
97.80 
Static 
93.29 +1.01 
97.61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparing dynamic and the static search. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 : Accuracies of the revised reranker with different revising depths on the development set.</head><label>7</label><figDesc></figDesc><table>Type 
static dynamic 
w/o mixture 93.29 
93.81 
w/ mixture 
93.53 
94.08 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Effects of interpolated reranking. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Comparison of dependency rerankers. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>Comparison with neural parsers. Speed: 
sentences per second.  †: results are reported on 
Stanford dependencies.  ‡: results are run by our-
self using their codes. 

System 
UAS 
dev 
test 
baseline 
91.80 
91.41 
dynamic 
93.44 (+1.64) 92.95 (+1.57) 
static 
93.09 (+1.29) 
92.57 (+1.16) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table>Dynamic reranking results on Stanford 
dependencies. 

</table></figure>

			<note place="foot" n="4"> Search-based Dynamic Reranking for Dependency Parsing Using the hierarchical parser of Section 2 as the baseline parser, we propose a search-based dynamic reranking model, which integrates search and learning by searching the reranking candidates dynamically, instead of limiting the scope to a fixed k-best list. The efficiency of the reranking model is guaranteed by 3 properties of the baseline parser, namely revising efficiency, probability diversity and search efficiency.</note>

			<note place="foot" n="1"> http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html 2 http://ronan.collobert.com/senna/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their insightful comments. Xin-Yu Dai is the corresponding author of this paper. This work was supported by the Natural Science Foundation of <ref type="bibr">China (61472183, 6130158, 61472191)</ref>, the 863 program via 2015AA015406 and Singapore Min-istratry of Education Tier 2 Grant T2MOE201301.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing using two heterogeneous gated recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE</title>
		<imprint>
			<biblScope unit="page" from="175" to="182" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Output space search for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hc-search: Learning heuristics and cost functions for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient implementation of beam-search incremental parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="628" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<idno>cmp-lg/9805007</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
<note type="report_type">Parsing inside-out. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Third-order variational reranking on packed-shared dependency forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1479" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient stacked dependency parsing by forest reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="139" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Better k-best parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology</title>
		<meeting>the Ninth International Workshop on Parsing Technology</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The insideoutside recursive neural network model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="729" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remko</forename><surname>Scha</surname></persName>
		</author>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, chapter Learning from errors: Using vector-based compositional semantics for parse reranking</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality, chapter Learning from errors: Using vector-based compositional semantics for parse reranking</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international workshop on artificial intelligence and statistics</title>
		<meeting>the international workshop on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
<note type="report_type">Citeseer</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atanas</forename><surname>Chanev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsen</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetoslav</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Maltparser</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
