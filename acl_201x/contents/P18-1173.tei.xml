<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Backpropagating through Structured Argmax using a SPIGOT</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
							<email>sthomson@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♦</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Washington</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Backpropagating through Structured Argmax using a SPIGOT</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1863" to="1873"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce the structured projection of intermediate gradients optimization technique (SPIGOT), a new method for backpropagating through neural networks that include hard-decision struc-tured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks (Kim et al., 2017) and some reinforcement learning-inspired solutions (Yogatama et al., 2017). Like so-called straight-through estimators (Hinton, 2012), SPIGOT defines gradient-like quantities associated with intermediate nondif-ferentiable operations, allowing backprop-agation before and after them; SPIGOT&apos;s proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning methods for natural language process- ing are increasingly dominated by end-to-end dif- ferentiable functions that can be trained using gradient-based optimization. Yet traditional NLP often assumed modular stages of processing that formed a pipeline; e.g., text was tokenized, then tagged with parts of speech, then parsed into a phrase-structure or dependency tree, then semanti- cally analyzed. Pipelines, which make "hard" (i.e., discrete) decisions at each stage, appear to be in- compatible with neural learning, leading many re- searchers to abandon earlier-stage processing.</p><p>Inspired by findings that continue to see benefit from various kinds of linguistic or domain-specific preprocessing ( <ref type="bibr" target="#b31">He et al., 2017;</ref><ref type="bibr">Oepen et al., 2017;</ref><ref type="bibr" target="#b35">Ji and Smith, 2017)</ref>, we argue that pipelines can be treated as layers in neural architectures for NLP tasks. Several solutions are readily available:</p><p>• Reinforcement learning (most notably the REINFORCE algorithm; <ref type="bibr">Williams, 1992)</ref>, and structured attention (SA; <ref type="bibr" target="#b36">Kim et al., 2017</ref>). These methods replace argmax with a sam- pling or marginalization operation. We note two potential downsides of these ap- proaches: (i) not all argmax-able operations have corresponding sampling or marginaliza- tion methods that are efficient, and (ii) in- spection of intermediate outputs, which could benefit error analysis and system improve- ment, is more straightforward for hard deci- sions than for posteriors.</p><p>• The straight-through estimator (STE; Hin- ton, 2012) treats discrete decisions as if they were differentiable and simply passes through gradients. While fast and surpris- ingly effective, it ignores constraints on the argmax problem, such as the requirement that every word has exactly one syntactic par- ent. We will find, experimentally, that the quality of intermediate representations de- grades substantially under STE. This paper introduces a new method, the struc- tured projection of intermediate gradients opti- mization technique (SPIGOT; §2), which defines a proxy for the gradient of a loss function with re- spect to the input to argmax. Unlike STE's gradi- ent proxy, SPIGOT aims to respect the constraints in the argmax problem. SPIGOT can be applied with any intermediate layer that is expressible as a constrained maximization problem, and whose feasible set can be projected onto. We show em- pirically that SPIGOT works even when the max- imization and the projection are done approxi- mately.</p><p>We offer two concrete architectures that employ structured argmax as an intermediate layer: se- mantic parsing with syntactic parsing in the mid- dle, and sentiment analysis with semantic parsing in the middle ( §3). These architectures are trained using a joint objective, with one part using data for the intermediate task, and the other using data for the end task. The datasets are not assumed to over- lap at all, but the parameters for the intermediate task are affected by both parts of the training data.</p><p>Our experiments ( §4) show that our architecture improves over a state-of-the-art semantic depen- dency parser, and that SPIGOT offers stronger per- formance than a pipeline, SA, and STE. On sen- timent classification, we show that semantic pars- ing offers improvement over a BiLSTM, more so with SPIGOT than with alternatives. Our analy- sis considers how the behavior of the intermedi- ate parser is affected by the end task ( §5). Our code is open-source and available at https:// github.com/Noahs-ARK/SPIGOT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our aim is to allow a (structured) argmax layer in a neural network to be treated almost like any other differentiable function. This would allow us to place, for example, a syntactic parser in the mid- dle of a neural network, so that the forward calcu- lation simply calls the parser and passes the parse tree to the next layer, which might derive syntactic features for the next stage of processing.</p><p>The challenge is in the backward computation, which is key to learning with standard gradient- based methods. When its output is discrete as we assume here, argmax is a piecewise constant func- tion. At every point, its gradient is either zero or undefined. So instead of using the true gradient, we will introduce a proxy for the gradient of the loss function with respect to the inputs to argmax, allowing backpropagation to proceed through the argmax layer. Our proxy is designed as an im- provement to earlier methods (discussed below) that completely ignore constraints on the argmax operation. It accomplishes this through a projec- tion of the gradients.</p><p>We first lay out notation, and then briefly review max-decoding and its relaxation ( §2.1). We define SPIGOT in §2.2, and show how to use it to back- propagate through NLP pipelines in §2.3.</p><p>Notation. Our discussion centers around two tasks: a structured intermediate task followed by an end task, where the latter considers the out- puts of the former (e.g., syntactic-then-semantic parsing). Inputs are denoted as x, and end task outputs as y. We use z to denote intermedi- ate structures derived from x. We will often re- fer to the intermediate task as "decoding", in the structured prediction sense. It seeks an outputˆz outputˆ outputˆz = argmax z∈Z S from the feasible set Z, max- imizing a (learned, parameterized) scoring func- tion S for the structured intermediate task. L de- notes the loss of the end task, which may or may not also involve structured predictions. We use</p><formula xml:id="formula_0">∆ k−1 = {p ∈ R k | 1 p = 1, p ≥ 0} to denote the (k − 1)-dimensional simplex.</formula><p>We de- note the domain of binary variables as B = {0, 1}, and the unit interval as U = <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. By projection of a vector v onto a set A, we mean the closest point in A to v, measured by Euclidean distance:</p><formula xml:id="formula_1">proj A (v) = argmin v ∈A v − v 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relaxed Decoding</head><p>Decoding problems are typically decomposed into a collection of "parts", such as arcs in a depen- dency tree or graph. In such a setup, each element of z, z i , corresponds to one possible part, and z i takes a boolean value to indicate whether the part is included in the output structure. The scoring function S is assumed to decompose into a vector s(x) of part-local, input-specific scores:</p><formula xml:id="formula_2">ˆ z = argmax z∈Z S(x, z) = argmax z∈Z z s(x) (1)</formula><p>In the following, we drop s's dependence on x for clarity.</p><p>In many NLP problems, the output space Z can be specified by linear constraints ( <ref type="bibr">Roth and Yih, 2004</ref>):</p><formula xml:id="formula_3">A z ψ ≤ b,<label>(2)</label></formula><p>where ψ are auxiliary variables (also scoped by argmax), together with integer constraints (typi- cally, each z i ∈ B). <ref type="figure">Figure 1</ref>: The original feasible set Z (red ver- tices), is relaxed into a convex polytope P (the area encompassed by blue edges). Left: making a gradient update tô z makes it step outside the polytope, and it is projected back to P, resulting in the projected point˜zpoint˜ point˜z. s L is then along the edge. Right: updatingˆzupdatingˆ updatingˆz keeps it within P, and</p><formula xml:id="formula_4">1865ˆz˜zˆz 1865ˆ 1865ˆz 1865ˆz˜ 1865ˆz˜z 1865ˆz˜zˆ 1865ˆz˜zˆz ⌘rˆz⌘rˆ ⌘rˆz L r s L ˆ z ˆ z ⌘rˆz⌘rˆ ⌘rˆz L r s L</formula><formula xml:id="formula_5">thus s L = η ˆ z L.</formula><p>The problem in Equation 1 can be NP-complete in general, so the {0, 1} constraints are often re- laxed to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> to make decoding tractable <ref type="bibr" target="#b47">(Martins et al., 2009)</ref>. Then the discrete combinatorial problem over Z is transformed into the optimiza- tion of a linear objective over a convex polytope P ={p ∈ R d |Ap≤b}, which is solvable in poly- nomial time <ref type="bibr" target="#b8">(Bertsimas and Tsitsiklis, 1997</ref>). This is not necessary in some cases, where the argmax can be solved exactly with dynamic programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">From STE to SPIGOT</head><p>We now view structured argmax as an activation function that takes a vector of input-specific part- scores s and outputs a solutionˆzsolutionˆ solutionˆz. For backpropa- gation, to calculate gradients for parameters of s, the chain rule defines:</p><formula xml:id="formula_6">s L = J ˆ z L,<label>(3)</label></formula><p>where the Jacobian matrix J = ∂ˆz∂ˆz ∂s contains the derivative of each element ofˆzofˆ ofˆz with respect to each element of s. Unfortunately, argmax is a piecewise constant function, so its Jacobian is ei- ther zero (almost everywhere) or undefined (in the case of ties).</p><p>One solution, taken in structured attention, is to replace the argmax with marginal inference and a softmax function, so thatˆzthatˆ thatˆz encodes probability distributions over parts ( <ref type="bibr" target="#b36">Kim et al., 2017;</ref><ref type="bibr" target="#b41">Liu and Lapata, 2018)</ref>. As discussed in §1, there are two reasons to avoid this modification. Softmax can only be used when marginal inference is feasible, by sum-product algorithms for example <ref type="bibr" target="#b21">(Eisner, 2016;</ref><ref type="bibr" target="#b24">Friesen and Domingos, 2016)</ref>; in general marginal inference can be #P-complete. Further, a soft intermediate layer will be less amenable to inspection by anyone wishing to understand and improve the model. In another line of work, argmax is aug- mented with a strongly-convex penalty on the so- lutions <ref type="bibr" target="#b44">(Martins and Astudillo, 2016;</ref><ref type="bibr" target="#b2">Amos and Kolter, 2017;</ref><ref type="bibr">Niculae and Blondel, 2017;</ref><ref type="bibr">Niculae et al., 2018;</ref><ref type="bibr" target="#b48">Mensch and Blondel, 2018)</ref>. How- ever, their approaches require solving a relaxation even when exact decoding is tractable. Also, the penalty will bias the solutions found by the de- coder, which may be an undesirable conflation of computational and modeling concerns.</p><p>A simpler solution is the STE method (Hin- ton, 2012), which replaces the Jacobian matrix in Equation 3 by the identity matrix. This method has been demonstrated to work well when used to "backpropagate" through hard threshold func- tions ( <ref type="bibr" target="#b25">Friesen and Domingos, 2018</ref>) and categorical random variables ( <ref type="bibr" target="#b34">Jang et al., 2016;</ref><ref type="bibr" target="#b12">Choi et al., 2017)</ref>.</p><p>Consider for a moment what we would do ifˆzifˆ ifˆz were a vector of parameters, rather than intermedi- ate predictions. In this case, we are seeking points in Z that minimize L; denote that set of minimiz- ers by Z * . GivenˆzGivenˆ Givenˆz L and step size η, we would updatê</p><formula xml:id="formula_7">z to bê z − η ˆ z L.</formula><p>This update, however, might not return a value in the feasible set Z, or even (if we are using a linear relaxation) the re- laxed set P.</p><p>SPIGOT therefore introduces a projection step that aims to keep the "updated" ˆ z in the feasible set. Of course, we do not directly updatê z; we continue backpropagation through s and onward to the parameters. But the projection step nonethe- less alters the parameter updates in the way that our proxy for " s L" is defined.</p><p>The procedure is defined as follows:</p><formula xml:id="formula_8">ˆ p = ˆ z − η ˆ z L, (4a) ˜ z = proj P (ˆ p), (4b) s L ˆ z − ˜ z. (4c)</formula><p>First, the method makes an "update" tô z as if it contained parameters (Equation 4a), lettingˆplettingˆ lettingˆp de- note the new value. Next, ˆ p is projected back onto the (relaxed) feasible set (Equation 4b), yielding a feasible new value˜zvalue˜ value˜z. Finally, the gradients with respect to s are computed by Equation 4c.</p><p>Due to the convexity of P, the projected point˜zpoint˜ point˜z will always be unique, and is guaranteed to be no farther thanˆpthanˆ thanˆp from any point in Z * ( <ref type="bibr" target="#b42">Luenberger and Ye, 2015)</ref>. <ref type="bibr">1</ref> Compared to STE, SPIGOT in-volves a projection and limits s L to a smaller space to satisfy constraints. See <ref type="figure">Figure 1</ref> for an illustration.</p><p>When efficient exact solutions (such as dynamic programming) are available, they can be used. Yet, we note that SPIGOT does not assume the argmax operation is solved exactly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Backpropagation through Pipelines</head><p>Using SPIGOT, we now devise an algorithm to "backpropagate" through NLP pipelines. In these pipelines, an intermediate task's output is fed into an end task for use as features. The parameters of the complete model are divided into two parts: de- note the parameters of the intermediate task model by φ (used to calculate s), and those in the end task model as θ. <ref type="bibr">2</ref> As introduced earlier, the end-task loss function to be minimized is L, which depends on both φ and θ.</p><p>Algorithm 1 describes the forward and back- ward computations. It takes an end task training pair x, y, along with the intermediate task's fea- sible set Z, which is determined by x. It first runs the intermediate model and decodes to get inter- mediate structurê z, just as in a standard pipeline. Then forward propagation is continued into the end-task model to compute loss L, usingˆzusingˆ usingˆz to de- fine input features. Backpropagation in the end- task model computes θ L andˆzandˆ andˆz L, and s L is then constructed using Equations 4. Backpropa- gation then continues into the intermediate model, computing φ L.</p><p>Due to its flexibility, SPIGOT is applicable to many training scenarios. When there is no x, z training data for the intermediate task, SPIGOT can be used to induce latent structures for the end-task ( <ref type="bibr">Yogatama et al., 2017;</ref><ref type="bibr" target="#b36">Kim et al., 2017;</ref><ref type="bibr" target="#b12">Choi et al., 2017</ref>, inter alia). When intermediate-task training data is available, one can use SPIGOT to adopt joint learning by minimizing an interpo- lation of L (on end-task data x, y) and an intermediate-task loss function L (on intermediate task data x, z). This is the setting in our experi- ments; note that we do not assume any overlap in the training examples for the two tasks.</p><p>Algorithm 1 Forward and backward computation with SPIGOT.</p><formula xml:id="formula_9">1: procedure SPIGOT(x, y, Z) 2: Construct A, b such that Z = {p ∈ Z d | Ap ≤ b} 3: P ← {p ∈ R d | Ap ≤ b} Relaxation 4:</formula><p>Forwardprop and compute s φ (x) 5:</p><formula xml:id="formula_10">ˆ z ← argmax z∈Z z s φ (x) Intermediate decoding 6:</formula><p>Forwardprop and compute L given x, y, andˆzandˆ andˆz 7:</p><p>Backprop and compute θ L andˆzandˆ andˆz L 8:</p><formula xml:id="formula_11">˜ z ← proj P (ˆ z − η ˆ z L) Projection 9: sL ← ˆ z − ˜ z 10:</formula><p>Backprop and compute φ L 11: end procedure considered in this work, arc-factored unlabeled de- pendency parsing and first-order semantic depen- dency parsing.</p><p>In early experiments we observe that for both tasks, projecting with respect to all constraints of their original formulations using a generic quadratic program solver was prohibitively slow. Therefore, we construct relaxed polytopes by con- sidering only a subset of the constraints. <ref type="bibr">3</ref> The projection then decomposes into a series of singly constrained quadratic programs (QP), each of which can be efficiently solved in linear time.</p><p>The two approximate projections discussed here are used in backpropagation only. In the forward pass, we solve the decoding problem using the models' original decoding algorithms.</p><p>Arc-factored unlabeled dependency parsing. For unlabeled dependency trees, we impose [0, 1] constraints and single-headedness constraints. <ref type="bibr">4</ref> Formally, given a length-n input sentence, ex- cluding self-loops, an arc-factored parser consid- ers d = n(n − 1) candidate arcs. Let i→j denote an arc from the ith token to the jth, and σ(i→j) denote its index. We construct the relaxed feasible set by:</p><formula xml:id="formula_12">P DEP =    p ∈ U d i =j p σ(i→j) = 1, ∀j    ,<label>(5)</label></formula><p>i.e., we consider each token j individually, and force single-headedness by constraining the num- ber of arcs incoming to j to sum to 1. Algorithm 2 summarizes the procedure to project onto P DEP .</p><p>Line 3 forms a singly constrained QP, and can be solved in O(n) time <ref type="bibr" target="#b11">(Brucker, 1984)</ref>.</p><p>Algorithm 2 Projection onto the relaxed polytope P DEP for dependency tree structures. Let bold σ(·→j) denote the index set of arcs incoming to j. For a vector v, we use v σ(·→j) to denote vector [v k ] k∈σ(·→j) .</p><p>1: procedure DEPPROJ(ˆ p) 2: for j = 1, 2, . . . , n do 3:</p><formula xml:id="formula_13">˜ z σ(·→j) ← proj ∆ n−2 ˆ p σ(·→j) 4:</formula><p>end for 5: returñ z 6: end procedure First-order semantic dependency parsing. Se- mantic dependency parsing uses labeled bilexical dependencies to represent sentence-level seman- tics ( <ref type="bibr">Oepen et al., 2014</ref><ref type="bibr">Oepen et al., , 2015</ref><ref type="bibr">Oepen et al., , 2016</ref>. Each de- pendency is represented by a labeled directed arc from a head token to a modifier token, where the arc label encodes broadly applicable semantic re- lations. <ref type="figure">Figure 2</ref> diagrams a semantic graph from the DELPH-IN MRS-derived dependencies (DM), together with a syntactic tree.</p><p>We use a state-of-the-art semantic dependency parser ( <ref type="bibr">Peng et al., 2017</ref>) that considers three types of parts: heads, unlabeled arcs, and labeled arcs. Let σ(i → j) denote the index of the arc from i to j with semantic role . In addition to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> constraints, we constrain that the predictions for labeled arcs sum to the prediction of their associ- ated unlabeled arc:</p><formula xml:id="formula_14">P SDP p ∈ U d p σ(i →j) = p σ(i→j) , ∀i = j .<label>(6)</label></formula><p>This ensures that exactly one label is predicted if and only if its arc is present. The projection onto P SDP can be solved similarly to Algorithm 2. We drop the determinism constraint imposed by <ref type="bibr">Peng et al. (2017)</ref> in the backward computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We empirically evaluate our method with two sets of experiments: using syntactic tree structures in semantic dependency parsing, and using semantic dependency graphs in sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Syntactic-then-Semantic Parsing</head><p>In this experiment we consider an intermedi- ate syntactic parsing task, followed by seman- Figure 2: A development instance annotated with both gold DM semantic dependency graph (red arcs on the top), and gold syntactic dependency tree (blue arcs at the bottom). A pretrained syn- tactic parser predicts the same tree as the gold; the semantic parser backpropagates into the interme- diate syntactic parser, and changes the dashed blue arcs into dashed red arcs ( §5).</p><p>tic dependency parsing as the end task. We first briefly review the neural network architectures for the two models ( §4.1.1), and then introduce the datasets ( §4.1.2) and baselines ( §4.1.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Architectures</head><p>Syntactic dependency parser. For intermedi- ate syntactic dependencies, we use the unlabeled arc-factored parser of <ref type="bibr" target="#b37">Kiperwasser and Goldberg (2016)</ref>. It uses bidirectional LSTMs (BiLSTM) to encode the input, followed by a multilayer- perceptron (MLP) to score each potential depen- dency. One notable modification is that we replace their use of Chu-Liu/Edmonds' algorithm (Chu and <ref type="bibr" target="#b13">Liu, 1965;</ref><ref type="bibr" target="#b19">Edmonds, 1967</ref>) with the Eisner algorithm <ref type="bibr" target="#b22">(Eisner, 1996</ref><ref type="bibr" target="#b20">(Eisner, , 2000</ref>), since our dataset is in English and mostly projective. </p><formula xml:id="formula_15">h j = [ − → h j ; ← − h j ]</formula><p>at each position j as the contextu- alized token representations. We then concatenate h j with the representation of its head h HEAD(j) by</p><formula xml:id="formula_16">h j = [h j ; h HEAD(j) ] =   h j ; i =jˆz =jˆ =jˆz σ(i→j) h i   ,<label>(7)</label></formula><p>wherê z ∈ B n(n−1) is a binary encoding of the tree structure predicted by by the intermediate parser.</p><p>We then use h j anywhere h j would have been used in NEURBOPARSER. In backpropagation, we computê z L with an automatic differentiation toolkit (DyNet; <ref type="bibr">Neubig et al., 2017)</ref>.</p><p>We note that this approach can be generalized to convolutional neural networks over graphs ( <ref type="bibr" target="#b49">Mou et al., 2015;</ref><ref type="bibr" target="#b18">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b38">Kipf and Welling, 2017</ref>, inter alia), recurrent neural net- works along paths ( <ref type="bibr" target="#b49">Xu et al., 2015;</ref><ref type="bibr">Roth and Lapata, 2016</ref>, inter alia) or dependency trees <ref type="bibr">(Tai et al., 2015)</ref>. We choose to use concatenations to control the model's complexity, and thus to better understand which parts of the model work.</p><p>We refer the readers to <ref type="bibr" target="#b37">Kiperwasser and Goldberg (2016)</ref> and <ref type="bibr">Peng et al. (2017)</ref> for further de- tails of the parsing models.</p><p>Training procedure. Following previous work, we minimize structured hinge loss ( <ref type="bibr">Tsochantaridis et al., 2004</ref>) for both models. We jointly train both models from scratch, by randomly sampling an in- stance from the union of their training data at each step. In order to isolate the effect of backpropaga- tion, we do not share any parameters between the two models. 5 Implementation details are summa- rized in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Datasets</head><p>• For semantic dependencies, we use the English dataset from <ref type="bibr">SemEval 2015</ref><ref type="bibr">Task 18 (Oepen et al., 2015</ref>). Among the three for- malisms provided by the shared task, we con- sider DELPH-IN MRS-derived dependencies (DM) and Prague Semantic Dependencies (PSD). <ref type="bibr">6</ref> It includes §00-19 of the WSJ cor- pus as training data, §20 and §21 for devel- opment and in-domain test data, resulting in a 33,961/1,692/1,410 train/dev./test split, and <ref type="bibr">5</ref> Parameter sharing has proved successful in many related tasks <ref type="bibr" target="#b15">(Collobert and Weston, 2008;</ref><ref type="bibr">Søgaard and Goldberg, 2016;</ref><ref type="bibr" target="#b1">Ammar et al., 2016;</ref><ref type="bibr">Swayamdipta et al., 2016</ref><ref type="bibr">Swayamdipta et al., , 2017</ref>, and could be easily combined with our approach. <ref type="bibr">6</ref> We drop the third (PAS) because its structure is highly predictable from parts-of-speech, making it less interesting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Baselines</head><p>We compare to the following baselines:</p><p>• A pipelined system (PIPELINE). The pre- trained parser achieves 92.9 test unlabeled at- tachment score (UAS). 8</p><p>• Structured attention networks (SA; <ref type="bibr" target="#b36">Kim et al., 2017)</ref>. We use the inside-outside algo- rithm <ref type="bibr" target="#b3">(Baker, 1979)</ref> to populate z with arcs' marginal probabilities, use log-loss as the ob- jective in training the intermediate parser.</p><p>• The straight-through estimator <ref type="bibr">(STE;</ref><ref type="bibr" target="#b32">Hinton, 2012)</ref>, introduced in §2.2. <ref type="table" target="#tab_3">Table 1</ref> compares the semantic dependency pars- ing performance of SPIGOT to all five baselines. FREDA3 ( <ref type="bibr">Peng et al., 2017</ref>) is a state-of-the-art variant of NEURBOPARSER that is trained using multitask learning to jointly predict three different semantic dependency graph formalisms. Like the basic NEURBOPARSER model that we build from, FREDA3 does not use any syntax. Strong DM per- formance is achieved in a more recent work by us- ing joint learning and an ensemble ( <ref type="bibr">Peng et al., 2018)</ref>, which is beyond fair comparisons to the models discussed here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Empirical Results</head><p>We found that using syntactic information improves semantic parsing performance: using pipelined syntactic head features brings 0.5- 1.4% absolute labeled F 1 improvement to NEUR- BOPARSER.</p><p>Such improvements are smaller compared to previous works, where depen- dency path and syntactic relation features are in- cluded ( <ref type="bibr" target="#b0">Almeida and Martins, 2015;</ref><ref type="bibr">Ribeyre et al., 2015;</ref><ref type="bibr">Zhang et al., 2016)</ref>, indicating the potential to get better performance by using more syntactic information, which we leave to future work.</p><p>Both STE and SPIGOT use hard syntactic fea- tures. By allowing backpropation into the inter- mediate syntactic parser, they both consistently outperform PIPELINE. On the other hand, when marginal syntactic tree structures are used, SA outperforms PIPELINE only on the out-of-domain PSD test set, and improvements under other cases are not observed.</p><p>Compared to STE, SPIGOT outperforms STE on DM by more than 0.3% absolute labeled F 1 , both in-domain and out-of-domain. For PSD, SPIGOT achieves similar performance to STE on in-domain test set, but has a 0.5% absolute labeled F 1 im- provement on out-of-domain data, where syntactic parsing is less accurate.</p><p>tecture achieves 93.5 UAS when trained and evaluated with the standard split, close to the results reported by <ref type="bibr" target="#b37">Kiperwasser and Goldberg (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semantic Dependencies for Sentiment Classification</head><p>Our second experiment uses semantic dependency graphs to improve sentiment classification perfor- mance. We are not aware of any efficient algo- rithm that solves marginal inference for seman- tic dependency graphs under determinism con- straints, so we do not include a comparison to SA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Architectures</head><p>Here we use NEURBOPARSER as the intermediate model, as described in §4.1.1, but with no syntac- tic enhancements.</p><p>Sentiment classifier. We first introduce a base- line that does not use any structural information. It learns a one-layer BiLSTM to encode the in- put sentence, and then feeds the sum of all hidden states into a two-layer ReLU-MLP.</p><p>To use semantic dependency features, we con- catenate a word's BiLSTM-encoded representa- tion to the averaged representation of its heads, to- gether with the corresponding semantic roles, sim- ilarly to that in Equation 7. <ref type="bibr">9</ref> Then the concatena- tion is fed into an affine transformation followed by a ReLU activation. The rest of the model is kept the same as the BiLSTM baseline.</p><p>Training procedure. We use structured hinge loss to train the semantic dependency parser, and log-loss for the sentiment classifier. Due to the dis- crepancy in the training data size of the two tasks (33K vs. 7K), we pre-train a semantic dependency parser, and then adopt joint training together with the classifier. In the joint training stage, we ran- domly sample 20% of the semantic dependency training instances each epoch. Implementations are detailed in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Datasets</head><p>For semantic dependencies, we use the DM dataset introduced in §4.1.2.</p><p>We consider a binary classification task using the Stanford Sentiment Treebank ( <ref type="bibr">Socher et al., 2013)</ref>. It consists of roughly 10K movie review sentences from Rotten Tomatoes. The full dataset includes a rating on a scale from 1 to 5 for each constituent (including the full sentences), resulting in more than 200K instances. Following previous work <ref type="bibr" target="#b33">(Iyyer et al., 2015)</ref>, we only use full-sentence  instances, with neutral instances excluded (3s) and the remaining four rating levels converted to bi- nary "positive" or "negative" labels. This results in a 6,920/872/1,821 train/dev./test split. <ref type="table" target="#tab_5">Table 2</ref> compares our SPIGOT method to three baselines. Pipelined semantic dependency predic- tions brings 0.9% absolute improvement in clas- sification accuracy, and SPIGOT outperforms all baselines. In this task STE achieves slightly worse performance than a fixed pre-trained PIPELINE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Empirical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We examine here how the intermediate model is affected by the end-task training signal. Is the end- task signal able to "overrule" intermediate predic- tions? We use the syntactic-then-semantic parsing model ( §4.1) as a case study. <ref type="table">Table 3</ref> compares a pipelined system to one jointly trained using SPIGOT. We consider the development set in- stances where both syntactic and semantic an- notations are available, and partition them based on whether the two systems' syntactic predic- tions agree (SAME), or not (DIFF  <ref type="table">Table 3</ref>: Syntactic parsing performance (in unla- beled attachment score, UAS) and DM semantic parsing performance (in labeled F 1 ) on different groups of the development data. Both systems predict the same syntactic parses for instances from SAME, and they disagree on instances from DIFF ( §5).</p><p>tree, we consider three cases: (a) h is a head of m in the semantic graph; (b) h is a modifier of m in the semantic graph; (c) h is the modifier of m in the semantic graph. The first two reflect mod- ifications to the syntactic parse that rearrange se- mantically linked words to be neighbors. Under (c), the semantic parser removes a syntactic depen- dency that reverses the direction of a semantic de- pendency. These cases account for 17.6%, 10.9%, and 12.8%, respectively (41.2% combined) of the total changes. Making these changes, of course, is complicated, since they often require other modi- fications to maintain well-formedness of the tree. <ref type="figure">Figure 2</ref> gives an example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Joint learning in NLP pipelines. To avoid cas- cading errors, much effort has been devoted to joint decoding in NLP pipelines <ref type="bibr" target="#b29">(Habash and Rambow, 2005;</ref><ref type="bibr" target="#b14">Cohen and Smith, 2007;</ref><ref type="bibr" target="#b26">Goldberg and Tsarfaty, 2008;</ref><ref type="bibr" target="#b40">Lewis et al., 2015;</ref><ref type="bibr">Zhang et al., 2015, inter alia)</ref>. However, joint inference can sometimes be prohibitively expensive. Recent advances in representation learning facilitate ex- ploration in the joint learning of multiple tasks by sharing parameters <ref type="bibr" target="#b15">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b9">Blitzer et al., 2006;</ref><ref type="bibr" target="#b23">Finkel and Manning, 2010;</ref><ref type="bibr">Zhang and Weiss, 2016;</ref><ref type="bibr" target="#b30">Hashimoto et al., 2017</ref>, inter alia).</p><p>Differentiable optimization. <ref type="bibr" target="#b28">Gould et al. (2016)</ref> review the generic approaches to differ- entiation in bi-level optimization <ref type="bibr" target="#b4">(Bard, 2010;</ref><ref type="bibr" target="#b39">Kunisch and Pock, 2013)</ref>. <ref type="bibr" target="#b2">Amos and Kolter (2017)</ref> extend their efforts to a class of subdif- ferentiable quadratic programs. However, they both require that the intermediate objective has an invertible Hessian, limiting their application in NLP. In another line of work, the steps of a gradient-based optimization procedure are un- rolled into a single computation graph <ref type="bibr">(Stoyanov et al., 2011;</ref><ref type="bibr" target="#b17">Domke, 2012;</ref><ref type="bibr" target="#b27">Goodfellow et al., 2013;</ref><ref type="bibr" target="#b10">Brakel et al., 2013)</ref>. This comes at a high computational cost due to the second-order derivative computation during backpropagation. Moreover, constrained optimization problems (like many NLP problems) often require projec- tion steps within the procedure, which can be difficult to differentiate through <ref type="bibr" target="#b5">(Belanger and McCallum, 2016;</ref><ref type="bibr" target="#b6">Belanger et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented SPIGOT, a novel approach to back- propagating through neural network architectures that include discrete structured decisions in in- termediate layers. SPIGOT devises a proxy for the gradients with respect to argmax's inputs, employing a projection that aims to respect the constraints in the intermediate task. We empiri- cally evaluate our method with two architectures: a semantic parser with an intermediate syntactic parser, and a sentiment classifier with an inter- mediate semantic parser. Experiments show that SPIGOT achieves stronger performance than base- lines under both settings, and outperforms state- of-the-art systems on semantic dependency pars- ing. Our implementation is available at https: //github.com/Noahs-ARK/SPIGOT.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Semantic dependency parsing perfor-
mance in both unlabeled (UF ) and labeled (LF ) 
F 1 scores. Bold font indicates the best perfor-
mance. Peng et al. (2017) does not report UF . 

1,849 out-of-domain test instances from the 
Brown corpus. 7 
• For syntactic dependencies, we use the Stan-
ford Dependency (de Marneffe and Manning, 
2008) conversion of the the Penn Treebank 
WSJ portion (Marcus et al., 1993). To avoid 
data leak, we depart from standard split and 
use  §20 and  §21 as development and test data, 
and the remaining sections as training data. 
The number of training/dev./test instances is 
40,265/2,012/1,671. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test accuracy of sentiment classification 
on Stanford Sentiment Treebank. Bold font indi-
cates the best performance. 

</table></figure>

			<note place="foot" n="1"> Note that this property follows from P&apos;s convexity, and we do not assume the convexity of L.</note>

			<note place="foot" n="3"> Solving the Projections In this section we discuss how to compute approximate projections for the two intermediate tasks 2 Nothing prohibits tying across pre-argmax parameters and post-argmax parameters; this separation is notationally convenient but not at all necessary.</note>

			<note place="foot" n="3"> A parallel work introduces an active-set algorithm to solve the same class of quadratic programs (Niculae et al., 2018). It might be an efficient approach to solve the projections in Equation 4b, which we leave to future work. 4 It requires O(n 2 ) auxiliary variables and O(n 3 ) additional constraints to ensure well-formed tree structures (Martins et al., 2013).</note>

			<note place="foot" n="7"> The organizers remove, e.g., instances with cyclic graphs, and thus only a subset of the WSJ corpus is included. See Oepen et al. (2015) for details. 8 Note that this number is not comparable to the parsing literature due to the different split. As a sanity check, we found in preliminary experiments that the same parser archi</note>

			<note place="foot" n="9"> In a well-formed semantic dependency graph, a token may have multiple heads. Therefore we use average instead of the sum in Equation 7.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the ARK, Julian Michael, Minjoon Seo, Eunsol Choi, and Maxwell Forbes for their helpful comments on an earlier version of this work, and the anonymous reviewers for their valuable feed-back. This work was supported in part by NSF grant IIS-1562364.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lisbon: Evaluating TurboSemanticParser on multiple languages and out-of-domain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Mariana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SemEval</title>
		<meeting>of SemEval</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Many languages, one parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="431" to="444" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OptNet: Differentiable optimization as a layer in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Trainable grammars for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Communication Papers for the 97th Meeting of the Acoustical Society of</title>
		<meeting><address><addrLine>America</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Practical Bilevel Optimization: Algorithms and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured prediction energy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end learning for structured prediction energy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Introduction to Linear Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Training energy-based models for time-series imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philémon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Stroobandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2771" to="2797" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An O(n) algorithm for quadratic knapsack problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Brucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="163" to="166" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised learning of task-specific tree structures with tree-LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02786</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint morphological and syntactic disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Stanford typed dependencies manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generic methods for optimization-based modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Edmonds</surname></persName>
		</author>
		<title level="m">Optimum branchings. Journal of Research of the National Bureau of Standards</title>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bilexical grammars and their cubic-time parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Probabilistic and Other Parsing Technologies</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="29" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inside-outside and forwardbackward algorithms are just backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP Workshop on Structured Prediction for NLP</title>
		<meeting>the EMNLP Workshop on Structured Prediction for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical joint learning: Improving joint parsing and named entity recognition with non-jointly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The sum-product theorem: A foundation for learning tractable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abram</forename><forename type="middle">L</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning as a mixed convex-combinatorial optimization problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abram</forename><forename type="middle">L</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A single generative model for joint morphological segmentation and syntactic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-prediction deep Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On differentiating parameterized argmin and argmax problems with application to bi-level optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edison</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05447</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A joint many-task model: Growing a neural network for multiple NLP tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and whats next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning. Coursera video lectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with Gumbel-Softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural discourse structure for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Structured attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional LSTM feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A bilevel optimization approach for parameter learning in variational models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Kunisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="938" to="983" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint A* CCG parsing and semantic role labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning structured text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="63" to="75" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Linear and Nonlinear Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinyu</forename><surname>Luenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order non-projective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">AD3: Alternating directions dual decomposition for map inference in graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M Q</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Aguiar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="495" to="545" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Polyhedral outer approximations with application to natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03676</idno>
		<title level="m">Differentiable dynamic programming for structured prediction and attention</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Discriminative neural sentence modeling by tree-based convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
