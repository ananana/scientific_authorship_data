<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Continuous Word Embedding with Metadata for Question Retrieval in Community Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="250" to="259"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Community question answering (cQA) has become an important issue due to the popularity of cQA archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in cQA archives aims to find the existing questions that are semantically equivalent or relevant to the queried questions. However, the lexical gap problem brings about new challenge for question retrieval in cQA. In this paper, we propose to learn continuous word embeddings with meta-data of category information within cQA pages for question retrieval. To deal with the variable size of word embedding vectors , we employ the framework of fisher kernel to aggregated them into the fixed-length vectors. Experimental results on large-scale real world cQA data set show that our approach can significantly out-perform state-of-the-art translation models and topic-based models for question retrieval in cQA.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years, a large amount of user- generated content have become an important in- formation resource on the web. These include the traditional Frequently Asked Questions (FAQ) archives and the emerging community question answering (cQA) services, such as Yahoo! An- swers 1 , Live QnA 2 , and Baidu Zhidao 3 . The con- tent in these web sites is usually organized as ques- tions and lists of answers associated with meta- data like user chosen categories to questions and askers' awards to the best answers. This data made cQA archives valuable resources for various tasks like question-answering ( <ref type="bibr" target="#b9">Jeon et al., 2005;</ref><ref type="bibr" target="#b23">Xue et al., 2008</ref>) and knowledge mining ( <ref type="bibr" target="#b0">Adamic et al., 2008)</ref>, etc.</p><p>One fundamental task for reusing content in cQA is finding similar questions for queried ques- tions, as questions are the keys to accessing the knowledge in cQA. Then the best answers of these similar questions will be used to answer the queried questions. Many studies have been done along this line ( <ref type="bibr" target="#b9">Jeon et al., 2005;</ref><ref type="bibr" target="#b23">Xue et al., 2008;</ref><ref type="bibr" target="#b7">Duan et al., 2008;</ref><ref type="bibr" target="#b11">Lee et al., 2008;</ref><ref type="bibr" target="#b2">Bernhard and Gurevych, 2009;</ref><ref type="bibr" target="#b4">Cao et al., 2010;</ref><ref type="bibr" target="#b17">Singh, 2012;</ref><ref type="bibr" target="#b26">Zhang et al., 2014a</ref>). One big challenge for question retrieval in cQA is the lexi- cal gap between the queried questions and the ex- isting questions in the archives. Lexical gap means that the queried questions may contain words that are different from, but related to, the words in the existing questions. For example shown in ( <ref type="bibr" target="#b26">Zhang et al., 2014a</ref>), we find that for a queried question "how do I get knots out of my cats fur?", there are good answers under an existing question "how can I remove a tangle in my cat's fur?" in Yahoo! Answers. Although the two questions share few words in common, they have very similar mean- ings, it is hard for traditional retrieval models (e.g., <ref type="bibr">BM25 (Robertson et al., 1994)</ref>) to determine their similarity. This lexical gap has become a major barricade preventing traditional IR models (e.g., BM25) from retrieving similar questions in cQA.</p><p>To address the lexical gap problem in cQA, pre- vious work in the literature can be divided into two groups. The first group is the translation models, which leverage the question-answer pairs to learn the semantically related words to improve tradi- tional IR models ( <ref type="bibr" target="#b9">Jeon et al., 2005;</ref><ref type="bibr" target="#b23">Xue et al., 2008;</ref>). The basic assumption is that question-answer pairs are "parallel texts" and relationship of words (or phrases) can be estab- lished through word-to-word (or phrase-to-phrase) translation probabilities ( <ref type="bibr" target="#b9">Jeon et al., 2005;</ref><ref type="bibr" target="#b23">Xue et al., 2008;</ref>). Experimental results show that translation models obtain state- of-the-art performance for question retrieval in cQA. However, questions and answers are far from "parallel" in practice, questions and answers are highly asymmetric on the information they con- tain ( <ref type="bibr" target="#b26">Zhang et al., 2014a</ref>). The second group is the topic-based models <ref type="bibr" target="#b10">Ji et al., 2012)</ref>, which learn the latent topics aligned across the question-answer pairs to alleviate the lexical gap problem, with the assumption that a question and its paired answers share the same topic distri- bution. However, questions and answers are het- erogeneous in many aspects, they do not share the same topic distribution in practice. Inspired by the recent success of continuous space word representations in capturing the se- mantic similarities in various natural language processing tasks, we propose to incorporate an embedding of words in a continuous space for question representations. Due to the ability of word embeddings, we firstly transform words in a question into continuous vector representations by looking up tables. These word embeddings are learned in advance using a continuous skip-gram model ( <ref type="bibr" target="#b13">Mikolov et al., 2013)</ref>, or other continuous word representation learning methods. Once the words are embedded in a continuous space, one can view a question as a Bag-of-Embedded-Words (BoEW). Then, the variable-cardinality BoEW will be aggregated into a fixed-length vector by using the Fisher kernel (FK) framework of <ref type="bibr" target="#b6">(Clinchant and Perronnin, 2013;</ref><ref type="bibr" target="#b16">Sanchez et al., 2013)</ref>. Through the two steps, the proposed approach can map a question into a length invariable compact vector, which can be efficiently and effectively for large-scale question retrieval task in cQA.</p><p>We test the proposed approach on large-scale Yahoo! Answers data and Baidu Zhidao data. Ya- hoo! Answers and Baidu Zhidao represent the largest and most popular cQA archives in English and Chinese, respectively. We conduct both quan- titative and qualitative evaluations. Experimental results show that our approach can significantly outperform state-of-the-art translation models and topic-based models for question retrieval in cQA.</p><p>Our contribution in this paper are three-fold: (1) we represent a question as a bag-of-embedded- words (BoEW) in a continuous space; (2) we in- troduce a novel method to aggregate the variable- cardinality BoEW into a fixed-length vector by us- ing the FK. The FK is just one possible way to sub- sequently transform this bag representation into a fixed-length vector which is more amenable to large-scale processing; (3) an empirical verifica- tion of the efficacy of the proposed framework on large-scale English and Chinese cQA data.</p><p>The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 describes our proposed framework for question re- trieval. Section 4 reports the experimental results. Finally, we conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Question Retrieval in cQA</head><p>Significant research efforts have been conducted over the years in attempt to improve question re- trieval in cQA ( <ref type="bibr" target="#b9">Jeon et al., 2005;</ref><ref type="bibr" target="#b23">Xue et al., 2008;</ref><ref type="bibr" target="#b11">Lee et al., 2008;</ref><ref type="bibr" target="#b7">Duan et al., 2008;</ref><ref type="bibr" target="#b2">Bernhard and Gurevych, 2009;</ref><ref type="bibr" target="#b4">Cao et al., 2010;</ref><ref type="bibr" target="#b17">Singh, 2012;</ref><ref type="bibr" target="#b26">Zhang et al., 2014a</ref>). Most of these works focus on finding similar questions for the user queried questions. The major chal- lenge for question retrieval in cQA is the lexical gap problem. <ref type="bibr" target="#b9">Jeon et al. (2005)</ref> proposed a word- based translation model for automatically fixing the lexical gap problem. <ref type="bibr" target="#b23">Xue et al. (2008)</ref> pro- posed a word-based translation language model for question retrieval. <ref type="bibr" target="#b11">Lee et al. (2008)</ref> tried to further improve the translation probabilities based on question-answer pairs by selecting the most im- portant terms to build compact translation mod- els. <ref type="bibr" target="#b2">Bernhard and Gurevych (2009)</ref> proposed to use as a parallel training data set the definitions and glosses provided for the same term by differ- ent lexical semantic resources. In order to improve the word-based translation model with some con- textual information, <ref type="bibr" target="#b14">Riezler et al. (2007)</ref> and  proposed a phrase-based translation model for question and answer retrieval. The phrase-based translation model can capture some contextual information in modeling the transla- tion of phrases as a whole, thus the more accurate translations can better improve the retrieval per- formance. Singh (2012) addressed the lexical gap issues by extending the lexical word-based trans- lation model to incorporate semantic information (entities).</p><p>In contrast to the works described above that as- sume question-answer pairs are "parallel text", our paper deals with the lexical gap by learning con-tinuous word embeddings in capturing the simi- larities without any assumptions, which is much more reasonable in practice.</p><p>Besides, some other studies model the semantic relationship between questions and answers with deep linguistic analysis ( <ref type="bibr" target="#b7">Duan et al., 2008;</ref><ref type="bibr" target="#b20">Wang et al., 2009;</ref><ref type="bibr" target="#b21">Wang et al., 2010;</ref><ref type="bibr" target="#b10">Ji et al., 2012;</ref><ref type="bibr" target="#b26">Zhang et al., 2014a</ref>) or a learning to rank strat- egy ( <ref type="bibr" target="#b18">Surdeanu et al., 2008;</ref><ref type="bibr" target="#b5">Carmel et al., 2014</ref>). Recently, <ref type="bibr" target="#b4">Cao et al. (2010)</ref> and <ref type="bibr" target="#b29">Zhou et al. (2013)</ref> exploited the category metadata within cQA pages to further improve the performance. On the con- trary, we focus on the representation learning for questions, with a different solution with those pre- vious works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Embedding Learning</head><p>Representation of words as continuous vectors has attracted increasing attention in the area of nat- ural language processing (NLP). Recently, a se- ries of works applied deep learning techniques to learn high-quality word representations. <ref type="bibr" target="#b1">Bengio et al. (2003)</ref> proposed a probabilistic neural net- work language model (NNLM) for word represen- tations. Furthermore, <ref type="bibr" target="#b13">Mikolov et al. (2013)</ref> pro- posed efficient neural network models for learn- ing word representations, including the continu- ous skip-gram model and the continuous bag-of- word model (CBOW), both of which are unsu- pervised models learned from large-scale text cor- pora. Besides, there are also a large number of works addressing the task of learning word repre- sentations ( <ref type="bibr" target="#b8">Huang et al., 2012;</ref><ref type="bibr" target="#b12">Maas et al., 2011;</ref><ref type="bibr" target="#b19">Turian et al., 2010)</ref>.</p><p>Nevertheless, since most the existing works learned word representations mainly based on the word co-occurrence information, the obtained word embeddings cannot capture the relationship between two syntactically or semantically similar words if either of them yields very little context in- formation. On the other hand, even though amount of context could be noisy or biased such that they cannot reflect the inherent relationship between words and further mislead the training process. Most recently, <ref type="bibr" target="#b24">Yu et al. (2014)</ref> used semantic prior knowledge to improve word representations. <ref type="bibr" target="#b22">Xu et al. (2014)</ref> used the knowledge graph to advance the learning of word embeddings. In contrast to all the aforementioned works, in this paper, we present a general method to leverage the metadata of category information within cQA pages to fur- ther improve the word embedding representations. To our knowledge, it is the first work to learn word embeddings with metadata on cQA data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>In this Section, we describe the proposed ap- proach: learning continuous word embedding with metadata for question retrieval in cQA. The pro- posed framework consists of two steps: (1) word embedding learning step: given a cQA data collec- tion, questions are treated as the basic units. For each word in a question, we firstly transform it to a continuous word vector through the looking up ta- bles. Once the word embeddings are learned, each question is represented by a variable-cardinality word embedding vector (also called BoEW); (2) fisher vector generation step: which uses a genera- tive model in the FK framework to generate fisher vectors (FVs) by aggregating the BoEWs for all the questions. Question retrieval can be performed through calculating the similarity between the FVs of a queried question and an existing question in the archive.</p><p>From the framework, we can see that although the word embedding learning computations and generative model estimation are time consuming, they can run only once in advance. Meanwhile, the computational requirements of FV generation and similarity calculation are limited. Hence, the pro- posed framework can efficiently achieve the large- scale question retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Embedding Learning</head><p>In this paper, we consider a context-aware pre- dicting model, more specifically, the Skip-gram model ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>) for learning word embeddings, since it is much more efficient as well as memory-saving than other approaches. <ref type="bibr">4</ref> Skip- gram is recently proposed for learning word rep- resentations using a neural network model, whose underlying idea is that similar words should have similar contexts. In the Skip-gram model (see <ref type="figure" target="#fig_0">Fig- ure 1)</ref>, a sliding window is employed on the input text stream to generate the training data, and l in- dicates the context window size to be 2l + 1. In each slide window, the model aims to use the cen- tral word w k as input to predict the context words. Let M d×N denote the learned embedding matrix, where N is the vocabulary size and d is the di- mension of word embeddings. Each column of M represents the embedding of a word. Let w k is first mapped to its embedding e w k by selecting the cor- responding column vector of M . The probability of its context word w k+j is then computed using a log-linear softmax function:</p><formula xml:id="formula_0">p(w k+j |w k ; θ) = exp(e T w k+j e w k ) N w=1 exp(e T w e w k )<label>(1)</label></formula><p>where θ are the parameters we should learned, k = 1 · · · d, and j ∈ [−l, l]. Then, the log-likelihood over the entire training data can be computed as:</p><formula xml:id="formula_1">J(θ) = (w k ,w k+j ) logp(w k+j |w k ; θ)<label>(2)</label></formula><p>To calculate the prediction errors for back prop- agation, we need to compute the derivative of p(w k+j |w k ; θ), whose computation cost is pro- portional to the vocabulary size N . As N is of- ten very large, it is difficult to directly compute the derivative. To deal this problem, <ref type="bibr" target="#b13">Mikolov et al. (2013)</ref> proposed a simple negative sam- pling method, which generates r noise samples for each input word to estimate the target word, in which r is a very small number compared with N . Therefore, the training time yields linear scale to the number of noise samples and it becomes independent of the vocabulary size. Suppose the frequency of word w is u(w), then the proba- bility of sampling w is usually set to p(w) ∝ u(w) 3/4 (Mikolov et al., 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Metadata Powered Model</head><p>After briefing the skip-gram model, we introduce how we equip it with the metadata information. In cQA sites, there are several metadata, such as "category","voting" and so on. In this paper, we only consider the metadata of category informa- tion for word embedding learning. All questions in cQA are usually organized into a hierarchy of categories. When an user asks a question, the user typically required to choose a category label for the question from a predefined hierarchy of cate- gories ( <ref type="bibr" target="#b4">Cao et al., 2010;</ref><ref type="bibr" target="#b29">Zhou et al., 2013)</ref>. Pre- vious work in the literature has demonstrated the effectiveness of the category information for ques- tion retrieval <ref type="bibr" target="#b4">(Cao et al., 2010;</ref><ref type="bibr" target="#b29">Zhou et al., 2013</ref></p><note type="other">). On the contrary, we argue that the category infor- mation benefits the word embedding learning in this work. The basic idea is that category informa- tion encodes the attributes or properties of words, from which we can group similar words according to their categories. Here, a word's category is as- signed based on the questions it appeared in. For example, a question "What are the security issues with java?" is under the category of "Computers &amp; Internet → Security", we simply put the cate- gory of a word java as "Computers &amp; Internet → Security". Then, we may require the representa- tions of words that belong to the same category to be close to each other.</note><p>Let s(w k , w i ) be the similarity score between w k and w i . Under the above assumption, we use the following heuristic to constrain the simi- lar scores:</p><formula xml:id="formula_2">s(w k , w i ) = 1 if c(w k ) = c(w i ) 0 otherwise<label>(3)</label></formula><p>where c(w k ) denotes the category of w k . If the central word w k shares the same category with the word w i , their similarity score will become 1, oth- erwise, we set to 0. Then we encode the category information using a regularization function E c :</p><formula xml:id="formula_3">E c = N k=1 N i=1 s(w k , w i )d(w k , w i )<label>(4)</label></formula><p>where d(w k , w i ) is the distance for the words in the embedding space and s(w k , w i ) serves as a weighting function. Again, for simplicity, we de- fine d(w k , w i ) as the Euclidean distance between w k and w i . We combine the skip-gram objective function and the regularization function derived from the metadata of category information, we get the fol- lowing combined objective J c that incorporates category information into the word representation learning process:</p><formula xml:id="formula_4">J c = J(θ) + βE c<label>(5)</label></formula><p>where β is the combination coefficient. Our goal is to maximize the combined objective J c , which can be optimized using back propagation neural networks. We call this model as metadata powered model (see <ref type="figure" target="#fig_1">Figure 2</ref>), and denote it by M-NET for easy of reference.</p><p>In the implementation, we optimize the regu- larization function derived from the metadata of category information along with the training pro- cess of the skip-gram model. During the pro- cedure of learning word representations from the context words in the sliding window, if the central word w k hits the category information, the cor- responding optimization process of the metadata powered regularization function will be activated. Therefore, we maximize the weighted Euclidean distance between the representation of the central word and that of its similar words according to the objective function in Equation (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fisher Vector Generation</head><p>Once the word embeddings are learned, ques- tions can be represented by variable length sets of word embedding vectors, which can be viewed as BoEWs. Semantic level similarities between queried questions and the existing questions rep- resented by BoEWs can be captured more accu- rately than previous bag-of-words (BoW) meth- ods. However, since BoEWs are variable-size sets of word embeddings and most of the index meth- ods in information retrieval field are not suitable for this kinds of issues, BoEWs cannot be directly used for large-scale question retrieval task.</p><p>Given a cQA data collection Q = {q i , 1 ≤ i ≤ |Q|}, where q i is the ith question and |Q| is the number of questions in the data collection. The ith question q i is composed by a sequence of words w i = {w ij , 1 ≤ j ≤ N i }, where N i denotes the length of q i . Through looking up table (word em- bedding matrix) of M , the ith question q i can be represented by E w i = {e w ij , 1 ≤ j ≤ N i }, where e w ij is the word embedding of w ij . According to the framework of FK <ref type="bibr" target="#b6">(Clinchant and Perronnin, 2013;</ref><ref type="bibr" target="#b16">Sanchez et al., 2013;</ref><ref type="bibr" target="#b27">Zhang et al., 2014b</ref>), questions are modeled by a probability density function. In this work, we use Gaussian mixture model (GMM) to do it. We assume that the con- tinuous word embedding E w i for question q i have been generated by a "universal" (e.g., question- independent) probability density function (pdf). As is a common practice, we choose this pdf to be a GMM since any continuous distribution can be approximated with arbitrary precision by a mix- ture of Gaussian. In what follows, the pdf is de- noted u λ where λ = {θ i , µ i , Σ i , i = 1 · · · K} is the set of parameters of the GMM. θ i , µ i and Σ i denote respectively the mixture weight, mean vector and covariance matrix of Gaussian i. For computational reasons, we assume that the covari- ance matrices are diagonal and denote σ 2 i the vari- ance vector of Gaussian i, e.g., σ 2 i = diag( i ). In real applications, the GMM is estimated of- fline with a set of continuous word embeddings extracted from a representative set of questions. The parameters λ are estimated through the op- timization of a Maximum Likelihood (ML) crite- rion using the Expectation-Maximization (EM) al- gorithm. In the following, we follow the notations used in ( <ref type="bibr" target="#b16">Sanchez et al., 2013)</ref>.</p><p>Given u λ , one can characterize the question q i using the following score function:</p><formula xml:id="formula_5">G q i λ = N i λ logu λ (q i )<label>(6)</label></formula><p>where G q i λ is a vector whose size depends only on the number of parameters in λ. Assuming that the word embedding e w ij is iid (a simplifying assump- tion), we get:</p><formula xml:id="formula_6">G q i λ = N i j=1 λ logu λ (e w ij )<label>(7)</label></formula><p>Following the literature ( <ref type="bibr" target="#b16">Sanchez et al., 2013)</ref>, we propose to measure the similarity between two questions q i and q j using the FK:</p><formula xml:id="formula_7">K(q i , q j ) = G q T i λ F −1 λ G q j λ (8)</formula><p>where F λ is the Fisher Information Matrix (FIM) of u λ :</p><formula xml:id="formula_8">F λ = E q i ∼u λ G q i λ G q T i λ (9)</formula><p>Since F λ is symmetric and positive definite, F −1 λ can be transformed to L T λ L λ based on the Cholesky decomposition. Hence, K F K (q i , q j ) can rewritten as follows:</p><formula xml:id="formula_9">K F K (q i , q j ) = G q T i λ G q j λ (10)</formula><p>where</p><formula xml:id="formula_10">G q i λ = L λ G q i λ = L λ λ logu λ (q i )<label>(11)</label></formula><p>In ( <ref type="bibr" target="#b16">Sanchez et al., 2013)</ref>, G q i λ refers to as the Fisher Vector (FV) of q i . The dot product between FVs can be used to calculate the semantic simi- larities. Based on the specific probability density function, GMM, FV of q i is respect to the mean µ and standard deviation σ of all the mixed Gaussian distributions. Let γ j (k) be the soft assignment of the jth word embedding e w ij in q i to Guassian k (u k ):</p><formula xml:id="formula_11">γ j (k) = p(k|e w ij ) θ i u k (e w ij ) K j=1 θ k u k (e w ij )<label>(12)</label></formula><p>Mathematical derivations lead to:</p><formula xml:id="formula_12">G q i µ,k = 1 N i √ θ i N i j=1 γ j (k) e w ij − µ k σ k (13) G q i σ,k = 1 N i √ 2θ i N i j=1 γ j (k) (e w ij − µ k ) 2 σ 2 k − 1</formula><p>The division by the vector σ k should be under- stood as a term-by-term operation. The final gradi- ent vector G q i λ is the concatenation of the G q i µ,k and G q i σ,k vectors for k = 1 · · · K. Let d denote the di- mensionality of the continuous word embeddings and K be the number of Gaussians. The final fisher vector G q i λ is therefore 2Kd-dimensional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present the experiments to eval- uate the performance of the proposed method for question retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Set and Evaluation Metrics</head><p>We collect the data sets from Yahoo! Answers and Baidu Zhidao. Yahoo! Answers and Baidu Zhidao represent the largest and the most popu- lar cQA archives in English and Chinese, respec- tively. More specifically, we utilized the resolved questions at Yahoo! Answers and Baidu Zhidao. The questions include 10 million items from Ya- hoo! Answers and 8 million items from Baidu Zhidao (also called retrieval data). Each resolved question consists of three fields: "title", "descrip- tion" and "answers", as well as some metadata, such as "category". For question retrieval, we use only the "title" field and "category" metadata. <ref type="table" target="#tab_0">It   #queries #candidate #relevant  Yahoo data  1,000  13,000  2,671  Baidu data  1,000  8,000  2,104   Table 1</ref>: Statistics on the manually labeled data.</p><p>is assumed that the titles of questions already pro- vide enough semantic information for understand- ing users' information needs ( <ref type="bibr" target="#b7">Duan et al., 2008)</ref>. We develop two test sets, one for "Yahoo data", and the other for "Baidu data". In order to create the test sets, we collect some extra questions that have been posted more recently than the retrieval data, and randomly sample 1, 000 questions for Yahoo! Answers and Baidu Zhidao, respectively. We take those questions as queries. All questions are lowercased and stemmed. Stopwords 5 are also removed.</p><p>We separately index all data from Yahoo! An- swers and Baidu Zhidao using an open source Lucene with the BM25 scoring function 6 . For each query from Yahoo! Answers and Baidu Zhi- dao, we retrieve the several candidate questions from the corresponding indexed data by using the BM25 ranking algorithm in Lucene. On average, each query from Yahoo! Answers has 13 candi- date questions and the average number of candi- date questions for Baidu Zhidao is 8.</p><p>We recruit students to label the relevance of the candidate questions regarding to the queries. Specifically, for each type of language, we let three native students. Given a candidate question, a student is asked to label it with "relevant" or "ir- relevant". If a candidate question is considered semantically similar to the query, the student will label it as "relevant"; otherwise, the student will label it as "irrelevant". As a result, each candi- date question gets three labels and the majority of the label is taken as the final decision for a query- candidate pair. We randomly split each of the two labeled data sets into a validation set and a test set with a ration 1 : 3. The validation set is used for tuning parameters of different models, while the test set is used for evaluating how well the models ranked relevant candidates in contrast to irrelevant candidates. <ref type="table">Table 1</ref> presents the manually labeled data.</p><p>Please note that rather than evaluate both re- trieval and ranking capability of different meth-ods like the existing work <ref type="bibr" target="#b4">(Cao et al., 2010)</ref>, we compare them in a ranking task. This may lose recall for some methods, but it can enable large- scale evaluation.</p><p>In order to evaluate the performance of dif- ferent models, we employ Mean Average Preci- sion (MAP), Mean Reciprocal Rank (MRR), R- Precision (R-Prec), and Precision at K (P@5) as evaluation measures. These measures are widely used in the literature for question retrieval in cQA ( <ref type="bibr" target="#b4">Cao et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter Setting</head><p>In our experiments, we train the word embeddings on another large-scale data set from cQA sites. For English, we train the word embeddings on the Ya- hoo! Webscope dataset 7 . For Chinese, we train the word embeddings on a data set with 1 billion web pages from Baidu Zhidao. These two data sets do not intersect with the above mentioned retrieval data. Little pre-processing is conducted for the training of word embeddings. The resulting text is tokenized using the Stanford tokenizer, 8 , and ev- ery word is converted to lowercase. Since the pro- posed framework has no limits in using which of the word embedding learning methods, we only consider the following two representative meth- ods: Skip-gram (baseline) and M-NET. To train the word embedding using these two methods, we ap- ply the same setting for their common parameters. Specifically, the count of negative samples r is set to 3; the context window size l is set to 5; each model is trained through 1 epoch; the learning rate is initialized as 0.025 and is set to decrease linearly so that it approached zero at the end of training.</p><p>Besides, the combination weight β used in M- NET also plays an important role in producing high quality word embedding. Overemphasizing the weight of the original objective of Skip-gram may result in weakened influence of metadata, while putting too large weight on metadata pow- ered objective may hurt the generality of learned word embedding. Based on our experience, it is a better way to decode the objective combination weight of the Skip-gram model and metadata in- formation based on the scale of their respective derivatives during optimization. Finally, we set β = 0.001 empirically. Note that if the parameter is optimized on the validation set, the final perfor- mance can be further improved.</p><p>For parameter K used in FV, we do an exper- iment on the validation data set to determine the best value among 1, 2, 4, · · · , 64 in terms of MAP. As a result, we set K = 16 in the experiments empirically as this setting yields the best perfor- mance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>In this subsection, we present the experimental re- sults on the test sets of Yahoo data and Baidu data. We compare the baseline word embedding trained by Skip-gram against this trained by M-NET. The dimension of word embedding is set as 50,100 and 300. Since the motivation of this paper attempts to tackle the lexical gap problem for queried ques- tions and questions in the archive, we also com- pare them with the two groups of methods which also address the lexical gap in the literature. The first group is the translation models: word-based translation model ( <ref type="bibr" target="#b9">Jeon et al., 2005</ref>), word-based translation language model ( <ref type="bibr" target="#b23">Xue et al., 2008)</ref>, and phrase-based translation model ( ). We implement those three translation mod- els based on the original papers and train those models with (question, best answer) pairs from the Yahoo! Webscope dataset Yahoo answers and the 1 billion web pages of Baidu Zhidao for English and Chinese, respectively. Training the translation models with different pairs (e.g., question-best an- swer, question-description, question-answer) may achieve inconsistent performance on Yahoo data and Baidu data, but its comparison and analysis are beyond the scope of this paper. The second group is the topic-based methods: unsupervised question-answer topic model ( <ref type="bibr" target="#b10">Ji et al., 2012</ref>) and supervised question-answer topic model ( <ref type="bibr" target="#b26">Zhang et al., 2014a</ref>). We re-implement these two topic- based models and tune the parameter settings on our data set. Besides, we also introduce a baseline language model (LM) <ref type="bibr" target="#b25">(Zhai and Lafferty, 2001</ref>) for comparison.  <ref type="table" target="#tab_0">Table 2</ref>: Evaluation results on Yahoo data and Baidu data, where dim denotes the dimension of the word embeddings. The bold formate indicates the best results for question retrieval. † indicates that the difference between the results of our proposed approach (Skip-gram + FV, M-NET + FV) and other methods are mildly significant with p &lt; 0.08 under a t-test; ‡ indicates the comparisons are statistically significant with p &lt; 0.05.</p><p>show that the improvements between the pro- posed M-NET + FV and the two groups of com- pared methods (translation-based approaches and topic-based approaches) are statistically signifi- cant (p &lt; 0.05), while the improvements be- tween Skip-gram + FV and the translation-based approaches are mildly significant (p &lt; 0.08). Moreover, the metadata of category information powered model (M-NET + FV) outperforms the baseline skip-gram model (Skip-gram + FV) and yields the largest improvements. These results can imply that the metadata powered word embedding is of higher quality than the baseline model with no metadata information regularization. Besides, we also note that setting higher dimension brings more improvements for question retrieval task.</p><p>Translation-based methods significantly outper- form LM, which demonstrate that matching ques- tions with the semantically related translation words or phrases from question-answer pairs can effectively address the word lexical gap problem. Besides, we also note that phrase-based translation model is more effective because it captures some contextual information in modeling the transla- tion of phrases as a whole. More precise transla- tion can be determined for phrases than for words. Similar observation has also been found in the pre- vious work ( ).</p><p>On both data sets, topic-based models achieve comparable performance with the translation- based models and but they perform better than LM. The results demonstrate that learning the latent topics aligned across the question-answer pairs can be an alternative for bridging lexical gap problem for question retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes to learn continuous vector representations for question retrieval in cQA. We firstly introduce a new metadata powered word embedding method, called M-NET, to leverage the category information within cQA pages to obtain word representations. Once the words are embed- ded in a continuous space, we treat each ques- tion as a BoEW. Then, the variable size BoEWs are aggregated into fixed-length vectors by using FK. Finally, the dot product between FVs are used to calculate the semantic similarities for question retrieval. Experiments on large-scale real world cQA data demonstrate that the efficacy of the pro- posed approach. For the future work, we will explore how to incorporate more types of meta- data information, such as the user ratings, like sig- nals and Poll and Survey signals, into the learning process to obtain more powerful word representa- tions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The continuous skip-gram model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The continuous skip-gram model with metadata of category information, called M-NET.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>the question retrieval perfor-
mance by using different evaluation metrics. From 
this table, we can see that learning continu-
ous word embedding representations (Skip-gram 
+ FV, M-NET + FV) for question retrieval can 
outperform the translation-based approaches and 
topic-based approaches on all evaluation metrics. 
We conduct a statistical test (t-test), the results </table></figure>

			<note place="foot" n="1"> http://answers.yahoo.com/ 2 http://qna.live.com/ 3 http://zhidao.baidu.com/</note>

			<note place="foot" n="4"> Note that although we use the skip-gram model as an example to illustrate our approach, the similar framework can be developed on the basis of any other word embedding models.</note>

			<note place="foot" n="5"> http://truereader.com/manuals/onix/stopwords1.html 6 We use the BM25 implementation provided by Apache Lucene (http://lucene.apache.org/), using the default parameter setting (k1 = 1.2, b = 0.75)</note>

			<note place="foot" n="7"> The Yahoo! Webscope dataset Yahoo answers comprehensive questions and answers version 1.0.2, available at http://reseach.yahoo.com/Academic Relations. 8 http://nlp.stanford.edu/software/tokenizer.shtml</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Natu-ral Science Foundation of <ref type="bibr">China (No. 61303180, 257</ref> No. 61272332 and 61402191), the Beijing Natu-ral Science Foundation (No. 4144087), the Ma-jor Project of National Social Science Found (No. 12&amp;2D223), the Fundamental Research Funds for the Central Universities (No. CCNU15ZD003), and also Sponsored by CCF-Tencent Open Re-search Fund. We thank the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge sharing and yahoo answers: Everyone knows something</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Bakshy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ackerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining lexical semantic resources with question &amp; answer archives for translation-based answer finding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning the latent topics for question retrieval in community qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="273" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A generalized framework of exploring category information for question retrieval in community question answer archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">S</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving term weighting for community question answering search using syntactic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Carmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avihai</forename><surname>Mejer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aggregating continuous word embeddings for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="100" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching questions by identifying question topic and question focus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunbo</forename><surname>Huizhong Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chin Yew Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding similar questions in large question and answer archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Jiwoon Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Ho</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Question-answer topic model for question retrieval in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2471" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bridging lexical gaps between queries and questions on large online q&amp;a collections with compact translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Tae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Bum</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<editor>Song, and Hae-Chang Rim</editor>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="410" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical machine translation for query expansion in answer retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Er</forename><surname>Vasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Okapi at trec-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hancockbeaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC</title>
		<meeting>TREC</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Entity based q&amp;a retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1266" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to rank answers on large online qa collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="719" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A syntactic tree matching approach to finding similar questions in community-based qa services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyan</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling semantic relevance for question-answer pairs in web social communities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rcnet: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Retrieval models for question and answer archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwoon</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Question retrieval with high quality answers in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haocheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Continuous word embeddings for detecting local text reuses at the semantic level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihua</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR &apos;14</title>
		<meeting>the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Phrase-based translation model for question retrieval in community question answer archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="653" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards faster and better retrieval models for question search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2139" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
