<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards more variation in text generation: Developing and evaluating variation models for choice of referential form</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thiago</forename><forename type="middle">Castro</forename><surname>Ferreira</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tilburg center for Cognition and Communication (TiCC</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tilburg center for Cognition and Communication (TiCC</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Wubben</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tilburg center for Cognition and Communication (TiCC</orgName>
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards more variation in text generation: Developing and evaluating variation models for choice of referential form</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="568" to="577"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this study, we introduce a non-deterministic method for referring expression generation. We describe two models that account for individual variation in the choice of referential form in automatically generated text: a Naive Bayes model and a Recurrent Neural Network. Both are evaluated using the VaREG corpus. Then we select the best performing model to generate referential forms in texts from the GREC-2.0 corpus and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original references and those produced by a random baseline model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic text generation is the process of con- verting non-linguistic data into coherent and com- prehensible text <ref type="bibr" target="#b16">(Reiter and Dale, 2000</ref>). In recent years, interest in text generation has substantially increased, due to the emergence of new applica- tions such as "robot-journalism" <ref type="bibr" target="#b6">(Clerwall, 2014)</ref>. Even though computers these days are perfectly capable of automatically producing text, the re- sults are arguably often rather rigid, always pro- ducing the same kind and style of text, which makes them somewhat "boring" to read, especially when reading multiple texts in succession.</p><p>Human-written texts, by contrast, do not suf- fer from this problem, presumably because hu- man authors have an innate tendency to produce variation in their use of words and constructions. Indeed, psycholinguistic research has shown that when speakers produce referring expressions in comparable contexts, they non-deterministically vary both the form and the contents of their refer- ences ( <ref type="bibr" target="#b7">Dale and Viethen, 2010;</ref><ref type="bibr" target="#b18">Van Deemter et al., 2012)</ref>. In this paper, we present and evaluate mod- els of referring expression generation that mimic this human non-determinacy and show that this enables us to generate varied references in texts, which, in terms of coherence and comprehensi- bility, did not yield significant differences from human-produced references according to human judges.</p><p>In particular, in this study we focus on the choice of referential form, which is the first de- cision to be made by referring expression gener- ation models <ref type="bibr" target="#b16">(Reiter and Dale, 2000</ref>) and which determines whether a reference takes the form of a proper name, a pronoun, a definite description, etc. Several such models have been proposed <ref type="bibr" target="#b16">(Reiter and Dale, 2000;</ref><ref type="bibr" target="#b11">Henschel et al., 2000;</ref><ref type="bibr" target="#b4">Callaway and Lester, 2002;</ref><ref type="bibr" target="#b12">Krahmer and Theune, 2002;</ref><ref type="bibr" target="#b10">Gupta and Bandopadhyay, 2009;</ref><ref type="bibr" target="#b9">Greenbacker and McCoy, 2009)</ref>. However, all of these are fully de- terministic, always choosing the same referential form in the same context.</p><p>The fact that these models are generally based on text corpora which have only one gold standard form per reference (the one produced by the orig- inal author) does not help either. When the corpus contains, say, a description at some point in the text, this does not mean that, for example, a proper name could not occur in that position as well <ref type="bibr" target="#b19">(Yeh and Mellish, 1997;</ref><ref type="bibr" target="#b8">Ferreira et al., 2016)</ref>. Gener- ally, we just don't know. To counter this prob- lem, a recent corpus, called VaREG, was devel- oped in which 20 different writers were asked to produce references for a particular topic in a vari- ety of texts, giving rise to a distribution over forms per reference <ref type="bibr" target="#b8">(Ferreira et al., 2016)</ref>. This gives us the possibility to distinguish situations where there is more or less agreement between writers in their choices of referential form. But it also enables a new paradigm for choosing referential forms, where instead of predicting the most likely refer- ential form, we can in fact predict the frequency in which a reference assumes a specific form, allow- ing us to turn the choice of referential form into a non-deterministic probabilistic model.</p><p>In this study, we introduce two different mod- els that take the individual variation into account for the choice of referential form, one based on Naive Bayes and one on Recurrent Neural Net- works. Both are evaluated using the VaREG cor- pus. Furthermore, we use the best performing model to generate referential forms in texts from the GREC-2.0 corpus, based on the roulette-wheel generation process <ref type="bibr" target="#b1">(Belz, 2008)</ref>, and conduct an evaluation experiment in which humans judge the coherence and comprehensibility of the generated texts, comparing them both with the original ref- erences and those produced by a random baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Studies</head><p>Several models for the choice of referential form have been proposed in the literature. They can roughly be distinguished in two groups: rule- based and data-driven models.</p><p>Many rule-based models were created for pronominalization, i.e, to choose whether an ob- ject or person should be referred to using a pro- noun or not. <ref type="bibr" target="#b16">Reiter and Dale (2000)</ref> proposed one of the first rule-based models, which opts for a pronominal reference only if the referent was pre- viously mentioned in the discourse and no men- tion to an entity of same gender can be found between the reference and its antecedent. <ref type="bibr" target="#b11">Henschel et al. (2000)</ref> presented a pronominalization model based on recency, discourse status, syntac- tic position, parallelism and ambiguity. To de- cide among a pronoun or a definite description, <ref type="bibr" target="#b4">Callaway and Lester (2002)</ref> also proposed a rule- based model which makes the choices based on information about the discourse, rhetorical struc- ture, recency and distance. <ref type="bibr" target="#b12">Krahmer and Theune (2002)</ref> extended the Incremental algorithm so that if a referent achieves a level of salience in the dis- course (measured by a salience weight), a pronoun is used. Otherwise, a definite description is pro- duced to distinguish the referent from the distrac- tors.</p><p>Aiming to make choices similar to humans, some studies proposed machine learning models trained on human choices of referential form. The GREC project <ref type="bibr" target="#b2">(Belz et al., 2010</ref>) motivated the de- velopment of many of those data-driven models. One of the project's shared tasks aimed to predict the form of the references to the main topics of texts taken from Wikipedia. Among the partici- pants of the task, <ref type="bibr" target="#b10">Gupta and Bandopadhyay (2009)</ref> presented a model that combined rules and a ma- chine learning technique based on semantic and syntactic category, paragraph and sentence posi- tions, and reference number. Similarly, <ref type="bibr" target="#b9">Greenbacker and McCoy (2009)</ref> proposed a decision tree that, besides the features used in <ref type="bibr" target="#b10">Gupta and Bandopadhyay (2009)</ref>, was also based on recency and part-of-speech features. For more information on the GREC shared task, see <ref type="bibr" target="#b2">Belz et al. (2010)</ref>.</p><p>One limitation that these models all have in common is that they fail to model individual vari- ation. According to their predictions, a refer- ence will always assume the most likely referential form. For example, a model that takes into account syntactic position will always choose the same ref- erential form for the subject of a sentence, while humans tend to vary in their choices of referential form. One of the reasons for this problem arises from the data these models are trained on. Most corpora only contain one referring expression per reference. Only the newly introduced VaREG cor- pus takes variation into account, containing 20 dif- ferent expressions for each reference, allowing us to model distributions over referential slots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The VaREG corpus</head><p>The VaREG corpus was collected for the study of individual variation in the choice of referential form <ref type="bibr" target="#b8">(Ferreira et al., 2016)</ref>. The corpus is based on a number of texts, which were presented to partic- ipants in such a way that all references to the main topic of the text had been replaced with gaps. Each participant was asked to fill each of those gaps with a referring expression for the topic.</p><p>The resulting corpus consists of 9,588 referring expressions, produced by 78 participants for 563 referential gaps -around 20 referring expressions per reference -in 36 English texts. The texts were equally distributed over 3 genres: news texts, re- views of commercial products and encyclopedic texts. The references were annotated according to their syntactic position (subject, object, etc.), ref- erential status (new or old, in text, paragraph and sentence) and recency (number of words between previous reference to the same object or entity), and the referring expressions of the participants were classified into 5 referential forms: proper names, pronouns, definite descriptions, demon- stratives and empty references.</p><p>The analysis of the corpus revealed consider- able variation among participants in their choices of referential forms. Various factors influenced the amount of variation that occurred. High amounts of variation, for example, were found in product reviews and also in the object position of sen- tences. Besides allowing us to distinguish between situations with relatively high and relatively low individual variation in choices of referential form, this corpus introduces a new paradigm for the de- velopment and evaluation of models for referen- tial choice. Rather than predicting the most likely form of a reference, as is usually done, the new corpus allows us to develop a model that can pre- dict the frequency with which a particular refer- ence can assume different referential forms. In this study, we explore this possibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>We model the individual variation in the choice of referential form in the following way: each refer- ence consists of a tuple (X, y), where X is the set of feature values that describes the reference and y is a distribution of referential forms that indicates the frequency (in proportion) in which X assumes each form. So given X, we expect to find a distri- butionˆybutionˆ butionˆy similar to y. <ref type="table">Table 1</ref> depicts the features used to describe X. The influence of those discourse factors in the choice of referential form has been often stud- ied in the literature. Concerning syntactic posi- tion, <ref type="bibr" target="#b3">Brennan (1995)</ref> argued that references in the subject position of a sentence are more likely to be shorter than references in the the object posi- tion. In favor of status and recency, <ref type="bibr" target="#b5">Chafe (1994)</ref> showed that references to previously mentioned referents in the discourse and ones that are close to their antecedents are more likely to be shorter than references to new referents or ones that are distant from their antecedents.</p><p>All features were defined categorically, includ- ing the recency. This latter is treated by describ- ing if a reference's antecedent is 10 or less words away, between 11 and 20 words, between 21 and 30 words, between 31 and 40 words and more than 40 words away.</p><p>To predict a distributionˆydistributionˆ distributionˆy based on X, we pro- pose two models: a Naive Bayes and a Recurrent Neural Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Naive Bayes</head><p>Given a set of referential forms F , the probability that a reference assumes a particular form f ∈ F according to this model is given by:</p><formula xml:id="formula_0">P (f | X) ∝ P (f ) x∈X P (x | f ) f ∈F P (f ) x∈X P (x | f )<label>(1)</label></formula><p>To avoid zero probabilities, we used additive smoothing with α = 2e −308 . So given a reference described by X, ˆ y is the distribution over F :</p><formula xml:id="formula_1">ˆ y =   P (f 1 | X) ... P (f |F | | X)   (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recurrent Neural Network</head><p>Some referential theories support the idea that a referential form is chosen based on previous choices to the same referent. Arnold (1998) ar- gued that subjects of a sentence are more likely to be later pronominalized, as well as references in parallel syntactic position with their antecedents. <ref type="bibr" target="#b5">Chafe (1994)</ref> sustained that referents mentioned in recent clauses also tend to be pronominalized. Since Naive Bayes does not take into account the sequential nature of text, we use a Recurrent Neu- ral Network (RNN) to be able to take context into account. RNN is a powerful structure to handle se- quences of data. It can map a sequence of refer- ences (X 1 , ..., X t ) to their referential forms distri- butions (y 1 , ..., y t ) based on the previous steps.</p><p>Our approach here is similar to the one pre- sented by <ref type="bibr" target="#b15">Mesnil et al. (2013)</ref>. But instead of word continuous representations, a referential em- bedding is created for each combination of feature values in X. So given a reference X t and a con- text window size win, the embeddings of the ref- erences X t−1 t−win/2 , X t and X t+win/2 t+1</p><p>are merged to form a representation e t . This representation is used in equations 3 and 4 to find a distribution over the referential forms that X t could assume.</p><formula xml:id="formula_2">h t = sigmoid(W hx e t + W hh h t−1 )<label>(3)</label></formula><formula xml:id="formula_3">ˆ y t = sof tmax(W yh h t )<label>(4)</label></formula><p>Feature Description Syntactic position Subject, object or a genitive noun phrase in the sentence. Referential Status First mention to the referent (new) or not (old) at the level of text, paragraph and sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recency</head><p>Distance between a given reference and the last, previous reference to the same referent. <ref type="table">Table 1</ref>: Features used to describe the references.</p><p>We assume a sequence of tuples {(X 1 , y 1 )..., (X t , y t )} as all the references to a referent throughout a text.</p><p>We trained our RNN using Backpropagation Through Time. To measure the error among y andˆy andˆ andˆy, we use cross entropy as a cost function. The val- ues for the remaining parameters of the RNN are introduced in <ref type="table" target="#tab_1">Table 2</ref>. We chose them based on an ad-hoc analysis, where we searched for an optimal combination to obtain the best predictions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Individual Variation Experiments</head><p>For each reference slot encountered in the VaREG corpus, we evaluated how well a model takes the individual variation into account in the choice of referential form by comparing its predicted distri- bution of referential forms (ˆ y) with the real distri- bution (y). We performed this comparison through two experiments. In the first, the models were trained and tested with VaREG corpus. In the second, we aimed to check to what extent the referring expressions from the GREC-2.0 corpus are similar in form to the referring expressions from VaREG corpus by training the models with the first corpus and test- ing with the second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Method</head><p>4-fold-cross-validation was used to train the mod- els in the first experiment. The number of folds was chosen based on the set-up of the VaREG cor- pus, which consists of 4 groups of texts. Given the structure of the corpus, we decided that training our model with 3 groups of texts and testing it on the held-out group was the most natural solution to avoid overfitting. Each fold has the same amount of texts per genre.</p><p>Unlike VaREG, GREC-2.0 corpus does not have a set of referring expressions for the exact same reference. So, in the second experiment, the referential form distributions y were defined glob- ally by grouping the references by X and comput- ing the frequency of each referential form.</p><p>We also re-annoted the GREC-2.0 corpus to make it compatible with the VaREG corpus. In particular, we added features for status and re- cency to the GREC-2.0 corpus and made the ter- minology consistent beween the two corpora 1 . Both the VaREG corpus and the re-annotated GREC-2.0 corpus are publicly available 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Metrics</head><p>For each reference, Jensen-Shannon divergence <ref type="bibr" target="#b14">(Lin, 1991)</ref> was used to measure the similarity be- tween y andˆyandˆ andˆy:</p><formula xml:id="formula_4">JSD(y||ˆyy||ˆy) = 1 2 D(y||m) + 1 2 D(ˆ y||m)<label>(5)</label></formula><p>where m = 1 2 (y + ˆ y)</p><p>In this measure, D is the Kullback-Leibler di- vergence <ref type="bibr" target="#b13">(Kullback, 1968)</ref>. The Jensen-Shannon divergence ranges from 0 to 1, in which 0 indicates full convergence of the two distributions and 1 full divergence. Therefore, a lower number indicates a better individual variation modeling.</p><p>To check the behaviour ofˆyofˆ ofˆy based on y in each reference, the referential forms of both distribu- tions were ranked and their relation were analysed with the Spearman's rank correlation coefficient. This measure ranges between -1 and 1, where - 1 indicates a fully opposed behaviour among the variables and 1 the exact same behaviour among them. 0 indicates a non-linear correlation among the involved variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines</head><p>We considered two baseline models in the exper- iments. The first, called Random, assumesˆyassumesˆ assumesˆy as a random distribution of forms for each reference.</p><p>The second model, called ParagraphStatus, al- ways chooses a proper name when the reference is to a new topic in the paragraph (the distribu- tion will assume the value 1 to the proper name form and 0 to the others), and a pronoun otherwise (value 1 to the pronoun form and 0 to the others).  <ref type="table">Table 3</ref>: Average Jensen-Shannon divergence and Spearman's correlation coefficient of the models in Experiment 1. <ref type="table">Table 3</ref> depicts the Jensen-Shannon divergence and Spearman's correlation coefficient of the mod- els cross-validated on VaREG corpus. All our models outperformed the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Cross-validation on VaREG corpus</head><p>Considering the models in which the references are described by only one kind of feature, it seems that the status features (+Status) are the ones that best contributed to model the individual variation in the choice of referential form, whereas the re- cency (+Recency) is the worst. Syntactic position is sandwiched among the previous two.</p><p>In the comparison within Naive Bayes and RNN models, the ones in which the references are de- scribed by syntactic position and referential sta- tus (+Syntax+Status−Recency) obtained the best results for both measures. encyclopedic texts, and the worst in product re- views.</p><p>Although RNNs are able to model the indi- vidual variation in a reference based on its an- tecedents, they did not introduce significantly better results than Naive Bayes.</p><p>In fact, NB+Syntax+Status−Recency is significantly bet- ter than RNN+Syntax+Status−Recency in mod- eling the individual variation in news (Wilcoxon Z = 11574.5, p &lt; 0.01) and encyclopedic texts (Wilcoxon Z = 4232.5, p &lt; 0.001).   <ref type="table" target="#tab_3">Table 4</ref> shows the results of models trained with GREC-2.0 and tested with VaREG corpus. These models are the two versions of Naive Bayes, and the two versions of RNN which were best evalu- ated in the previous experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Training on GREC-2.0 and evaluating on VaREG corpus</head><p>The results of this experiment follow the results of the previous one. Our models outperformed the baselines and NB+Syntax+Status−Recency was the model that obtained the best results for both measures.  As in the previous experiment, both Naive Bayes and RNN models best mod- eled the individual variation in encyclopedic texts. Moreover, there was not significant dif- ference among NB+Syntax+Status-Recency and RNN+Syntax+Status-Recency in the three text genres.</p><p>In general, the models trained with VaREG cor- pus seemed to model the individual variation in the choice of referential form better than the mod- els trained with GREC-2.0 corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Coherence and comprehensibility of the texts</head><p>In this section, we investigate to what extent texts generated by our method, including variation of referential form, are judged coherent and compre- hensible by readers. We do this by comparing texts from the GREC-2.0 corpus in which all refer- ences were (re)generated using our method, with the original text and with a variant that includes random variation of referential form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Our model for choice of referential form</head><p>To generate the referring expressions for the topic of a given text of GREC-2.0, we first group all ref- erences by syntactic position and referential sta- tus values. Then for each group, we shuffle the references and choose their forms according to the distribution predicted by our best performing model (the NB+Syntax+Status−Recency trained on VaREG). The choice of referential forms fol- lows the roulette-wheel generation process <ref type="bibr" target="#b1">(Belz, 2008)</ref>. This process entails that if a group has 5 references and our model predicts a distribution of 0.75 proper names and 0.25 pronouns, 4 ref- erences of the group will be proper names and 1 a pronoun. This covers the selection of referential forms (deciding which form to use at which particular point in the text). To deal with their linguistic realisation, we implemented the following heuris- tics. For the cases in which a proper name refer- ence is selected, we choose a realization depend- ing on referential status. If the reference is the first mention to the topic in the text, the reference is realized with the topic's longest proper name. Otherwise, the reference is realized with its short- est proper name. For the cases in which a defi- nite description is selected, but where the original GREC-2.0 corpus does not provide a description for the topic, we select the shortest predicate ad- jective of the first sentence of the text, immedi- ately following the main verb. For instance, for the sentence "Alan Mathison Turing was an English mathematician, logician, and cryptographer.", the selected definite description would be "The En- glish mathematician". In the cases where a refer- ence should assume the form of a demonstrative, the definite article of the definite description is re- placed by the demonstrative "this" (In the previous example, "This English mathematician").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Method</head><p>We evaluated three versions of each text. The Original is the original text in the corpus, includ- ing the original referring expressions selected by the author. We compare this with a Random vari- ant, which does include variation of referential forms, but selects them in a fully random way. Finally, in the third, Generated version, all refer- ences are generated according to the method out- lined at Section 6.1. <ref type="table">Table 5</ref> depicts an example of text in the three versions.</p><p>In total, we make 3 versions of 9 pseudo- randomly selected texts (5 covering animate top- ics and 4 inanimate ones, varying in length) from the GREC-2.0 corpus, yielding 27 texts in total. These were distributed over 3 lists, such that each list contains one variant of each text, and there is an equal number of texts from the 3 conditions (Original, Random, Generated). In all texts, all</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Version</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text</head><p>Original Spain, officially the Kingdom of Spain, is a country located in Southern Europe, with two small exclaves in North Africa (both bordering Morocco). Spain is a democracy which is organized as a parliamentary monarchy. It is a developed country with the ninth-largest economy in the world. It is the largest of the three sovereign nations that make up the Iberian Peninsula-the others are Portugal and the microstate of Andorra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random</head><p>It, officially the Kingdom of Spain, is a country located in Southern Europe, with two small exclaves in North Africa (both bordering Morocco). The country is a democracy that is organized as a parliamentary monarchy. It is a developed country with the ninth-largest economy in the world. This country is the largest of the three sovereign nations that make up the Iberian Peninsula-the others are Portugal and the microstate of Andorra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated</head><p>Spain, officially the Kingdom of Spain, is a country located in Southern Europe, with two small exclaves in North Africa (both bordering Morocco). Spain is a democracy that is organized as a parliamentary monarchy. The country is a developed country with the ninth-largest economy in the world. It is the largest of the three sovereign nations that make up the Iberian Peninsula-the others are Portugal and the microstate of Andorra. <ref type="table">Table 5</ref>: Example of text in the Original, Random and Generated version.</p><p>references to the topic were highlighted in yellow. The experiment was run on CrowdFlower and is publicly available 3 .</p><p>The experiment was performed by 30 partici- pants (10 per list). Their average age was 36 years, and 22 were female. All were proficient in En- glish (the language of the experiment), 26 partic- ipants were native speakers. They were asked to rate each text in terms of how coherent and com- prehensible they considered it, on a scale from 1 (Very Bad) to 5 (Very Good). <ref type="figure" target="#fig_4">Figure 3</ref> depicts the average coherence and com- prehensibility of the texts where their topics are described by the Original, Random and Generated approaches, respectively. Inspection of this Fig- ure clearly shows that the Random texts are rated lower than both the Original and the Generated texts, and that the latter are rated very similarly on both dimensions. This is confirmed by the statistical analysis. Ac- cording to a Friedman test, there is statistically sig- nificant difference in the coherence (χ 2 = 11.79, p &lt; 0.005) and comprehensibility (χ 2 = 8.98, p = 0.01) for the three kinds of texts. We then conducted a post hoc analysis with Wilcoxon signed-rank test corrected for multiple compar- isons using the Bonferroni method, resulting in a significance level set at p &lt; 0.017. Texts of the Original approach are statistically more coher- ent (Z = 322, p &lt; 0.017) and comprehensible (Z = 407.5, p &lt; 0.017) than texts of the Random one. Texts of the Generated approach are also sta- tistically more coherent (Z = 275, p &lt; 0.017), but not more comprehensible (Z = 378, p &lt; 0.05) than texts of the Random one. Finally, and cru-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>In this paper we explored the possibilities of in- troducing more variation in automatically gener- ated texts, by trying to model individual variation in the selection of referential form. We relied on a new corpus <ref type="bibr">(VaREG (Ferreira et al., 2016)</ref>), which does not contain a single expression for each ref- erence in a text, but rather a distribution of ref- erential forms produced by 20 different people. In contrast to earlier models for referential choice which always deterministically choose the most likely form of a reference, we proposed a Naive Bayes and a Recurrent Neural Network model which aimed to predict the frequency distribution with which a reference can assume a specific refer- ential form, based on discourse features including syntactic position, referential status and recency. Given a reference, we evaluated how well each different model could capture the individual vari- ation found in the VaREG corpus by comparing its predicted distribution of referential forms with the real one in the corpus. We trained the models in two different ways: first using the VaREG, and second using the GREC-2.0 corpus. The Naive Bayes model, trained on VaREG corpus, in which the references were described by syntactic posi- tion and referential status features was the one that best modeled the individual variation in the choice of referential form.</p><p>Features Referential status features were the most helpful for modeling the individual variation in the choice of referential form. They were fol-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Generated</head><p>3.4</p><p>3.6</p><p>3.8 lowed by the syntactic position feature. Both of these findings are consistent with the observations about human variation in the selection of referen- tial forms, as discussed by <ref type="bibr" target="#b8">Ferreira et al. (2016)</ref>. This study argued that writers are more likely to vary in their choices when a reference is in the object position, and when it is an old mention in the text, but new in the sentence. Recency was not a helpful feature for our models, and this may be due to the way the feature was represented - i.e., as a categorical rather than a continuous fea- ture. Moreover, the recency feature was measured in terms of words between the current reference and the most recent previous one to the same refer- ent. Perhaps, it would be better to measure recency in terms of different discourse entities mentioned between two references to the same referent.</p><p>Genre In agreement with <ref type="bibr" target="#b8">Ferreira et al. (2016)</ref>, we also found that genre mattered. For model- ing variation, our models performed best when ap- plied to encyclopedic texts, and worst in product reviews, with news sandwiched in between.</p><p>Naive Bayes model vs. RNNs Although the RNNs were able to model individual variation in the choice of referential form to some extent, they did not perform significantly better than the Naive Bayes models, which might have to do with the relatively small dataset. However, we think the size of the corpus matches the relatively low complexity of the problem we address. In the most complex case (i.e., when a reference is de- scribed by its syntactic position, status and re- cency), an input can be represented in 120 differ- ent ways to predict a multinomial distribution of size 5 (number of referential forms). This com- plexity is much smaller than other problems typi- cally modeled by RNNs. In text production, for in- stance, an input may be represented by thousands of words to predict a large multinomial distribu- tion over a vocabulary <ref type="bibr" target="#b17">(Sutskever et al., 2014</ref>). Additionally, it is important to stress that we ac- tually have a real multinomial distribution to com- pare with the distribution predicted by the RNN in each situation. We observed that it is possi- ble to compute more fine-grained error costs in our case, which makes the RNN converge faster when it is backpropagated. In sum, we believe that those two factors combined compensate for the size of the dataset. A possible explanation for the non-difference among the Naive Bayes model and RNNs is the use of the referential status fea- tures, which perhaps are already enough to model the relation among a reference and its antecedents.</p><p>VaREG corpus vs. GREC-2.0 corpus Inter- estingly, our proposed models yielded better per- formance when trained on the VaREG than on the GREC-2.0 corpus. This shows a difference among the referential choices of both corpora. We conjecture this difference is partly due to differ- ences in text genres, since the VaREG corpus con- tains texts from three different genres, whereas the GREC-2.0 corpus only has encyclopedic texts. Earlier work has also highlighted the influence of text genre on the amount of individual variation in writers' choices for referential forms <ref type="bibr" target="#b8">(Ferreira et al., 2016</ref>).</p><p>Coherence and comprehensibility In the sec- ond part of the study, we used the best perform- ing model to generate referential forms in texts from the GREC-2.0 corpus, using a roulette-based model sampling from the predicted distributions over referential forms. We evaluated the texts gen-erated in this way in an experiment in which hu- mans were asked to judge the coherence and com- prehensibility of the generated texts, comparing them both with the original references and those produced by a random baseline model. In terms of coherence and comprehensibility, we found that the texts in which the references were generated by our model were not significantly different than the human generated ones, and significantly bet- ter than the randomly generated ones. This shows that our solution does not only model the individ- ual variation in the choice of referential form, but that this also does not negatively affect the quality of the texts. This is an important step towards de- veloping new models for automatic text generation that are less predictable and more varied.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure 1: Jensen-Shannon divergence of NB+Syntax+Status−Recency (NB) and RNN+Syntax+Status−Recency (RNN) by genre in Experiment 1. Error bars represent 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average Jensen-Shannon divergence of NB+Syntax+Status−Recency (NB) and RNN+Syntax+Status-Recency (RNN) by genre in Experiment 2. Error bars represent 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 depicts</head><label>2</label><figDesc>Figure 2 depicts the Jensen-Shannon divergence measures of models NB+Syntax+StatusRecency and RNN+Syntax+Status-Recency by text genre. As in the previous experiment, both Naive Bayes and RNN models best modeled the individual variation in encyclopedic texts. Moreover, there was not significant difference among NB+Syntax+Status-Recency and RNN+Syntax+Status-Recency in the three text genres. In general, the models trained with VaREG corpus seemed to model the individual variation in the choice of referential form better than the models trained with GREC-2.0 corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average coherence (3a) and comprehensibility (3b) of the texts with the original, randomized and generated referring expressions. Error bars represent 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : RNN Settings</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Average Jensen-Shannon divergence and 
Spearman's correlation coefficient of the models 
in Experiment 2. 

</table></figure>

			<note place="foot" n="1"> Texts also used in VaREG had their references removed from the GREC-2.0 version used in here. 2 http://ilk.uvt.nl/ ˜ tcastrof/acl2016</note>

			<note place="foot" n="3"> http://ilk.uvt.nl/ ˜ tcastrof/acl2016 cially, comparing Original and Generated texts revealed no significant differences for coherence (Z = 540, p &lt; 0.5) nor for comprehensibility (Z = 391.5, p &lt; 0.5).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the National Council of Scientific and Technological Develop-ment from Brazil (CNPq).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Reference form and discourse patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Stanford University Stanford, CA</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="431" to="455" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Heidelberg, chapter Generating Referring Expressions in Context: The GREC Task Evaluation Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jette</forename><surname>Viethen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="294" to="327" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>Empirical methods in natural language generation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Centering attention in discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="167" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pronominalization in generated discourse and dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">B</forename><surname>Callaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">C</forename><surname>Lester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, ACL &apos;02</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Discourse, Consciousness, and Time: The Flow and Displacement of Conscious Experience in Speaking and Writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wallace</forename><forename type="middle">L</forename><surname>Chafe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>University of Chicago Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enter the robot journalist: Users&apos; perceptions of automated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christer</forename><surname>Clerwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Practice</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="519" to="531" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attributecentric referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jette</forename><surname>Viethen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical methods in natural language generation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="163" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Individual variation in the choice of referential form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Thiago Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wubben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature selection for reference generation as informed by psycholinguistic research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">F</forename><surname>Greenbacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CogSci 2009 Workshop on Production of Referring Expressions</title>
		<meeting>the CogSci 2009 Workshop on Production of Referring Expressions</meeting>
		<imprint>
			<publisher>PRECogsci</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Junlg-msr: A machine learning approach of main subject reference selection with rule based improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaji</forename><surname>Bandopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Workshop on Language Generation and Summarisation. Association for Computational Linguistics</title>
		<meeting>the 2009 Workshop on Language Generation and Summarisation. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, UCNLG+Sum &apos;09</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="103" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pronominalization revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renate</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th conference on Computational linguistics</title>
		<meeting>the 18th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="306" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Information sharing: Reference and presupposition in language generation and interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariët</forename><surname>Theune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CSLI</title>
		<editor>K. van Deemter and R. Kibble</editor>
		<imprint>
			<biblScope unit="page" from="223" to="264" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Efficient context-sensitive generation of referring expressions</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Information theory and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<publisher>Courier Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Divergence measures based on the shannon entropy. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-25" />
			<biblScope unit="page" from="3771" to="3775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Building natural language generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Toward a computational psycholinguistics of reference production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kees</forename><surname>Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Van Gompel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Topics in cognitive science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="166" to="183" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An empirical study on the generation of anaphora in chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Long</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mellish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="190" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
