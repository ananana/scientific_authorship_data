<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lexical Inference over Multi-Word Predicates: A Distributional Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<settlement>Edinburgh</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lexical Inference over Multi-Word Predicates: A Distributional Approach</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="644" to="654"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Representing predicates in terms of their argument distribution is common practice in NLP. Multi-word predicates (MWPs) in this context are often either disregarded or considered as fixed expressions. The latter treatment is unsatisfactory in two ways: (1) identifying MWPs is notoriously difficult , (2) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts. We propose a novel approach that integrates the distributional representation of multiple subsets of the MWP&apos;s words. We assume a latent distribution over subsets of the MWP, and estimate it relative to a downstream prediction task. Focusing on the supervised identification of lexical inference relations, we compare against state-of-the-art baselines that consider a single subset of an MWP, obtaining substantial improvements. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-word expressions (MWEs) constitute a large part of the lexicon and account for much of its growth <ref type="bibr" target="#b16">(Jackendoff, 2002;</ref><ref type="bibr">Seaton and Macaulay, 2002</ref>). However, despite their impor- tance, MWEs remain difficult to define and model, and consequently pose serious difficulties for NLP applications ( <ref type="bibr" target="#b35">Sag et al., 2001</ref>). Multi-word Predi- cates (MWPs; sometimes termed Complex Predi- cates) form an important and much addressed sub- class of MWEs and are the focus of this paper.</p><p>MWPs are informally defined as multiple words that constitute a single predicate ( <ref type="bibr" target="#b0">Alsina et al., 1997</ref>). MWPs encompass a wide range of phe- nomena, including causatives, light verbs, phrasal verbs, serial verb constructions and many others, and pose considerable challenges to both linguistic theory and NLP applications (see Section 2). Part of the difficulty in treating them stems from their position on the borderline between syntax and the lexicon. It is therefore often unclear whether they should be treated as fixed expressions, as compo- sitional phrases that reflect the properties of their component parts or as both.</p><p>This work addresses the modelling of MWPs within the context of distributional semantics <ref type="bibr" target="#b43">(Turney and Pantel, 2010)</ref>, in which predicates are represented through the distribution of arguments they may take. In order to collect meaningful statistics, the predicate's lexical unit should be suf- ficiently frequent and semantically unambiguous.</p><p>MWPs pose a challenge to such models, as na¨ıvelyna¨ıvely collecting statistics over all instances of highly ambiguous verbs is likely to result in noisy representations. For instance, the verb "take" may appear in MWPs as varied as "take time", "take effect" and "take to the hills". This heterogene- ity of "take" is likely to have a negative effect on downstream systems that use its distributional rep- resentation. For instance, while "take" and "ac- cept" are often considered lexically similar, the high frequency in which "take" participates in non-compositional MWPs is likely to push the two verbs' distributional representations apart.</p><p>A straightforward approach to this problem is to represent the predicate as a conjunction of mul- tiple words, thereby trading ambiguity for spar- sity. For instance, the verb "take" could be con- joined with its object (e.g., "take care", "take a bus"). This approach, however, raises the chal- lenge of identifying the sub-set of the predicate's words that should be taken to represent it (hence- forth, its lexical components or LCs).</p><p>We propose a novel approach that addresses this challenge in the context of identifying lexical in- ference relations between predicates ( <ref type="bibr" target="#b19">Lin and Pantel, 2001</ref>; <ref type="bibr" target="#b36">Schoenmackers et al., 2010;</ref><ref type="bibr" target="#b24">Melamud et al., 2013a</ref>, inter alia). A (lexical) inference rela- tion p L → p R is said to hold if the relation denoted by p R generally holds between a set of arguments whenever the relation p L does. For instance, an in- ference relation holds between "annex" and "con- trol" since if a country annexes another, it gener- ally controls it. Most works to this task use dis- tributional similarity, either as their main compo- nent ( <ref type="bibr" target="#b41">Szpektor and Dagan, 2008;</ref><ref type="bibr" target="#b25">Melamud et al., 2013b</ref>), or as part of a more comprehensive system <ref type="bibr" target="#b3">(Berant et al., 2011;</ref><ref type="bibr" target="#b18">Lewis and Steedman, 2013)</ref>. For example, consider the verb "take". While the inference relation "have → take" does not gen- erally hold, it does hold in the case of some light verbs, such as "have a look → take a look", under- scoring the importance of taking more inclusive LCs into account. On the other hand, the pred- icate "likely to give a green light" is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., "likely" or "give a green light") into account.</p><p>We present a novel approach to the task that models the selection and relative weighting of the predicate's LCs using latent variables. This ap- proach allows the classifier that uses the distri- butional representations to take into account the most relevant LCs in order to make the predic- tion. By doing so, we avoid the notoriously dif- ficult problem of defining and identifying MWPs and account for predicates of various sizes and de- grees of compositionality. To our knowledge, this is the first work to address lexical relations be- tween MWPs of varying degrees of composition- ality within distributional semantics.</p><p>We conduct experiments on the dataset of Ze- ichner et al. (2012) and compare our methods with analogous ones that select a fixed LC, using state- of-the-art feature sets. Our method obtains sub- stantial performance gains across all scenarios.</p><p>Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker's lexicon consists of par- tially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance <ref type="bibr" target="#b16">(Jackendoff, 2002;</ref><ref type="bibr" target="#b49">Wray, 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Inference Relations. The detection of inference relations between predicates has become a central task over the past few years <ref type="bibr" target="#b39">(Sekine, 2005;</ref><ref type="bibr" target="#b50">Zanzotto et al., 2006;</ref><ref type="bibr" target="#b36">Schoenmackers et al., 2010;</ref><ref type="bibr" target="#b3">Berant et al., 2011;</ref><ref type="bibr" target="#b24">Melamud et al., 2013a</ref>, in- ter alia). Inference rules are used in a wide va- riety of applications including Question Answer- ing ( <ref type="bibr" target="#b31">Ravichandran and Hovy, 2002</ref>), Information Extraction ( <ref type="bibr" target="#b40">Shinyama and Sekine, 2006</ref>), and as a main component in Textual Entailment systems ( <ref type="bibr" target="#b10">Dinu and Wang, 2009;</ref><ref type="bibr" target="#b8">Dagan et al., 2013</ref>). Most approaches to the task used distributional similarity as a major component within their sys- tem. <ref type="bibr" target="#b19">Lin and Pantel (2001)</ref> introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline <ref type="bibr" target="#b24">(Melamud et al., 2013a)</ref>, and is often used as a component within larger sys- tems. <ref type="bibr" target="#b36">Schoenmackers et al. (2010)</ref> presented an unsupervised system for learning inference rules directly from open-domain web data. <ref type="bibr" target="#b24">Melamud et al. (2013a)</ref> used topic models to combine type- level predicate inference rules with token-level in- formation from their arguments in a specific con- text. <ref type="bibr" target="#b25">Melamud et al. (2013b)</ref> used lexical expan- sion to improve the representation of infrequent predicates. <ref type="bibr" target="#b18">Lewis and Steedman (2013)</ref> combined distributional and symbolic representations, eval- uating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using super- vised systems. <ref type="bibr" target="#b47">Weisman et al. (2012)</ref> used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost en- tirely of single-word predicates. <ref type="bibr" target="#b26">Mirkin et al. (2006)</ref> presented a system for learning inference rules between nouns, using distributional similar- ity and pattern-based features. <ref type="bibr" target="#b13">Hagiwara et al. (2009)</ref> identified synonyms using a supervised ap- proach relying on distributional and syntactic fea- tures. <ref type="bibr" target="#b3">Berant et al. (2011)</ref> used distributional simi- larity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtained a more accurate set of inference rules.</p><p>Previous work used simple methods to select the predicate's LC. Some filtered out frequent highly ambiguous verbs ( <ref type="bibr" target="#b18">Lewis and Steedman, 2013)</ref>, others selected a single representative word ( <ref type="bibr" target="#b24">Melamud et al., 2013a)</ref>, while yet others used multi-word LCs but treated them as fixed expres- sions ( <ref type="bibr" target="#b19">Lin and Pantel, 2001;</ref><ref type="bibr" target="#b3">Berant et al., 2011</ref>).</p><p>The goals of the above studies are largely com-plementary to ours. While previous work focused either on improving the quality of the distribu- tional representations themselves or on their incor- poration into more elaborate systems, we focus on the integration of the distributional representation of multiple LCs to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexi- con, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguis- tic discussion ( <ref type="bibr" target="#b0">Alsina et al., 1997;</ref><ref type="bibr" target="#b6">Butt, 2010)</ref>.</p><p>In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years <ref type="bibr" target="#b21">(Lin, 1999;</ref><ref type="bibr" target="#b1">Baldwin et al., 2003;</ref><ref type="bibr" target="#b4">Biemann and Giesbrecht, 2011</ref>). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See ( <ref type="bibr" target="#b30">Ramisch et al., 2013</ref>) for an overview.</p><p>Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs ( <ref type="bibr" target="#b42">Tu and Roth, 2011;</ref><ref type="bibr" target="#b45">Vincze et al., 2013</ref>) and phrasal verbs ( <ref type="bibr" target="#b23">McCarthy et al., 2003;</ref><ref type="bibr" target="#b29">Pichotta and DeNero, 2013)</ref>. Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs ex- plicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representa- tions of composite phrases and the representations of their component sub-parts <ref type="bibr" target="#b48">(Widdows, 2008;</ref><ref type="bibr" target="#b27">Mitchell and Lapata, 2010;</ref><ref type="bibr" target="#b2">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b7">Coecke et al., 2010)</ref>. Several works have used compositional distributional semantics (CDS) representations to assess the composition- ality of MWEs, such as noun compounds <ref type="bibr" target="#b32">(Reddy et al., 2011</ref>) or verb-noun combinations <ref type="bibr" target="#b17">(Kiela and Clark, 2013)</ref>. Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Proposal: A Latent LC Approach</head><p>This section details our approach for distribu- tionally representing MWPs by leveraging their component LCs. Section 3.1 describes our gen- eral approach, Section 3.2 presents our model and Section 3.3 details the feature set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Approach and Notation</head><p>We propose a method for addressing MWPs of varying degrees of compositionality through the integration of the distributional representation of multiple sub-sets of the predicate's words (LCs). We use it to tackle a supervised prediction task that represents predicates distributionally. Our model assumes a latent distribution over the LCs, and es- timates its parameters so to best conform to the goals of the target prediction task.</p><p>Formally, given a predicate p, we denote the set of words comprising it as W (p). The set of al- lowable LCs for p is denoted with H p ⊂ 2 W (p) . H p contains all sub-sets of p that we consider as apriori possible to represent p. For instance, if p is "likely to give a green light", H p may include LCs such as "likely" or "give light". As our method is aimed at discovering the most relevant LCs, we do not attempt to analyze the MWPs in advance, but rather take an inclusive H p , allowing the model to estimate the relative weights of the LCs.</p><p>The task we use as a testbed for our approach is the lexical inference identification task between predicates. Given a pair of predicates p = (p L , p R ), the task is to predict whether an infer- ence relation holds between them. For instance, if p L is "devour" and p R is "eat greedily", the clas- sifier should use the similarity between "devour" and "eat" in order to correctly predict an infer- ence relation in this case. Selecting the wider LC "eat greedily" might result in sparser statistics. In other examples, however, taking a wider LC is po- tentially beneficial. For instance, the dissimilar- ity between "take" and "make" should not prevent the classifier from identifying the inference rela- tion between "take a step" and "make a step".</p><p>Our statistical model aims at predicting the cor- rect label by making use of partially overlapping LCs of various sizes, both for the premise left- hand side (LHS) predicate p L and the hypothesis right-hand side (RHS) predicate p R . More for- mally, we take the space of values for our latent LC variables to be</p><formula xml:id="formula_0">H p L ,p R = H p L × H p R .</formula><p>Our evaluation dataset consists of pairs</p><formula xml:id="formula_1">p (i) = (p (i) L , p (i) R ) for i ∈ {1, . . . , M },</formula><p>where M is the number of examples available, coupled with their gold-standard labels y (i) ∈ {1, −1}. For brevity, we denote</p><formula xml:id="formula_2">H (i) = H p (i) = H p (i) L ,p (i) R</formula><p>. We also as-sume the existence of a feature function Φ(p, y, h) which maps a triplet of a predicate pair p, an infer- ence label y, and a latent state h ∈ H p to R d for some integer d. We denote the training set by D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Model</head><p>We address the task with a latent variable log- linear model, representing the LCs of the predi- cates. We choose this model for its generality, con- ceptual simplicity, and because it allows to easily incorporate various feature sets and sets of latent variables. We introduce L 2 regularization to avoid over-fitting. We use maximum likelihood estima- tion, and arrive at the following objective function:</p><formula xml:id="formula_3">L(w|D) = 1 M M X i=1 log P (y (i) |p (i) , w) − λ 2 w 2 = = 1 n n X i=1 0 @ log X h∈H (i) exp " w Φ(p (i) , y (i) , h) " − log Z(w, i) « − λ 2 w 2</formula><p>where:</p><formula xml:id="formula_4">Z(w, i) = X y∈{−1,1} X h∈H i exp(w Φ(pi, y, h)).</formula><p>We maximize L using the BFGS algorithm <ref type="bibr" target="#b28">(Nocedal and Wright, 1999</ref>). The gradient (with re- spect to w) is the following:</p><formula xml:id="formula_5">L = E h [Φ(pi, yi, h)] − E h,y [Φ(pi, y, h)] − λ · w</formula><p>H p can be defined to be any sub-set of 2 W (p) given that taking an expectation over H can be done efficiently. It is therefore possible to use prior linguistic knowledge to consider only sub-sets of p that are likely to be non-compositional (e.g., verb- preposition or verb-noun pairs).</p><p>In our experiments we attempt to keep the ap- proach maximally general, and define H p to be the set of all subsets of size 1 or 2 of content words in W p 1 . We bound the size of h ∈ H p in order to re- tain computational efficiency and a sufficient fre- quency of the LCs in H p . MWPs of length greater than 2 are effectively approximated by their set of subsets of sizes 1 and 2.</p><p>Each h can therefore be written as a 4-tuple </p><formula xml:id="formula_6">(h A L , h B L , h A R , h B R ),</formula><formula xml:id="formula_7">P (y|p (i) ) over y. As |H p | = O(k 4 ),</formula><p>where k is the number of content words in p, and as the number of content words is usually small 2 , inference can be carried out by directly summing over H (i) . Initialization. The introduction of latent vari- ables into the log-linear model leads to a non- convex objective function. Consequently, BFGS is not guaranteed to converge to the global opti- mum, but rather to a stationary point. The result may therefore depend on the parameter initializa- tion. Indeed, preliminary experiments showed that both initializing w to be zero and using a random initializer results in lower performance.</p><p>Instead, we initialize our model with a simpli- fied convex model that fixes the LCs to be the pair of left-most content words comprising each of the predicates. This is a common method for selecting the predicate's LC (e.g., <ref type="bibr" target="#b24">Melamud et al., 2013a)</ref>. Once h has been fixed, the model col- lapses to a convex log-linear model. The optimal w is then taken as an initialization point for the la- tent variable model. While this method may still not converge to the global maximum, our experi- ments show that this initialization technique yields high quality values for w (see Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Feature Set</head><p>This section lists the features used for our exper- iments. We intentionally select a feature set that relies on either completely unsupervised or shal- low processing tools that are available for a wide variety of languages and domains.</p><p>Given a predicate pair p (i) , a label y ∈ {1, −1} and a latent state h ∈ H (i) , we define their feature vector as Φ(p (i) , y, h) = y · Φ(p (i) , h). The com- putation of Φ(p (i) , h) requires a reference corpus R that contains triplets of the type (p, x, y) where p is a binary predicate and x and y are its argu- ments. We use the Reverb corpus as R in our ex- periments <ref type="bibr" target="#b11">(Fader et al., 2011</ref>; see Section 4). We refrain from encoding features that directly reflect the vocabulary of the training set. Such features are not applicable beyond that set's vocabulary, and as available datasets contain no more than a few thousand examples, these features are unlikely to generalize well.  Distributional Similarity Features. The distri- butional similarity features are based on the DIRT system ( <ref type="bibr" target="#b19">Lin and Pantel, 2001</ref>). The score defines for each predicate p and for each argument slot s ∈ {L, R} (corresponding to the arguments to the right and left of that predicate) a vector v p s which represents the distribution of arguments appearing in that slot. We take v p s (x) to be the number of times that the argument x appeared in the slot s of the predicate p. Given these vectors, the similarity between the predicates p 1 and p 2 is defined as:</p><formula xml:id="formula_8">score(p1, p2) = q sim(v p 1 L , v p 2 L ) · sim(v p 1 R , v p 2 R )</formula><p>where sim is some vector similarity measure. We use two common similarity measures: the vector cosine metric, and the BInc (Szpektor and Dagan, 2008) similarity measure. These measures give complementary perspectives on the similar- ity between the predicates, as the cosine similar- ity is symmetric between the LHS and RHS predi- cates, while BInc takes into account the direction- ality of the inference relation. Preliminary exper- iments with other measures, such as those of Lin (1998) and <ref type="bibr" target="#b46">Weeds and Weir (2003)</ref> did not yield additional improvements.</p><p>We encode the similarity of all measures for the pair h L and h R as well as the pair h A L and h A R . The latter feature is an approximation to the similar- ity between the heads of the predicates, as heads in English tend to be to the left of the predicates. These two features coincide for h values of size 1. Word and Pair Features. These features en- code the basic properties of the LC. The motiva- tion behind them is to allow a more accurate lever- aging of the similarity features, as well as to better determine the relative weights of h ∈ H (i) .</p><p>The feature set is composed of four analogous sets corresponding to h A L ,h B L ,h A R and h B R , as well as two sets of features that capture relations be- tween h A L , h B L and h A R , h B R (in cases h is of size 2). The features include the ordinal index of the word within the predicate, the lemma's frequency ac- cording to R, and a feature that indicates whether that word's lemma also appears in both predicates of the pair. For instance, when considering the predicates "likely to come" and "likely to leave", "likely" appears in both predicates, while "come" and "leave" appear only in one of them.</p><p>In addition, we use POS-based features that encode the most frequent POS tag for the word lemma and the second most frequent POS tag (ac- cording to R). Information about the second most frequent POS tag can be important in identifying light verb constructions, such as "take a swim" or "give a smile", where the object is derived from a verb. It can thus be interpreted as a generalization of the feature that indicates whether the object is a deverbal noun, which is used by some light verb identification algorithms ( <ref type="bibr" target="#b42">Tu and Roth, 2011</ref>).</p><p>In cases where h L is of size 2, we additionally encode features that apply to the conjunction of h A L and h B L . We encode the conjunction of their POS and the number of times the two lemmas oc- curred together in R. We also introduce features that capture the statistical correlation between the words of h L . To do so, we use point-wise mu- tual information, and the conditional probabili- ties P (h A L |h B L ) and P (h B L |h A L ). Similar measures have often been used for the unsupervised detec- tion of MWEs ( <ref type="bibr" target="#b44">Villavicencio et al., 2007;</ref><ref type="bibr" target="#b12">Fazly and Stevenson, 2006</ref>). We also include the analo- gous set of features for h R . LDA-based Features. We further incorporate features based on a Latent Dirichlet Allocation (LDA) topic model ( <ref type="bibr" target="#b5">Blei et al., 2003)</ref>. Several recent works have underscored the usefulness of using topic models to model a predicate's selec- tional preferences ( <ref type="bibr" target="#b34">Ritter et al., 2010;</ref><ref type="bibr" target="#b9">Dinu and Lapata, 2010;</ref><ref type="bibr" target="#b37">Séaghdha, 2010;</ref><ref type="bibr" target="#b18">Lewis and Steedman, 2013;</ref><ref type="bibr" target="#b24">Melamud et al., 2013a</ref>). We adopt the approach of <ref type="bibr" target="#b18">Lewis and Steedman (2013)</ref>, and de- fine a pseudo-document for each LC in the evalu- ation corpus. We populate the pseudo-documents of an LC with its arguments according to R. We then train an LDA model with 25 topics over these documents. This yields a probability distribution P (topic|h) for each LC h, reflecting the types of arguments h may take.</p><p>We further include a feature for the entropy of the topic distribution of the predicate, which re- flects its heterogeneity. This feature is motivated by the assumption that a heterogeneous predicate is more likely to benefit from selecting a more in- clusive LC than a homogeneous one. Technical Issues. All features used, except the similarity ones and the topic distribution features are binary. Frequency features are binned into 4 bins of equal frequency. We conjoin some of the feature sets by multiplying their values. Specifi- cally, we add the cross product of the features of the category "Similarity" (see <ref type="table" target="#tab_1">Table 1</ref>) with the rest of the features. In addition, we conjoin all LHS (RHS) features with an indicator feature that indicates whether h L (h R ) is of size two. This re- sults in 1605 non-constant features.</p><p>We further note that some LCs that appear in the evaluation corpus do not appear at all in R. In our experiments they amounted to 0.2% of the LCs in our evaluation dataset. While previous work of- ten discarded predicates below a certain frequency from the evaluation, we include them in order to facilitate comparison to future work. We assign the similarity features of such examples a 0 value, and assign their other numerical features the mean value of those features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Corpora and Preprocessing. As a reference corpus R, we use <ref type="bibr">Reverb (Fader et al., 2011</ref>), a web-based corpus consisting of 15M web extrac- tions of binary relations. Each relation is a triplet of a predicate and two arguments, one preceding it and one following it. Relations were extracted us- ing regular expressions over the output of a POS tagger and an NP chunker. Each predicate may consist of a single verb, a verb and a preposi- tion or a sequence of words starting in a verb and ending in a preposition, between which there may nouns, adjectives, adverbs, pronouns, determiners and verbs. The verb may also be a copula. Exam- ples of predicates are "make the most of", "could be exchanged for" and "is happy with".</p><p>Reverb is an appealing reference corpus for this task for several reasons. First, it uses fairly shal- low preprocessing technology which is available for many domains and languages. Second, Reverb applies considerable noise filtering, which results in extractions of fair quality. Third, our evaluation dataset is based on Reverb extractions.</p><p>We evaluate our algorithm on the dataset of <ref type="bibr" target="#b51">Zeichner et al. (2012)</ref>. This publicly available corpus 3 provides pairs of Reverb binary relations and an indication of whether an inference rela- tion holds between them within the context of a specific pair of argument fillers. The corpus was compiled using distributional methods to de- tect pairs of relations in Reverb that are likely to have an inference relation between. Annota- tors, employed through Amazon Mechanical Turk, were then asked to determine whether each pair is meaningful, and if so, to determine whether an inference relation holds. Further measures were taken to monitor the accuracy of the annotation.</p><p>For example, the pair of predicates "make the most of" and "take advantage of" appears in the corpus as a pair between which an inference rela- tion holds. The arguments in this case are "stu- dents" and "their university experience". An ex-ample of a pair between which an inference rela- tion does not hold is "tend to neglect" and "under- estimate the importance of", where the arguments are "Robert" and "his family".</p><p>The dataset contains 6,565 instances in total. We use 5,411 pairs of them, discarding instances that were deemed as meaningless by the annota- tors. We also discard cases where the set of ar- guments is reversed between the LHS and RHS predicates. In these examples, p R (x, y) is infer- able from p L (y, x), rather than from p L (x, y). As there are less than 150 reversed instances in the corpus, experimenting on this sub-set is unlikely to be informative.</p><p>The average length of a predicate in the cor- pus is 2.7 words (including function words). In 87.3% of the predicate pairs, there was more than one LC (i.e., |H p | &gt; 1), underscoring the im- portance of correctly leveraging the different LCs. We randomly partition the corpus into a training set which contains 4,343 instances (∼80%), and a test set that contains 1,068 instances, maintaining the same positive to negative label ratio in both datasets <ref type="bibr">4</ref> . Development was carried out using cross-validation on the training data (see below).</p><p>We use a Maximum Entropy POS Tagger, trained on the Penn Treebank, and the WordNet lemmatizer, both implemented within the NLTK package ( <ref type="bibr" target="#b22">Loper and Bird, 2002</ref>). To obtain a coarse-grained set of POS tags, we collapse the tag set to 7 categories: nouns, verbs, adjectives, adverbs, prepositions, the word "to" and a cate- gory that includes all other words. A Reverb argu- ment is represented as the conjunction of its con- tent words that appear more than 10 times in the corpus. Function words are defined according to their POS tags and include determiners, possessive pronouns, existential "there", numbers and coordi- nating conjunctions. Auxiliary verbs and copulas are also considered function words.</p><p>To compute the LDA features, we use the on- line variational Bayes algorithm of <ref type="bibr" target="#b15">(Hoffman et al., 2010)</ref> as implemented in the Gensim software package <ref type="bibr" target="#b33">(Rehurek and Sojka, 2010)</ref>.</p><p>Evaluated Algorithms. The only two previous works on this dataset ( <ref type="bibr" target="#b24">Melamud et al., 2013a;</ref><ref type="bibr" target="#b25">Melamud et al., 2013b</ref>) are not directly compara- ble, as they used unsupervised systems and evalu-ated on sub-sets of the evaluation dataset. Instead, we use several baselines to demonstrate the use- fulness of integrating multiple LCs, as well as the relative usefulness of our feature sets.</p><p>The simplest baseline is ALLNEG, which pre- dicts the most frequent label in the dataset (in our case: "no inference"). The other evaluated sys- tems are formed by taking various subsets of our feature set. We experiment with 4 feature sets. The smallest set, SIM, includes only the similarity fea- tures. This feature set is related to the composi- tional distributional model of Mitchell and Lap- ata (2010) (see Section 6). We note that despite recent advances in identifying predicate inference relations, the DIRT system ( <ref type="bibr" target="#b19">Lin and Pantel, 2001</ref>) remains a strong baseline, and is often used as a component in state-of-the-art systems <ref type="bibr" target="#b3">(Berant et al., 2011</ref>), and specifically in the two aforemen- tioned works that used the same evaluation corpus.</p><p>The next feature set BASIC includes the features found to be most useful during the development of the model: the most frequent POS tag, the fre- quency features and the feature Common. More inclusive is the feature set NO-LDA, which in- cludes all features except the LDA features. Ex- periments with this set were performed in order to isolate the effect of the LDA features. Finally, ALL includes our complete set of features.</p><p>The more direct comparison is against partial implementations of our system where the LC h is deterministically selected. Determining h for each predicate yields a regular log-linear binary classi- fication model. We use two variants of this base- line. The first, LEFTMOST, selects the left-most content word for each predicate. Similar selec- tion strategy was carried out by <ref type="bibr" target="#b24">Melamud et al. (2013a)</ref>. The second, VPREP, selects h to be the verb along with its following preposition. In cases the predicate contains multiple verbs, the one pre- ceding the preposition is selected, and where the predicate does not contain any non-copula verbs, it regresses to LEFTMOST. This LC selection method approximates a baseline that includes sub- categorized prepositions. Such cases are highly frequent and account for a large portion of the MWPs in English. Including a verb's preposition in its LC was commonly done in previous work (e.g., <ref type="bibr" target="#b18">Lewis and Steedman, 2013</ref>).</p><p>We also attempted to identify verb-preposition constructions using a dependency parser. Unfor- tunately, our evaluation dataset is only available in a lemmatized version, which posed a difficulty for the parser. Due to the low quality of the resulting parses, we implemented VPREP using POS-based regular expressions as defined above.</p><p>The full model is denoted with LATENTLC. For each system and feature set, we report results us- ing 10-fold cross-validation on the training set, as well as results on the test set. Both cases use the same set of parameters determined by cross- validation on the training set. As the task at hand is a binary classification problem, we use accuracy scores to rate the performance of our systems. <ref type="table" target="#tab_3">Table 2</ref> presents the results of our experi- ments. Rows correspond to the evaluated algo- rithms, while columns correspond to the feature sets used and the evaluation scenarios (i.e., train- ing set cross-validation or test set evaluation). Our experiments make first use of this dataset in its fullest form for the problem of supervised learning of inference relations, and may serve as a starting point for further exploration of this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>For all feature sets and settings, LATENTLC scored highest, often with a considerable margin of up to 3.0% in the cross-validation and up to 4.6% on the test set relative to the LEFTMOST baseline, and 5.1% (cross-validation) and 6.8% (test) margins relative to VPREP.</p><p>The best scoring result of our LATENTLC model in the cross-validation scenario is 65.72%, obtained by the feature set All. The best scoring result by any of the baseline models in this sce- nario is 62.7%, obtained by the same feature set. For the test set scenario, LATENTLC obtained its highest accuracy, 65.73%, when using the feature set Basic. This is a substantial improvement over the highest scoring baseline model in this scenario that obtained 61.6% accuracy, using the feature set All. This performance gap is substantial when tak- ing into consideration that the improvements ob- tained by the highly competitive DIRT similarity features using the stronger LEFTMOST baseline, result in an improvement of 3.1% and 5.3% over the trivial ALLNEG baseline in the test set and cross-validation scenarios respectively.</p><p>Comparing the different feature sets on our pro- posed model, we find that the Basic feature set gives a consistent and substantial increase over the Sim feature set. Improvements are of 2.8% (test) and 2.2% (cross-validation). Introducing more elaborate features (i.e., the feature sets NoLDA and All) yields some improvements in the cross- validation, but these improvements are not repli- cated on the test set. This may be due to idiosyn- crasies in the test set that are averaged out in the cross-validation scenario.</p><p>For a qualitative analysis, we took the best per- forming model of the data set (i.e., with the Basic feature set), and extracted the set of instances where it made a correct prediction while both baselines made an error. This set contains many verb-preposition pairs, such as "list as → report as" or "submit via → deliver by", underscoring the utility of leveraging multiple LCs rather than con- sidering only a head word (as with LEFTMOST) or the entire phrase (as with VPREP). Other ex- amples in this set contain more complex patterns. These include the positive pairs "talk much about → have much to say about" and "increase with → go up with", and the negative "make predic- tion about → meet the challenge of" and "enjoy watching → love to play".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Relation to CDS. Much recent work subsumed under the title Compositional Distributional Se- mantics addressed the distributional representa- tion of multi-word phrases (see Section 2). This line of work focuses on compositional predicates, such as "kick the ball" and not on idiosyncratic predicates such as "kick the bucket".</p><p>A variant of the CDS approach can be framed within ours. Assume we wish to compute the similarity of the predicates p L = (w 1 , ..., w n ) and p R = (w 1 , ..., w m ). Let us denote the vec- tor space representations of the individual words as v 1 , ..., v n and v 1 , ..., v m respectively. A stan- dard approach in CDS is to compose distributional representations by taking their vector sum . One of the most effective sim- ilarity measures is the cosine similarity, which is a normalized dot product. The distributional sim- ilarity between p L and p R under this model is sim(p L , p R ) = n i=1 m j=1 sim(w i , w j ), where sim(w i , w j ) is the dot product between v i and v j . This similarity score is similar in spirit to a simplified version of our statistical model that restricts the set of allowable LCs H p to be {({w i }, {w j })|i ≤ n, j ≤ m}, i.e., only LCs of size 1. Indeed, taking H p as above, and cosine similarity as the only feature (i.e., w ∈ R), yields the distribution Test Set</p><formula xml:id="formula_9">v L = v 1 + v 2 ... + v n and v R = v 1 + ... + v m (Mitchell</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross Validation Algorithm</head><p>Sim Basic NoLDA All Sim Basic NoLDA All LATENTLC 62.9 65.7 64.4 64.6 62.7 ± 1.9 64.9 ± 1.9 65.0 ± 1.7 65.7 ±1.9 LEFTMOST 59.0 61.1 60.0 60.4 61.2 ± 2.1 62.5 ± 2.4 62.4 ±2.2 62.7 ± 2.0 VPREP 56.1 60.9 60.7 61.6 * 58.1 ± 1.7 60.8 ± 2.2 60.4 ± 2.6 60.6 ± 2.2 ALLNEG 55.9 55.9 This derivation highlights the relation of a sim- plified version of our approach to the additive CDS model, as both approaches effectively aver- age over the similarities of all pairs of words in p L and p R . The derivation also highlights a few ad- vantages of our approach. First, our approach al- lows to straightforwardly introduce additional fea- tures and to weight them in a way most consistent with the task at hand. Second, it allows much more flexibility in defining the set of allowable LCs, H p . Specifically, H p may contain LCs of sizes greater than 1. Third, our approach uses standard proba- bilistic modelling, and therefore has a natural sta- tistical interpretation.</p><p>In order to appreciate the effect of these advan- tages, we perform an experiment that takes H to be the set of all LCs of size 1, and uses a sin- gle similarity measure. We run a 10-fold cross- validation on our training data, obtaining 61.3% accuracy using COSINE and 62.2% accuracy us- ing BInc. The performance gap between these re- sults and the accuracy obtained by our full model (65.7%) underscores the latter's effectiveness in integrating multiple features and LCs. Effectiveness of Optimization Method. Our maximization of the log-likelihood function is not guaranteed to converge to a global optimum. Therefore, the quality of the learned parameters may be sensitive to the initialization point. We hereby describe an experiment that tests the sen- sitivity of our approach to such variance.</p><p>Selecting the highest scoring feature set on our test set (i.e., BASIC), we ran the model with mul- tiple initializers, by randomly perturbing our stan- dard convex initializer (see Section 3). Concretely, given a convex initializer w, we select the starting point to be w + η, where η i ∼ N (0, α|w i |). We ran this experiment 400 times with α = 0.8.</p><p>To combine the resulting weight vectors into a single classifier, we apply two types of standard approaches: a Product of Experts <ref type="bibr" target="#b14">(Hinton, 2002)</ref>, as well as a voting approach that selects the most frequently predicted label. Neither of these exper- iments yielded any significant performance gain. This demonstrates the robustness of our optimiza- tion method to the initialization point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a novel approach to the distributional representation of multi-word pred- icates. Since MWPs demonstrate varying levels of compositionality, a uniform treatment of MWPs either as fixed expressions or through head words is lacking. Instead, our approach integrates mul- tiple lexical units contained in the predicate. The approach takes into account both multi-word LCs that address low compositionality cases, as well as single-word LCs that address compositional cases and are more frequent. It assumes a latent distribu- tion over the LCs of the predicates, and estimates it relative to a target application task.</p><p>We addressed the supervised inference identi- fication task, obtaining substantial improvement over state-of-the-art baseline systems. In future work we intend to assess the benefit of this ap- proach in MWP classes that are well-known from the literature. We believe that a permissive ap- proach that integrates multiple analyses would perform better than standard single-analysis meth- ods in a wide range of applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 presents</head><label>1</label><figDesc>the set of features we use in our experiments. The features can be divided into two main categories: similarity features between the LHS and the RHS predicates (table's top), and fea- tures that reflect the individual properties of each</figDesc><table>Category 

Name 
Description 
Similarity 
COSINE 
DIRT cosine similarity between the vectors of hL and hR 
COSINEA 
DIRT cosine similarity between the vectors of h A 
L and h A 

R 

BInc 
DIRT BInc similarity between the vectors of hL and hR 
BIncA 
DIRT BInc similarity between the vectors of h A 
L and h A 

R 

Word A LHS 

POS A 

L 

The most frequent POS tag for the lemma of h A 

L 

POS2 A 

L 

The second most frequent POS tag for the word lemma of h A 

L 

FREQ A 

L 

The number of occurrences of h A 
L in the reference corpus 
COMMON A 

L 

A binary feature indicating whether h A 
L appears in both predicates 
ORDINAL A 

L 

The ordinal number of h A 
L among the content words of the LHS predicate 

Pair LHS 

POS AB 

L 

The conjunction of P OS A 
L and P OS B 

L 

FREQ AB 

L 

The frequency of h A 
L and h B 
L in the reference corpus 
PREFABL 
P (h A 
L |h A 
L ) as estimated from the reference corpus 
PREFBAL 
P (h B 
L |h A 
L ) as estimated from the reference corpus 
PMIABL 
The point-wise mutual information of h A 
L and h B 

L 

LDA 
TOPICSL 
P (topic|hL) for each of the induced topics. 
TOPICENTL The entropy of the topic distribution P (topic|hL) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : The feature set used in our experiments. The top part presents the similarity measures based on the DIRT approach.</head><label>1</label><figDesc></figDesc><table>The rest of the listed features apply to the LHS predicate (hL), and to the first word in it (h A 
L ). Analogous features are 
introduced for the second word, h B 
L , and for the RHS predicate. The upper-middle part presents the word features for h A 
L . The 
lower-middle part presents features that apply where hL is of size 2. The bottom part lists the LDA-based features. 

of them. Within the LHS feature set, we distin-
guish between two sub-types of features: word 
features that encode the individual properties of 
h A 
L and h B 
L (table's upper middle part), and pair 
features that only apply to LCs of size 2 and re-
flect the relation between h A 
L and h B 
L (table's lower 
middle part). We further incorporate LDA-based 
features that reflect the selectional preferences of 
the predicates (table's bottom). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Results for the various evaluated systems. Accuracy results are presented in percents, followed in the cross vali-</head><label>2</label><figDesc></figDesc><table>dation scenario by the standard deviation over the folds. The rows correspond to the various systems as defined in Section 4. 
LATENTLC is our proposed model. The columns correspond to the various feature sets, from the least to the most inclusive. 
SIM includes only similarity features. BASIC additionally includes POS-based and frequency features. NOLDA includes all 
features except LDA-based features. ALL is the full feature set. ALLNEG is the classifier that invariably predicts the label "no 
inference". Bold marks best overall accuracy per column, and  *  marks figures that are not significantly worse (McNemar's test, 
p &lt; 0.05). The same positive to negative label ratio was maintained in both the cross validation and test set scenarios. In all 
cases, LATENTLC obtains substantial improvements over the baseline systems. 

P (y|p) ∝ 
X 

(w i ,w 
j )∈Hp 

exp`w exp` exp`w · y · sim(wi, w 
j ) 
´ . 

</table></figure>

			<note place="foot" n="1"> We use a POS tagger to identify content words. Prepositions are considered content words under this definition.</note>

			<note place="foot" n="2"> |Hp| is about 15 on average in our dataset, where less than 5% of the H (i) are of size greater than 50.</note>

			<note place="foot" n="3"> http://tinyurl.com/krx2acd</note>

			<note place="foot" n="4"> A script that replicates our train-test partition of the corpus can be found here: http://homepages.inf.ed. ac.uk/oabend/mwpreds.html</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Complex predicates. Center for the Study of Language and Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Alsina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><forename type="middle">Wanda</forename><surname>Bresnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sells</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An empirical model of multiword expression decomposability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Widdows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment</title>
		<meeting>the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global learning of typed entailment rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="610" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributional semantics and compositionality 2011: Shared task description and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenie</forename><surname>Giesbrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Distributional Semantics and Compositionality</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent Dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The light verb jungle: still hacking away</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complex predicates: cross-linguistic perspectives on event structure</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="48" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Analysis</title>
		<editor>J. van Bentham, M. Moortgat, and W. Buszkowski</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="435" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recognizing textual entailment: Models and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zanzotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="220" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Topic models for meaning similarity in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING: Posters</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="250" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inference rules and their application to recognizing textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="211" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying relations for open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatically constructing a lexicon of verb phrase idiomatic combinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afsaneh</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised synonym acquisition using distributional features and syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masato</forename><surname>Hagiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhiro</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation and Media Technologies</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="558" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geoffrey E Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online learning for latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="856" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Foundations of language: Brain, meaning, grammar, evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Jackendoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting compositionality of multi-word expressions using nearest neighbours in vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1427" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combined distributional and logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DIRT-discovery of inference rules from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD 2001</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="323" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-ACL</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic identification of noncompositional phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="317" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NLTK: The natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detecting a continuum of compositionality in phrasal verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop on Multiword expressions: analysis, acquisition and treatment</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A two level model for context sensitive inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1331" to="1340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using lexical expansion to learn inference rules from sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: Short Papers</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="283" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Integrating pattern-based and distributional similarity methods for lexical entailment acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Shachar Mirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-ACL: Poster Session</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Numerical optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Identifying phrasal verbs using many bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="636" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on multiword expressions: From theory to practice and use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Ramisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valia</forename><surname>Kordoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing (TSLP)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning surface text patterns for a question answering system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An empirical study on compositionality in compound nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Software framework for topic modelling with large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radim</forename><surname>Rehurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2010 workshop New Challenges for NLP Frameworks</title>
		<meeting>LREC 2010 workshop New Challenges for NLP Frameworks</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A latent Dirichlet allocation method for selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="424" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiword expressions: A pain in the neck for NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Sag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CICLing</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning first-order Horn clauses from web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schoenmackers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1088" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Latent variable models of selectional preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><surname>Diarmuid´o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Séaghdha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="435" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Collins COBUILD Idioms Dictionary</title>
		<editor>Maggie Seaton and Alison Macaulay</editor>
		<imprint>
			<date type="published" when="2002" />
			<publisher>HarperCollins Publishers</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic paraphrase discovery based on context and keywords between NE pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWP</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="4" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Preemptive information extraction using unrestricted relation discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Shinyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning entailment rules for unary templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning English light verb constructions: contextual or statistical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL HLT 2011</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Validation and evaluation of automatically acquired multiword expressions for grammar engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valia</forename><surname>Kordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Idiart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Ramisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1034" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Identifying English and Hungarian light verb constructions: A contrastive approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">István</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: Short Papers</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="255" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A general framework for distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning verb inference rules from linguistically-motivated evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hila Weisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="194" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic vector products: Some initial investigations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Widdows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second AAAI Symposium on Quantum Interaction</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Formulaic language: Pushing the boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Wray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discovering asymmetric entailment relations between verbs using selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Teresa</forename><surname>Pazienza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-COLING</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Crowdsourcing inference-rule evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Zeichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: Short Papers</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="156" to="160" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
