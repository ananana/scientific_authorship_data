<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generative Attentional Neural Network Model for Dialogue Act Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><forename type="middle">Hung</forename><surname>Tran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Zukerman</surname></persName>
						</author>
						<title level="a" type="main">A Generative Attentional Neural Network Model for Dialogue Act Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="524" to="529"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2083</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel generative neural network architecture for Dialogue Act classification. Building upon the Recurrent Neural Network framework, our model incorporates a new attentional technique and a label-to-label connection for sequence learning, akin to Hidden Markov Models. Our experiments show that both of these innovations enable our model to out-perform strong baselines for dialogue-act classification on the MapTask and Switchboard corpora. In addition, we analyse empirically the effectiveness of each of these innovations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialogue Act (DA) classification is a sequence- to-sequence learning task where a sequence of utterances is mapped into a sequence of DAs. Some works in DA classification treat each ut- terance as an independent instance ( <ref type="bibr" target="#b9">Julia et al., 2010;</ref><ref type="bibr" target="#b5">Gambäck et al., 2011)</ref>, which leads to ig- noring important long-range dependencies in the dialogue history. Other works have captured inter-utterance relationships using models such as Hidden Markov Models (HMMs) ( <ref type="bibr" target="#b15">Stolcke et al., 2000;</ref><ref type="bibr" target="#b16">Surendran and Levow, 2006</ref>) or Recur- rent Neural Networks (RNNs) <ref type="bibr" target="#b11">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b8">Ji et al., 2016)</ref>, where RNNs have been particularly successful.</p><p>In this paper, we present a generative model of utterances and dialogue acts which conditions on the relevant part of the dialogue history. To this effect, we use the attention mechanism ( ) developed originally for sequence-to- sequence models, which has proven effective in Machine Translation ( <ref type="bibr" target="#b12">Luong et al., 2015)</ref> and DA classification <ref type="bibr" target="#b13">(Shen and Lee, 2016</ref>). The intuition is that different parts of an input sequence have different levels of impor- tance with respect to the objective, and this mech- anism enables the selection of the important parts. However, the traditional attention mechanism suf- fers from the attention-bias problem ( <ref type="bibr" target="#b17">Wang et al., 2016)</ref>, where the attention mechanism tends to fa- vor the inputs at the end of a sequence. To address this problem, we propose a gated attention mech- anism, where the attention signal is represented as a gate over the input vector.</p><p>In addition, when generating a dialogue act, we capture its direct dependence on the previous di- alogue act -a reasonable source of information, which, surprisingly, has not been explored in the RNN literature for DA classification.</p><p>Our experiments show that our model signifi- cantly outperforms variants that do not have our innovations, i.e., the gated attention mechanism and direct label-to-label dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Description</head><p>Assume that we have a training dataset D com- prising a collection of dialogues, where each dia- logue consists of a sequence of utterances {y t } T t=1</p><p>and the corresponding sequence of dialogue acts {z t } T t=1 . Each utterance y t is a sequence of to- kens, and its n-th token is denoted y t,n .</p><p>We propose a generative neural model for dialogue P Θ Θ Θ (y 1:T , z 1:T ), which specifies a joint probability distribution over a sequence of utterances y 1:T and the corresponding sequence of dialogue acts z 1:T . This gener- ative model is then trained discriminatively by maximising the conditional log-likelihood (z 1:T ,y 1:T )∈D log P Θ Θ Θ (z 1:T |y 1:T ):</p><p>arg max</p><formula xml:id="formula_0">Θ Θ Θ (y 1:T ,z 1:T )∈D log P Θ Θ Θ (y 1:T , z 1:T ) z 1:T P Θ Θ Θ (y 1:T , z 1:T )</formula><p>Figure 1: Graphical model representation of our model. Red connections depict dialogue-act genera- tion (1); purple connections (dashed and continuous) depict utterance generation (2).</p><p>where Θ Θ Θ represents all neural network param- eters. Discriminative training is employed in order to match the use of the model for pre- dicting dialogue acts during test time, using arg max z 1:T P Θ Θ Θ (z 1:T |y 1:T ). The generative story of our model is as follows:</p><p>(1) generate the dialogue act of the current dia- logue turn conditioned on the previous dialogue act and the previous utterance P Θ Θ Θ (z t |z t−1 , y t−1 ); and (2) generate the current utterance condi- tioned on the previous utterance and the current dialogue act P Θ Θ Θ (y t |z t , y t−1 ). In other words, P Θ Θ Θ (z 1:T , y 1:T ) is decomposed as:</p><formula xml:id="formula_1">T t=1 P Θ Θ Θ (z t |z t−1 , y t−1 )P Θ Θ Θ (y t |z t , y t−1 ).<label>(1)</label></formula><p>Furthermore, each utterance is generated by a sequential process whereby each token y t,n is con- ditioned on all the previously generated tokens y t,&lt;n , as well as the external conditioning context consisting of the dialogue act z t and the previous turn's utterance y t−1 , i.e.,</p><formula xml:id="formula_2">P Θ Θ Θ (y t |z t , y t−1 ) = |yt| n=1 P Θ Θ Θ (y t,n |y t,&lt;n , z t , y t−1 ). (2)</formula><p>Importantly, the decomposition of the joint dis- tribution in Equation 1 allows dynamic program- ming for exact decoding ( §2.2). One possible extension of our framework is to investigate a higher-order Markov model, although one needs to be conscious about the trade-off between the in- crease in the computational complexity of train- ing/decoding with higher-order Markov models versus the potential gain in classification quality.</p><p>We now turn our attention to the neural architec- ture used to realise the components of our prob- abilistic model ( <ref type="figure">Figure 1</ref>). We define the neural model for the conditional probability of the next dialogue act as follows:</p><formula xml:id="formula_3">P Θ Θ Θ (z t |z t−1 , y t−1 ) = softmax(W (z t−1 ) cz c t + b (z t−1 ) z ),<label>(3)</label></formula><p>where c t is the context vector summarising the in- formation from the previous utterance y t−1 , and</p><formula xml:id="formula_4">W (z t−1 ) cz</formula><p>and b</p><formula xml:id="formula_5">(z t−1 ) z</formula><p>are the softmax parameter gated on the previous dialogue act z t−1 . Due to gating, the number of parameters of the model may increase significantly; therefore, we have also explored a variant where only the bias term b</p><formula xml:id="formula_6">(z t−1 ) z</formula><p>is gated. We define the neural model for generat- ing the tokens of the current utterance as follows:</p><formula xml:id="formula_7">P Θ Θ Θ (y t,n |y t,&lt;n , z t , y t−1 ) = softmax(W (zt) hy h t,n−1 + W c c t + b y ),<label>(4)</label></formula><p>where the weight matrix W (zt) hy is gated based on z t , c t summarises the previous utterance, and h t,n−1 is the state of an utterance-level RNN sum- marising all the previously generated tokens:</p><formula xml:id="formula_8">h t,n−1 = f f f (h t,n−2 , E E E y t,n−1 ),<label>(5)</label></formula><p>where E E E y t,n−1 provides the embedding of the to- ken y t,n−1 from the embedding table E E E, and f f f can be any non-linear function, i.e., the sim- ple sigmoid applied to elements of a vec- tor, or the more complex Long-Short-Term- Memory unit (LSTM) <ref type="bibr" target="#b6">(Graves, 2013;</ref><ref type="bibr" target="#b7">Hochreiter and Schmidhuber, 1997)</ref>, or the Gated-Recurrent- Unit (GRU) ( <ref type="bibr" target="#b3">Chung et al., 2014;</ref>).</p><p>In what follows, we elaborate on how to best summarise the information from the previous ut- terance in c t , and how to decode for the best se- quence of dialogue acts given a trend model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>525</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Gated Attention Mechanism</head><p>Given a sequence of words in an utterance {y 1 , . . . , y n }, we would like to compress its infor- mation in c, which is then used in the conditioning contexts of other components of the model. Typ- ically, the last hidden state of the utterance-level RNN is taken to be the summary vector: c = h n . However, it has been shown that attending to all RNN states is more effective.</p><p>The traditional attention mechanism ( ) employs a probability vector a over the words of the input utterance to summarise it. The attention elements in a are typically calcu- lated from the current input y n , and the previous hidden state h n−1 :</p><formula xml:id="formula_9">α n = g(h n−1 , E E E yn ) , a n = e αn n n =1 e α n ,</formula><p>where g is a non-linear function. Once the atten- tion is defined, the representation of the input is constructed as c = n a n h h h n .</p><p>The problem with this traditional attention model is that the final hidden state is a function of all the inputs, hence it is usually more "infor- mative" than the earlier hidden states due to se- mantic accumulation ( <ref type="bibr" target="#b17">Wang et al., 2016</ref>). Thus, most of the attention signal is assigned to the hid- den states toward the end of a sequence. In DA classification, this may not be desirable, since an important token with respect to a dialogue act can appear anywhere in an utterance. We call this the attention bias problem.</p><p>We propose a novel gated attention mechanism, which is inspired by the gating mechanism in LSTMs, to fix the attention bias problem. Simi- lar to the forget gate of LSTMs, we use the avail- able information to calculate an attention gate that learns whether to allow the whole input signal to pass through or to forget all or a part of the input signal: a n = g g g(h n−1 , E E E yn )</p><p>x n = a n E E E yn (8)</p><formula xml:id="formula_12">h n = f f f (h n−1 , x n )<label>(9)</label></formula><p>where represents element-wise multiplication. After filtering the important signal from the in- put token, the information from our tokens is accu- mulated in the last hidden state of the RNN, which we take as the summary vector c = h h h n . Note that since the gated attention is applied to the input be- fore the RNN calculations, it is not affected by the attention bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inference: Viterbi Decoding</head><p>For prediction, we choose the sequence of dia- logue acts with the highest posterior probability: arg max Since the joint probability is decomposed fur- ther according to Equation 1, we can make use of dynamic programming to find the highest prob- ability sequence of dialogue acts. Specifically, the model endows each latent variable z t with a unary potential P Θ Θ Θ (y t |z t , y t−1 ) and binary poten- tial P Θ Θ Θ (z t |z t−1 , y t−1 ) functions. P Θ Θ Θ (y t |z t , y t−1 ) and P Θ Θ Θ (z t |z t−1 , y t−1 ) are akin to the emission and transition functions of an HMM, and are calcu- lated using Equations 2 and 3 respectively. Fur- thermore, the model has been carefully designed so that the hidden states in the RNNs encod- ing the utterances to form the context vector c t (the representation of the previous utterance) are not affected by the sequence of dialogue acts, which is crucial to making the inference amenable to dynamic programming. The resulting infer- ence algorithm is akin to the Viterbi algorithm for HMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Datasets. We conduct our experiments on the MapTask and Switchboard corpora. The MapTask Dialog Act corpus ( <ref type="bibr" target="#b0">Anderson et al., 1991)</ref> con- sists of 128 conversations and more than 27000 utterances in an instruction-giving scenario. There are 13 DA types in this corpus. For the experi- ments, the available data is split into three parts, train/test/validation with 103, 13 and 12 conversa- tions respectively.</p><p>The Switchboard Dialog Act corpus ( <ref type="bibr" target="#b10">Jurafsky et al., 1997</ref>) consists of 1155 transcribed telephone conversations with around 205000 utterances. In contrast with the MapTask conversations, which are task-oriented, the Switchboard corpus con- sists mostly of general topic conversations. The Switchboard tag set has 42 DAs. 1 without gate bias gate all HMM HMM HMM no attn.</p><p>60.97% 64.60% 63.55% traditional 61.72% 64.73% 65.19% gated attn. 62.21% 65.94% 65.94% <ref type="table">Table 1</ref>: Comparison of our model variants on the MapTask corpus.</p><p>Baselines. On MapTask, to the best of our knowledge, there is no standard data split, thus, we make the comparison against our implementation of strong baselines such as HMM-trigram <ref type="bibr" target="#b15">(Stolcke et al., 2000</ref>) and instance-based random forest classifier (1/2/3-gram features). <ref type="bibr">Ji et al.'s (2016)</ref> results for this corpus are obtained by running their publicly available code with the same hyper parameters as those used by our models. We also report the results of <ref type="bibr" target="#b9">Julia et al. (2010)</ref>  <ref type="bibr">2 and Surendran et al. (2006)</ref>. However, the experimental setup of these two works differs from ours, hence their results are not directly comparable to ours.</p><p>On Switchboard, we compare our results with strong baselines using the experimental setup from <ref type="bibr" target="#b11">Kalchbrenner and Blunsom (2013)</ref> and <ref type="bibr">Stolcke et al. (2000). 3</ref> Our Model Configurations. We experiment with several variants of our model to explore the effectiveness of our two improvements: the HMM-like connection and the gated attention mechanism. For the HMM connection, we con- sider three choices: gating all parameters (Equa- tion 3), gating only the bias, and no connection. For the attention, we consider three choices: our new gated attention mechanism, the traditional at- tention, and no attention. Thus, in total, we ex- plore nine model variants.</p><p>All the model variants are implemented with the CNN package <ref type="bibr">4</ref> and trained with Adagrad (Duchi et al., 2011) using dropout ( <ref type="bibr" target="#b14">Srivastava et al., 2014</ref>). They share the same word-embedding size (128) and hidden vector size (64). <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Accuracy <ref type="bibr" target="#b9">Julia et al. (2010)</ref> 55.40% <ref type="bibr" target="#b16">Surendran et al. (2006)</ref> 59.10% HMM ( <ref type="bibr" target="#b15">Stolcke et al. (2000))</ref> 51.40% Random Forest (n-gram) 55.72% <ref type="bibr" target="#b8">Ji et al. (2016)</ref> 60.97% Our model gated attn. + gated HMM bias 65.94% gated attn. + gated HMM all 65.94% <ref type="table">Table 2</ref>: Results on MapTask data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Accuracy <ref type="bibr" target="#b15">Stolcke et al. (2000)</ref> 71.0% <ref type="bibr" target="#b11">Kalchbrenner and Blunsom (2013)</ref> 73.9% <ref type="bibr" target="#b8">Ji et al. (2016)</ref> 72.5% <ref type="bibr" target="#b13">Shen and Lee (2016)</ref> 72.6% our model gated attn. + gated HMM bias 74.2% gated attn. + gated HMM all 74.0% <ref type="table">Table 3</ref>: Results on Switchboard data.</p><p>Results and Analysis. <ref type="table">Table 1</ref> shows the classi- fication accuracy of the nine variants of our model on the MapTask corpus. The classification accu- racy of the two best variants of our model and the baselines appears in <ref type="table">Tables 2 and 3</ref> for MapTask and Switchboard respectively. The bold numbers in each table show the best accuracy achieved by the systems. As seen in these tables, our best mod- els outperform strong baselines for both corpora. 6 <ref type="table">Table 1</ref> shows that adding the attention mecha- nism is beneficial, as the traditional attention mod- els always outperform their non-attention coun- terparts. The gated attention configurations, in turn, outperform those with the traditional atten- tion mechanism by 0.49%-1.21%. Interestingly, the accuracy of Shen and Lee's (2016) classifier, which employs an attention mechanism, is lower than that obtained by <ref type="bibr" target="#b11">Kalchbrenner and Blunsom (2013)</ref>, whose mechanism does not use atten- tion. We believe that the difference in performance is not due to the attention mechanism being inef- fective, but because Shen and Lee (2016) treat the classification of each utterance independently. In contrast, Kalchbrenner and Blunsom (2013) take the sequential nature of dialog acts into account, and run an RNN across the conversation, which conditions the generation of a dialogue act on the dialogue acts and utterances in all the previous di- alogue turns.</p><p>As seen in <ref type="table">Table 1</ref>, the performance gain from the HMM connection is larger than the gain from the attention mechanism. Without the attention mechanism, the HMM connection brings an in- crease of 3.63% with the gated bias HMM config- uration and 2.58% with the fully gated HMM con- figuration. With the use of traditional attention, the improvement is 3.01% for the bias HMM con- figuration and 3.47% for the gated HMM config- uration. Finally with the gated attention in place, the two HMM configurations improve the accu- racy by 3.73%.</p><p>We used McNemar's test to determine the statis- tical significance between the predictions of differ- ent models, and found that our model with both in- novations (HMM connections and gated attention) is statistically significantly better than the variant without these innovations with α &lt; 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we have proposed a new gated at- tention mechanism and a novel HMM-like con- nection in a generative model of utterances and dialogue acts. Our experiments show that these two innovations significantly improve the accu- racy of DA classification on the MapTask and Switchboard corpora. In the future, we plan to apply these two innovations to other sequence-to- sequence learning tasks. Furthermore, DA classi- fication itself can be seen as a preprocessing step in a dialogue system's pipeline. Thus, we also plan to investigate the effect of improvements in DA classification on the downstream components of a dialogue system.</p></div>
			<note place="foot" n="1"> The original size of the tag set for Switchboard is 226, which was then collapsed into 42</note>

			<note place="foot" n="2"> Julia et al. (2010) employed both text transcription and audio signal. Here, we report the results obtained with the transcription. 3 There have been other works with different experimental setups (Gambäck et al., 2011; Webb and Ferguson, 2010) that obtained accuracies ranging from 77.85% to 80.72%. However, these results are not directly comparable to ours. 4 https://github.com/clab/cnn-v1. 5 The experiments were executed on an Intel Xeon E52667 CPU with 16GB of RAM. The training time for each MapTask model is less than a day, the training time for each Switchboard model takes up to four weeks.</note>

			<note place="foot" n="6"> Ji et al. (2016) reported an accuracy of 77.0% on the Switchboard corpus, but their paper does not provide enough information about the experimental setup to replicate this result (hyper-parameters, train/test/development split). Thus, we ran the paper&apos;s publicly available code with our experimental settings, and report the result in our comparison.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The HCRC map task corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Anne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><forename type="middle">Gurman</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gwyneth</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Garrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kowtko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and speech</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="366" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active learning for dialogue act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Gambäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2011-Proceedings of the International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1329" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A latent variable recurrent neural network for discourse-driven language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="332" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dialog act classification using acoustic and discourse information of maptask data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Julia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atiq U Islam</forename><surname>Khan M Iftekharuddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Intelligence and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="289" to="311" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Switchboard SWBD-DAMSL ShallowDiscourse-Function Annotation Coders Manual, Draft 13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debra</forename><surname>Biasca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>University of Colorado</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.3584</idno>
		<title level="m">Recurrent convolutional neural networks for discourse compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural attention models for sequence classification: Analysis and application to key term extraction and dialogue act detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Sheng-Syun Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00077</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Van Ess-Dykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="373" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dialog act tagging with support vector machines and hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinoj</forename><surname>Surendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2006-Proceedings of the International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1950" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inner attention based recurrent neural networks for answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1122" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1288" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic extraction of cue phrases for cross-corpus dialogue act classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1310" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
