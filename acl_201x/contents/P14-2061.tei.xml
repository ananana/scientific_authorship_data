<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Feature Learning for Visual Sign Language Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyam</forename><surname>Gebrekidan Gebre</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Psycholinguistics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onno</forename><surname>Crasborn</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Radboud University Nijmegen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wittenburg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Psycholinguistics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Drude</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Psycholinguistics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Heskes</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Radboud University Nijmegen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Feature Learning for Visual Sign Language Identification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="370" to="376"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Prior research on language identification fo-cused primarily on text and speech. In this paper, we focus on the visual modality and present a method for identifying sign languages solely from short video samples. The method is trained on unlabelled video data (un-supervised feature learning) and using these features, it is trained to discriminate between six sign languages (supervised learning). We ran experiments on short video samples involving 30 signers (about 6 hours in total). Using leave-one-signer-out cross-validation, our evaluation shows an average best accuracy of 84%. Given that sign languages are under-resourced, unsupervised feature learning techniques are the right tools and our results indicate that this is realistic for sign language identification .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of automatic language identification is to quickly identify the identity of the language given utterances. Performing this task is key in applications involving multiple languages such as machine translation and information retrieval (e.g. metadata creation for large audiovisual archives).</p><p>Prior research on language identification is heavily biased towards written and spoken lan- guages <ref type="bibr" target="#b8">(Dunning, 1994;</ref><ref type="bibr" target="#b27">Zissman, 1996;</ref><ref type="bibr" target="#b14">Li et al., 2007;</ref><ref type="bibr" target="#b19">Singer et al., 2012</ref>). While language iden- tification in signed languages is yet to be studied, significant progress has been recorded for written and spoken languages.</p><p>Written languages can be identified to about 99% accuracy using Markov models <ref type="bibr" target="#b8">(Dunning, 1994)</ref>. This accuracy is so high that current research has shifted to related more challeng- ing problems: language variety identification ( <ref type="bibr" target="#b26">Zampieri and Gebre, 2012)</ref>, native language iden- tification ( <ref type="bibr" target="#b24">Tetreault et al., 2013</ref>) and identification at the extremes of scales; many more languages, smaller training data, shorter document lengths ( <ref type="bibr" target="#b0">Baldwin and Lui, 2010)</ref>.</p><p>Spoken languages can be identified to accura- cies that range from 79-98% using different mod- els <ref type="bibr" target="#b27">(Zissman, 1996;</ref><ref type="bibr" target="#b18">Singer et al., 2003</ref>). The methods used in spoken language identification have also been extended to a related class of prob- lems: native accent identification ( <ref type="bibr" target="#b2">Chen et al., 2001;</ref><ref type="bibr" target="#b3">Choueiter et al., 2008;</ref><ref type="bibr" target="#b25">Wu et al., 2010)</ref> and foreign accent identification ( <ref type="bibr" target="#b23">Teixeira et al., 1996)</ref>.</p><p>While some work exists on sign language recognition 1 <ref type="bibr" target="#b20">(Starner and Pentland, 1997;</ref><ref type="bibr" target="#b21">Starner et al., 1998;</ref><ref type="bibr" target="#b9">Gavrila, 1999;</ref><ref type="bibr" target="#b6">Cooper et al., 2012)</ref>, very little research exists on sign language iden- tification except for the work by <ref type="bibr" target="#b10">(Gebre et al., 2013)</ref>, where it is shown that sign language identi- fication can be done using linguistically motivated features. Accuracies of 78% and 95% are reported on signer independent and signer dependent iden- tification of two sign languages. This paper has two goals. First, to present a method to identify sign languages using features learned by unsupervised techniques <ref type="bibr" target="#b12">(Hinton and Salakhutdinov, 2006;</ref><ref type="bibr" target="#b5">Coates et al., 2011</ref>). Sec- ond, to evaluate the method on six sign languages under different conditions.</p><p>Our contributions: a) show that unsupervised feature learning techniques, currently popular in many pattern recognition problems, also work for visual sign languages. More specifically, we show how K-means and sparse autoencoder can be used to learn features for sign language identification. b) demonstrate the impact on performance of vary- ing the number of features (aka, feature maps or filter sizes), the patch dimensions (from 2D to 3D) and the number of frames (video length).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The challenges in sign language identification</head><p>The challenges in sign language identification arise from three sources as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Iconicity in sign languages</head><p>The relationship between forms and meanings are not totally arbitrary ( <ref type="bibr" target="#b17">Perniss et al., 2010)</ref>. Both signed and spoken languages manifest iconicity, that is forms of words or signs are somehow mo- tivated by the meaning of the word or sign. While sign languages show a lot of iconicity in the lex- icon <ref type="bibr" target="#b22">(Taub, 2001</ref>), this has not led to a universal sign language. The same concept can be iconi- cally realised by the manual articulators in a way that conforms to the phonological regularities of the languages, but still lead to different sign forms. Iconicity is also used in the morphosyntax and discourse structure of all sign languages, however, and there we see many similarities between sign languages. Both real-world and imaginary objects and locations are visualised in the space in front of the signer, and can have an impact on the artic- ulation of signs in various ways. Also, the use of constructed action appears to be used in many sign languages in similar ways. The same holds for the rich use of non-manual articulators in sentences and the limited role of facial expressions in the lexicon: these too make sign languages across the world very similar in appearance, even though the meaning of specific articulations may differ <ref type="bibr" target="#b7">(Crasborn, 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Differences between signers</head><p>Just as speakers have different voices unique to each individual, signers have also different sign- ing styles that are likely unique to each individual. Signers' uniqueness results from how they articu- late the shapes and movements that are specified by the linguistic structure of the language. The variability between signers either in terms of phys- ical properties (hand sizes, colors, etc) or in terms of articulation (movements) is such that it does not affect the understanding of the sign language by humans, but that it may be difficult for machines to generalize over multiple individuals. At present we do not know whether the differences between signers using the same language are of a similar or different nature than the differences between dif- ferent languages. At the level of phonology, there are few differences between sign languages, but the differences in the phonetic realization of words (their articulation) may be much larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Diverse environments</head><p>The visual 'activity' of signing comes in a context of a specific environment. This environment can include the visual background and camera noises. The background objects of the video may also in- clude dynamic objects -increasing the ambiguity of signing activity. The properties and configu- rations of the camera induce variations of scale, translation, rotation, view, occlusion, etc. These variations coupled with lighting conditions may introduce noise. These challenges are by no means specific to sign interaction, and are found in many other computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our method performs two important tasks. First, it learns a feature representation from patches of unlabelled raw video data <ref type="bibr" target="#b12">(Hinton and Salakhutdinov, 2006;</ref><ref type="bibr" target="#b5">Coates et al., 2011</ref>). Second, it looks for activations of the learned representation (by convolution) and uses these activations to learn a classifier to discriminate between sign languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised feature learning</head><p>Given samples of sign language videos (unknown sign language with one signer per video), our sys- tem performs the following steps to learn a feature representation (note that these video samples are separate from the video samples that are later used for classifier learning or testing):</p><p>1. Extract patches. Extract small videos (here- after called patches) randomly from any- where in the video samples. We fix the size of the patches such that they all have r rows, c columns and f frames and we ex- tract patches m times. This gives us X = {x (1) , x (1) , . . . , x (m) }, where x (i) ∈ R N and N = r * c * f (the size of a patch). For our ex- periments, we extract 100,000 patches of size 15 * 15 * 1 (2D) and 15 * 15 * 2 (3D).</p><p>2. Normalize the patches. There is evidence that normalization and whitening <ref type="bibr" target="#b13">(Hyvärinen and Oja, 2000</ref>) improve performance in un- supervised feature learning <ref type="bibr" target="#b5">(Coates et al., 2011</ref>). We therefore normalize every patch x (i) by subtracting the mean and dividing by the standard deviation of its elements. For vi- sual data, normalization corresponds to local brightness and contrast normalization.</p><p>3. Learn a feature-mapping. Our unsuper- vised algorithm takes in the normalized and whitened dataset X = {x (1) , x (1) , . . . , x (m) } and maps each input vector</p><formula xml:id="formula_0">x (i) to a new fea- ture vector of K features (f : R N → R K ).</formula><p>We use two unsupervised learning algorithms a) K-means b) sparse autoencoders.</p><p>(a) K-means clustering: we train K-means to learns K c (k) centroids that mini- mize the distance between data points and their nearest centroids <ref type="bibr" target="#b4">(Coates and Ng, 2012)</ref>. Given the learned centroids c (k) , we measure the distance of each data point (patch) to the centroids. Natu- rally, the data points are at different dis- tances to each centroid, we keep the dis- tances that are below the average of the distances and we set the other to zero:</p><formula xml:id="formula_1">f k (x) = max{0, µ(z) − z k }<label>(1)</label></formula><p>where z k = ||x − c (k) || 2 and µ(z) is the mean of the elements of z. </p><formula xml:id="formula_2">f (x) = g(W x + b)<label>(2)</label></formula><p>where g(z) = max(z, 0). Note that ReL nodes have advantages over sigmoid or tanh functions; they create sparse repre- sentations and are suitable for naturally sparse data <ref type="bibr" target="#b11">(Glorot et al., 2011</ref>).</p><p>From K-means, we get K R N centroids and from the sparse autoencoder, we get W ∈ R KxN and b ∈ R K filters. We call both the centroids and filters as the learned features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classifier learning</head><p>Given the learned features, the feature mapping functions and a set of labeled training videos, we extract features as follows:</p><p>1. Convolutional extraction: Extract features from equally spaced sub-patches covering the video sample.</p><p>2. Pooling: Pool features together over four non-overlapping regions of the input video to reduce the number of features. We perform max pooling for K-means and mean pooling for the sparse autoencoder over 2D regions (per frame) and over 3D regions (per all se- quence of frames).</p><p>3. Learning: Learn a linear classifier to predict the labels given the feature vectors. We use logistic regression classifier and support vec- tor machines <ref type="bibr" target="#b16">(Pedregosa et al., 2011</ref>).</p><p>The extraction of classifier features through convolution and pooling is illustrated in <ref type="figure" target="#fig_0">figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Our experimental data consist of videos of 30 signers equally divided between six sign lan- guages: British sign language (BSL), Danish (DSL), French Belgian (FBSL), Flemish (FSL), Greek (GSL), and Dutch (NGT). The data for the unsupervised feature learning comes from half of the BSL and GSL videos in the Dicta-Sign cor- pus 2 . Part of the other half, involving 5 signers, is used along with the other sign language videos for learning and testing classifiers.</p><p>For the unsupervised feature learning, two types of patches are created: 2D dimensions <ref type="bibr">(15 * 15)</ref> and 3D <ref type="bibr">(15 * 15 * 2)</ref>. Each type consists of ran- domly selected 100,000 patches and involves 16 different signers. For the supervised learning, 200 videos (consisting of 1 through 4 frames taken at a step of 2) are randomly sampled per sign language per signer (for a total of 6,000 samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data preprocessing</head><p>The data preprocessing stage has two goals.</p><p>First, to remove any non-signing signals that re- main constant within videos of a single sign lan- guage but that are different across sign languages. For example, if the background of the videos is different across sign languages, then classifying the sign languages could be done with perfection by using signals from the background. To avoid this problem, we removed the background by us- ing background subtraction techniques and manu- ally selected thresholds.</p><p>The second reason for data preprocessing is to make the input size smaller and uniform. The videos are colored and their resolutions vary from 320 * 180 to 720 * 576. We converted the videos to grayscale and resized their heights to 144 and cropped out the central 144 * 144 patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>We evaluate our system in terms of average accu- racies. We train and test our system in leave-one- signer-out cross-validation, where videos from four signers are used for training and videos of the remaining signer are used for testing. Classifica- tion algorithms are used with their default settings and the classification strategy is one-vs.-rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>Our best average accuracy (84.03%) is obtained using 500 K-means features which are extracted over four frames (taken at a step of 2). This ac- curacy obtained for six languages is much higher than the 78% accuracy obtained for two sign lan- guages <ref type="figure" target="#fig_0">(Gebre et al., 2013)</ref>. The latter uses lin- guistically motivated features that are extracted over video lengths of at least 10 seconds. Our sys- tem uses learned features that are extracted over much smaller video lengths (about half a second).</p><p>All classification accuracies are presented in ta- ble 5 for 2D and table 5 for 3D. Classification con- fusions are shown in table 5. <ref type="figure" target="#fig_2">Figure 2</ref> shows fea- tures learned by K-means and sparse autoencoder.   <ref type="table" target="#tab_2">1  2  3  4  5  6  7  8  9  10   BSL   1 2 3 4 5 6 7 8 9 10  1  2  3  4  5  6  7  8  9  10   DSL   1 2 3 4 5 6 7 8 9 10  1  2  3  4  5  6  7  8  9</ref>      <ref type="table">Tables 5 and 5</ref> indicate that K-means performs better with 2D filters and that sparse autoencoder performs better with 3D filters. Note that features from 2D filters are pooled over each frame and concatenated whereas, features from 3D filters are pooled over all frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">2 3 4 5 6 7 8 9 10</head><p>Which filters are active for which language? <ref type="figure" target="#fig_3">Figure 3</ref> shows visualization of the strength of fil- ter activation for each sign language. The figure shows what Lasso looks for when it identifies any of the six sign languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>Given that sign languages are under-resourced, unsupervised feature learning techniques are the right tools and our results show that this is realis- tic for sign language identification.</p><p>Future work can extend this work in two direc- tions: 1) by increasing the number of sign lan- guages and signers to check the stability of the learned feature activations and to relate these to iconicity and signer differences 2) by comparing our method with deep learning techniques. In our experiments, we used a single hidden layer of fea- tures, but it is worth researching into deeper layers to improve performance and gain more insight into the hierarchical composition of features.</p><p>Other questions for future work. How good are human beings at identifying sign languages? Can a machine be used to evaluate the quality of sign language interpreters by comparing them to a na- tive language model? The latter question is partic- ularly important given what happened at the Nel- son Mandela's memorial service 3 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of feature extraction: convolution and pooling.</figDesc><graphic url="image-1.png" coords="3,117.36,62.81,362.85,181.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(b) Sparse autoencoder: we train a sin- gle layer autoencoder with K hid- den nodes using backpropagation to minimize squared reconstruction error. At the hidden layer, the features are mapped using a rectified linear (ReL) function (Maas et al., 2013) as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: All 100 features learned from 100,000 patches of size 15 * 15. K-means learned relatively more curving edges than the sparse auto encoder.</figDesc><graphic url="image-2.png" coords="4,295.88,295.34,127.54,95.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of coefficients of Lasso (logistic regression with L1 penalty) for each sign language with respect to each of the 100 filters of the sparse autoencoder. The 100 filters are shown in figure 2(b). Each grid cell represents a frame and each filter is activated in 4 non-overlapping pooling regions.</figDesc><graphic url="image-7.png" coords="5,162.74,169.21,72.49,72.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>3D filters (15  *  15  *  2): Leave-one-signer-
out cross-validation average accuracies. 

BSL DSL FBSL FSL GSL NGT 
BSL 56.11 2.98 
1.79 
3.38 24.11 11.63 
DSL 2.87 92.37 0.95 
0.46 
3.16 
0.18 
FBSL 1.48 
1.96 79.04 4.69 
6.62 
6.21 
FSL 6.96 
2.96 
2.06 60.81 18.15 9.07 
GSL 5.50 
2.55 
1.67 
2.57 86.05 1.65 
NGT 9.08 
1.33 
3.98 18.76 4.41 62.44 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Confusion matrix -confusions averaged 
over all settings for K-means and sparse autoen-
coder with 2D and 3D filters (i.e. for all # of 
frames, all filter sizes and all classifiers). 

</table></figure>

			<note place="foot" n="1"> There is a difference between sign language recognition and identification. Sign language recognition is the recognition of the meaning of the signs in a given known sign language, whereas sign language identification is the recognition of the sign language itself from given signs.</note>

			<note place="foot" n="2"> http://www.dictasign.eu/</note>

			<note place="foot" n="3"> http://www.youtube.com/watch?v=X-DxGoIVUWo</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language identification: The long and the short of the matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic accent identification using gaussian mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding, 2001. ASRU &apos;01. IEEE Workshop on</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="343" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical study of automatic accent classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghinwa</forename><surname>Choueiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4265" to="4268" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning feature representations with k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="561" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sign language recognition using sub-units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2205" to="2231" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Nonmanual structures in sign languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onno</forename><surname>Crasborn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Oxford</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="668" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Statistical identification of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dunning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Computing Research Laboratory, New Mexico State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The visual analysis of human movement: A survey. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavrila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="82" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic sign language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Binyam Gebrekidan Gebre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Wittenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP 2013</title>
		<meeting>ICIP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A vector space modeling approach to spoken language identification. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Hui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="271" to="284" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML</title>
		<meeting>the ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Iconicity as a general property of language: evidence from spoken and signed languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Perniss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Robin L Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vigliocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Acoustic, phonetic, and discriminative approaches to automatic language identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Gleason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The mitll nist lre 2011 language recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torres-Carrasquillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sturim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey 2012-The Speaker and Language Recognition Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time american sign language recognition from video using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thad</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Motion-Based Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="227" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time american sign language recognition using desk and wearable computer based video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thad</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1371" to="1375" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Language from the body: iconicity and metaphor in American Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Taub</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accent identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Serralheiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1784" to="1787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A report on the first native language identification shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoife</forename><surname>Cahill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL/HLT 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature subset selection for improved native accent identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Duchateau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Pierre</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Van Compernolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic identification of language varieties: The case of portuguese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binyam Gebrekidan Gebre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KONVENS</title>
		<meeting>KONVENS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="233" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comparison of four approaches to automatic language identification of telephone speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Zissman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="44" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
