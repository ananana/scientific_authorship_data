<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What do Neural Machine Translation Models Learn about Morphology?</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">What do Neural Machine Translation Models Learn about Morphology?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="861" to="872"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1080</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network models are quickly becoming the predominant approach to machine translation (MT). Training neural MT (NMT) models can be done in an end-to-end fashion, which is sim- pler and more elegant than traditional MT sys- tems. Moreover, NMT systems have become competitive with, or better than, the previous state-of-the-art, especially since the introduction of sequence-to-sequence models and the atten- tion mechanism ( <ref type="bibr" target="#b2">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b38">Sutskever et al., 2014</ref>). The improved translation quality is often attributed to better handling of non-local dependencies and morphology generation <ref type="bibr" target="#b23">(Luong and Manning, 2015;</ref><ref type="bibr" target="#b4">Bentivogli et al., 2016;</ref><ref type="bibr" target="#b39">Toral and Sánchez-Cartagena, 2017</ref>).</p><p>However, little is known about what and how much these models learn about each language and its features. Recent work has started ex- ploring the role of the NMT encoder in learn- ing source syntax ( <ref type="bibr" target="#b37">Shi et al., 2016)</ref>, but research studies are yet to answer important questions such as: (i) what do NMT models learn about word morphology? (ii) what is the effect on learning when translating into/from morphologically-rich languages? (iii) what impact do different repre- sentations (character vs. word) have on learning? and (iv) what do different modules learn about the syntactic and semantic structure of a language? Answering such questions is imperative for fully understanding the NMT architecture. In this pa- per, we strive towards exploring (i), (ii), and (iii) by providing quantitative, data-driven answers to the following specific questions:</p><p>• Which parts of the NMT architecture capture word structure?</p><p>• What is the division of labor between differ- ent components (e.g. different layers or en- coder vs. decoder)?</p><p>• How do different word representations help learn better morphology and modeling of in- frequent words?</p><p>• How does the target language affect the learn- ing of word structure?</p><p>To achieve this, we follow a simple but effective procedure with three steps: (i) train a neural MT system on a parallel corpus; (ii) use the trained model to extract feature representations for words in a language of interest; and (iii) train a classi- fier using extracted features to make predictions for another task. We then evaluate the quality of the trained classifier on the given task as a proxy to the quality of the extracted representations. In this way, we obtain a quantitative measure of how well the original MT system learns features that are relevant to the given task.</p><p>We focus on the tasks of part-of-speech (POS) and full morphological tagging. We investigate how different neural MT systems capture POS and morphology through a series of experiments along several parameters. For instance, we con- trast word-based and character-based representa- tions, use different encoding layers, vary source and target languages, and compare extracting fea- tures from the encoder vs. the decoder.</p><p>We experiment with several languages with varying degrees of morphological richness: French, German, Czech, Arabic, and Hebrew. Our analysis reveals interesting insights such as:</p><p>• Character-based representations are much better for learning morphology, especially for low-frequency words. This improvement is correlated with better BLEU scores. On the other hand, word-based models are sufficient for learning the structure of common words.</p><p>• Lower layers of the encoder are better at cap- turing word structure, while deeper networks improve translation quality, suggesting that higher layers focus more on word meaning.</p><p>• The target language impacts the kind of in- formation learned by the MT system. Trans- lating into morphologically-poorer languages leads to better source-side word representa- tions. This is partly, but not completely, cor- related with BLEU scores.</p><p>• The neural decoder learns very little about word structure. The attention mechanism re- moves much of the burden of learning word representations from the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Given a source sentence s = {w 1 , w 2 , ..., w N } and a target sentence t = {u 1 , u 2 , ..., u M }, we first generate a vector representation for the source sentence using an encoder (Eqn. 1) and then map this vector to the target sentence using a decoder (Eqn. 2) (Sutskever et al., 2014): <ref type="figure">Figure 1</ref>: Illustration of our approach: (i) NMT system trained on parallel data; (ii) features ex- tracted from pre-trained model; (iii) classifier trained using the extracted features. Here a POS tagging classifier is trained on features from the first hidden layer.</p><formula xml:id="formula_0">ENC : s = {w 1 , w 2 , ..., w N } 7 ! s 2 R k (1) DEC : s 2 R k 7 ! t = {u 1 , u 2 , ..., u M }<label>(2)</label></formula><p>In this work, we use long short-term mem- ory (LSTM) <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref> encoder-decoders with attention ( <ref type="bibr" target="#b2">Bahdanau et al., 2014</ref>), which we train on parallel data.</p><p>After training the NMT system, we freeze the parameters of the encoder and use ENC as a feature extractor to generate vectors representing words in the sentence. Let ENC i (s) denote the encoded rep- resentation of word w i . For example, this may be the output of the LSTM after word w i . We feed ENC i (s) to a neural classifier that is trained to pre- dict POS or morphological tags and evaluate the quality of the representation based on our ability to train a good classifier. By comparing the perfor- mance of classifiers trained with features from dif- ferent instantiations of ENC, we can evaluate what MT encoders learn about word structure. <ref type="figure">Figure 1</ref> illustrates this process. We follow a similar proce- dure for analyzing representation learning in DEC.</p><p>The classifier itself can be modeled in differ- ent ways. For example, it may be an LSTM over outputs of the encoder. However, as we are inter- ested in assessing the quality of the representations learned by the MT system, we choose to model the classifier as a simple feed-forward neural network with one hidden layer and a ReLU non-linearity. Arguably, if the learned representations are good, then a non-linear classifier should be able to ex- tract useful information from them. <ref type="bibr">2</ref>   <ref type="table">Table 1</ref>: Statistics for annotated corpora in Arabic (Ar), German (De), French (Fr), and Czech (Cz).</p><p>size that our goal is not to beat the state-of-the-art on a given task, but rather to analyze what NMT models learn about morphology. The classifier is trained with a cross-entropy loss; more details about its architecture are given in the supplemen- tary material (appendix A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>Language pairs We experiment with several language pairs, including morphologically-rich languages, that have received relatively significant attention in the MT community. These include Arabic-, German-, French-, and Czech-English pairs. To broaden our analysis and study the effect of having morphologically-rich languages on both source and target sides, we also include Arabic- Hebrew, two languages with rich and similar mor- phological systems, and Arabic-German, two lan- guages with rich but different morphologies.  <ref type="table">Table 2</ref>: POS accuracy on gold and predicted tags using word-based and character-based representa- tions, as well as corresponding BLEU scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Encoder Analysis</head><p>Recall that after training the NMT system we freeze its parameters and use it only to gener- ate features for the POS/morphology classifier. Given a trained encoder ENC and a sentence s with POS/morphology annotation, we generate word features ENC i (s) for every word in the sentence.</p><p>We then train a classifier that uses the features ENC i (s) to predict POS or morphological tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effect of word representation</head><p>In this section, we compare different word repre- sentations extracted with different encoders. Our word-based model uses a word embedding ma- trix which is initialized randomly and learned with other NMT parameters. For a character-based model we adopt a convolutional neural network (CNN) over character embeddings that is also learned during training ( <ref type="bibr" target="#b18">Kim et al., 2015)</ref>; see ap- pendix A.1 for specific settings. In both cases we run the encoder over these representations and use its output ENC i (s) as features for the classifier. <ref type="table">Table 2</ref> shows POS tagging accuracy using features from different NMT encoders. Char- based models always generate better represen- tations for POS tagging, especially in the case of morphologically-richer languages like Arabic and Czech. We observed a similar pattern in the full morphological tagging task. For exam- ple, we obtain morphological tagging accuracy of 65.2/79.66 and 67.66/81.66 using word/char- based representations from the Arabic-Hebrew and Arabic-English encoders, respectively. <ref type="bibr">3</ref> The superior morphological power of the char-based model also manifests in better translation quality (measured by BLEU), as shown in <ref type="table">Table 2</ref>.  Impact of word frequency Let us look more closely at an example case: Arabic POS and mor- phological tagging. <ref type="figure" target="#fig_1">Figure 3</ref> shows the effect of using word-based vs. char-based feature represen- tations, obtained from the encoder of the Arabic- Hebrew system (other language pairs exhibit sim- ilar trends). Clearly, the char-based model is su- perior to the word-based one. This is true for the overall accuracy (+14.3% in POS, +14.5% in mor- phology), but more so in OOV words (+37.6% in POS, +32.7% in morphology). <ref type="figure" target="#fig_0">Figure 2</ref> shows that the gap between word-based and char-based repre- sentations increases as the frequency of the word in the training data decreases. In other words, the more frequent the word, the less need there is for character information. These findings make intu- itive sense: the char-based model is able to learn character n-gram patterns that are important for identifying word structure, but as the word be- comes more frequent the word-based model has seen enough examples to make a decision. Analyzing specific tags In <ref type="figure" target="#fig_4">Figure 5</ref> we plot confusion matrices for POS tagging using word- based and char-based representations (from Ara- bic encoders). While the char-based represen- tations are overall better, the two models still share similar misclassified tags. Much of the confusion comes from wrongly predicting nouns (NN, NNP). In the word-based case, relatively many tags with determiner (DT+NNP, DT+NNPS, DT+NNS, DT+VBG) are wrongly predicted as non-determined nouns (NN, NNP). In the char- based case, this hardly happens. This suggests that the char-based representations are predictive of the presence of a determiner, which in Arabic is ex- pressed as the prefix "Al-" (the definite article), a pattern easily captured by a char-based model.</p><p>In <ref type="figure" target="#fig_2">Figure 4</ref> we plot the difference in POS accu- racy when moving from word-based to char-based representations, per POS tag frequency in the training data. Tags closer to the upper-right corner occur more frequently in the training set and are  where the char-based model really shines, which makes sense linguistically as plurality in Arabic is usually expressed by certain suffixes ("-wn/yn" for masc. plural, "-At" for fem. plural). The char- based model is thus especially good with frequent tags and infrequent words, which is understand- able given that infrequent words typically belong to frequent open categories like nouns and verbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effect of encoder depth</head><p>Modern NMT systems use very deep architectures with up to 8 or 16 layers ( <ref type="bibr" target="#b42">Zhou et al., 2016)</ref>. We would like to understand what kind of information different layers capture. Given a trained NMT model with multiple layers, we extract feature representations from the different layers in the encoder. Let ENC l i (s) denote the encoded representation of word w i after the l-th layer. We can vary l and train different classi- fiers to predict POS or morphological tags. Here we focus on the case of a 2-layer encoder-decoder model for simplicity (l 2 {1, 2}).  <ref type="figure" target="#fig_5">Figure 6</ref> shows POS tagging results using rep- resentations from different encoding layers across five language pairs. The general trend is that pass- ing word vectors through the NMT encoder im- proves POS tagging, which can be explained by the contextual information contained in the repre- sentations after one layer. However, it turns out that representations from the 1st layer are bet- ter than those from the 2nd layer, at least for the purpose of capturing word structure. <ref type="figure" target="#fig_6">Fig- ure 7</ref> demonstrates that the same pattern holds for both word-based and char-based representations, on Arabic POS and morphological tagging. In all cases, layer 1 representations are better than layer 2 representations. <ref type="bibr">4</ref> In contrast, BLEU scores ac- tually increase when training 2-layer vs. 1-layer models (+1.11/+0.56 BLEU for Arabic-Hebrew word/char-based models). Thus translation qual- ity improves when adding layers but morphol- ogy quality degrades. Intuitively, it seems that lower layers of the network learn to represent word structure while higher layers are more focused on word meaning. A similar pattern was recently ob- served in a joint language-vision deep recurrent network (Gelderloos and Chrupała, 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of target language</head><p>While translating from morphologically-rich lan- guages is challenging, translating into such lan- guages is even harder. For instance, our ba- sic system obtains BLEU scores of 24.69/23.2 on Arabic/Czech to English, but only 13.37/13.9 on English to Arabic/Czech. How does the target language affect the learned source lan- guage representations? Does translating into a morphologically-rich language require more knowledge about source language morphology? In order to investigate these questions, we fix the source language and train NMT models us- ing different target languages. For example, given an Arabic source side, we train Arabic-to- English/Hebrew/German systems. These target languages represent a morphologically-poor lan- guage (English), a morphologically-rich language with similar morphology to the source language (Hebrew), and a morphologically-rich language with different morphology (German). To make a fair comparison, we train the models on the inter- section of the training data based on the source language. In this way the experimental setup is  As expected, translating into English is easier than translating into the morphologically- richer Hebrew and German, resulting in higher BLEU scores. Despite their similar morphologi- cal systems, translating Arabic to Hebrew is worse than Arabic to German, which can be attributed to the richer Hebrew morphology compared to German. POS and morphology accuracies share an intriguing pattern: the representations that are learned when translating into English are better for predicting POS or morphology than those learned when translating into German, which are in turn better than those learned when translating into He- brew. This is remarkable given that English is a morphologically-poor language that does not dis- play many of the morphological properties that are found in the Arabic source. In contrast, Ger- man and Hebrew have richer morphologies, so one could expect that translating into them would make the model learn more about morphology.</p><p>A possible explanation for this phenomenon is that the Arabic-English model is simply better than the Arabic-Hebrew and Arabic-German mod- els, as hinted by the BLEU scores in <ref type="table">Table 2</ref>. The inherent difficulty in translating Arabic to He- brew/German may affect the ability to learn good representations of word structure. To probe this more, we trained an Arabic-Arabic autoencoder on the same training data. We found that it learns to recreate the test sentences extremely well, with very high BLEU scores <ref type="figure" target="#fig_7">(Figure 8</ref>). However, its word representations are actually inferior for the purpose of POS/morphological tagging. This im- plies that higher BLEU does not necessarily en- tail better morphological representations. In other words, a better translation model learns more in- formative representations, but only when it is actu- ally learning to translate rather than merely mem- orizing the data as in the autoencoder case. We found this to be consistently true also for char- based experiments, and in other language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Decoder Analysis</head><p>So far we only looked at the encoder. However, the decoder DEC is a crucial part in an MT system with access to both source and target sentences. In order to examine what the decoder learns about morphology, we first train an NMT system on the parallel corpus. Then, we use the trained model to encode a source sentence and extract features for words in the target sentence. These features are used to train a classifier on POS or morphological tagging on the target side. <ref type="bibr">5</ref> Note that in this case the decoder is given the correct target words one- by-one, similar to the usual NMT training regime. <ref type="table">Table 3 (1st row)</ref> shows the results of using rep- resentations extracted with ENC and DEC from the Arabic-English and English-Arabic models, re- spectively. There is clearly a huge drop in rep- resentation quality with the decoder. <ref type="bibr">6</ref> At first, this drop seems correlated with lower BLEU scores in English to Arabic vs. Arabic to English. However, we observed similar low POS tagging accuracy using decoder representations from high-quality NMT models. For instance, the French-to-English system obtains 37.8 BLEU, but its decoder rep- resentations give a mere 54.26% accuracy on En- glish POS tagging.</p><p>As an alternative explanation for the poor qual- ity of the decoder representations, consider the fundamental tasks of the two NMT modules: en- coder and decoder. The encoder's task is to create a generic, close to language-independent represen- tation of the source sentence, as shown by recent evidence from multilingual NMT ( <ref type="bibr" target="#b13">Johnson et al., 2016</ref>  <ref type="table">Table 3</ref>: POS tagging accuracy using encoder and decoder representations with/without attention.</p><p>language. Presumably, it is sufficient for the de- coder to learn a strong language model in order to produce morphologically-correct output, with- out learning much about morphology, while the encoder needs to learn quite a lot about source language morphology in order to create a good generic representation. In the following section we show that the attention mechanism also plays an important role in the division of labor between encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of attention</head><p>Consider the role of the attention mechanism in learning useful representations: during decoding, the attention weights are combined with the de- coder's hidden states to generate the current trans- lation. These two sources of information need to jointly point to the most relevant source word(s) and predict the next most likely word. Thus, the decoder puts significant emphasis on mapping back to the source sentence, which may come at the expense of obtaining a meaningful representa- tion of the current word. We hypothesize that the attention mechanism hurts the quality of the target word representations learned by the decoder.</p><p>To test this hypothesis, we train NMT models with and without attention and compare the quality of their learned representations. As <ref type="table">Table 3</ref> shows (compare 1st and 2nd rows), removing the atten- tion mechanism decreases the quality of the en- coder representations, but improves the quality of the decoder representations. Without the attention mechanism, the decoder is forced to learn more informative representations of the target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of word representation</head><p>We also conducted experiments to verify our find- ings regarding word-based versus character-based representations on the decoder side. By charac- ter representation we mean a character CNN on the input words. The decoder predictions are still done at the word-level, which enables us to use its hidden states as word representations. <ref type="table">Table 4</ref> shows POS accuracy of word-based vs. char-based representations in the encoder and de- coder. While char-based representations improve the encoder, they do not help the decoder. BLEU scores behave similarly: the char-based model leads to better translations in Arabic-to-English, but not in English-to-Arabic. A possible expla- nation for this phenomenon is that the decoder's predictions are still done at word level even with the char-based model (which encodes the target in- put but not the output). In practice, this can lead to generating unknown words. Indeed, in Arabic- to-English the char-based model reduces the num- ber of generated unknown words in the MT test set by 25%, while in English-to-Arabic the num- ber of unknown words remains roughly the same between word-based and char-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Analysis of neural models The opacity of neu- ral networks has motivated researchers to ana- lyze such models in different ways. One line of work visualizes hidden unit activations in recur- rent neural networks that are trained for a given task <ref type="bibr" target="#b10">(Elman, 1991;</ref><ref type="bibr" target="#b16">Karpathy et al., 2015;</ref><ref type="bibr" target="#b15">Kádár et al., 2016;</ref><ref type="bibr" target="#b30">Qian et al., 2016a</ref>). While such vi- sualizations illuminate the inner workings of the network, they are often qualitative in nature and somewhat anecdotal. A different approach tries to provide a quantitative analysis by correlating parts of the neural network with linguistic properties, for example by training a classifier to predict fea- tures of interest. Different units have been used, from word embeddings <ref type="bibr" target="#b22">(Köhn, 2015;</ref><ref type="bibr" target="#b31">Qian et al., 2016b</ref>), through LSTM gates or states <ref type="bibr" target="#b30">(Qian et al., 2016a)</ref>, to sentence embeddings ( <ref type="bibr" target="#b0">Adi et al., 2016)</ref>. Our work is most similar to <ref type="bibr" target="#b37">Shi et al. (2016)</ref>, who use hidden vectors from a neural MT encoder to predict syntactic properties on the English source side. In contrast, we focus on representations in morphologically-rich languages and evaluate both source and target sides across several criteria. <ref type="bibr" target="#b40">Vylomova et al. (2016)</ref> also analyze different repre- sentations for morphologically-rich languages in MT, but do not directly measure the quality of the learned representations.</p><p>Word representations in MT Machine transla- tion systems that deal with morphologically-rich languages resort to various techniques for repre- senting morphological knowledge, such as word segmentation <ref type="bibr" target="#b28">(Nieflen and Ney, 2000</ref>  <ref type="table">Table 4</ref>: POS tagging accuracy using word-based and char-based encoder/decoder representations. <ref type="bibr" target="#b21">Knight, 2003;</ref><ref type="bibr" target="#b1">Badr et al., 2008)</ref> and factored translation and reordering models <ref type="bibr" target="#b20">(Koehn and Hoang, 2007;</ref><ref type="bibr" target="#b8">Durrani et al., 2014</ref>). Charac- ters and other sub-word units have become in- creasingly popular in neural MT, although they had also been used in phrase-based MT for han- dling morphologically-rich ( <ref type="bibr" target="#b25">Luong et al., 2010)</ref> or closely related language pairs ( <ref type="bibr" target="#b9">Durrani et al., 2010;</ref><ref type="bibr" target="#b27">Nakov and Tiedemann, 2012)</ref>. In neural MT, such units are obtained in a pre-processing step-e.g. by byte-pair encoding <ref type="bibr" target="#b36">(Sennrich et al., 2016)</ref> or the word-piece model ( )- or learned during training using a character-based convolutional/recurrent sub-network <ref type="bibr" target="#b7">(Costa-jussà and Fonollosa, 2016;</ref><ref type="bibr" target="#b24">Luong and Manning, 2016;</ref><ref type="bibr" target="#b40">Vylomova et al., 2016</ref>). The latter approach has the advantage of keeping the original word bound- aries without requiring pre-and post-processing.</p><p>Here we focus on a character CNN which has been used in language modeling and machine translation ( <ref type="bibr" target="#b18">Kim et al., 2015;</ref><ref type="bibr" target="#b3">Belinkov and Glass, 2016;</ref><ref type="bibr" target="#b7">Costa-jussà and Fonollosa, 2016;</ref><ref type="bibr" target="#b14">Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b33">Sajjad et al., 2017)</ref>. We evaluate the quality of different representations learned by an MT system augmented with a character CNN in terms of POS and morphological tagging, and contrast them with a purely word-based system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Neural networks have become ubiquitous in ma- chine translation due to their elegant architecture and good performance. The representations they use for linguistic units are crucial for obtaining high-quality translation. In this work, we inves- tigated how neural MT models learn word struc- ture. We evaluated their representation quality on POS and morphological tagging in a number of languages. Our results lead to the following con- clusions:</p><p>• Character-based representations are better than word-based ones for learning morphol- ogy, especially in rare and unseen words.</p><p>• Lower layers of the neural network are better at capturing morphology, while deeper net- works improve translation performance. We hypothesize that lower layers are more fo- cused on word structure, while higher ones are focused on word meaning.</p><p>• Translating into morphologically-poorer lan- guages leads to better source-side representa- tions. This is partly, but not completely, cor- related with BLEU scores.</p><p>• The attentional decoder learns impoverished representations that do not carry much infor- mation about morphology.</p><p>These insights can guide further development of neural MT systems. For instance, jointly learn- ing translation and morphology can possibly lead to better representations and improved translation. Our analysis indicates that this kind of approach should take into account factors such as the en- coding layer and the type of word representation.</p><p>Another area for future work is to extend the analysis to other word representations (e.g. byte-pair encoding), deeper networks, and more semantically-oriented tasks such as semantic role- labeling or semantic parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplementary Material</head><p>A.1 Training Details POS/Morphological classifier The classifier used for all prediction tasks is a feed-forward net- work with one hidden layer, dropout (⇢ = 0.5), a ReLU non-linearity, and an output layer mapping to the tag set (followed by a Softmax). The size of the hidden layer is set to be identical to the size of the encoder's hidden state (typically 500 dimen- sions). We use Adam ( <ref type="bibr" target="#b19">Kingma and Ba, 2014</ref>) with default parameters to minimize the cross-entropy objective. Training is run with mini-batches of size 16 and stopped once the loss on the dev set stops improving; we allow a patience of 5 epochs.</p><p>Neural MT system We train a 2-layer LSTM encoder-decoder with attention. We use the seq2seq-attn implementation <ref type="bibr" target="#b17">(Kim, 2016)</ref> with the following default settings: word vec- tors and LSTM states have 500 dimensions, SGD with initial learning rate of 1.0 and rate decay of 0.5, and dropout rate of 0.3. The character- based model is a CNN with a highway network over characters ( <ref type="bibr" target="#b18">Kim et al., 2015</ref>) with 1000 fea- ture maps and a kernel width of 6 characters. This model was found to be useful for translating morphologically-rich languages <ref type="bibr" target="#b7">(Costa-jussà and Fonollosa, 2016</ref>). The MT system is trained for 20 epochs, and the model with the best dev loss is used for extracting features for the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Data and Taggers</head><p>Datasets All of the translation models are trained on the Ted talks corpus included in WIT 3 ( <ref type="bibr" target="#b6">Cettolo et al., 2012;</ref><ref type="bibr" target="#b5">Cettolo, 2016)</ref>. Statistics about each language pair are available on the WIT 3 website: https://wit3.fbk.eu. For experiments using gold tags, we used the Arabic Treebank for Arabic (with the versions and splits described in the MADAMIRA manual ( <ref type="bibr" target="#b29">Pasha et al., 2014)</ref>) and the Tiger corpus for German. <ref type="bibr">7</ref> POS and morphological taggers We used the following tools to annotate the MT corpora: MADAMIRA ( <ref type="bibr" target="#b29">Pasha et al., 2014</ref>) for Arabic POS and morphological tags, Tree-Tagger ( <ref type="bibr" target="#b34">Schmid, 1994)</ref> for Czech and French POS tags, LoPar <ref type="bibr" target="#b35">(Schmid, 2000</ref>) for German POS and morpholog- ical tags, and MXPOST <ref type="bibr" target="#b32">(Ratnaparkhi, 1998)</ref> for English POS tags. These tools are recommended <ref type="bibr">7</ref> http://www.ims.uni-stuttgart.de/ forschung/ressourcen/korpora/tiger.html on the Moses website. 8 As mentioned before, our goal is not to achieve state-of-the-art results, but rather to study what different components of the NMT architecture learn about word morphology. Please refer to <ref type="bibr" target="#b26">Mueller et al. (2013)</ref> for represen- tative POS and morphological tagging accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Supplementary Results</head><p>We report here results that were omitted from the paper due to the space limit. <ref type="table" target="#tab_5">Table 5</ref> shows en- coder results using different layers, languages, and representations (word/char-based). As noted in the paper, all the results consistently show that i) layer 1 performs better than layers 0 and 2; and ii) char- based representations are better than word-based for learning morphology. <ref type="table" target="#tab_6">Table 6</ref> shows that trans- lating into a morphologically-poor language (En- glish) leads to better source representations, and    <ref type="table" target="#tab_4">Table 7</ref>: POS accuracy and BLEU using decoder representations from different language pairs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: POS and morphological tagging accuracy of word-based and character-based models per word frequency in the training data. Best viewed in color.</figDesc><graphic url="image-2.png" coords="4,83.25,80.66,193.92,145.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Improvement in POS/morphology accuracy of character-based vs. word-based models for words unseen/seen in training, and for all words.</figDesc><graphic url="image-4.png" coords="4,83.25,270.78,207.39,138.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Increase in POS accuracy with char-vs. word-based representations per tag frequency in the training set; larger bubbles reflect greater gaps.</figDesc><graphic url="image-5.png" coords="4,306.80,270.78,207.38,138.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Confusion matrices for POS tagging using word-based and character-based representations.</figDesc><graphic url="image-8.png" coords="5,306.80,321.35,207.39,138.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: POS tagging accuracy using representations from layers 0 (word vectors), 1, and 2, taken from encoders of different language pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: POS and morphological tagging accuracy across layers. Layer 0: word vectors or charbased representations before the encoder; layers 1 and 2: representations after the 1st and 2nd layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effect of target language on representation quality of the Arabic source.</figDesc><graphic url="image-10.png" coords="6,306.80,80.66,207.39,138.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 shows</head><label>8</label><figDesc>Figure 8 shows POS and morphological tagging accuracy of word-based representations from the NMT encoders, as well as corresponding BLEU scores. As expected, translating into English is easier than translating into the morphologicallyricher Hebrew and German, resulting in higher BLEU scores. Despite their similar morphological systems, translating Arabic to Hebrew is worse than Arabic to German, which can be attributed to the richer Hebrew morphology compared to German. POS and morphology accuracies share an intriguing pattern: the representations that are learned when translating into English are better for predicting POS or morphology than those learned when translating into German, which are in turn better than those learned when translating into Hebrew. This is remarkable given that English is a morphologically-poor language that does not display many of the morphological properties that are found in the Arabic source. In contrast, German and Hebrew have richer morphologies, so one could expect that translating into them would make the model learn more about morphology.</figDesc><graphic url="image-9.png" coords="6,83.25,80.66,207.39,138.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>). The decoder's task is to use this represen- tation to generate the target sentence in a specific</figDesc><table>POS Accuracy 
BLEU 
Attn 
ENC 
DEC Ar-En En-Ar 

3 
89.62 43.93 24.69 13.37 
7 
74.10 50.38 11.88 
5.04 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 7 provides additional decoder results.</head><label>7</label><figDesc></figDesc><table>Layer 0 
Layer 1 
Layer 2 

Word/Char (POS) 

De 91.1/92.0 93.6/95.2 93.5/94.6 
Fr 92.1/92.9 95.1/95.9 94.6/95.6 
Cz 76.3/78.3 77.0/79.1 75.7/80.6 

Word/Char (Morphology) 

De 87.6/88.8 89.5/91.2 88.7/90.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>POS and morphology accuracy on pre-
dicted tags using word-and char-based represen-
tations from different layers of *-to-En systems. 

Source 
Target English Arabic Self 

German 
93.5 
92.7 89.3 
Czech 
75.7 
75.2 71.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Impact of changing the target language 
on POS tagging accuracy. Self = German/Czech 
in rows 1/2 respectively. 

En-De En-Cz De-En Fr-En 

POS 
53.6 
36.3 
53.3 
54.1 
BLEU 
23.4 
13.9 
29.6 
37.8 

</table></figure>

			<note place="foot" n="1"> Our code is available at https://github.com/ boknilev/nmt-repr-analysis.</note>

			<note place="foot" n="2"> We also experimented with a linear classifier and observed similar trends to the non-linear case, but overall lower results; Qian et al. (2016b) reported similar findings.</note>

			<note place="foot" n="3"> The results are not far below dedicated taggers (e.g. 95.1/84.1 on Arabic POS/morphology (Pasha et al., 2014)), indicating that NMT models learn quite good representations.</note>

			<note place="foot" n="4"> We found this result to be also true in French, German, and Czech experiments; see appendix A.3.</note>

			<note place="foot" n="5"> In this section we only experiment with predicted tags as there are no parallel data with gold POS/morphological tags that we are aware of. 6 Note that the decoder results are above a majority baseline of 20%, so the decoder is still learning something about the target language.</note>

			<note place="foot" n="8"> http://www.statmt.org/moses/?n=Moses. ExternalTools</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Helmut Schmid for provid-ing the Tiger corpus, members of the MIT Spoken Language Systems group for helpful comments, and the three anonymous reviewers for their use-ful suggestions. This research was carried out in collaboration between the HBKU Qatar Comput-ing Research Institute (QCRI) and the MIT Com-puter Science and Artificial Intelligence Labora-tory (CSAIL).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04207</idno>
		<title level="m">Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segmentation for English-to-Arabic Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Badr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers. Columbus, Ohio, HLT-Short &apos;08</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers. Columbus, Ohio, HLT-Short &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="153" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-Scale Machine Translation between Arabic and Hebrew: Available Corpora and Initial Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semitic Machine Translation</title>
		<meeting>the Workshop on Semitic Machine Translation<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural versus PhraseBased Machine Translation Quality: a Case Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="257" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Arabic-Hebrew parallel corpus of TED talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semitic Machine Translation</title>
		<meeting>the Workshop on Semitic Machine Translation<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">WIT 3 : Web Inventory of Transcribed and Translated Talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16 th Conference of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 16 th Conference of the European Association for Machine Translation (EAMT)<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Character-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonollosa</surname></persName>
		</author>
		<ptr target="http://anthology.aclweb.org/P16-2058" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="357" to="361" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Investigating the Usefulness of Generalized Word Representations in SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C14-1041" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hindi-to-Urdu Machine Translation through Transliteration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P10-1048" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="465" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations, simple recurrent networks, and grammatical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="195" to="225" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieke</forename><surname>Gelderloos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1124" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1309" to="1319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Google&apos;s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<title level="m">Exploring the Limits of Language Modeling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>´ Akos Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alishahi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.08952</idno>
		<title level="m">Representation of linguistic form and function in recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and Understanding Recurrent Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://github.com/harvardnlp/seq2seq-attn" />
		<title level="m">Seq2seq-attn</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06615</idno>
		<title level="m">Character-aware Neural Language Models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factored Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D07-1091" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL). Association for Computational Linguistics</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL). Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="868" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Empirical Methods for Compound Splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E03-1076" />
	</analytic>
	<monogr>
		<title level="m">10th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What&apos;s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Köhn</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1246" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2067" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stanford Neural Machine Translation Systems for Spoken Language Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation. Da Nang</title>
		<meeting>the International Workshop on Spoken Language Translation. Da Nang<address><addrLine>Vietnam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D. Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1100</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1100" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1054" to="1063" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D10-1015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient Higher-Order CRFs for Morphological Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1032" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Combining Word-Level and Character-Level Models for Machine Translation Between Closely-Related Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P12-2059" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="301" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving SMT quality with morpho-syntactic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonja</forename><surname>Nieflen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/C00-2162" />
	</analytic>
	<monogr>
		<title level="m">The 18th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arfath</forename><surname>Pasha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Al-Badrashiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Eskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Pooleery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1094" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyzing Linguistic Knowledge in Sequential Model of Sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Peng Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1079" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="826" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Investigating Language Universal and Specific Properties in Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Peng Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1140" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1478" to="1488" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Maximum Entropy Models for Natural Language Ambiguity Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Challenging Language-Dependent Segmentation for Arabic: An Application to Machine Translation and Part-of-Speech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Part-of-Speech Tagging with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Computational Linguistics (Coling</title>
		<meeting>the 15th International Conference on Computational Linguistics (Coling<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="172" to="176" />
		</imprint>
	</monogr>
	<note>Coling</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">LoPar: Design and Implementation. Bericht des Sonderforschungsbereiches &quot;Sprachtheoretische Grundlagen fr die Computerlinguistik&quot; 149, Institute for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>University of Stuttgart</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Does String-Based Neural MT Learn Source Syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1159" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Víctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sánchez-Cartagena</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/E17-1100" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers. Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1063" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Word Representation Models for Morphologically Rich Languages in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04217</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/863" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="371" to="383" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
