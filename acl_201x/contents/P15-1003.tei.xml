<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Encoding Source Language with Convolutional Neural Network for Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Noah&apos;s Ark Lab, Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">ADAPT Centre</orgName>
								<orgName type="department" key="dep2">School of Computing</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Encoding Source Language with Convolutional Neural Network for Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="20" to="30"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The recently proposed neural network joint model (NNJM) (Devlin et al., 2014) augments the n-gram target language model with a heuristically chosen source context window, achieving state-of-the-art performance in SMT. In this paper, we give a more systematic treatment by summarizing the relevant source information through a convolutional architecture guided by the target information. With different guiding signals during decoding , our specifically designed convolu-tion+gating architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English translation tasks show that the proposed model can achieve significant improvements over the previous NNJM by up to +1.08 BLEU points on average.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning of continuous space representation for source language has attracted much at- tention in both traditional statistical machine translation (SMT) and neural machine trans- lation (NMT). Various models, mostly neural network-based, have been proposed for repre- senting the source sentence, mainly as the en- coder part in an encoder-decoder framework ( <ref type="bibr">Bengio et al., 2003;</ref><ref type="bibr" target="#b0">Auli et al., 2013;</ref><ref type="bibr" target="#b8">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b3">Cho et al., 2014;</ref><ref type="bibr" target="#b19">Sutskever et al., 2014</ref>). There has been some quite recent work on encoding only "relevant" part of source sentence during the decoding process, most notably neural network joint model (NNJM) in <ref type="bibr" target="#b4">(Devlin et al., 2014</ref>), which extends the n-grams target language model by additionally taking a fixed-length window of source sentence, achieving state-of-the-art per- formance in statistical machine translation.</p><p>In this paper, we propose novel convolu- tional architectures to dynamically encode the relevant information in the source language. Our model covers the entire source sentence, but can effectively find and properly summa- rize the relevant parts, guided by the informa- tion from the target language. With the guiding signals during decoding, our specifically de- signed convolution architectures can pinpoint the parts of a source sentence that are relevant to predicting a target word, and fuse them with the context of entire source sentence to form a unified representation. This representation, to- gether with target words, are fed to a deep neu- ral network (DNN) to form a stronger NNJM. Since our proposed joint model is purely lexi- calized, it can be integrated into any SMT de- coder as a feature.</p><p>Two variants of the joint model are also proposed, with coined name tagCNN and inCNN, with different guiding signals used from the decoding process. We integrate the proposed joint models into a state-of-the-art dependency-to-string translation system (Xie et al., 2011) to evaluate their effectiveness. Experiments on NIST Chinese-English trans- lation tasks show that our model is able to achieve significant improvements of +2.0 BLEU points on average over the baseline. Our model also outperforms <ref type="bibr" target="#b4">Devlin et al. (2014)</ref>'s NNJM by up to +1.08 BLEU points.  RoadMap: In the remainder of this paper, we start with a brief overview of joint language model in Section 2, while the convolutional en- coders, as the key component of which, will be described in detail in Section 3. Then in Sec- tion 4 we discuss the decoding algorithm with the proposed models. The experiment results are reported in Section 5, followed by Section 6 and 7 for related work and conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Joint Language Model</head><p>Our joint model with CNN encoders can be il- lustrated in <ref type="figure" target="#fig_1">Figure 1</ref> (a) &amp; (b), which consists 1) a CNN encoder, namely tagCNN or inCNN, to represent the information in the source sen- tences, and 2) an NN-based model for predict- ing the next words, with representations from CNN encoders and the history words in target sentence as inputs.</p><p>In the joint language model, the probabil- ity of the target word e n , given previous k target words {e n−k , · · ·, e n−1 } and the repre- sentations from CNN-encoders for source sen- tence S are tagCNN: p(e n |φ 1 (S, {a(e n )}), {e} n−1 n−k ) inCNN: p(e n | φ 2 (S, h({e} n−1 n−k )), {e} n−1 n−k ), where φ 1 (S, {a(e n )}) stands for the represen- tation given by tagCNN with the set of indexes {a(e n )} of source words aligned to the target word e n , and φ 2 (S, h({e} n−1 n−k )) stands for the representation from inCNN with the attention signal h({e} n−1 n−k ).</p><p>Let us use the example in <ref type="figure" target="#fig_1">Figure 1</ref>, where the task is to translate the Chinese sentence into English. In evaluating a target lan- guage sequence "holds parliament and presidential", with "holds parliament and" as the proceeding words (assume 4-gram LM), and the affiliated source word 1 of "presidential" being "ZˇongtˇongZˇongtˇZˇongtˇong" (determined by word align- ment), tagCNN generates φ 1 (S, {4}) (the in- dex of "ZˇongtˇongZˇongtˇZˇongtˇong" is 4), and inCNN gener- ates φ 2 (S, h(holds parliament and)). The DNN component then takes "holds parliament and" and (φ 1 or φ 2 ) as input to give the con- ditional probability for next word, e.g., p("presidential"|φ 1|2 , {holds, parliament, and}).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convolutional Models</head><p>We start with the generic architecture for convolutional encoder, and then proceed to tagCNN and inCNN as two extensions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generic CNN Encoder</head><p>The basic architecture is of a generic CNN en- coder is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref> (a), which has a fixed architecture consisting of six layers:</p><p>Layer-0: the input layer, which takes words in the form of embedding vectors. In our work, we set the maximum length of sen- tences to 40 words. For sentences shorter than that, we put zero padding at the be- ginning of sentences.</p><p>Layer-1: a convolution layer after Layer-0, with window size = 3. As will be dis- cussed in Section 3.2 and 3.3, the guid- ing signal are injected into this layer for "guided version".</p><p>Layer-2: a local gating layer after Layer- 1, which simply takes a weighted sum over feature-maps in non-adjacent win- dow with size = 2.</p><p>Layer-3: a convolution layer after Layer-2, we perform another convolution with window size = 3.</p><p>Layer-4: we perform a global gating over feature-maps on Layer-3.</p><p>Layer-5: fully connected weights that maps the output of Layer-4 to this layer as the final representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Convolution</head><p>As shown in <ref type="figure" target="#fig_2">Figure 2</ref> (a), the convolution in Layer-1 operates on sliding windows of words (width k 1 ), and the similar definition of win- dows carries over to higher layers. Formally, for source sentence input x = {x 1 , · · · , x N }, the convolution unit for feature map of type-f (among F of them) on Layer-is</p><formula xml:id="formula_0">z (,f ) i (x) = σ(w (,f ) ˆ z (−1) i + b (,f ) ), = 1, 3, f = 1, 2, · · · , F (1)</formula><p>where</p><formula xml:id="formula_1">• z (,f ) i (x)</formula><p>gives the output of feature map of type-f for location i in Layer-;</p><p>• w (,f ) is the parameters for f on Layer-;</p><p>• σ(·) is the Sigmoid activation function;</p><p>• ˆ z (−1) i denotes the segment of Layer-−1 for the convolution at location i , whilê</p><formula xml:id="formula_2">whilê z (0) i def = [x i , x i+1 , x i+2 ]</formula><p>concatenates the vectors for 3 words from sentence input x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Gating</head><p>Previous CNNs, including those for NLP tasks ( <ref type="bibr">Hu et al., 2014;</ref><ref type="bibr" target="#b8">Kalchbrenner et al., 2014</ref>), take a straightforward convolution- pooling strategy, in which the "fusion" deci- sions (e.g., selecting the largest one in max- pooling) are based on the values of feature- maps. This is essentially a soft template match- ing, which works for tasks like classification, but harmful for keeping the composition func- tionality of convolution, which is critical for modeling sentences. In this paper, we propose to use separate gating unit to release the score function duty from the convolution, and let it focus on composition.</p><p>We take two types of gating: 1) for Layer- 2, we take a local gating with non-overlapping windows (size = 2) on the feature-maps of con- volutional Layer-1 for representation of seg- ments, and 2) for Layer-4, we take a global gating to fuse all the segments for a global rep- resentation. We found that this gating strategy can considerably improve the performance of both tagCNN and inCNN over pooling.</p><p>• Local Gating: On Layer-1, for every gat- ing window, we first find its original in- put (before convolution) on Layer-0, and merge them for the input of the gating net- work. For example, for the two windows: word (3,4,5) and word (4,5,6) on Layer-0, we use concatenated vector consisting of embedding for word (3,4,5,6) as the input of the local gating network (a logistic re- gression model) to determine the weight for the convolution result of the two win- dows (on Layer-1), and the weighted sum are the output of Layer-2.</p><p>• Global Gating: On Layer-3, for feature- maps at each location i, denoted z <ref type="formula" target="#formula_4">(3)</ref> i , the global gating network (essentially soft- max, parameterized w g ), assigns a nor- malized weight</p><formula xml:id="formula_3">ω(z (3) i ) = e w g z (3) i / j e w g z (3) j ,</formula><p>and the gated representation on Layer- 4 is given by the weighted sum i ω(z</p><formula xml:id="formula_4">(3) i )z<label>(3)</label></formula><p>i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Training of CNN encoders</head><p>The CNN encoders, including tagCNN and inCNN that will be discussed right below, are trained in a joint language model described in Section 2, along with the following parameters</p><p>• the embedding of the words on source and the proceeding words on target;</p><p>• the parameters for the DNN of joint lan- guage model, include the parameters of soft-max for word probability.</p><p>The training procedure is identical to that of neural network language model, except that the parallel corpus is used instead of a monolin- gual corpus. We seek to maximize the log- likelihood of training samples, with one sam- ple for every target word in the parallel corpus. Optimization is performed with the conven- tional back-propagation, implemented as sto- chastic gradient descent ( <ref type="bibr" target="#b12">LeCun et al., 1998</ref>) with mini-batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">tagCNN</head><p>tagCNN inherits the convolution and gating from generic CNN (as described in Section 3.1), with the only modification in the input layer. As shown in <ref type="figure" target="#fig_2">Figure 2</ref> (b), in tagCNN, we append an extra tagging bit (0 or 1) to the embedding of words in the input layer to indi- cate whether it is one of affiliated words</p><formula xml:id="formula_5">x (AFF) i = [x i 1] , x (NON-AFF) j = [x j 0] .</formula><p>Those extended word embedding will then be treated as regular word-embedding in the con- volutional neural network. This particular en- coding strategy can be extended to embed more complicated dependency relation in source lan- guage, as will be described in Section 5.4. This particular "tag" will be activated in a parameterized way during the training for pre- dicting the target words. In other words, the supervised signal from the words to predict will find, through layers of back-propagation, the importance of the tag bit in the "affiliated words" in the source language, and learn to put proper weight on it to make tagged words stand out and adjust other parameters in tagCNN accordingly for the optimal predictive perfor- mance. In doing so, the joint model can pin- point the parts of a source sentence that are rel- evant to predicting a target word through the already learned word alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">inCNN</head><p>Unlike tagCNN, which directly tells the loca- tion of affiliated words to the CNN encoder, inCNN sends the information about the pro- ceeding words in target side to the convolu- tional encoder to help retrieve the information relevant for predicting the next word. This is essentially a particular case of attention model, analogous to the automatic alignment mecha- nism in ( <ref type="bibr" target="#b3">Bahdanau et al., 2014)</ref>, where the at- tention signal is from the state of a generative recurrent neural network (RNN) as decoder. Basically, the information from proceeding words, denoted as h({e} n−1 n−k ), is injected into every convolution window in the source lan- guage sentence, as illustrated in <ref type="figure" target="#fig_2">Figure 2 (c)</ref>. More specifically, for the window indexed by t, the input to convolution is given by the con- catenated vectorˆz</p><formula xml:id="formula_6">vectorˆ vectorˆz t = [h({e} n−1 n−k ), x t , x t+1 , x t+2 ] .</formula><p>In this work, we use a DNN to transform the vector concatenated from word-embedding for words {e n−k · · · , e n−k } into h({e} n−1 n−k ), with sigmoid activation function. Through lay- ers of convolution and gating, inCNN can 1) retrieve the relevant segments of source sen- tences, and 2) compose and transform the retrieved segments into representation recog- nizable by the DNN in predicting the words in target language. Different from that of tagCNN, inCNN uses information from pro- ceeding words, hence provides complementary information in the augmented joint language model of tagCNN. This has been empirically verified when using feature based on tagCNN and that based on inCNN in decoding with greater improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoding with the Joint Model</head><p>Our joint model is purely lexicalized, and therefore can be integrated into any SMT de- coders as a feature. For a hierarchical SMT decoder, we adopt the integrating method pro- posed by <ref type="bibr" target="#b4">Devlin et al. (2014)</ref>. As inherited from the n-gram language model for perform- ing hierarchical decoding, the leftmost and rightmost n − 1 words from each constituent should be stored in the state space. We ex- tend the state space to also include the in- dexes of the affiliated source words for each of these edge words. For an aligned target word, we take its aligned source words as its affiliated source words. And for an unaligned word, we use the affiliation heuristic adopted by <ref type="bibr" target="#b4">Devlin et al. (2014)</ref>. In this paper, we in- tegrate the joint model into the state-of-the-art dependency-to-string machine translation de- coder as a case study to test the efficacy of our proposed approaches. We will briefly describe the dependency-to-string translation model and then the description of MT system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dependency-to-String Translation</head><p>In this paper, we use a state-of-the-art dependency-to-string (Xie et al., 2011) decoder (Dep2Str), which is also a hierarchical de- coder. This dependency-to-string model em- ploys rules that represent the source side as head-dependents relations and the target side as strings. A head-dependents relation (HDR) is composed of a head and all its dependents in dependency trees. <ref type="figure" target="#fig_3">Figure 3</ref> shows a depen- dency tree (a) with three HDRs (in shadow), an example of HDR rule (b) for the top level of (a), and an example of head rule (c). HDR rules are constructed from head-dependents re- lations. HDR rules can act as both translation rules and reordering rules. And head rules are used for translating source words.</p><p>We adopt the decoder proposed by <ref type="bibr" target="#b13">Meng et al. (2013)</ref> as a variant of Dep2Str trans- lation that is easier to implement with com- parable performance. Basically they extract the HDR rules with GHKM ( <ref type="bibr">Galley et al., 2004</ref>) algorithm. For the decoding procedure, given a source dependency tree T , the de- coder transverses T in post-order. The bottom- up chart-based decoding algorithm with cube pruning <ref type="bibr" target="#b2">(Chiang, 2007;</ref><ref type="bibr" target="#b7">Huang and Chiang, 2007</ref>) is used to find the k-best items for each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MT Decoder</head><p>Following <ref type="bibr" target="#b14">Och and Ney (2002)</ref>, we use a gen- eral loglinear framework. Let d be a derivation that convert a source dependency tree into a tar- get string e. The probability of d is defined as:</p><formula xml:id="formula_7">P (d) ∝ i φ i (d) λ i<label>(2)</label></formula><p>where φ i are features defined on derivations and λ i are the corresponding weights. Our de- coder contains the following features: Baseline Features:</p><p>• translation probabilities P (t|s) and P (s|t) of HDR rules;</p><p>• lexical translation probabilities P LEX (t|s) and P LEX (s|t) of HDR rules;</p><p>• rule penalty exp(−1);</p><p>• pseudo translation rule penalty exp(−1);</p><p>• target word penalty exp(|e|);</p><p>• n-gram language model P LM (e);</p><p>Proposed Features:</p><p>• n-gram tagCNN joint language model P TLM (e);</p><p>• n-gram inCNN joint language model P ILM (e).</p><p>Our baseline decoder contains the first eight features. The pseudo translation rule (con- structed according to the word order of a HDR) is to ensure the complete translation when no matched rules is found during decoding. The weights of all these features are tuned via minimum error rate training (MERT) . For the dependency-to-string decoder, we set rule-threshold and stack-threshold to 10 −3 , rule-limit to 100, stack-limit to 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The experiments in this Section are designed to answer the following questions:</p><p>1. Are our tagCNN and inCNN joint lan- guage models able to improve translation quality, and are they complementary to each other?</p><p>2. Do inCNN and tagCNN benefit from their guiding signal, compared to a generic CNN?</p><p>3. For tagCNN, is it helpful to embed more dependency structure, e.g., dependency head of each affiliated word, as additional information?</p><p>4. Can our gating strategy improve the per- formance over max-pooling?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Data: Our training data are extracted from LDC data 2 . We only keep the sentence pairs that the length of source part no longer than 40 words, which covers over 90% of the sen- tence. The bilingual training data consist of 221K sentence pairs, containing 5.0 million Chinese words and 6.8 million English words. The development set is NIST MT03 (795 sen- tences) and test sets are MT04 (1499 sen- tences) and MT05 (917 sentences) after filter- ing with length limit.</p><p>Preprocessing: The word alignments are ob- tained with GIZA++ (Och and Ney, 2003) on the corpora in both directions, using the "grow- diag-final-and" balance strategy ( <ref type="bibr" target="#b10">Koehn et al., 2003</ref> Optimization of NN: In training the neural network, we limit the source and target vocab- ulary to the most frequent 20K words for both Chinese and English, covering approximately 97% and 99% of two corpus respectively. All the out-of-vocabulary words are mapped to a special token UNK. We used stochastic gradient descent to train the joint model, setting the size of minibatch to 500. All joint models used a 3- word target history (i.e., 4-gram LM). The di- mension of word embedding and the attention signal h({e} n−1 n−k ) for inCNN are 100. For the convolution layers (Layer 1 and Layer 3), we apply 100 filters. And the final representation of CNN encoders is a vector with dimension 100. The final DNN layer of our joint model is the standard multi-layer perceptron with soft- max at the top layer.</p><p>Metric: We use the case-insensitive 4- gram NIST BLEU 3 as our evaluation met- ric, with statistical significance test with sign- test <ref type="bibr">(Collins et al., 2005</ref>) between the proposed models and two baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Setting for Model Comparisons</head><p>We use the tagCNN and inCNN joint lan- guage models as additional decoding fea- tures to a dependency-to-string baseline sys- tem (Dep2Str), and compare them to the neu- ral network joint model with 11 source con- text words <ref type="bibr" target="#b4">(Devlin et al., 2014</ref>). We use the implementation of an open source toolkit <ref type="bibr">4</ref> with default configuration except the global settings described in Section 5.1. Since our tagCNN and inCNN models are source-to- target and left-to-right (on target side), we only take the source-to-target and left-to-right type NNJM in <ref type="bibr" target="#b4">(Devlin et al., 2014</ref>) in compari- son. We call this type NNJM as BBN-JM here- after. Although the BBN-JM in <ref type="bibr" target="#b4">(Devlin et al., 2014</ref>) is originally tested in the hierarchical phrase-based <ref type="bibr" target="#b2">(Chiang, 2007)</ref> SMT and string- to-dependency ( <ref type="bibr" target="#b17">Shen et al., 2008</ref>) SMT, it is fairly versatile and can be readily integrated into Dep2Str.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Main Results</head><p>The main results of different models are given in <ref type="table">Table 1</ref>. Before proceeding to more detailed comparison, we first observe that</p><p>• the baseline Dep2Str system gives BLEU 0.5+ higher than the open-source phrase- based system Moses ( <ref type="bibr" target="#b11">Koehn et al., 2007</ref>);</p><p>• BBN-JM can give about +0.92 BLEU score over Dep2Str, a result similar as re- ported in <ref type="bibr" target="#b4">(Devlin et al., 2014</ref>).   <ref type="table">Table 2</ref>: BLEU-4 scores (%) of tagCNN model with dependency head words as addi- tional tags (tagCNN dep).</p><p>Clearly from <ref type="table">Table 1</ref>, tagCNN and inCNN improve upon the Dep2Str baseline by +1.28 and +1.75 BLEU, outperforming BBN-JM in the same setting by respectively +0.36 and +0.83 BLEU, averaged on NIST MT04 and MT05. These indicate that tagCNN and inCNN can individually provide discrimina- tive information in decoding. It is worth not- ing that inCNN appears to be more informative than the affiliated words suggested by the word alignment (GIZA++). We conjecture that this is due to the following two facts</p><p>• inCNN avoids the propagation of mis- takes and artifacts in the already learned word alignment;</p><p>• the guiding signal in inCNN provides complementary information to evaluate the translation.</p><p>Moreover, when tagCNN and inCNN are both used in decoding, it can further increase its winning margin over BBN-JM to +1.08 BLEU points (in the last row of <ref type="table">Table 1</ref>), indicating that the two models with different guiding sig- nals are complementary to each other.  <ref type="table">Table 3</ref>: BLEU-4 scores (%) of inCNN mod- els implemented with gating strategy and k max-pooling, where k is of {2, 4, 8}.</p><p>power of CNN-based encoder, as can be eas- ily seen from the difference between the BLEU scores achieved by generic CNN, tagCNN, and inCNN. Indeed, with the signal from the al- ready learned word alignment, tagCNN can gain +0.25 BLEU over its generic counterpart, while for inCNN with the guiding signal from the proceeding words in target, the gain is more saliently +0.72 BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Dependency Head in tagCNN</head><p>In this section, we study whether tagCNN can further benefit from encoding richer depen- dency structure in source language in the input. More specifically, the dependency head words can be used to further improve tagCNN model. As described in Section 3.2, in tagCNN, we append a tagging bit (0 or 1) to the embedding of words in the input layer as tags on whether they are affiliated source words. To incorpo- rate dependency head information, we extend the tagging rule in Section 3.2 to add another tagging bit (0 or 1) to the word-embedding for original tagCNN to indicate whether it is part of dependency heads of the affiliated words. For example, if x i is the embedding of an af- filiated source word and x j the dependency head of word x i , the extended input of tagCNN would contain</p><formula xml:id="formula_8">x (AFF, NON-HEAD) i = [x i 1 0] x (NON-AFF, HEAD) j = [x j 0 1]</formula><p>If the affiliated source word is the root of a sentence, we only append 0 as the second tag- ging bit since the root has no dependency head. From <ref type="table">Table 2</ref>, with the help of dependency head information, we can improve tagCNN by +0.23 BLEU points averagely on two test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Gating Vs. Max-pooling</head><p>In this section, we investigate to what extent that our gating strategy can improve the trans- lation performance over max pooling, with the comparisons on inCNN model as a case study. For implementation of inCNN with max- pooling, we replace the local-gating (Layer-2) with max-pooling with size 2 (2-pooling for short), and global gating (Layer-4) with k max- pooling ("k-pooling"), where k is of {2, 4, 8}. Then, we use the mean of the outputs of k- pooling as the final input of Layer-5. In do- ing so, we can guarantee the input dimension of Layer-5 is the same as the architecture with gating. From <ref type="table">Table 3</ref>, we can clearly see that our gating strategy can improve translation performance over max-pooling by 0.34∼0.71 BLEU points. Moreover, we find 8-pooling yields performance better than 2-pooling. We conjecture that this is because the useful rel- evant parts for translation are mainly concen- trated on a few words of the source sentence, which can be better extracted with a larger pool size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The seminal work of neural network language model (NNLM) can be traced to <ref type="bibr">Bengio et al. (2003)</ref> on monolingual text. It is recently ex- tended by <ref type="bibr" target="#b4">Devlin et al. (2014)</ref> to include ad- ditional source context (11 source words) in modeling the target sentence, which is clearly most related to our work, with however two im- portant differences: 1) instead of the ad hoc way of selecting a context window in <ref type="bibr" target="#b4">(Devlin et al., 2014</ref>), our model covers the entire source sentence and automatically distill the context relevant for target modeling; 2) our convo- lutional architecture can effectively leverage guiding signals of vastly different forms and nature from the target. Prior to our model there is also work on representing source sentences with neural net- works, including RNN ( <ref type="bibr" target="#b3">Cho et al., 2014;</ref><ref type="bibr" target="#b19">Sutskever et al., 2014</ref>) and CNN <ref type="bibr" target="#b8">(Kalchbrenner and Blunsom, 2013)</ref>. These work typi- cally aim to map the entire sentence to a vec- tor, which will be used later by RNN/LSTM- based decoder to generate the target sentence. As demonstrated in Section 5, the representa- tion learnt this way cannot pinpoint the rele- vant parts of the source sentences (e.g., words or phrases level) and therefore is inferior to be directly integrated into traditional SMT de- coders.</p><p>Our model, especially inCNN, is inspired by is the automatic alignment model proposed in ( <ref type="bibr" target="#b3">Bahdanau et al., 2014</ref>). As the first effort to apply attention model to machine transla- tion, it sends the state of a decoding RNN as attentional signal to the source end to obtain a weighted sum of embedding of source words as the summary of relevant context. In con- trast, inCNN uses 1) a different attention sig- nal extracted from proceeding words in partial translations, and 2) more importantly, a con- volutional architecture and therefore a highly nonlinear way to retrieve and summarize the relevant information in source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We proposed convolutional architectures for obtaining a guided representation of the entire source sentence, which can be used to augment the n-gram target language model. With differ- ent guiding signals from target side, we devise tagCNN and inCNN, both of which are tested in enhancing a dependency-to-string SMT with +2.0 BLEU points over baseline and +1.08 BLEU points over the state-of-the-art in <ref type="bibr" target="#b4">(Devlin et al., 2014</ref>). For future work, we will con- sider encoding more complex linguistic struc- tures to further enhance the joint model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration for joint LM based on CNN encoder.</figDesc><graphic url="image-2.png" coords="2,304.68,71.82,191.34,180.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration for the CNN encoders.</figDesc><graphic url="image-4.png" coords="3,95.67,70.86,403.95,134.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration for a dependency tree (a) with three head-dependents relations in shadow, an example of head-dependents relation rule (b) for the top level of (a), and an example of head rule (c). "X 1 :NN" indicates a substitution site that can be replaced by a subtree whose root has part-of-speech "NN". The underline denotes a leaf node.</figDesc><graphic url="image-17.png" coords="5,174.89,127.54,85.77,58.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Systems</head><label></label><figDesc></figDesc></figure>

			<note place="foot" n="1"> For an aligned target word, we take its aligned source words as its affiliated source words. And for an unaligned word, we inherit its affiliation from the closest aligned word, with preference given to the right (Devlin et al., 2014). Since the word alignment is of many-to-many, one target word may has multi affiliated source words.</note>

			<note place="foot" n="2"> The corpora include LDC2002E18, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T06.</note>

			<note place="foot" n="3"> ftp://jaguar.ncsl.nist.gov/mt/ resources/mteval-v11b.pl</note>

			<note place="foot" n="4"> http://nlg.isi.edu/software/nplm/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Meng, Wang, Jiang and Liu are supported by National Natural Science Foundation of China (Contract 61202216). Liu is partially supported by the Science Foundation Ireland (Grant 12/CE/I2267 and 13/RC/2106) as part of the ADAPT Centre at Dublin City Univer-sity. We sincerely thank the anonymous re-viewers for their thorough reviewing and valu-able suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28</title>
		<meeting>the 28</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<idno type="arXiv">arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Bengio, Rjean Ducharme, Pascal Vincent, and Christian Jauvin</editor>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>A neural probabilistic language model</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Michael Collins</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
	<note>Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Baltimore, Maryland; Mark Hopkins, Kevin Knight</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
	<note>Michel Galley. and Daniel Marcu</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/NAACL</title>
		<editor>Boston. [Hu et al.2014] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen</editor>
		<meeting>HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Forest rescoring: Faster decoding with integrated language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting-Association For Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>ward Grefenstette, and Phil Blunsom</editor>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
	<note>Recurrent continuous translation models</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast exact inference with a factored model for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2002] Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical phrasebased translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Czech Republic</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Translation with source constituency and dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1066" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative training and maximum entropy models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Ney2002] Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="295" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Och and Ney2003</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new string-to-dependency machine translation algorithm with a target dependency language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on spoken language processing</title>
		<meeting>the international conference on spoken language processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
	<note>Stolcke and others2002</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1409.3215</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A novel dependency-to-string model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="216" to="226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
