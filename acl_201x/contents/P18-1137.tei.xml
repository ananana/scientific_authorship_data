<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tailored Sequence to Sequence Models to Different Conversation Scenarios</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hainan</forename><surname>Zhang</surname></persName>
							<email>zhanghainan@software.ict.ac.cn, {lanyanyan, guojiafeng, junxu, cxq}@ict.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CAS Key Lab of Network Data Science and Technology</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tailored Sequence to Sequence Models to Different Conversation Scenarios</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1479" to="1488"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sequence to sequence (Seq2Seq) models have been widely used for response generation in the area of conversation. However , the requirements for different conversation scenarios are distinct. For example , customer service requires the generated responses to be specific and accurate , while chatbot prefers diverse responses so as to attract different users. The current Seq2Seq model fails to meet these diverse requirements, by using a general average likelihood as the optimization criteria. As a result, it usually generates safe and commonplace responses , such as &apos;I don&apos;t know&apos;. In this paper , we propose two tailored optimization criteria for Seq2Seq to different conversation scenarios, i.e., the maximum generated likelihood for specific-requirement scenario, and the conditional value-at-risk for diverse-requirement scenario. Experimental results on the Ubuntu dialogue corpus (Ubuntu service scenario) and Chinese Weibo dataset (social chatbot scenario) show that our proposed models not only satisfies diverse requirements for different scenarios, but also yields better performances against traditional Seq2Seq models in terms of both metric-based and human evaluations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper focuses on the problem of the single- turn dialogue generation, which is critical in many natural language processing applications such as customer services, intelligent assistant and chat- bot. Recently, sequence to sequence (Seq2Seq) models <ref type="bibr" target="#b15">(Sutskever et al., 2014</ref>) have been widely used in this area. In these Seq2Seq models, a re- current neural network (RNN) based encoder is first utilized to encode the input post to a vec- tor, and another RNN decoder is then used to au- tomatically generate the response word by word. The parameters of the encoder and decoder are learned by maximizing the averaged likelihood of the training data.</p><p>It is clear that the requirements for generated responses are distinct in different dialogue sce- narios. For instance, in the scenario of customer service or mobile assistant, users mainly expect the system to help them solve a problem. There- fore, the responses should be specific and accu- rate to provide useful assistance. For example, if the user asks a question 'How can I get the AMD driver running on Ubuntu 12.10?', the sys- tem is expected to reply 'The fglrx driver is in the repo. But it may depend on your exact chipset.', rather than 'I do not know about the package.', even though the latter can also be viewed as rel- evant for the proposed question. We called this kind of scenario as specific-requirement scenario. While in other scenarios such as chatbot, users are interacting with the dialogue system for fun. Therefore, the generated responses should be di- verse to attract different users. Take the post 'Can you recommend me a tourist city?' as an example. If the user prefers the magnificent mountains and rivers, it is better to reply 'You may like the Bern- ina Express to the Alps'. While if the user loves literature, it is better to reply 'Paris is a beautiful city with full of the literary atmosphere'. This kind of scenario is called diverse-requirement scenario.</p><p>However, the current generation model Seq2Seq ( <ref type="bibr" target="#b15">Sutskever et al., 2014</ref>) usually tend to generate common responses, such as 'I don't know' and 'What does this mean?' ( <ref type="bibr">Li et al., 2016a,b;</ref>, which fails to meet diverse requirements for different conversation scenarios. Intrinsically, conversation is a typical one-to-many application, i.e., multiple responses with different semantic meanings are correspon- dent to a same post. That means there are various post-response matching patterns in the training data. Seq2Seq optimizes an averaged likelihood, so it can only capture the common matching patterns, leading to common responses.</p><p>The purpose of this paper is to propose two tailored optimization criteria for Seq2Seq mod- els to accommodate different conversation scenar- ios, i.e. specific-requirement scenario and diverse- requirement scenario. The key idea is to how cap- ture the required post-response matching patterns. For the specific-requirement scenario, we define the maximum generated likelihood as the objec- tive function. With this kind of criterion, we just require one ground-truth response to be close to the given post, instead of requiring the average of multiple ground-truth responses to be close to the post. Therefore, the most significant post-response matching pattern will be learned from the data, to facilitate generating a specific response. While for the diverse-requirement scenario, the condi- tional value-at-risk (CVaR) is used as the objective function. CVaR is a risk-sensitive function widely used in finances ( <ref type="bibr" target="#b13">Rockafellar and Uryasev, 2002;</ref><ref type="bibr" target="#b0">Alexander et al., 2006;</ref>, defined to assessing the likelihood (at a specific confidence level) that a specific loss will exceed the value at risk. With CVaR as the objective function, the worst 1-α responses are required to be close to the post, therefore various post-response patterns can be captured, and the learned model has the ability to generate diverse responses.</p><p>We use public data to evaluate our pro- posed models. For the specific-requirement sce- nario, the experiments on public Ubuntu dia- logue corpus(Ubuntu service) show that optimiz- ing the maximum generated likelihood produces more specific and accurate responses than tradi- tional Seq2Seq models. While for the diverse- requirement scenario, the experiments on the pub- lic Chinese Weibo dataset (social chatbot) show that optimizing CVaR produces diverse responses, as compared with Seq2Seq and the variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The basic neural-based Seq2Seq framework for dialogue generation is inspired by the studies of statistical machine translation. <ref type="bibr" target="#b15">Sutskever et al. (Sutskever et al., 2014)</ref> proposed the origi- nal Seq2Seq framework(Seq2Seq), which used a multilayered Long Short-Term Memory(LSTM) to map the input sequence to a fixed dimension vector and then used another LSTM to decode the target sequence from the vector. Then <ref type="bibr" target="#b4">Cho et al. (Cho et al., 2014</ref>) followed the above archi- tecture, and proposed to feed the last hidden state of encoder to every cell of decoder(RNN-encdec), which enhanced the influence of contexts in gen- erating each word of the targets. To further alle- viate the long dependency problem, <ref type="bibr" target="#b2">Bahdanau et al. (Bahdanau et al., 2015</ref>) introduced the attention mechanism into the neural network and achieved encouraging performances(Seq2Seq-att). Many studies ( <ref type="bibr" target="#b14">Shang et al., 2015;</ref><ref type="bibr" target="#b18">Vinyals and Le, 2015)</ref> directly applied the above neural SMT models to the task of dialogue generation, and gained some promising performances.</p><p>Although the current Seq2Seq model is capable to generate fluent responses, these responses are usually general. Therefore, many researchers fo- cused on how to improve the generation quality and specification. <ref type="bibr" target="#b8">Li et al. (Li et al., 2016a</ref>) pro- posed a mutual information model(MMI) to tackle this problem. However, it is not a unified train- ing model, instead it still trained original Seq2Seq model, and used the Maximum Mutual Informa- tion criterion only for testing to rerank the primary top-n list. <ref type="bibr" target="#b12">Mou et al. (Mou et al., 2017)</ref> proposed a forward-backward keyword method which used a pointwise mutual information to predict a noun as a keyword and then used two Seq2Seq models to generate the forward sentence and the backward sentence. <ref type="bibr" target="#b19">Xing et al. (Xing et al., 2017</ref>) proposed a joint attention mechanism model, which modified the generation probability by adding the topic key- words likelihood to the generated maximum like- lihood with extra corpus. The recent works such as seqGAN (  and Adver-REGS ( <ref type="bibr" target="#b12">Li et al., 2017</ref>) try to use Generative Adversarial Net- works(GAN) for generation, where the discrimi- nator scores are used as rewards for reinforcement learning.</p><p>For the study of generating diverse responses, <ref type="bibr" target="#b17">Vijayakumar et al. (Vijayakumar et al., 2016</ref>) in- troduced a diverse beam search which decoded a list of diverse outputs by optimizing for a diversity-augmented objective, which can control for the exploration and exploitation of the search space. Zhou (  proposed to apply a hidden state as a generating style(Mechanism). They make an assumption that some latent re- sponding mechanisms can generate different re- sponses, and model these mechanisms as latent embedding. With these latent embedding in the mid of Seq2Seq, the mechanism-aware Seq2Seq can generate different mechanism responses.</p><p>However, most of these models are using an av- eraged approach for optimization, similar to that in Seq2Seq. This paper proposes two new crite- ria for different conversation scenarios. For the specific-requirement scenario, the maximum gen- erated likelihood is used as the objective function. While for the diverse-requirement scenario, CVaR is used for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequence to Sequence Models</head><p>We first introduce the typical LSTM-based Seq2Seq framework ( <ref type="bibr" target="#b2">Bahdanau et al., 2015</ref>) used in dialogue generation.</p><p>Given a post X = {x 1 , . . . , x M } as the input, a standard LSTM first maps the input sequence to a fixed-dimension vector h M as follows.</p><formula xml:id="formula_0">i k = σ(W i [h k−1 , w k ]), f k = σ(W f [h k−1 , w k ]), o k = σ(W o [h k−1 , w k ]), l k = tanh(W l [h k−1 , w k ]), c k = f k c k−1 + i k l k , h i = o k tanh(c k ),<label>(1)</label></formula><p>where i k , f k and o k are the input gate, the mem- ory gate, and the output gate, respectively. w k is the word embedding for x k , and h k stands for the vector computed by LSTM at time k by combin- ing w k and h k−1 . c k is the cell at time k, and σ de- notes the sigmoid function.</p><formula xml:id="formula_1">W i , W f , W o and W l are parameters.</formula><p>Then another LSTM is used as the decoder to map the vector h M to the ground-truth response Y = {y 1 , · · · , y N }. Typically, the decoder is trained to predict the next word g i , given the con- text vector h M and the previous generated words {g 1 , . . . , g i−1 }. In other words, the decoder de- fines a probability over the output Y by decom- posing the joint probability into the ordered con- ditionals by chain rule in the probability theory:</p><formula xml:id="formula_2">P (Y |X) = N i=1 p(y i |h M , y 1 , . . . , y i−1 ) = N i=1 g(h M , y i−1 , h i ),</formula><p>where g θ is a softmax function, h i is the hidden state in the decoder LSTM.</p><p>Usually the attention mechanism is further in- troduced to the above Seq2Seq framework in real applications. Instead of using h M as the con- text vector in the decoder, we let the context vec- tor, denoted as s i , to be dependent on the se- quence (h 1 , · · · , h M ). Each h k contains informa- tion about the input sequence with a strong focus on the parts surrounding the k-th word of the input sentence. The context vector s i is then computed as a weighted sum of these h k :</p><formula xml:id="formula_3">s i = M k=1 α ik h k .</formula><p>The weight α ik of each representation h k is com- puted by:</p><formula xml:id="formula_4">α ik = exp (e ik ) M j=1 exp (e ij ) , e ik = v T tanh(W 1 h i−1 + W 2 h k ),</formula><p>where v T ,W 1 and W 2 are learned parameters. e ik is an alignment model which scores how well the inputs around position k and the output at position i match. The score is based on the LSTM hidden state h i−1 (just before emitting y i ), and h k of the input sentence.</p><p>Given a set of training data D, Seq2Seq assumes that data are i.i.d. sampled from a probability P , and uses the following log likelihood as the objec- tive for maximization:</p><formula xml:id="formula_5">L = (X,Y )∈D log P (Y |X).<label>(2)</label></formula><p>4 Tailored Sequence to Sequence Models</p><p>We can see that a general averaged likelihood of the training data is used as the objective func- tion in Seq2Seq. However, this objective func- tion is usually criticized for generating common responses, such as 'I don't know' and 'What does this mean?'. Clearly, this kind of responses can- not satisfy either the specific or the diverse re- quirements. The underlying reason is not diffi- cult to understand. Intrinsically, conversation is a typical one-to-many application, i.e., multiple responses with different semantic meanings are correspondent to a same post. That means there are various post-response matching patterns in the training data. If we optimize an averaged likeli- hood, we can only capture the common matching patterns, which leads to generating common re- sponses. Therefore, if we want to generate specific responses, we need to capture the most significant matching pattern; while if we want to generate di- verse responses, we need to define a criteria which has the ability to capture the various matching pat- terns. Motivated by this idea, we propose two op- timization criteria, i.e. maximum generated likeli- hood, and CVaR, to adapt two different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Maximum Generated Likelihood Criteria</head><p>To meet the specific requirement, we need to capture a specific matching pattern between post and response, rather than the common match- ing pattern. Therefore, instead of optimizing the averaged likelihood, we turn to use the max- imum generated likelihood (MGL) as the ob- jective function. Mathematically, for a given post X and its associated ground-truth responses</p><formula xml:id="formula_6">(Y (1) X , Y (2) X , · · · , Y (m X ) X</formula><p>), the objective function is defined as:</p><formula xml:id="formula_7">L = X m X max k=1 log P (Y (k) X |X).</formula><p>From the definition, we can see that we aim to capture the most significant post-response match- ing pattern in the training data. Therefore, the learned model can output specific responses for a given post. Since there is a max operator in the objective function, which is difficult for accurate optimization, we approximate it by the softmax function. Then the objective function becomes the following form:</p><formula xml:id="formula_8">L = X m X k=1 log P (Y (k) X |X) m X j=1 P (Y (j) X |X)</formula><p>.</p><p>If the probability for one ground-truth Y (k) X is small, it contributes little to the objective func- tion. That is to say, we just require the top ground- truth responses with relative large probabilities to be close to the post.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CVaR Criteria</head><p>To meet the diverse requirements, we need to capture various matching patterns between post and its multiple ground-truth responses. There- fore, instead of optimizing the averaged likeli- hood, we turn to optimize the conditional value- at-risk, named CVaR for short. CVaR is a promi- nent risk measure used extensively in finance, and it is proved to be coherent ( <ref type="bibr" target="#b1">Artzner et al., 1999</ref>) and numerically effective ( <ref type="bibr" target="#b7">Krokhmal et al., 2002;</ref><ref type="bibr" target="#b16">Uryasev, 2013)</ref>.</p><p>The definitions of VaR and CVaR are as follows. For a confidence level α ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, and a continu- ous random cost Z whose distribution is parame- terized by a controllable parameter θ, the α-VaR of the cost Z, denoted by ν α (θ), is defined as:</p><formula xml:id="formula_9">ν α (θ)= inf{ν ∈ R|P (Z ≤ ν) ≥ α}.</formula><p>α-VaR denotes the maximum cost that might be incurred with probability at least α, or can be sim- ply regarded as the α-quantile of Z. And the α- CVaR, denoted by Φ α (θ), is defined as:</p><formula xml:id="formula_10">Φ α (θ)= 1 1 − α 1 α ν r (θ)dr=E θ [Z|Z ≥ ν α (θ)].</formula><p>It can be viewed as the expected cost over the (1 − α) worst outcomes of Z.</p><p>Applying CVaR to generating diverse re- sponses, we can define the random cost Z as − log P (Y |X), the corresponding CVaR is:</p><formula xml:id="formula_11">Φ α (θ) = 1 1 − α 1 α ν r (θ)dr,</formula><p>where ν r (θ) = inf{ν ∈ R|P (− log P (Y |X) ≤ ν) ≥ r}, and θ are parameters of the Seq2Seq model. Therefore, we have:</p><formula xml:id="formula_12">ν r (θ) = inf{ν ∈ R|P (P (Y |X) ≥ e ν ) ≥ r}.</formula><p>Therefore, for a given post X and its ground- truth responses (Y</p><formula xml:id="formula_13">(1) X , Y (2) X , · · · , Y (m X ) X</formula><p>), opti- mizing CVaR is equivalent to maximizing the fol- lowing objective function:</p><formula xml:id="formula_14">L = X 1 1 − α Y (k) X ∈Y 1−α P (Y (k) X |X),</formula><p>where Y 1−α is a collection of ground-truth re- sponses such that:</p><formula xml:id="formula_15">sup{P (Y (i) X |X) : Y i X ∈ Y 1−α } ≤ α.</formula><p>We can see that maximizing the above objec- tive function requires the worst 1 − α responses to be close to the post. Therefore, we aim to cap- ture each distinct post-response matching pattern by optimizing the CVaR criteria, which can meet the requirement for generating diverse responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we conduct experiments on both specific-requirement and diverse-requirement sce- narios, to evaluate the performances of our pro- posed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>We use two public datasets in our experiments. For the specific-requirement scenario, we use the Ubuntu dialogue corpus 1 extracted from Ubuntu question-answering forum, named Ubuntu ( <ref type="bibr" target="#b11">Lowe et al., 2015</ref>). The original training data consists of 7 million conversational post-responses pairs from 2014 to April 27,2012. The validation data are conversational pairs from April 27,2014 to Au- gust 7,2012, and the test data are from August 7,2012 to December 1,2012. We set the number of positive examples as 4,000,000 in the Github to directly sample data from the whole corpus. Then we construct post and response pairs based on the period from both context and utterance. We also conduct some data pro-processing. For ex- ample, we use the official script to tokenize, stem and lemmatize, and the duplicates and sentences with length less than 5 or longer than 50 are re- moved. Finally, we obtain 3,200,000, 100,000 and 100,000 for training, validation and testing, re- spectively.</p><p>For the diverse-requirement scenario, we use the Chinese Weibo dataset, named STC ( <ref type="bibr" target="#b14">Shang et al., 2015</ref>). It consists of 3,788,571 post- response pairs extracted from the Chinese Weibo website and cleaned by the data publishers. We randomly split the data to training, validation, and testing sets, which contains 3,000,000, 388,571 and 400,000 pairs, respectively. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baseline Methods</head><p>Six baseline methods are used for compari- son, including traditional Seq2Seq ( <ref type="bibr" target="#b15">Sutskever et al., 2014</ref>), RNN-encdec ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>), Seq2Seq with attention(Seq2Seq-att) ( <ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>, mutual information(MMI) ( <ref type="bibr" target="#b9">Li et al., 2016b</ref>), Adver-REGS ( <ref type="bibr" target="#b12">Li et al., 2017)</ref> and Mech- anism model ( . Here are some empirical settings. We first introduce the input em-beddings. For STC, we utilize character-level em- beddings rather than word-level embeddings, due to the word sparsity, segmentation mistakes and unknown Chinese words which may lead to infe- rior performance ( <ref type="bibr" target="#b6">Hu et al., 2015)</ref>. For Ubuntu, we use word embeddings trained by word2vec on the training dataset. In the training process, the dimension is set to be 300, the size of negative sample is set to be 3, and the learning rate is 0.05. For fair comparison among all the base- line methods and our methods, the number of hid- den nodes is all set to 300, and batch size is set to 200. Stochastic gradient decent (SGD) is uti- lized in our experiment for optimization, instead of Adam, because SGD yields better performances in our experiments. The learning rate is set to be 0.5, and adaptively decays with rate 0.99 in the op- timization process. We run our model on a Tesla K80 GPU card with Tensorflow framework. All the methods are pretrained with the same Seq2Seq model. For maximum generated likelihood(MGL) model, some people may argue that the specific results may be due to the usage of single post- response pair. Thus we also implement the base- line of using a single post-response pair, by ran- dom selecting the response from the ground-truth for each post, denoted as Single Model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Evaluation Measures</head><p>We use both quantitative metrics and human judgements to evaluate the proposed MGL model and the CVaR model. Specifically, we use two kinds of metrics for quantitative comparisons. The first one kind is the traditional metric, including PPL and Bleu score ( <ref type="bibr" target="#b19">Xing et al., 2017</ref>). They are both widely used in natural language processing, and here we use them to evaluate the quality of the generated responses. The other kind is to eval- uate the specific degree 3 in <ref type="figure">(Li et al., 2016a,b)</ref>. It measures the specific degree of the generated responses, by calculating the number of distinct unigrams and bigrams in the generated responses, denoted as distinct. If a model usually generates common responses, the distinct will be low.</p><p>For the diverse-requirement scenario, we define two measures to evaluate the performance. Specif- ically, we set the beam as 10. Group-diversity is  defined to calculate the difference between each two generations for one post, denoted as divrs.</p><p>Group-overlap is defined to calculate the over- lap between each two generations for one post, denoted as overlap. The detailed definitions are shown as follows.</p><formula xml:id="formula_16">divrs = 1 N N i=1 X i cosine(G i1 , G i2 ), overlap = 1 N N i=1 X i overlap(G i1 , G i2 ),</formula><p>where G i1 and G i2 are the generated responses from the model for post X, cosine(G i1 , G i2 ) is the cosine similarity, and the overlap(G i1 , G i2 ) is defined as the intersection divided by union. For human evaluation, given 200 randomly sampled post and it's generated responses, three annotators, randomly selected from a class of computer science majored students(48 students), are required to give 3-graded judgements. The an- notation criteria are defined as follows:</p><p>1. the response is nonfluent or has wrong logic; or the response is fluent but not related with the post; 2. the response is fluent and weak related, but it's common which can reply many other posts; 3. the response is fluent and strong related with its post, which is like following a real per- son's tone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Specific-Requirement Scenario</head><p>We demonstrate the experimental results on the specific-requirement scenario, based on the Ubuntu dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Metric-based Evaluation</head><p>The quantitative evaluation results are shown in  with the Single model, MGL is also better because MGL considers more data in the model computation process. The distinct scores of MGL are also higher than baseline models, which indi- cate that our model can generate more specific re- sponses. That's because it has the ability to learn the significant matching pattern between post and responses, by optimizing the maximum generated likelihood rather than the averaged one. In sum- mary, our maximum generated likelihood model produces more fluent and specific results, as com- pared with baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Human Evaluation</head><p>The human evaluation results are shown in Ta- ble 2, in which the percentage of sentences be- longing to each grade and the averaged grade are demonstrated to evaluate the quality of generated responses. Kappa <ref type="bibr" target="#b5">(Fleiss, 1971)</ref> value is presented to demonstrate the consistency of different anno- tators. From the results, we can see that MGL significantly outperforms baseline methods. The averaged score of MGL Model is 2.18, which is much higher than that of MMI and Adver-REGS, i.e., 1.78 and 1.9, respectively. The percentage of post Install grub to the hdd where the bios is set to boot. response1</p><p>So I would not just install it to /boot? Seq2Seq-att A log file a bug with the install cd. MMI What do you mean? Adver-REGS How do I install grub? Single Do you delete grub? MGL I have boot with the window from the grub and it go from the boot menu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CVaR</head><p>If you have grub on the same pc, it is 0 0 boot partition.</p><p>post I want to have a backup , sad I use up all my space with dual boot and game. response1</p><p>Oh you can get a sata to usb adapt : ) Seq2Seq-att What about your computer ? MMI It is not in the install , you can do that. Adver-REGS Game game game game. Single And other user will do too ? MGL And do you buy direct driver from the kernel and it cause run ? CVaR</p><p>Yes , you can also use a text file to your file .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>post</head><p>Take a look at install . response1</p><p>I am to cd to the directories contain the folder. Seq2Seq-att</p><p>How do you install it? MMI I think it be a good idea to do that. Adver-REGS I have no idea what I am looking for. Single I think it is a bite , but I do not know a good thing to do that. I am use. MGL I think so, I have a lot of nautilus. I am already install. CVaR I just install it from synaptics, but I want to install it on the same repository. strongly related sentences (i.e., the grade '3') of MGL Model is 51%, which is also higher than that of MMI, Adver-REGS and Single Model, i.e., 20% , 32% and 37%. In summary, our max- imum generated likelihood model produces better responses compared with baselines. As compared with MMI and Adver-REGS, both the metric- based improvements and human evaluation im- provements of MGL are significant on Ubuntu datasets (p-value &lt; 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Case Study</head><p>Here we show some generated responses for demonstration. Specifically, <ref type="table" target="#tab_3">Table 3</ref> gives one ex- ample post and its ground-truth responses from Ubuntu. We also list the generated responses from different models. We can see that Seq2Seq-att, MMI and Adver-REGS all produce common re- sponses, such as 'What do you mean?','I have no idea what I am looking for.' and'What about your computer?'. Our models give interesting re- sponses with specific meanings. Take the post 'In- stall grub to the hdd where the bios is set to boot.' as an example, our model conveys more specific information by replying 'I have boot with the win- dow from the grub and it go from the boot menu.' . And in another case, for the given post 'I want to have a backup , sad I use up all my space with dual boot and game.', our MGL model generates a question for the post 'And do you buy direct driver from the kernel and it cause run?', which is more intelligent. Similar observations have been ob- tained for many other posts, and we omit them for space limitations.   of different models on STC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Diverse-Requirement Scenario</head><p>Now we introduce the experimental results for the diverse-requirement scenario, based on STC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Parameters Setting</head><p>First, we study the influences of different parame- ter α in CVaR. Specifically, we show the validation result with α ranging from 0 to 0.9 with step 0.1, to see the change of CVaR performances. <ref type="figure">Figure 1</ref> show the results of different α in terms of divrs , overlap, distinct-2 and PPL. From the results, we can see that the performances of divrs , overlap and PPL are all changing in a similar trend, i.e. first drop and then increase. The best α for CVaR is 0.3, which is used in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Metric-based Evaluation</head><p>The quantitative evaluation results are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Human Evaluation</head><p>The human evaluation results are shown in <ref type="table" target="#tab_7">Ta- ble 5</ref>. From the results, we can see MGL and CVaR models achieve comparable results, which are significantly better than baseline meth- ods. Specifically, the averaged score of MGL and CVaR is 2.15 and 1.995, which is significantly higher than that of Adver-REGS and Mechanism, i.e., 1.83 and 1.775, respectively. The percentage of strongly related sentences (i.e., the grade '3') of MGL Model and CVaR are 52% and 44%, which are also significantly higher than that of Adver- REGS and Mechanism, i.e., 31.5% and 30%. We conducted significant test for the improvement.</p><p>As compared with Adver-REGS and Mechanism, both the metric-based improvements and human evaluation improvements of CVaR are significant on STC datasets (p-value &lt; 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Case Study</head><p>Here we show some generated responses for demonstration. Specifically,  post and its three ground-truth responses from STC. We also give three generated responses from Mechanism and CVaR model. We can see that Mechanism produces responses with the same meaning, such as 'Wade is so amazing' and 'It is really good'. However, our CVaR models give specific responses with diverse meanings. Take the post 'Waiting for Wade in the final games.' for example, CVaR's responses are related to differ- ent topics. The response 'I must go and see the final games ' focuses on the game, while another response of 'James is so fast ' focuses on the per- son, James. For the other case, the post is about the docking of two spacecrafts and the CVaR re- sponses are related to different users, such as the supporter of the event, the newspaper reader and the children who have a father concerned with the current news . We have obtained similar observa- tions for many other posts, but we have to omit them for space limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose two new optimization criteria for Seq2Seq model to adapt different con- versation scenario. For the specific-requirement scenario, such as customer service, which requires specific and high quality responses, maximum generated likelihood is used as the objective func- tion instead of the averaged one. While for the diverse-requirement, such as chatbot, which re- quires diverse and high quality responses even if for the same post, CVaR is used as the objec- tive function for worst case optimization. Ex- perimental results on both specific-requirement (Ubuntu data) and diverse-requirement scenarios (STC data) demonstrate that the proposed opti- mization criteria can meet the corresponding re- quirement, yielding better performances against traditional Seq2Seq models in terms of both metric-based and human evaluations. The contribution of this paper is to use tailored Seq2Seq model for different conversation scenar- ios. The study shows that if we want to gener- ate specific responses, it is important to design the model to learn the most significant matching pattern between post and response. While if we want to generate diverse responses, a risk-sensitive objective functions is helpful. In future work, we plan to further investigate the impact of risk- sensitive objective functions, including the rela- tions between model robustness and diverse gen- erations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 . From the results, we can see that both</head><label>1</label><figDesc></figDesc><table>model 
human score distribution(%) 
Ave. 
Kappa 
1 
2 
3 
Seq2Seq-att 
46.5 38.6 
14.9 
1.684 0.387 
MMI 
42 
38 
20 
1.78 
0.395 
Adver-REGS 
42 
26 
32 
1.9 
0.379 
Single 
49 
14 
37 
1.88 
0.383 
MGL 
33 
16 
51 
2.18 
0.372 
CVaR 
40 
12 
48 
2.08 
0.381 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The comparisons of different models by 
human evaluation on Ubuntu. 

MMI and Adver-REGS outperform Seq2Seq base-
lines in terms of BLUE, PPL and distinct mea-
sures. That's because both MMI and Adver-
REGS further consider some reward functions in 
the optimization process to encourage specific re-
sults. Specifically, MMI uses a predefined re-
ward function to penalize generating common re-
sponses, and Adver-REGS uses a learned discrim-
inator to define the reward function. Our MGL 
model obtains higher BLEU and lower PPL than 
baseline models. Take the BLEU score on Ubuntu 
dataset for example, the BLEU score of MGL 
model is 1.354, which is significantly better than 
that of MMI and Adver-REGS, i.e., 1.297 and 
1.279. These results indicate that our MGL gen-
erates responses with higher quality. When com-
pared </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The generated responses from different 
models on Ubuntu. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The metric-based evaluation results(%) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>From the results, we can see that 
both Adver-REGS and Mechanism outperform 
Seq2Seq models in terms of BLUE and PPL mea-
sures. That's because they both use some tech-
niques to enhance the generation ability. Adver-
REGS uses a learned discriminator to define the 
reward function, while Mechanism uses a style </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The comparisons of different models by 
human evaluation on STC. 

hidden state to describe the generation mecha-
nism. Both MGL and CVaR obtain better results 
in terms of BLUE and PPL, compared with other 
baselines. These results indicate that our pro-
posed models generate more fluent responses in 
the diverse-requirement scenario. As for the evalu-
ation for the diversity, we can see that CVaR model 
obtains the lowest overlap and divrs among all the 
baseline models. Take the overlap score on STC 
for example, the overlap score of CVaR model is 
38.86, which is significantly lower than that of 
Adver-REGS, Mechanism and GLM, i.e., 57.96, 
57.67 and 66.92. These results indicate that our 
CVaR model can generate responses with higher 
diversity. That's because it has the capability to 
capture various matching patterns in the training 
data, by optimizing the worst 1 − α costs. There-
fore, our CVaR model produces both fluent and di-
verse results, as compared with baseline methods. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 6 gives one</head><label>6</label><figDesc></figDesc><table>post 
.(Waiting for Wade in the final games.) 
response 
(Everyone has his favorite stars.) 
response 
(Analysis is much better than Sina) 
response 
(Waiting for the explosion of Mr.Flash) 

Mechanism 
!!(Wade is really great! Support him!) 
Mechanism 
(Wade will be better) 
Mechanism 
!(Wade is mighty) 

CVaR 
!(I must go and see the final games) 
CVaR 
(James is so fast) 

CVaR 
, 
(The final games is a blow for the opposite. Heat come on) 

post 
. 
(Shenzhou 8 spacecraft and Tiangong-1 has the second successful docking) 
response 
(Hope other aspects will soon lead the world) 

response 

Strong technology and close cooperation contributed to this success 
response 
.(Next will have a human in it) 

Mechanism 
?(Will broadcast in the Weekend Newspaper? ) 
Mechanism 
(It is really good) 
Mechanism 
(It is really good, they should be together) 

CVaR 
(Yes, they should insisted on being together) 
CVaR 
(Are you see it in the front page of the newspaper?) 

CVaR 

(It is really good, you could recommend it to your father if you have time) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>The generated responses from different 
models on STC. 

</table></figure>

			<note place="foot" n="1"> https://github.com/rkadlec/ubuntu-ranking-datasetcreator 2 https://github.com/zhanghainan/TailoredSeq2Seq2 DifferentConversationScenarios</note>

			<note place="foot" n="3"> Though it is named as diversity in Li&apos;s paper, this diversity is not the same as that used in our paper. This diversity measures the specific degree of the generated responses over all generations. While the diversity used in our paper means that the responses are required to be relevant to a post from different aspects.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Minimizing cvar and var for a portfolio of derivatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Banking and Finance</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="583" to="605" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coherent measures of risk mathematical finance 9</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Artzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Delbaen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Eber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Finance Theory Modeling Implementation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="228" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Technical note-a risk-averse newsvendor model under the cvar criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe George</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1040" to="1044" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joseph L Fleiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<publisher>American Psychological Association</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangze</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05865</idno>
		<title level="m">Lcsts: A large scale chinese short text summarization dataset</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Portfolio optimization with conditional value-at-risk objective and constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Krokhmal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Palmquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Uryasev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Risk</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="11" to="27" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A diversity-promoting objective function for neural conversation models. The North American Chapter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adversarial learning for neural dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional value-at-risk for general loss distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyrrell</forename><surname>Rockafellar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Uryasev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Banking and Finance</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Probabilistic constrained optimization: methodology and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Uryasev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science and Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasath</forename><forename type="middle">R</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3351" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mechanism-aware neural machine for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganbin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3400" to="3407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
