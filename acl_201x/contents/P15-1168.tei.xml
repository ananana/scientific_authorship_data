<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Recursive Neural Network for Chinese Word Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gated Recursive Neural Network for Chinese Word Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1744" to="1753"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability of alleviating the burden of manual feature engineering. However, the previous neural models cannot extract the complicated feature compositions as the traditional methods with discrete features. In this paper, we propose a gated recursive neural network (GRNN) for Chinese word segmen-tation, which contains reset and update gates to incorporate the complicated combinations of the context characters. Since GRNN is relative deep, we also use a supervised layer-wise training method to avoid the problem of gradient diffusion. Experiments on the benchmark datasets show that our model outperforms the previous neural network models as well as the state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlike English and other western languages, Chi- nese do not delimit words by white-space. There- fore, word segmentation is a preliminary and im- portant pre-process for Chinese language process- ing. Most previous systems address this problem by treating this task as a sequence labeling prob- lem and have achieved great success. Due to the nature of supervised learning, the performance of these models is greatly affected by the design of features. These features are explicitly represented by the different combinations of context charac- ters, which are based on linguistic intuition and sta- tistical information. However, the number of fea- tures could be so large that the result models are too large to use in practice and prone to overfit on training corpus. <ref type="figure">Figure 1</ref>: Illustration of our model for Chinese word segmentation. The solid nodes indicate the active neurons, while the hollow ones indicate the suppressed neurons. Specifically, the links denote the information flow, where the solid edges de- note the acceptation of the combinations while the dashed edges means rejection of that. As shown in the right figure, we receive a score vector for tag- ging target character "地" by incorporating all the combination information.</p><p>Recently, neural network models have been in- creasingly focused on for their ability to minimize the effort in feature engineering. <ref type="bibr" target="#b3">Collobert et al. (2011)</ref> developed a general neural network archi- tecture for sequence labeling tasks. Following this work, many methods ( <ref type="bibr" target="#b25">Zheng et al., 2013;</ref><ref type="bibr" target="#b8">Pei et al., 2014;</ref><ref type="bibr" target="#b10">Qi et al., 2014</ref>) applied the neural net- work to Chinese word segmentation and achieved a performance that approaches the state-of-the-art methods.</p><p>However, these neural models just concatenate the embeddings of the context characters, and feed them into neural network. Since the concatena- tion operation is relatively simple, it is difficult to model the complicated features as the traditional discrete feature based models. Although the com- plicated interactions of inputs can be modeled by the deep neural network, the previous neural model shows that the deep model cannot outperform the one with a single non-linear model. Therefore, the neural model only captures the interactions by the simple transition matrix and the single non-linear transformation . These dense features extracted via these simple interactions are not nearly as good as the substantial discrete features in the traditional methods.</p><p>In this paper, we propose a gated recursive neu- ral network (GRNN) to model the complicated combinations of characters, and apply it to Chi- nese word segmentation task. Inspired by the suc- cess of gated recurrent neural network <ref type="bibr" target="#b2">(Chung et al., 2014</ref>), we introduce two kinds of gates to con- trol the combinations in recursive structure. We also use the layer-wise training method to avoid the problem of gradient diffusion, and the dropout strategy to avoid the overfitting problem. <ref type="figure">Figure 1</ref> gives an illustration of how our ap- proach models the complicated combinations of the context characters. Given a sentence "雨 (Rainy) 天 (Day) 地面 (Ground) 积水 (Accumu- lated water)", the target character is "地". This sentence is very complicated because each consec- utive two characters can be combined as a word. To predict the label of the target character "地" un- der the given context, GRNN detects the combina- tions recursively from the bottom layer to the top. Then, we receive a score vector of tags by incorpo- rating all the combination information in network.</p><p>The contributions of this paper can be summa- rized as follows:</p><p>• We propose a novel GRNN architecture to model the complicated combinations of the context characters. GRNN can select and pre- serve the useful combinations via reset and update gates. These combinations play a sim- ilar role in the feature engineering of the tra- ditional methods with discrete features.</p><p>• We evaluate the performance of Chinese word segmentation on PKU, MSRA and CTB6 benchmark datasets which are com- monly used for evaluation of Chinese word segmentation. Experiment results show that our model outperforms other neural network models, and achieves state-of-the-art perfor- mance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Model for Chinese Word Segmentation</head><p>Chinese word segmentation task is usually re- garded as a character-based sequence labeling  problem. Each character is labeled as one of {B, M, E, S} to indicate the segmentation. {B, M, E} represent Begin, Middle, End of a multi-character segmentation respectively, and S represents a Sin- gle character segmentation. The general neural network architecture for Chi- nese word segmentation task is usually character- ized by three specialized layers: (1) a character embedding layer; (2) a series of classical neural network layers and (3) tag inference layer. A il- lustration is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><formula xml:id="formula_0">Input Window Characters C i-2 C i-1 C i+1 C i+2 C i Lookup</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of tags</head><formula xml:id="formula_1">· · · Tag Inference f(t|1) f(t|2) f(t|i) f(t|n-1) f(t|n</formula><p>The most common tagging approach is based on a local window. The window approach assumes that the tag of a character largely depends on its neighboring characters.</p><p>Firstly, we have a character set C of size |C|. Then each character c ∈ C is mapped into an d- dimensional embedding space as c ∈ R d by a lookup table M ∈ R d×|C| .</p><p>For each character c i in a given sentence c 1:n , the context characters c i−w 1 :i+w2 are mapped to their corresponding character embeddings as c i−w 1 :i+w 2 , where w 1 and w 2 are left and right context lengths respectively. Specifically, the un- known characters and characters exceeding the sentence boundaries are mapped to special sym- bols, "unknown", "start" and "end" respectively. In addition, w 1 and w 2 satisfy the constraint w 1 + w 2 + 1 = w, where w is the window size of the model. As an illustration in <ref type="figure" target="#fig_0">Figure 2</ref>, w 1 , w 2 and w are set to 2, 2 and 5 respectively.</p><p>The embeddings of all the context characters are then concatenated into a single vector a i ∈ R H 1 as input of the neural network, where H 1 = w × d is the size of Layer 1. And a i is then fed into a con- ventional neural network layer which performs a linear transformation followed by an element-wise activation function g, such as tanh.</p><formula xml:id="formula_2">h i = g(W 1 a i + b 1 ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">W 1 ∈ R H 2 ×H 1 , b 1 ∈ R H 2 , h i ∈ R H 2 . H 2</formula><p>is the number of hidden units in Layer 2. Here, w, H 1 and H 2 are hyper-parameters chosen on devel- opment set. Then, a similar linear transformation is per- formed without non-linear function followed:</p><formula xml:id="formula_4">f (t|c i−w 1 :i+w 2 ) = W 2 h i + b 2 ,<label>(2)</label></formula><p>where W 2 ∈ R |T |×H 2 , b 2 ∈ R |T | and T is the set of 4 possible tags. Each dimension of vector f (t|c i−w 1 :i+w 2 ) ∈ R |T | is the score of the corre- sponding tag.</p><p>To model the tag dependency, a transition score A ij is introduced to measure the probability of jumping from tag i ∈ T to tag j ∈ T (Collobert et al., 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gated Recursive Neural Network for Chinese Word Segmentation</head><p>To model the complicated feature combinations, we propose a novel gated recursive neural network (GRNN) architecture for Chinese word segmenta- tion task (see <ref type="figure" target="#fig_1">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recursive Neural Network</head><p>A recursive neural network (RNN) is a kind of deep neural network created by applying the same set of weights recursively over a given struc- ture(such as parsing tree) in topological order <ref type="bibr" target="#b9">(Pollack, 1990;</ref><ref type="bibr" target="#b14">Socher et al., 2013a</ref>).</p><p>In the simplest case, children nodes are com- bined into their parent node using a weight matrix W that is shared across the whole network, fol- lowed by a non-linear function g(·). Specifically, if h L and h R are d-dimensional vector representa- tions of left and right children nodes respectively, their parent node h P will be a d-dimensional vec- tor as well, calculated as:</p><formula xml:id="formula_5">E M B S …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… c i-2 c i-1 c i c i+1 c i+2 ……</formula><formula xml:id="formula_6">h P = g ( W [ h L h R ]) ,<label>(3)</label></formula><p>where W ∈ R d×2d and g is a non-linear function as mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Gated Recursive Neural Network</head><p>The RNN need a topological structure to model a sequence, such as a syntactic tree. In this paper, we use a directed acyclic graph (DAG), as showing in <ref type="figure" target="#fig_1">Figure 3</ref>, to model the combinations of the input characters, in which two consecutive nodes in the lower layer are combined into a single node in the upper layer via the operation as Eq. <ref type="formula" target="#formula_6">(3)</ref>. In fact, the DAG structure can model the com- binations of characters by continuously mixing the information from the bottom layer to the top layer. Each neuron can be regarded as a complicated fea- ture composition of its governed characters, simi- lar to the discrete feature based models. The differ- ence between them is that the neural one automat- ically learns the complicated combinations while the conventional one need manually design them.</p><p>When the children nodes combine into their parent node, the combination information of two children nodes is also merged and preserved by their parent node.</p><p>Although the mechanism above seem to work well, it can not sufficiently model the complicated combination features for its simplicity in practice.</p><p>Inspired by the success of the gated recurrent neural network ( <ref type="bibr" target="#b1">Cho et al., 2014b;</ref><ref type="bibr" target="#b2">Chung et al., 2014</ref>), we propose a gated recursive neural net- work (GRNN) by introducing two kinds of gates, namely "reset gate" and "update gate". Specifi- cally, there are two reset gates, r L and r R , par- tially reading the information from left child and right child respectively. And the update gates z N , z L and z R decide what to preserve when combin- ing the children's information. Intuitively, these gates seems to decide how to update and exploit the combination information.</p><p>In the case of word segmentation, for each char- acter c i of a given sentence c 1:n , we first repre- sent each context character c j into its correspond- ing embedding c j , where i − w 1 ≤ j ≤ i + w 2 and the definitions of w 1 and w 2 are as same as mentioned above.</p><p>Then, the embeddings are sent to the first layer of GRNN as inputs, whose outputs are recursively applied to upper layers until it outputs a single fixed-length vector.</p><p>The outputs of the different neurons can be re- garded as the different feature compositions. After concatenating the outputs of all neurons in the net- work, we get a new big vector x i . Next, we receive the tag score vector y i for character c j by a linear transformation of x i :</p><formula xml:id="formula_7">y i = W s × x i + b s ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_8">b s ∈ R |T | , W s ∈ R |T |×Q . Q = q × d is di- mensionality of the concatenated vector x i , where</formula><p>q is the number of nodes in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Gated Recursive Unit</head><p>GRNN consists of the minimal structures, gated re- cursive units, as showing in <ref type="figure" target="#fig_2">Figure 4</ref>. By assuming that the window size is w, we will have recursion layer l ∈ <ref type="bibr">[1, w]</ref>. At each recursion layer l, the activation of the j-th hidden node h</p><formula xml:id="formula_9">(l) j ∈ R d is computed as h (l) j = { zN ⊙ ˆ h l j + zL ⊙ h l−1 j−1 + zR ⊙ h l−1 j , l &gt; 1, cj, l = 1,<label>(5)</label></formula><p>Gate z</p><p>Gate r L Gate r R h j-1 The update gates can be formalized as:</p><formula xml:id="formula_10">(l-1) h j (l-1) h j ^(l) h j (l)</formula><formula xml:id="formula_11">z =   z N z L z R   =   1/Z 1/Z 1/Z   ⊙ exp(U    ˆ h l j h l−1 j−1 h l−1 j   ),<label>(6)</label></formula><p>where U ∈ R 3d×3d is the coefficient of update gates, and Z ∈ R d is the vector of the normal- ization coefficients,</p><formula xml:id="formula_12">Z k = 3 ∑ i=1 [exp(U    ˆ h l j h l−1 j−1 h l−1 j   )] d×(i−1)+k , (7) where 1 ≤ k ≤ d.</formula><p>Intuitively, three update gates are constrained by:</p><formula xml:id="formula_13">           [z N ] k + [z L ] k + [z R ] k = 1, 1 ≤ k ≤ d; [z N ] k ≥ 0, 1 ≤ k ≤ d; [z L ] k ≥ 0, 1 ≤ k ≤ d; [z R ] k ≥ 0, 1 ≤ k ≤ d.<label>(8)</label></formula><p>The new activationˆhactivationˆ activationˆh l j is computed as:</p><formula xml:id="formula_14">ˆ h l j = tanh(W ˆ h [ r L ⊙ h l−1 j−1 r R ⊙ h l−1 j ] ),<label>(9)</label></formula><p>where</p><formula xml:id="formula_15">W ˆ h ∈ R d×2d , r L ∈ R d , r R ∈ R d</formula><p>. r L and r R are the reset gates for left child node h l−1 j−1 and right child node h l−1 j respectively, which can be formalized as:</p><formula xml:id="formula_16">[ r L r R ] = σ(G [ h l−1 j−1 h l−1 j ] ),<label>(10)</label></formula><p>where G ∈ R 2d×2d is the coefficient of two reset gates and σ indicates the sigmoid function. Intuiativly, the reset gates control how to select the output information of the left and right chil- dren, which results to the current new activationˆh activationˆ activationˆh.</p><p>By the update gates, the activation of a parent neuron can be regarded as a choice among the the current new activationˆhactivationˆ activationˆh, the left child, and the right child. This choice allows the overall structure to change adaptively with respect to the inputs.</p><p>This gating mechanism is effective to model the combinations of the characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>In Chinese word segmentation task, it is usually to employ the Viterbi algorithm to inference the tag sequence t 1:n for a given input sentence c 1:n .</p><p>In order to model the tag dependencies, the previous neural network models <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr" target="#b25">Zheng et al., 2013;</ref><ref type="bibr" target="#b8">Pei et al., 2014</ref>) intro- duce a transition matrix A, and each entry A ij is the score of the transformation from tag i ∈ T to tag j ∈ T . Thus, the sentence-level score can be formu- lated as follows:</p><formula xml:id="formula_18">s(c 1:n , t 1:n , θ) = n ∑ i=1 ( A t i−1 t i + f θ (t i |c i−w 1 :i+w 2 ) ) ,<label>(12)</label></formula><p>where f θ (t i |c i−w 1 :i+w 2 ) is the score for choosing tag t i for the i-th character by our proposed GRNN (Eq. <ref type="formula" target="#formula_7">(4)</ref>). The parameter set of our model is</p><formula xml:id="formula_19">θ = (M, W s , b s , W ˆ h , U, G, A).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Layer-wise Training</head><p>Deep neural network with multiple hidden layers is very difficult to train for its problem of gradient diffusion and risk of overfitting.</p><p>Following <ref type="bibr" target="#b6">(Hinton and Salakhutdinov, 2006</ref>), we employ the layer-wise training strategy to avoid problems of overfitting and gradient vanishing. The main idea of layer-wise training is to train the network with adding the layers one by one. Specif- ically, we first train the neural network with the first hidden layer only. Then, we train at the net- work with two hidden layers after training at first layer is done and so on until we reach the top hid- den layer. When getting convergency of the net- work with layers 1 to l , we preserve the current parameters as initial values of that in training the network with layers 1 to l + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Max-Margin Criterion</head><p>We use the Max-Margin criterion ( <ref type="bibr" target="#b18">Taskar et al., 2005</ref>) to train our model. Intuitively, the Max- Margin criterion provides an alternative to prob- abilistic, likelihood based estimation methods by concentrating directly on the robustness of the de- cision boundary of a model. We use Y (x i ) to de- note the set of all possible tag sequences for a given sentence x i and the correct tag sequence for x i is y i . The parameter set of our model is θ. We first define a structured margin loss ∆(y i , ˆ y) for pre- dicting a tag sequencê y for a given correct tag se- quence y i :</p><formula xml:id="formula_20">∆(y i , ˆ y) = n ∑ j η1{y i,j ̸ = ˆ y j },<label>(13)</label></formula><p>where n is the length of sentence x i and η is a dis- count parameter. The loss is proportional to the number of characters with an incorrect tag in the predicted tag sequence. For a given training in- stance (x i , y i ), we search for the tag sequence with the highest score:</p><formula xml:id="formula_21">y * = arg maxˆy∈Y maxˆ maxˆy∈Y (x) s(x i , ˆ y, θ),<label>(14)</label></formula><p>where the tag sequence is found and scored by the proposed model via the function s(·) in Eq. (12). The object of Max-Margin training is that the tag sequence with highest score is the correct one: y * = y i and its score will be larger up to a margin to other possible tag sequencesˆysequencesˆ sequencesˆy ∈ Y (x i ):</p><formula xml:id="formula_22">s(x, y i , θ) ≥ s(x, ˆ y, θ) + ∆(y i , ˆ y).<label>(15)</label></formula><p>This leads to the regularized objective function for m training examples:</p><formula xml:id="formula_23">J(θ) = 1 m m ∑ i=1 l i (θ) + λ 2 ∥θ∥ 2 2 ,<label>(16)</label></formula><formula xml:id="formula_24">l i (θ) = maxˆy∈Y maxˆ maxˆy∈Y (x i ) (s(x i , ˆ y, θ)+∆(y i , ˆ y))−s(x i , y i , θ).<label>(17)</label></formula><p>By minimizing this object, the score of the correct tag sequence y i is increased and score of the high- est scoring incorrect tag sequencê y is decreased. The objective function is not differentiable due to the hinge loss. We use a generalization of gradient descent called subgradient method ( <ref type="bibr" target="#b13">Ratliff et al., 2007</ref>) which computes a gradient-like direction.</p><p>Following <ref type="figure" target="#fig_0">(Socher et al., 2013a)</ref>, we minimize the objective by the diagonal variant of AdaGrad ( <ref type="bibr" target="#b4">Duchi et al., 2011</ref>) with minibatchs. The parame- ter update for the i-th parameter θ t,i at time step t is as follows:</p><formula xml:id="formula_25">θ t,i = θ t−1,i − α √ ∑ t τ =1 g 2 τ,i g t,i ,<label>(18)</label></formula><p>where α is the initial learning rate and g τ ∈ R |θ i | is the subgradient at time step τ for parameter θ i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our model on two different kinds of texts: newswire texts and micro-blog texts. For evaluation, we use the standard Bakeoff scoring program to calculate precision, recall, F1-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word Segmentation on Newswire Texts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets</head><p>We use three popular datasets, PKU, MSRA and CTB6, to evaluate our model on newswire texts. The PKU and MSRA data are provided by the second International Chinese Word Segmentation Bakeoff ( <ref type="bibr" target="#b5">Emerson, 2005)</ref>, and CTB6 is from Chinese TreeBank 6.0 (LDC2007T36) <ref type="bibr" target="#b20">(Xue et al., 2005</ref>), which is a segmented, part-of-speech tagged, and fully bracketed corpus in the con- stituency formalism. These datasets are commonly used by previous state-of-the-art models and neu- ral network models. In addition, we use the first 90% sentences of the training data as training set and the rest 10% sentences as development set for PKU and MSRA datasets, and we divide the train- ing, development and test sets according to <ref type="bibr" target="#b22">(Yang and Xue, 2012</ref>) for the CTB6 dataset. All datasets are preprocessed by replacing the Chinese idioms and the continuous English char- acters and digits with a unique flag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Hyper-parameters</head><p>We set the hyper-parameters of the model as list in <ref type="table" target="#tab_1">Table 1</ref> via experiments on development set. In addition, we set the batch size to 20. And we Window size k = 5 Character embedding size d = 50</p><p>Initial learning rate α = 0.3 Margin loss discount η = 0.2 Regularization λ = 10 −4 Dropout rate on input layer p = 20%  find that it is a good balance between model per- formance and efficiency to set character embed- ding size d = 50. In fact, the larger embedding size leads to higher cost of computational resource, while lower dimensionality of the character em- bedding seems to underfit according to the experi- ment results.</p><p>Deep neural networks contain multiple non- linear hidden layers are always hard to train for it is easy to overfit. Several methods have been used in neural models to avoid overfitting, such as early stop and weight regularization. Dropout <ref type="bibr" target="#b16">(Srivastava et al., 2014</ref>) is also one of the popular strate- gies to avoid overfitting when training the deep neural networks. Hence, we utilize the dropout strategy in this work. Specifically, dropout is to temporarily remove the neuron away with a fixed probability p independently, along with the incom- ing and outgoing connections of it. As a result, we find dropout on the input layer with probability p = 20% is a good tradeoff between model effi- ciency and performance. models without layer-wise with layer-wise P R F P R F GRNN (1 layer) 90.7 89.6 90.2 - - - GRNN (2 layers) 96.0 95.6 95.8 96.0 95.6 95.8 GRNN (3 layers) 95.9 95.4 95.7 96.0 95.7 95.9 GRNN (4 layers) 95.6 95.2 95.4 96.1 95.7 95.9 GRNN (5 layers) 95.3 94.7 95.0 96.1 95.7 95.9 <ref type="table">Table 2</ref>: Performance of different models with or without layer-wise training strategy on PKU test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Layer-wise Training</head><p>We first investigate the effects of the layer-wise training strategy. Since we set the size of context window to five, there are five recursive layers in our architecture. And we train the networks with the different numbers of recursion layers. Due to the limit of space, we just give the results on PKU dataset. <ref type="figure" target="#fig_3">Figure 5</ref> gives the convergence speeds of the five models with different numbers of layers and the model with layer-wise training strategy on de- velopment set of PKU dataset. The model with one layer just use the neurons of the lowest layer in final linear score function. Since there are no non-linear layer, its seems to underfit and perform poorly. The model with two layers just use the neurons in the lowest two layers, and so on. The model with five layers use all the neurons in the network. As we can see, the layer-wise training strategy lead to the fastest convergence and the best performance. <ref type="table">Table 2</ref> shows the performances on PKU test set. The performance of the model with layer-wise training strategy is always better than that with- out layer-wise training strategy. With the increase of the number of layers, the performance also in- creases and reaches the stable high performance until getting to the top layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Results</head><p>We first compare our model with the previous neu- ral approaches on PKU, MSRA and CTB6 datasets as showing in <ref type="table" target="#tab_3">Table 3</ref>. The character embed- dings of the models are random initialized. The performance of word segmentation is significantly boosted by exploiting the gated recursive archi- tecture, which can better model the combinations of the context characters than the previous neural models.</p><p>Previous works have proven it will greatly im- prove the performance to exploit the pre-trained character embeddings instead of that with random initialization. Thus, we pre-train the embeddings on a huge unlabeled data, the Chinese Wikipedia corpus, with word2vec toolkit ( <ref type="bibr" target="#b7">Mikolov et al., 2013)</ref>. By using these obtained character embed- dings, our model receives better performance and still outperforms the previous neural models with pre-trained character embeddings. The detailed re- sults are shown in <ref type="table" target="#tab_4">Table 4</ref> (1st to 3rd rows).</p><p>Inspired by <ref type="bibr" target="#b8">(Pei et al., 2014</ref>), we utilize the bi- gram feature embeddings in our model as well. The concept of feature embedding is quite similar to that of character embedding mentioned above. Specifically, each context feature is represented as a single vector called feature embedding. In this paper, we only use the simply bigram feature em- beddings initialized by the average of two embed- dings of consecutive characters element-wisely.</p><p>Although the model of <ref type="bibr" target="#b8">Pei et al. (2014)</ref> greatly benefits from the bigram feature embeddings, our model just obtains a small improvement with them. This difference indicates that our model has well modeled the combinations of the characters and do not need much help of the feature engineering. The detailed results are shown in <ref type="table" target="#tab_4">Table 4</ref> (4-th and 6-th rows). <ref type="table" target="#tab_5">Table 5</ref> shows the comparisons of our model with the state-of-the-art systems on F-value. The model proposed by <ref type="bibr" target="#b23">Zhang and Clark (2007)</ref> is a word-based segmentation method, which ex- ploit features of complete words, while remains of the list are all character-based word segmenters, whose features are mostly extracted from the con- text characters. Moreover, some systems (such as Sun and Xu (2011) and ) also exploit kinds of extra information such as the un- labeled data or other knowledge. Although our model only uses simple bigram features, it outper- forms the previous state-of-the-art methods which use more complex features.      <ref type="table">Table 6</ref>. To train our model, we also use the first 90% sentences of the training data as training set and the rest 10% sentences as development set.</p><p>Here, we use the default setting of CRF++ toolkit with the feature templates as shown in Ta- ble 7. The same feature templates are also used for FNLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results</head><p>Since the NLPCC 2015 dataset is a new released dataset, we compare our model with the two popu- lar open source toolkits for sequence labeling task: FNLP 3 (Qiu et al., 2013) and CRF++ 4 . Our model uses pre-trained and bigram character embeddings. <ref type="table" target="#tab_7">Table 8</ref> shows the comparisons of our model with the other systems on NLPCC 2015 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular word segmentation method is based on sequence labeling <ref type="bibr" target="#b21">(Xue, 2003)</ref>. Recently, re- searchers have tended to explore neural network <ref type="table" target="#tab_1">Dataset   Sents  Words  Chars  Word Types Char Types OOV Rate  Training 10,000 215,027 347,984  28,208  39,71  - Test  5,000 106,327 171,652  18,696  3,538  7.25%  Total  15,000 322,410 520,555  35,277</ref> 4,243 - <ref type="table">Table 6</ref>: Statistical information of NLPCC 2015 dataset.   <ref type="bibr" target="#b3">(Collobert et al., 2011</ref>) to re- duce efforts of the feature engineering ( <ref type="bibr" target="#b25">Zheng et al., 2013;</ref><ref type="bibr" target="#b10">Qi et al., 2014</ref>). However, the features of all these methods are the concatenation of the embeddings of the context characters. <ref type="bibr" target="#b8">Pei et al. (2014)</ref> also used neural tensor model ( <ref type="bibr" target="#b15">Socher et al., 2013b</ref>) to capture the complicated interactions between tags and context characters. But the interactions depend on the number of the tensor slices, which cannot be too large due to the model complexity. The experiments also show that the model of ( <ref type="bibr" target="#b8">Pei et al., 2014</ref>) greatly bene- fits from the further bigram feature embeddings, which shows that their model cannot even handle the interactions of the consecutive characters. Dif- ferent with them, our model just has a small im- provement with the bigram feature embeddings, which indicates that our approach has well mod- eled the complicated combinations of the context characters, and does not need much help of further feature engineering.</p><formula xml:id="formula_26">unigram feature c −2 , c −1 , c 0 , c +1 , c +2 bigram feature c −1 • c 0 , c 0 • c +1 trigram feature c −2 •c −1 •c 0 , c −1 •c 0 •c +1 , c 0 • c +1 • c +2</formula><p>More recently, <ref type="bibr" target="#b0">Cho et al. (2014a)</ref> also proposed a gated recursive convolutional neural network in machine translation task to solve the problem of varying lengths of sentences. However, their ap- proach only models the update gate, which can not tell whether the information is from the current state or from sub notes in update stage without re- set gate. Instead, our approach models two kinds of gates, reset gate and update gate, by incorporat- ing which we can better model the combinations of context characters via selection function of re- set gate and collection function of update gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a gated recursive neu- ral network (GRNN) to explicitly model the com- binations of the characters for Chinese word seg- mentation task. Each neuron in GRNN can be re- garded as a different combination of the input char- acters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods. Experiments show that our proposed model outperforms the state-of-the-art methods on three popular benchmark datasets.</p><p>Despite Chinese word segmentation being a spe- cific case, our model can be easily generalized and applied to other sequence labeling tasks. In future work, we would like to investigate our proposed GRNN on other sequence labeling tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: General architecture of neural model for Chinese word segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of Gated Recursive Neural Network for Chinese word segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Our proposed gated recursive unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of different models with or without layer-wise training strategy on PKU development set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>models</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table</head><label></label><figDesc></figDesc><table>· 
· 
· 

· 
· 
· 

· 
· 
· 

· 
· 
· 

· 
· 
· 

3 
4 
5 

2 

6 

1 

· 
· 
· 
d-1 
d 

Features 

Linear 
W 1 ×□+b 1 

· 
· 
· 

Number of Hidden Units 

Sigmoid 
g(□) 

· 
· 
· 

Number of Hidden Units 

Linear 
W 2 ×□+b 2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Hyper-parameter settings. 

0 
10 
20 
30 
40 
88 

90 

92 

94 

96 

epoches 

F-value(%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performances on PKU, MSRA and CTB6 test sets with random initialized character embeddings. 

models 
PKU 
MSRA 
CTB6 
P 
R 
F 
P 
R 
F 
P 
R 
F 
+Pre-train 
(Zheng et al., 2013) 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7* 
(Pei et al., 2014) 
94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* 
GRNN 
96.3 95.9 96.1 96.2 96.3 96.2 95.8 
95.4 
95.6 

+bigram 
GRNN 
96.6 96.2 96.4 97.5 97.3 97.4 95.9 
95.7 
95.8 
+Pre-train+bigram 
(Pei et al., 2014) 
-
95.2 
-
-
97.2 
-
-
-
-
GRNN 
96.5 96.3 96.4 97.4 97.8 97.6 95.8 
95.7 
95.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performances on PKU, MSRA and CTB6 test sets with pre-trained and bigram character em-
beddings. 

models 
PKU MSRA CTB6 
(Tseng et al., 2005) 
95.0 96.4 
-
(Zhang and Clark, 2007) 95.1 97.2 
-
(Sun and Xu, 2011) 
-
-
95.7 
(Zhang et al., 2013) 
96.1 97.4 
-
This work 
96.4 97.6 95.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of GRNN with the state-of-
the-art methods on PKU, MSRA and CTB6 test 
sets. 

5.2 Word Segmentation on Micro-blog Texts 

5.2.1 Dataset 

We use the NLPCC 2015 dataset 1 (Qiu et al., 2015) 
to evaluate our model on micro-blog texts. The 
NLPCC 2015 data are provided by the shared task 
in the 4th CCF Conference on Natural Language 
Processing &amp; Chinese Computing (NLPCC 2015): 
Chinese Word Segmentation and POS Tagging for 
micro-blog Text. Different with the popular used 
newswire dataset, the NLPCC 2015 dataset is col-
lected from Sina Weibo 2 , which consists of the 
relatively informal texts from micro-blog with the 
various topics, such as finance, sports, entertain-
ment, and so on. The information of the dataset is shown in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Templates of CRF++ and FNLP. 

models 
P 
R 
F 
CRF++ 
93.3 93.2 93.3 
FNLP 
94.1 93.9 94.0 
This work 94.7 94.8 94.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 8 :</head><label>8</label><figDesc>Performances on NLPCC 2015 dataset. based approaches</figDesc><table></table></figure>

			<note place="foot" n="1"> http://nlp.fudan.edu.cn/nlpcc2015/ 2 http://www.weibo.com/</note>

			<note place="foot" n="3"> https://github.com/xpqiu/fnlp/ 4 http://taku910.github.io/crfpp/ * The result is from our own implementation of the corresponding method.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their valuable comments. This work was partially funded by the National Natural Sci-ence Foundation of China (61472088, 61473092), the National High Technology Research and De-velopment Program of China (2015AA015408), Shanghai Science and Technology Development Funds (14ZR1403200), Shanghai Leading Aca-demic Discipline Project (B114).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The second international Chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Baobao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan B Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for character-based information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sujatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="668" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FudanNLP: A toolkit for Chinese natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liusong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07599</idno>
		<title level="m">Overview of the NLPCC 2015 shared task: Chinese word segmentation and POS tagging for micro-blog texts</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">(online) subgradient methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Nathan D Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh International Conference on Artificial Intelligence and Statistics (AIStats)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhancing Chinese word segmentation using unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="970" to="979" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning structured prediction models: A large margin approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassil</forename><surname>Chatalbashev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Machine learning</title>
		<meeting>the international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A conditional random field word segmenter for sighan bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth SIGHAN workshop on Chinese language Processing</title>
		<meeting>the fourth SIGHAN workshop on Chinese language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Penn Chinese TreeBank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chinese word segmentation as character tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Chinese Language Processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chinese comma disambiguation for discourse analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="786" to="794" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Chinese segmentation with a word-based perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring representations from unlabeled data with co-training for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mairgup</forename><surname>Mansur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
