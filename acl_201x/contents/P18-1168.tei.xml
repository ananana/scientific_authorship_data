<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Semantic Parsing with Abstract Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Goldman</surname></persName>
							<email>{omergoldman@mail,veronical@mail, ehudnave@mail,gamir@post,joberant@cs}.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Latcinnik</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udi</forename><surname>Naveh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Semantic Parsing with Abstract Examples</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1809" to="1819"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential programs needs to be explored at training time to find a correct program. Second, spurious programs that accidentally lead to a correct denotation add noise to training. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spurious-ness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our method substantially improves performance, and reaches 82.5% accuracy, a 14.7% absolute accuracy improvement compared to the best reported accuracy so far.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of semantic parsing is to map language utterances to executable programs. Early work on statistical learning of semantic parsers utilized * Authors equally contributed to this work. x :There is a small yellow item not touching any wall y :True z :Exist(Filter <ref type="bibr">(ALL ITEMS, λx.And(And(IsYellow(x)</ref>,</p><p>IsSmall(x)), Not(IsTouchingWall(x, Side.Any))))))</p><p>Figure 1: Overview of our visual reasoning setup for the CN- LVR dataset. Given an image rendered from a KB k and an utterance x, our goal is to parse x to a program z that re- sults in the correct denotation y. Our training data includes (x, k, y) triplets.</p><p>supervised learning, where training examples in- cluded pairs of language utterances and programs <ref type="bibr" target="#b30">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b15">Kate et al., 2005;</ref><ref type="bibr">Collins, 2005, 2007)</ref>. However, col- lecting such training examples at scale has quickly turned out to be difficult, because expert annota- tors who are familiar with formal languages are re- quired. This has led to a body of work on weakly- supervised semantic parsing ( <ref type="bibr" target="#b8">Clarke et al., 2010;</ref><ref type="bibr" target="#b22">Liang et al., 2011;</ref><ref type="bibr" target="#b18">Krishnamurthy and Mitchell, 2012;</ref><ref type="bibr" target="#b19">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b4">Berant et al., 2013;</ref><ref type="bibr" target="#b5">Cai and Yates, 2013;</ref>. In this setup, training examples correspond to utterance-denotation pairs, where a denotation is the result of executing a program against the en- vironment (see <ref type="figure">Fig. 1</ref>). Naturally, collecting deno- tations is much easier, because it can be performed by non-experts.</p><p>Training semantic parsers from denotations rather than programs complicates training in two ways: (a) Search: The algorithm must learn to search through the huge space of programs at training time, in order to find the correct program. This is a difficult search problem due to the com- binatorial nature of the search space. (b) Spurious-ness: Incorrect programs can lead to correct deno- tations, and thus the learner can go astray based on these programs. Of the two mentioned problems, spuriousness has attracted relatively less attention <ref type="bibr" target="#b25">(Pasupat and Liang, 2016;</ref><ref type="bibr" target="#b9">Guu et al., 2017)</ref>.</p><p>Recently, the Cornell Natural Language for Vi- sual Reasoning corpus (CNLVR) was released ( <ref type="bibr" target="#b27">Suhr et al., 2017)</ref>, and has presented an opportu- nity to better investigate the problem of spurious- ness. In this task, an image with boxes that con- tains objects of various shapes, colors and sizes is shown. Each image is paired with a complex nat- ural language statement, and the goal is to deter- mine whether the statement is true or false <ref type="figure">(Fig. 1)</ref>. The task comes in two flavors, where in one the input is the image (pixels), and in the other it is the knowledge-base (KB) from which the image was synthesized. Given the KB, it is easy to view CNLVR as a semantic parsing problem: our goal is to translate language utterances into programs that will be executed against the KB to determine their correctness <ref type="bibr" target="#b14">(Johnson et al., 2017b;</ref><ref type="bibr" target="#b11">Hu et al., 2017)</ref>. Because there are only two return values, it is easy to generate programs that execute to the right denotation, and thus spuriousness is a major problem compared to previous datasets.</p><p>In this paper, we present the first semantic parser for CNLVR. Semantic parsing can be coarsely divided into a lexical task (i.e., mapping words and phrases to program constants), and a structural task (i.e., mapping language composi- tion to program composition operators). Our core insight is that in closed worlds with clear seman- tic types, like spatial and visual reasoning, we can manually construct a small lexicon that clusters language tokens and program constants, and create a partially abstract representation for utterances and programs <ref type="table">(Table 1</ref>) in which the lexical prob- lem is substantially reduced. This scenario is ubiq- uitous in many semantic parsing applications such as calendar, restaurant reservation systems, hous- ing applications, etc: the formal language has a compact semantic schema and a well-defined typ- ing system, and there are canonical ways to ex- press many program constants.</p><p>We show that with abstract representations we can share information across examples and bet- ter tackle the search and spuriousness challenges. By pulling together different examples that share the same abstract representation, we can identify programs that obtain high reward across multiple examples, thus reducing the problem of spurious- ness. This can also be done at search time, by augmenting the search state with partial programs that have been shown to be useful in earlier itera- tions. Moreover, we can annotate a small number of abstract utterance-program pairs, and automati- cally generate training examples, that will be used to warm-start our model to an initialization point in which search is able to find correct programs.</p><p>We develop a formal language for visual rea- soning, inspired by <ref type="bibr" target="#b14">Johnson et al. (2017b)</ref>, and train a semantic parser over that language from weak supervision, showing that abstract exam- ples substantially improve parser accuracy. Our parser obtains an accuracy of 82.5%, a 14.7% ab- solute accuracy improvement compared to state- of-the-art.</p><p>All our code is publicly avail- able at https://github.com/udiNaveh/ nlvr_tau_nlp_final_proj.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Setup</head><p>Problem Statement Given a training set of</p><formula xml:id="formula_0">N examples {(x i , k i , y i )} N i=1</formula><p>, where x i is an utter- ance, k i is a KB describing objects in an image and y i ∈ {TRUE, FALSE} denotes whether the utter- ance is true or false in the KB, our goal is to learn a semantic parser that maps a new utterance x to a program z such that when z is executed against the corresponding KB k, it yields the correct de- notation y (see <ref type="figure">Fig. 1</ref>).</p><p>Programming language The original KBs in CNLVR describe an image as a set of objects, where each object has a color, shape, size and location in absolute coordinates. We define a programming language over the KB that is more amenable to spatial reasoning, inspired by work on the CLEVR dataset <ref type="bibr" target="#b14">(Johnson et al., 2017b</ref>). This programming language provides access to func- tions that allow us to check the size, shape, and color of an object, to check whether it is touch- ing a wall, to obtain sets of items that are above and below a certain set of items, etc. 1 More for- mally, a program is a sequence of tokens describ- ing a possibly recursive sequence of function ap- plications in prefix notation. Each token is either a function with fixed arity (all functions have either one or two arguments), a constant, a variable or a λ term used to define Boolean functions. Functions, constants and variables have one of the following x: "There are exactly 3 yellow squares touching the wall." z: <ref type="bibr">Equal(3, Count(Filter(ALL ITEMS, λx. And (And (IsYellow(x)</ref>, IsSquare(x), IsTouchingWall(x)))))) ¯ x: "There are C-QuantMod C-Num C-Color C-Shape touching the wall." ¯ z: C-QuantMod(C-Num, <ref type="bibr">Count(Filter(ALL ITEMS, λx. And (And (IsC-Color(x)</ref>, IsC-Shape(x), IsTouchingWall(x)))))) <ref type="table">Table 1</ref>: An example for an utterance-program pair (x, z) and its abstract counterpart (¯ x, ¯ z)</p><p>x: "There is a small yellow item not touching any wall.   <ref type="table" target="#tab_0">Tables 1 and 2</ref> provide examples for utterances and their correct programs. The supplementary material provides a full description of all program tokens, their argu- ments and return types. Unlike CLEVR, CNLVR requires substantial set-theoretic reasoning (utterances refer to various aspects of sets of items in one of the three boxes in the image), which required extending the language described by <ref type="bibr" target="#b14">Johnson et al. (2017b)</ref> to include set operators and lambda abstraction. We manually sampled 100 training examples from the training data and estimate that roughly 95% of the utter- ances in the training data can be expressed with this programming language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We base our model on the semantic parser of <ref type="bibr" target="#b9">Guu et al. (2017)</ref>. In their work, they used an encoder- decoder architecture <ref type="bibr" target="#b28">(Sutskever et al., 2014</ref>) to de- fine a distribution p θ (z | x). The utterance x is encoded using a bi-directional LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>) that creates a contextual- ized representation h i for every utterance token x i , and the decoder is a feed-forward network com- bined with an attention mechanism over the en- coder outputs ( <ref type="bibr" target="#b3">Bahdanau et al., 2015</ref>). The feed- forward decoder takes as input the last K tokens that were decoded.</p><p>More formally the probability of a program is the product of the probability of its tokens given the history: p θ (z | x) = t p θ (z t | x, z 1:t−1 ), and the probability of a decoded token is com- puted as follows. First, a Bi-LSTM encoder con- verts the input sequence of utterance embeddings into a sequence of forward and backward states h</p><formula xml:id="formula_1">{F,B} 1 , . . . , h {F,B} |x| . The utterance representationˆx representationˆ representationˆx isˆxisˆ isˆx = [h F |x| ; h B 1 ].</formula><p>Then decoding produces the program token-by-token:</p><formula xml:id="formula_2">q t = ReLU(W q [ˆ x; ˆ v; z t−K−1:t−1 ]), α t,i ∝ exp(q t W α h i ) , c t = i α t,i h i , p θ (z t | x, z 1:t−1 ) ∝ exp(φ zt W s [q t ; c t ]),</formula><p>where φ z is an embedding for program token z, ˆ v is a bag-of-words vector for the tokens in x,</p><formula xml:id="formula_3">z i:j = (z i , . . . , z j )</formula><p>is a history vector of size K, the matrices W q , W α , W s are learned parameters (along with the LSTM parameters and embedding matrices), and ';' denotes concatenation.</p><p>Search: Searching through the large space of programs is a fundamental challenge in semantic parsing. To combat this challenge we apply sev- eral techniques. First, we use beam search at de- coding time and when training from weak super- vision (see Sec. 4), similar to prior work ( <ref type="bibr" target="#b9">Guu et al., 2017)</ref>. At each decoding step we maintain a beam B of program prefixes of length n, expand them exhaustively to programs of length n+1 and keep the top-|B| program prefixes with highest model probability. Second, we utilize the semantic typing sys- tem to only construct programs that are syntacti- cally valid, and substantially prune the program search space (similar to type constraints in Krish-  <ref type="formula">(2017)</ref>). We maintain a stack that keeps track of the expected semantic type at each de- coding step. The stack is initialized with the type Bool. Then, at each decoding step, only tokens that return the semantic type at the top of the stack are allowed, the stack is popped, and if the de- coded token is a function, the semantic types of its arguments are pushed to the stack. This dra- matically reduces the search space and guarantees that only syntactically valid programs will be pro- duced. <ref type="figure" target="#fig_3">Fig. 2</ref> illustrates the state of the stack when decoding a program for an input utterance.  Given the constrains on valid programs, our model p θ (z | x) is defined as:</p><formula xml:id="formula_4">t p θ (z t | x, z 1:t−1 ) · 1(z t | z 1:t−1 ) z p θ (z | x, z 1:t−1 ) · 1(z | z 1:t−1 )</formula><p>, where 1(z t | z 1:t−1 ) indicates whether a certain program token is valid given the program prefix.</p><p>Discriminative re-ranking: The above model is a locally-normalized model that provides a dis- tribution for every decoded token, and thus might suffer from the label bias problem ( <ref type="bibr" target="#b0">Andor et al., 2016;</ref><ref type="bibr" target="#b20">Lafferty et al., 2001</ref>). Thus, we add a globally-normalized re-ranker p ψ (z | x) that scores all |B| programs in the final beam produced by p θ (z | x). Our globally-normalized model is:</p><formula xml:id="formula_5">p g ψ (z | x) ∝ exp(s ψ (x, z)),</formula><p>and is normalized over all programs in the beam. The scoring function s ψ (x, z) is a neural net- work with identical architecture to the locally- normalized model, except that (a) it feeds the de- coder with the candidate program z and does not generate it. (b) the last hidden state is inserted to a feed-forward network whose output is s ψ (x, z).</p><formula xml:id="formula_6">Our final ranking score is p θ (z|x)p g ψ (z | x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>We now describe our basic method for training from weak supervision, which we extend upon in Sec. 5 using abstract examples. To use weak su- pervision, we treat the program z as a latent vari- able that is approximately marginalized. To de- scribe the objective, define R(z, k, y) ∈ {0, 1} to be one if executing program z on KB k results in denotation y, and zero otherwise. The objective is then to maximize p(y | x) given by:</p><formula xml:id="formula_7">z∈Z p θ (z | x)p(y | z, k) = z∈Z p θ (z | x)R(z, k, y) ≈ z∈B p θ (z | x)R(z, k, y)</formula><p>where Z is the space of all programs and B ⊂ Z are the programs found by beam search.</p><p>In most semantic parsers there will be relatively few z that generate the correct denotation y. How- ever, in CNLVR, y is binary, and so spuriousness is a central problem. To alleviate it, we utilize a property of CNLVR: the same utterance appears 4 times with 4 different images. <ref type="bibr">2</ref> If a program is spurious it is likely that it will yield the wrong de- notation in one of those 4 images.</p><p>Thus, we can re-define each training example to be (x, {(k j , y j )} 4 j=1 ), where each utterance x is paired with 4 different KBs and the denotations of the utterance with respect to these KBs. Then, we maximize p({y j } 4 j=1 | x, ) by maximizing the ob- jective above, except that R(z, {k j , y j } 4 j=1 ) = 1 iff the denotation of z is correct for all four KBs. This dramatically reduces the problem of spuri- ousness, as the chance of randomly obtaining a correct denotation goes down from 1 2 to 1 16 . This is reminiscent of <ref type="bibr" target="#b25">Pasupat and Liang (2016)</ref>, where random permutations of Wikipedia tables were shown to crowdsourcing workers to eliminate spu- rious programs.</p><p>We train the discriminative ranker analogously by maximizing the probability of programs with correct denotation z∈B p g ψ (z | x)R(z, k, y). This basic training method fails for CNLVR (see Sec. 6), due to the difficulties of search and spuriousness. Thus, we turn to learning from ab- stract examples, which substantially reduce these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning from Abstract Examples</head><p>The main premise of this work is that in closed, well-typed domains such as visual reasoning, the main challenge is handling language composition- ality, since questions may have a complex and nested structure. Conversely, the problem of map- ping lexical items to functions and constants in the programming language can be substantially alleviated by taking advantage of the compact KB schema and typing system, and utilizing a</p><formula xml:id="formula_8">Utterance Program Cluster # "yellow" IsYellow C-Color 3 "big" IsBig C-Size 3 "square" IsSquare C-Shape 4 "3" 3 C-Num 2 "exactly" EqualInt C-QuantMod 5 "top"</formula><p>Side.Top C-Location 2 "above" GetAbove C-SpaceRel 6 Total: 25 small lexicon that maps prevalent lexical items into typed program constants. Thus, if we abstract away from the actual utterance into a partially ab- stract representation, we can combat the search and spuriousness challenges as we can generalize better across examples in small datasets.</p><p>Consider the utterances:</p><p>1. "There are exactly 3 yellow squares touching the wall." 2. "There are at least 2 blue circles touching the wall." While the surface forms of these utterances are dif- ferent, at an abstract level they are similar and it would be useful to leverage this similarity.</p><p>We therefore define an abstract representation for utterances and logical forms that is suitable for spatial reasoning. We define seven abstract clus- ters (see <ref type="table" target="#tab_2">Table 3</ref>) that correspond to the main se- mantic types in our domain. Then, we associate each cluster with a small lexicon that contains language-program token pairs associated with this cluster. These mappings represent the canonical ways in which program constants are expressed in natural language. <ref type="table" target="#tab_2">Table 3</ref> shows the seven clusters we use, with an example for an utterance-program token pair from the cluster, and the number of mappings in each cluster. In total, 25 mappings are used to define abstract representations.</p><p>As we show next, abstract examples can be used to improve the process of training semantic parsers. Specifically, in sections 5.1-5.3, we use abstract examples in several ways, from generat- ing new training data to improving search accu- racy. The combined effect of these approaches is quite dramatic, as our evaluation demonstrates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">High Coverage via Abstract Examples</head><p>We begin by demonstrating that abstraction leads to rather effective coverage of the types of ques- tions asked in a dataset. Namely, that many ques- tions in the data correspond to a small set of ab- stract examples. We created abstract representa- tions for all 3,163 utterances in the training exam- ples by mapping utterance tokens to their cluster label, and then counted how many distinct abstract utterances exist. We found that as few as 200 ab- stract utterances cover roughly half of the training examples in the original training set.</p><p>The above suggests that knowing how to answer a small set of abstract questions may already yield a reasonable baseline. To test this baseline, we constructured a "rule-based" parser as follows. We manually annotated 106 abstract utterances with their corresponding abstract program (including alignment between abstract tokens in the utterance and program). For example, <ref type="table">Table 1</ref> shows the abstract utterance and program for the utterance "There are exactly 3 yellow squares touching the wall". Note that the utterance "There are at least 2 blue circles touching the wall" will be mapped to the same abstract utterance and program.</p><p>Given this set of manual annotations, our rule- based semantic parser operates as follows: Given an utterance x, create its abstract representation ¯ x. If it exactly matches one of the manually anno- tated utterances, map it to its corresponding ab- stract program ¯ z. Replace the abstract program to- kens with real program tokens based on the align- ment with the utterance tokens, and obtain a final program z. If ¯ x does not match return TRUE, the majority label. The rule-based parser will fail for examples not covered by the manual annotation. However, it already provides a reasonable baseline (see <ref type="table" target="#tab_4">Table 4</ref>). As shown next, manual annotations can also be used for generating new training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Augmentation</head><p>While the rule-based semantic parser has high precision and gauges the amount of structural variance in the data, it cannot generalize be- yond observed examples. However, we can auto- matically generate non-abstract utterance-program pairs from the manually annotated abstract pairs and train a semantic parser with strong supervi- sion that can potentially generalize better. E.g., consider the utterance "There are exactly 3 yellow squares touching the wall", whose abstract repre- sentation is given in <ref type="table">Table 1</ref>. It is clear that we can use this abstract pair to generate a program for a new utterance "There are exactly 3 blue squares touching the wall". This program will be identical</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Decoding with an Abstract Cache</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1: procedure DECODE(x, y, C, D) 2:</head><p>// C is a map where the key is an abstract utterance and the value is a pair (Z, ˆ R) of a list of abstract pro- grams Z and their average rewardsˆRrewardsˆ rewardsˆR. D is an integer. 3: ¯ x ← Abstract utterance of x 4:</p><p>A ← D programs in C[¯ x] with top reward values 5:</p><p>B1 ← compute beam of programs of length 1 6:</p><p>for t = 2 . . . T do // Decode with cache 7:</p><p>Bt ← construct beam from Bt−1 8: At = truncate(A, t) 9:</p><p>Bt.add(de-abstract(At)) 10:</p><p>for z ∈ BT do //Update cache 11:</p><p>Update rewards in C[¯ x] using (¯ z, R(z, y)) 12:</p><p>return BT ∪ de-abstract(A).</p><p>to the program of the first utterance, with IsBlue replacing IsYellow. More generally, we can sample any abstract ex- ample and instantiate the abstract clusters that ap- pear in it by sampling pairs of utterance-program tokens for each abstract cluster. Formally, this is equivalent to a synchronous context-free gram- mar <ref type="bibr" target="#b7">(Chiang, 2005</ref>) that has a rule for generat- ing each manually-annotated abstract utterance- program pair, and rules for synchronously gener- ating utterance and program tokens from the seven clusters.</p><p>We generated 6,158 (x, z) examples using this method and trained a standard sequence to se- quence parser by maximizing log p θ (z|x) in the model above. Although these are generated from a small set of 106 abstract utterances, they can be used to learn a model with higher coverage and ac- curacy compared to the rule-based parser, as our evaluation demonstrates. <ref type="bibr">3</ref> The resulting parser can be used as a standalone semantic parser. However, it can also be used as an initialization point for the weakly-supervised se- mantic parser. As we observe in Sec. 6, this results in further improvement in accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Caching Abstract Examples</head><p>We now describe a caching mechanism that uses abstract examples to combat search and spurious- ness when training from weak supervision. As shown in Sec. 5.1, many utterances are identical at the abstract level. Thus, a natural idea is to keep track at training time of abstract utterance- program pairs that resulted in a correct denotation, and use this information to direct the search pro- cedure.</p><p>Concretely, we construct a cache C that maps abstract utterances to all abstract programs that were decoded by the model, and tracks the aver- age reward obtained for those programs. For every utterance x, after obtaining the final beam of pro- grams, we add to the cache all abstract utterance- program pairs (¯ x, ¯ z), and update their average re- ward (Alg. 1, line 10). To construct an abstract example (¯ x, ¯ z) from an utterance-program pair (x, z) in the beam, we perform the following pro- cedure. First, we create ¯ x by replacing utterance tokens with their cluster label, as in the rule-based semantic parser. Then, we go over every program token in z, and replace it with an abstract cluster if the utterance contains a token that is mapped to this program token according to the mappings from <ref type="table" target="#tab_2">Table 3</ref>. This also provides an alignment from abstract program tokens to abstract utterance tokens that is necessary when utilizing the cache.</p><p>We propose two variants for taking advantage of the cache C. Both are shown in Algorithm 1. 1. Full program retrieval (Alg. 1, line 12): Given utterance x, construct an abstract utterance ¯ x, re- trieve the top D abstract programs A from the cache, compute the de-abstracted programs Z us- ing alignments from program tokens to utterance tokens, and add the D programs to the final beam. 2. Program prefix retrieval (Alg. 1, line 9): Here, we additionally consider prefixes of abstract pro- grams to the beam, to further guide the search pro- cess. At each step t, let B t be the beam of de- coded programs at step t. For every abstract pro- gram ¯ z ∈ A add the de-abstracted prefix z 1:t to B t and expand B t+1 accordingly. This allows the parser to potentially construct new programs that are not in the cache already. This approach com- bats both spuriousness and the search challenge, because we add promising program prefixes to the beam that might have fallen off of it earlier. <ref type="figure" target="#fig_4">Fig. 3</ref> visualizes the caching mechanism.</p><p>A high-level overview of our entire approach for utilizing abstract examples at training time for both data augmentation and model training is given in <ref type="figure" target="#fig_5">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Evaluation</head><p>Model and Training Parameters The Bi- LSTM state dimension is 30. The decoder has one hidden layer of dimension 50, that takes the  In the weakly-supervised parser we encour- age exploration with meritocratic gradient updates with β = 0.5 ( <ref type="bibr" target="#b9">Guu et al., 2017</ref>). In the weakly- supervised parser we warm-start the parameters with the supervised parser, as mentioned above. For optimization, Adam is used ( <ref type="bibr" target="#b16">Kingma and Ba, 2014)</ref>), with learning rate of 0.001, and mini-batch size of 8.</p><p>Pre-processing Because the number of utter- ances is relatively small for training a neural model, we take the following steps to reduce spar- sity. We lowercase all utterance tokens, and also use their lemmatized form. We also use spelling correction to replace words that contain typos. Af- ter pre-processing we replace every word that oc- curs less than 5 times with an UNK symbol.</p><p>Evaluation We evaluate on the public develop- ment and test sets of CNLVR as well as on the hidden test set. The standard evaluation metric is accuracy, i.e., how many examples are cor- rectly classified. In addition, we report consis- tency, which is the proportion of utterances for which the decoded program has the correct deno- tation for all 4 images/KBs. It captures whether a model consistently produces a correct answer.</p><p>Baselines We compare our models to the MA- JORITY baseline that picks the majority class (TRUE in our case). We also compare to the state- of-the-art model reported by <ref type="bibr" target="#b27">Suhr et al. (2017)</ref> Dev.</p><p>Test-P  when taking the KB as input, which is a maximum entropy classifier (MAXENT). For our models, we evaluate the following variants of our approach:</p><p>• RULE: The rule-based parser from Sec. 5.1.</p><p>• SUP.: The supervised semantic parser trained on augmented data as in Sec. 5.2 (5, 598 exam- ples for training and 560 for validation).</p><p>• WEAKSUP.: Our full weakly-supervised se- mantic parser that uses abstract examples.</p><p>• +DISC: We add a discriminative re-ranker (Sec. 3) for both SUP. and WEAKSUP.</p><p>Main results <ref type="table" target="#tab_4">Table 4</ref> describes our main re- sults. Our weakly-supervised semantic parser with re-ranking (W.+DISC) obtains 84.0 accuracy and 65.0 consistency on the public test set and 82.5 accuracy and 63.9 on the hidden one, improving accuracy by 14.7 points compared to state-of-the- art. The accuracy of the rule-based parser (RULE) is less than 2 points below MAXENT, showing that a semantic parsing approach is very suitable for this task. The supervised parser obtains better performance (especially in consistency), and with re-ranking reaches 76.6 accuracy, showing that generalizing from generated examples is better than memorizing manually-defined patterns. Our weakly-supervised parser significantly improves over SUP., reaching an accuracy of 81.7 before re- ranking, and 84.0 after re-ranking (on the public test set). Consistency results show an even crisper trend of improvement across the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Analysis</head><p>We analyze our results by running multiple abla- tions of our best model W.+DISC on the develop- ment set.</p><p>To examine the overall impact of our pro- cedure, we trained a weakly-supervised parser from scratch without pre-training a supervised parser nor using a cache, which amounts to a re-implementation of the RANDOMER algorithm ( <ref type="bibr" target="#b9">Guu et al., 2017)</ref>. We find that the algorithm is  <ref type="table">Table 5</ref>: Results of ablations of our main models on the de- velopment set. Explanation for the nature of the models is in the body of the paper.</p><p>unable to bootstrap in this challenging setup and obtains very low performance. Next, we exam- ined the importance of abstract examples, by pre- training only on examples that were manually an- notated (utterances that match the 106 abstract pat- terns), but with no data augmentation or use of a cache (−ABSTRACTION). This results in perfor- mance that is similar to the MAJORITY baseline.</p><p>To further examine the importance of abstrac- tion, we decoupled the two contributions, train- ing once with a cache but without data augmen- tation for pre-training (−DATAAUGMENTATION), and again with pre-training over the augmented data, but without the cache (−BEAMCACHE). We found that the former improves by a few points over the MAXENT baseline, and the latter per- forms comparably to the supervised parser, that is, we are still unable to improve learning by training from denotations.</p><p>Lastly, we use a beam cache without line 9 in Alg. 1 (−EVERYSTEPBEAMCACHE). This al- ready results in good performance, substantially higher than SUP. but is still 3.4 points worse than our best performing model on the development set.</p><p>Orthogonally, to analyze the importance of ty- ing the reward of all four examples that share an utterance, we trained a model without this ty- ing, where the reward is 1 iff the denotation is correct (ONEEXAMPLEREWARD). We find that spuriousness becomes a major issue and weakly- supervised learning fails.</p><p>Error Analysis We sampled 50 consistent and 50 inconsistent programs from the development set to analyze the weaknesses of our model. By and large, errors correspond to utterances that are more complex syntactically and semantically. In about half of the errors an object was described by two or more modifying clauses: "there is a box with a yellow circle and three blue items"; or nest- ing occurred: "one of the gray boxes has exactly three objects one of which is a circle". In these cases the model either ignored one of the condi- tions, resulting in a program equivalent to "there is a box with three blue items" for the first case, or applied composition operators wrongly, out- putting an equivalent to "one of the gray boxes has exactly three circles" for the second case. How- ever, in some cases the parser succeeds on such examples and we found that 12% of the sampled utterances that were parsed correctly had a similar complex structure. Other, less frequent reasons for failure were problems with cardinality interpreta- tion, i.e. ,"there are 2" parsed as "exactly 2" in- stead of "at least 2"; applying conditions to items rather than sets, e.g., "there are 2 boxes with a tri- angle closely touching a corner" parsed as "there are 2 triangles closely touching a corner"; and ut- terances with questionable phrasing, e.g., "there is a tower that has three the same blocks color".</p><p>Other insights are that the algorithm tended to give higher probability to the top ranked program when it is correct (average probability 0.18), com- pared to cases when it is incorrect (average proba- bility 0.08), indicating that probabilities are corre- lated with confidence. In addition, sentence length is not predictive for whether the model will suc- ceed: average sentence length of an utterance is 10.9 when the model is correct, and 11.1 when it errs.</p><p>We also note that the model was successful with sentences that deal with spatial relations, but struggled with sentences that refer to the size of shapes. This is due to the data distribution, which includes many examples of the former case and fewer examples of the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Training semantic parsers from denotations has been one of the most popular training schemes for scaling semantic parsers since the beginning of the decade. Early work focused on traditional log-linear models <ref type="bibr" target="#b8">(Clarke et al., 2010;</ref><ref type="bibr" target="#b22">Liang et al., 2011;</ref><ref type="bibr" target="#b19">Kwiatkowski et al., 2013</ref>), but recently de- notations have been used to train neural semantic parsers ( <ref type="bibr" target="#b17">Krishnamurthy et al., 2017;</ref><ref type="bibr" target="#b26">Rabinovich et al., 2017;</ref><ref type="bibr" target="#b6">Cheng et al., 2017)</ref>.</p><p>Visual reasoning has attracted considerable at- tention, with datasets such as VQA ( <ref type="bibr" target="#b1">Antol et al., 2015</ref>) and CLEVR <ref type="bibr" target="#b13">(Johnson et al., 2017a</ref>). The advantage of CNLVR is that language utterances are both natural and compositional. Treating vi- sual reasoning as an end-to-end semantic parsing problem has been previously done on CLEVR ( <ref type="bibr" target="#b11">Hu et al., 2017;</ref><ref type="bibr" target="#b14">Johnson et al., 2017b)</ref>.</p><p>Our method for generating training data resem- bles data re-combination ideas in <ref type="bibr" target="#b12">Jia and Liang (2016)</ref>, where examples are generated automati- cally by replacing entities with their categories.</p><p>While spuriousness is central to semantic pars- ing when denotations are not very informative, there has been relatively little work on explicitly tackling it. <ref type="bibr" target="#b24">Pasupat and Liang (2015)</ref> used man- ual rules to prune unlikely programs on the WIK- ITABLEQUESTIONS dataset, and then later uti- lized crowdsourcing <ref type="bibr" target="#b25">(Pasupat and Liang, 2016)</ref> to eliminate spurious programs. <ref type="bibr" target="#b9">Guu et al. (2017)</ref> proposed RANDOMER, a method for increasing exploration and handling spuriousness by adding randomness to beam search and a proposing a "meritocratic" weighting scheme for gradients. In our work we found that random exploration during beam search did not improve results while merito- cratic updates slightly improved performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>k</head><label></label><figDesc>:[[{y loc: ..., color: 'Black', type: 'square', x loc: ... size: 20}, ...}]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>" z: Exist(Filter(ALL ITEMS, λx.And(And(IsYellow(x), IsSmall(x)), Not(IsTouchingWall(x, Side.Any))))) x: "One tower has a yellow base." z: GreaterEqual(1, Count(Filter(ALL ITEMS, λx.And(IsYellow(x), IsBottom(x)))))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>namurthy et al. (2017); Xiao et al. (2016); Liang et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example for the state of the type stack s while decoding a program z for an utterance x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A visualization of the caching mechanism. At each decoding step, prefixes of high-reward abstract programs are added to the beam from the cache.</figDesc><graphic url="image-3.png" coords="7,82.87,317.40,431.81,147.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An overview of our approach for utilizing abstract examples for data augmentation and model training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples for utterance-program pairs. Commas and parenthesis provided for readability only. 

atomic types: Int, Bool, Item, Size, Shape, 
Color, Side (sides of a box in the image); or a 
composite type Set(?), and Func(?,?). Valid 
programs have a return type Bool. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Example mappings from utterance tokens to pro-
gram tokens for the seven clusters used in the abstract repre-
sentation. The rightmost column counts the number of map-
ping in each cluster, resulting in a total of 25 mappings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on the development, public test (Test-P) and 
hidden test (Test-H) sets. For each model, we report both 
accuracy and consistency. 

</table></figure>

			<note place="foot" n="1"> We leave the problem of learning the programming language functions from the original KB for future work.</note>

			<note place="foot" n="2"> We used the KBs in CNLVR, for which there are 4 KBs per utterance. When working over pixels there are 24 images per utterance, as 6 images were generated from each KB.</note>

			<note place="foot" n="3"> Training a parser directly over the 106 abstract examples results in poor performance due to the small number of examples.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>In this work we presented the first semantic parser for the CNLVR dataset, taking structured repre-sentations as input. Our main insight is that in closed, well-typed domains we can generate ab-stract examples that can help combat the diffi-culties of training a parser from delayed super-vision. First, we use abstract examples to semi-automatically generate utterance-program pairs that help warm-start our parameters, thereby re-ducing the difficult search challenge of finding correct programs with random parameters. Sec-ond, we focus on an abstract representation of ex-amples, which allows us to tackle spuriousness and alleviate search, by sharing information about promising programs between different examples. Our approach dramatically improves performance on CNLVR, establishing a new state-of-the-art.</p><p>In this paper, we used a manually-built high-precision lexicon to construct abstract examples. This is suitable for well-typed domains, which are ubiquitous in the virtual assistant use case. In fu-ture work we plan to extend this work and au-tomatically learn such a lexicon. This can re-duce manual effort and scale to larger domains where there is substantial variability on the lan-guage side.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06042</idno>
		<title level="m">Globally normalized transition-based neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning structured natural language representations for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saraswat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From language to programs: Bridging reinforcement learning and maximum marginal likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to reason: End-toend module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to transform natural to formal languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1062" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised training of semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="754" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inferring logical forms from denotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A corpus of natural language for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence-based structured prediction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gardent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online learning of relaxed CCG grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
