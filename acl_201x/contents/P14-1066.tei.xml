<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Continuous Phrase Representations for Translation Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research One Microsoft Way Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Continuous Phrase Representations for Translation Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="699" to="709"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations. A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a neural network whose weights are learned on parallel training data. Experimental evaluation has been performed on two WMT translation tasks. Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The phrase translation model, also known as the phrase table, is one of the core components of phrase-based statistical machine translation (SMT) systems. The most common method of construct- ing the phrase table takes a two-phase approach ( <ref type="bibr" target="#b22">Koehn et al. 2003</ref>). First, the bilingual phrase pairs are extracted heuristically from an automat- ically word-aligned training data. The second phase, which is the focus of this paper, is parame- ter estimation where each phrase pair is assigned with some scores that are estimated based on counting these phrases or their words using the same word-aligned training data.</p><p>Phrase-based SMT systems have achieved state-of-the-art performance largely due to the fact that long phrases, rather than single words, are used as translation units so that useful context in- formation can be captured in selecting translations. However, longer phrases occur less often in train- ing data, leading to a severe data sparseness prob- lem in parameter estimation. There has been a plethora of research reported in the literature on improving parameter estimation for the phrase translation model (e.g., <ref type="bibr" target="#b7">DeNero et al. 2006;</ref><ref type="bibr" target="#b50">Wuebker et al. 2010;</ref><ref type="bibr" target="#b14">He and Deng 2012;</ref><ref type="bibr" target="#b12">Gao and He 2013)</ref>.</p><p>This paper revisits the problem of scoring a phrase translation pair by developing a Continu- ous-space Phrase Translation Model (CPTM). The translation score of a phrase pair in this model is computed as follows. First, we represent each phrase as a bag-of-words vector, called word vec- tor henceforth. We then project the word vector, in either the source language or the target lan- guage, into a respective continuous feature vector in a common low-dimensional space that is lan- guage independent. The projection is performed by a multi-layer neural network. The projected feature vector forms the continuous representa- tion of a phrase. Finally, the translation score of a source-target phrase pair is computed by the dis- tance between their feature vectors.</p><p>The main motivation behind the CPTM is to alleviate the data sparseness problem associated with the traditional counting-based methods by grouping phrases with a similar meaning across different languages. This style of grouping is made possible because of the distributed nature of the continuous-space representations for phrases. No such sharing was possible in the original sym- bolic space for representing words or phrases. In this model, semantically or grammatically related phrases, in both the source and the target lan- guages, would tend to have similar (close) feature vectors in the continuous space, guided by the training objective. Since the translation score is a smooth function of these feature vectors, a small change in the features should only lead to a small change in the translation score.</p><p>The primary research task in developing the CPTM is learning the continuous representation of a phrase that is effective for SMT. Motivated by recent studies on continuous-space language models (e.g., <ref type="bibr" target="#b3">Bengio et al. 2003;</ref><ref type="bibr" target="#b28">Mikolov et al. 2011;</ref><ref type="bibr" target="#b41">Schwenk et al., 2012</ref>), we use a neural net- work to project a word vector to a feature vector. Ideally, the projection would discover those latent features that are useful to differentiate good trans- lations from bad ones, for a given source phrase. However, there is no training data with explicit annotation on the quality of phrase translations. The phrase translation pairs are hidden in the par- allel source-target sentence pairs, which are used to train the traditional translation models. The quality of a phrase translation can only be judged implicitly through the translation quality of the sentences, as measured by BLEU, which contain the phrase pair. In order to overcome this chal- lenge and let the BLEU metric guide the projec- tion learning, we propose a new method to learn the parameters of a neural network. This new method, via the choice of an appropriate objective function in training, automatically forces the fea- ture vector of a source phrase to be closer to the feature vectors of its candidate translations. As a result, the BLEU score is improved when these translations are selected by an SMT decoder to produce final, sentence-level translations. The new learning method makes use of the L-BFGS algorithm and the expected BLEU as the objective function defined on N-best lists.</p><p>To the best of our knowledge, the CPTM pro- posed in this paper is the first continuous-space phrase translation model that makes use of joint representations of a phrase in the source language and its translation in the target language (to be de- tailed in Section 4) and that is shown to lead to significant improvement over a standard phrase- based SMT system (to be detailed in Section 6).</p><p>Like the traditional phrase translation model, the translation score of each bilingual phrase pair is modeled explicitly in our model. However, in- stead of estimating the phrase translation score on aligned parallel data, our model intends to capture the grammatical and semantic similarity between a source phrase and its paired target phrase by pro- jecting them into a common, continuous space that is language independent.</p><p>The rest of the paper is organized as follows. Section 2 reviews previous work. Section 3 re- views the log-linear model for phrase-based SMT and Sections 4 presents the CPTM. Section 5 de- scribes the way the model parameters are esti- mated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Representations of words or documents as contin- uous vectors have a long history. Most of the ear- lier latent semantic models for learning such vec- tors are designed for information retrieval <ref type="bibr" target="#b6">(Deerwester et al. 1990;</ref><ref type="bibr" target="#b17">Hofmann 1999;</ref><ref type="bibr" target="#b4">Blei et al. 2003</ref>). In contrast, recent work on continuous space language models, which estimate the prob- ability of a word sequence in a continuous space ( <ref type="bibr" target="#b3">Bengio et al. 2003;</ref><ref type="bibr" target="#b27">Mikolov et al. 2010</ref>), have ad- vanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition ( <ref type="bibr" target="#b29">Mikolov et al. 2012;</ref><ref type="bibr" target="#b46">Sundermeyer et al. 2013</ref>) and machine translation <ref type="bibr" target="#b29">(Mikolov 2012;</ref><ref type="bibr" target="#b1">Auli et al. 2013)</ref>.</p><p>Because these models are developed for mono- lingual settings, word embedding from these mod- els is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in dif- ferent languages are projected into the shared la- tent vector space ( <ref type="bibr" target="#b10">Dumais et al. 1997;</ref><ref type="bibr" target="#b37">Platt et al. 2010;</ref><ref type="bibr" target="#b48">Vinokourov et al. 2002;</ref><ref type="bibr" target="#b51">Yih et al. 2011;</ref><ref type="bibr" target="#b13">Gao et al. 2011;</ref><ref type="bibr" target="#b18">Huang et al. 2013;</ref><ref type="bibr" target="#b54">Zou et al. 2013</ref>). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6.</p><p>Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continu- ous space n-gram translation models ( <ref type="bibr" target="#b39">Schwenk et al. 2007;</ref><ref type="bibr" target="#b40">Schwenk 2012;</ref><ref type="bibr" target="#b45">Son et al. 2012</ref>), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the n- gram translation models, where the translation probability of a phrase or a sentence is decom- posed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model 1 , which is much more version of such a model can be trained efficiently because the factor models used by Son et al. cannot be applied directly.</p><p>widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its trans- lation in the target language. The recurrent contin- uous translation models proposed by <ref type="bibr" target="#b19">Kalchbrenner and Blunsom (2013)</ref> also adopt the recurrent language model ( <ref type="bibr" target="#b27">Mikolov et al. 2010)</ref>. But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating trans- lations for new words ( <ref type="bibr" target="#b30">Mikolov et al. 2013a</ref>) and ITG reordering ( <ref type="bibr" target="#b24">Li et al. 2013</ref>).</p><p>There has been a lot of research on improving the phrase table in phrase-based SMT <ref type="bibr" target="#b26">(Marcu and Wong 2002;</ref><ref type="bibr">Lamber and Banchs 2005;</ref><ref type="bibr" target="#b7">Denero et al. 2006;</ref><ref type="bibr" target="#b50">Wuebker et al. 2010;</ref><ref type="bibr" target="#b52">Zhang et al., 2011;</ref><ref type="bibr" target="#b14">He and Deng 2012;</ref><ref type="bibr" target="#b12">Gao and He 2013)</ref>. Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discrimi- native training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous repre- sentations of phrases, integrating the strengths as- sociated with these earlier studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Log-Linear Model for SMT</head><p>Phrase-based SMT is based on a log-linear model which requires learning a mapping between input í µí°¹ ∈ ℱ to output í µí°¸∈µí°¸∈ ℰ. We are given  Training samples (í µí°¹ í µí± , í µí°¸íµí°¸í µí± ) for í µí± = 1 … í µí±, where each source sentence í µí°¹ í µí± is paired with a reference translation in target language í µí°¸íµí°¸í µí± ;  A procedure GEN to generate a list of N-best candidates GEN(í µí°¹ í µí± ) for an input í µí°¹ í µí± , where GEN in this study is the baseline phrase- based SMT system, i.e., an in-house implementation of the Moses system ( <ref type="bibr" target="#b20">Koehn et al. 2007</ref>) that does not use the CPTM, and each í µí°¸∈µí°¸∈ GEN(í µí°¹ í µí± ) is labeled by the sentence-level BLEU score (He and Deng 2012), denoted by sBleu(í µí°¸íµí°¸í µí± , í µí°¸) , which measures the quality of í µí°¸withµí°¸with respect to its reference translation í µí°¸íµí°¸í µí± ;  A vector of features í µí°¡ ∈ ℝ í µí± that maps each (í µí°¹ í µí± , í µí°¸) to a vector of feature values 2 ; and  A parameter vector í µí» ∈ ℝ í µí± , which assigns a real-valued weight to each feature.</p><p>The components GEN(. ), í µí°¡ and í µí» define a log- linear model that maps í µí°¹ í µí± to an output sentence as follows:</p><formula xml:id="formula_0">í µí°¸ µí°¸ * = argmax (í µí°¸,í µí°´)∈GEN(í µí°¹ í µí± ) í µí» T í µí°¡(í µí°¹ í µí± , í µí°¸, í µí°´) (1)</formula><p>which states that given í µí» and í µí°¡, argmax returns the highest scoring translation í µí°¸ µí°¸ * , maximizing over correspondences í µí°´. In phrase-based SMT, í µí°´consists µí°´consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since computing the argmax exactly is intractable, it is commonly performed approximatedly by beam search <ref type="bibr" target="#b35">(Och and Ney 2004)</ref>. Following Liang et al. <ref type="formula" target="#formula_5">(2006)</ref>, we assume that every translation candidate is always coupled with a corresponding í µí°´, called the Viterbi derivation, generated by (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Continuous-Space Phrase Transla- tion Model (CPTM)</head><p>The architecture of the CPTM is shown in <ref type="figure" target="#fig_0">Figures  1 and 2</ref>, where for each pair of source and target phrases (í µí± í µí± , í µí± í µí± ) in a source-target sentence pair, we first project them into feature vectors í µí°² í µí± í µí± and í µí°² í µí± í µí± in a latent, continuous space via a neural net- work with one hidden layer (as shown in <ref type="figure">Figure  2</ref>), and then compute the translation score, score(í µí± í µí± , í µí± í µí± ), by the distance of their feature vec- tors in that space.</p><p>We start with a bag-of-words representation of a phrase í µí°± ∈ ℝ í µí± , where í µí°± is a word vector and í µí± is the size of the vocabulary consisting of words in both source and target languages, which is set to 200K in our experiments. We then learn to pro- ject í µí°± to a low-dimensional continuous space ℝ í µí± :</p><formula xml:id="formula_1">í µí¼(í µí°±): ℝ í µí± → ℝ í µí±</formula><p>The projection is performed using a fully con- nected neural network with one hidden layer and tanh activation functions. Let í µí° 1 be the projec- tion matrix from the input layer to the hidden layer and í µí° 2 the projection matrix from the hidden layer to the output layer, we have í µí°² ≡ í µí¼(í µí°±) = tanh (í µí° 2 T (tanh(í µí° 1 T í µí°±))) (2) <ref type="figure">Figure 2</ref>. A neural network model for phrases giving rise to their continuous representations.</p><p>The model with the same form is used for both source and target languages.</p><p>The translation score of a source phrase f and a target phrase e can be measured as the similarity (or distance) between their feature vectors. We choose the dot product as the similarity function 3 :</p><p>score(í µí±, í µí±) ≡ sim í µí» (í µí°± í µí± , í µí°± í µí± ) = í µí°² í µí± T í µí°² í µí±</p><p>According to (2), we see that the value of the scor- ing function is determined by the projection ma- trices í µí» = {í µí° 1 , í µí° 2 }. The CPTM of <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref> can be incorporated into the log-linear model for SMT (1) by <ref type="bibr">3</ref> In our experiments, we compare dot product and the cosine similarity functions and find that the former works better for nonlinear multi-layer neural networks, and the latter works better for linear neural networks. For the sake of clarity, we choose dot product when we describe the CPTM and its train- ing in Sections 4 and 5, respectively. <ref type="bibr">4</ref> The baseline SMT needs to be reasonably good in the sense that the oracle BLEU score on the generated n-best introducing a new feature ℎ í µí±+1 and a new feature weight í µí¼ í µí±+1 . The new feature is defined as</p><formula xml:id="formula_3">ℎ í µí±+1 (í µí°¹ í µí± , í µí°¸, í µí°´) = ∑ sim í µí» (í µí°± í µí± , í µí°± í µí± ) (í µí±,í µí± )∈í µí°´( µí°´(4)</formula><p>Thus, the phrase-based SMT system, into which the CPTM is incorporated, is parameterized by (í µí», í µí»), where í µí» is a vector of a handful of param- eters used in the log-linear model of <ref type="formula">(1)</ref>, with one weight for each feature; and í µí» is the projection matrices used in the CPTM defined by (2) and (3).</p><p>In our experiments we take three steps to learn (í µí», í µí»):</p><p>1. We use a baseline phrase-based SMT sys- tem to generate for each source sentence in training data an N-best list of translation hy- potheses 4 . 2. We set í µí» to that of the baseline system and let í µí¼ í µí±+1 = 1, and optimize í µí» w.r.t. a loss function on training data 5 . 3. We fix í µí» , and optimize í µí» using MERT <ref type="bibr" target="#b34">(Och 2003</ref>) to maximize BLEU on dev data.</p><p>In the next section, we will describe Step 2 in de- tail as it is directly related to the CPTM training.</p><p>lists needs to be significantly higher than that of the top-1 translations so that the CPTM can be effectively trained. <ref type="bibr">5</ref> The initial value of í µí¼ í µí±+1 can also be tuned using the dev set. However, we find in a pilot study that it is good enough to set it to 1 when the values of all the baseline feature weights, used in the log-linear model of <ref type="formula">(1)</ref>, are properly nor- malized, such as by setting í µí¼ í µí± = í µí¼ í µí± /í µí° ¶ for í µí± = 1 … í µí± , where í µí° ¶ is the unnormalized weight value of the target lan- guage model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training CPTM</head><p>This section describes the loss function we em- ploy with the CPTM and the algorithm to train the neural network weights. We define the loss function ℒ(í µí») as the nega- tive of the N-best list based expected BLEU, de- noted by xBleu(í µí»). In the reranking framework of SMT outlined in Section 3, xBleu(í µí») over one training sample (í µí°¹ í µí± , í µí°¸íµí°¸í µí± ) is defined as xBleu(í µí») = ∑ í µí±(í µí°¸|í µí°¹ í µí± )sBleu(í µí°¸íµí°¸í µí± , í µí°¸)</p><formula xml:id="formula_4">í µí°¸∈GEN(í µí°¹ í µí± )<label>(5)</label></formula><p>where sBleu(í µí°¸íµí°¸í µí± , í µí°¸) is the sentence-level BLEU score, and í µí±(í µí°¸|í µí°¹ í µí± ) is the translation probability from í µí°¹ í µí± to í µí°çomputed using softmax as </p><p>where í µí» T í µí°¡ is the log-linear model of <ref type="formula">(1)</ref>, which also includes the feature derived from the CPTM as defined by <ref type="formula">(4)</ref>, and í µí»¾ is a tuned smoothing fac- tor. Let ℒ(í µí») be a loss function which is differen- tiable w.r.t. the parameters of the CPTM, í µí». We can compute the gradient of the loss and learn í µí» using gradient-based numerical optimization al- gorithms, such as L-BFGS or stochastic gradient descent (SGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Computing the Gradient</head><p>Since the loss does not explicitly depend on í µí», we use the chain rule for differentiation: </p><p>which takes the form of summation over all phrase pairs occurring either in a training sample (sto- chastic mode) or in the entire training data (batch mode). í µí»¿ (í µí±,í µí±) in <ref type="formula" target="#formula_6">(7)</ref> is known as the error term of the phrase pair (í µí±, í µí±), and is defined as </p><p>It describes how the overall loss changes with the translation score of the phrase pair (í µí±, í µí±). We will leave the derivation of í µí»¿ (í µí±,í µí±) to Section 5.1.2, and will first describe how the gradient of sim í µí» (í µí°± í µí± , í µí°± í µí± ) w.r.t. í µí» is computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Computing í µí½í µí°¬í µí°¢í µí°¦ í µí» (í µí°± í µí² , í µí°± í µí² )/í µí½í µí»</head><p>Without loss of generality, we use the following notations to describe a neural network:</p><p> í µí° í µí± is the projection matrix for the l-th layer of the neural network;  í µí°± is the input word vector of a phrase;  í µí°³ í µí± is the sum vector of the l-th layer; and  í µí°² í µí± = í µí¼(í µí°³ í µí± ) is the output vector of the l-th layer, where í µí¼ is an activation function;</p><p>Thus, the CPTM defined by <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>  </p><p>The derivation can be easily extended to a neural network with multiple hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Computing í µí¼¹ (í µí²,í µí²)</head><p>To simplify the notation, we rewrite our loss func- tion of (5) and (6) over one training sample as = í µí¼ í µí±+1 í µí±(í µí±, í µí±; í µí°´)</p><formula xml:id="formula_9">ℒ(í µí») = −xBleu(í µí») = − G(í µí») Z(í µí»)<label>(11)</label></formula><p>where í µí±(í µí±, í µí±; í µí°´) is the number of times the phrase pair (í µí±, í µí±) occurs in í µí°´.µí°´. Combining <ref type="formula">(12)</ref> and <ref type="formula" target="#formula_2">(13)</ref>, we end up with the following equation where <ref type="formula">(14)</ref> U(í µí», í µí°¸) = sBleu(í µí°¸íµí°¸í µí± , í µí°¸) − xBleu(í µí»).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Training Algorithm</head><p>In our experiments we train the parameters of the CPTM, í µí», using the L-BFGS optimizer described in <ref type="bibr" target="#b0">Andrew and Gao (2007)</ref>, together with the loss function described in (5). The gradient is com- puted as described in Sections 5.1. Although SGD has been advocated for neural network training due to its simplicity and its robustness to local minima (Bengio 2009), we find that in our task that the L-BFGS minimizes the loss in a desirable fashion empirically when iterating over the com- plete training data (batch mode). For example, the convergence of the algorithm was found to be smooth, despite the non-convexity in our loss. An- other merit of batch training is that the gradient over all training data can be computed efficiently. As shown in Section 5.1, computing í µí¼sim θ (x í µí± , x í µí± )/í µí¼θ requires large-scale matrix multiplications, and is expensive for multi-layer neural networks. Eq. <ref type="formula" target="#formula_6">(7)</ref> suggests that í µí¼sim θ (x í µí± , x í µí± )/í µí¼θ and í µí»¿ (í µí±,í µí±) can be computed separately, thus making the computation cost of the former term only depends on the number of phrase pairs in the phrase table, but not the size of training data. Therefore, the training method de- scribed here can be used on larger amounts of training data with little difficulty. As described in Section 4, we take three steps to learn the parameters for both the log-linear model of SMT and the CPTM. While steps 1 and 3 can be easily parallelized on a computer cluster, the CPTM training is performed on a single ma- chine. For example, given a phrase table contain- ing 16M pairs and a 1M-sentence training set, it takes a couple of hours to generate the N-best lists on a cluster, and about 10 hours to train the CPTM on a Xeon E5-2670 2.60GHz machine.</p><p>For a non-convex problem, model initialization is important. In our experiments we always initial- ize í µí° 1 using a bilingual topic model trained on parallel data (see detail in Section 6.2), and í µí° 2 as an identity matrix. In principle, the loss function of <ref type="formula" target="#formula_4">(5)</ref> can be further regularized (e.g. by adding a term of í µí°¿ 2 norm) to deal with overfitting. How- ever, we did not find clear empirical advantage over the simpler early stop approach in a pilot study, which is adopted in the experiments in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>This section evaluates the CPTM presented on two translation tasks using WMT data sets. We first describe the data sets and baseline setup. Then we present experiments where we compare different versions of the CPTM and previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Baseline. We experiment with an in-house phrase-based system similar to <ref type="bibr">Moses (Koehn et al. 2007)</ref>, where the translation candidates are scored by a set of common features including maximum likelihood estimates of source given target phrase mappings í µí± í µí±í µí°¿í µí°¸(µí°¸(í µí±|í µí±) and vice versa í µí± í µí±í µí°¿í µí°¸(µí°¸(í µí±|í µí±), as well as lexical weighting estimates í µí± í µí°¿í µí± (í µí±|í µí±) and í µí± í µí°¿í µí± (í µí±|í µí±), word and phrase penal- ties, a linear distortion feature, and a lexicalized reordering feature. The baseline includes a stand- ard 5-gram modified Kneser-Ney language model trained on the target side of the parallel corpora described below. Log-linear weights are estimated with the MERT algorithm <ref type="bibr" target="#b34">(Och 2003</ref>). <ref type="bibr" target="#b21">(Koehn and Monz 2006</ref>). The parallel corpus in- cludes 688K sentence pairs of parliamentary pro- ceedings for training. The development set con- tains 2000 sentences, and the test set contains other 2000 sentences, all from the official WMT 2006 shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation. We test our models on two different data sets. First, we train an English to French sys- tem based on the data of WMT 2006 shared task</head><p>Second, we experiment with a French to Eng- lish system developed using 2.1M sentence pairs of training data, which amounts to 102M words, from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sen- tences, as the development set. We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 system combination test set, containing 2034 to 3003 sentences.</p><p>In this study we perform a detailed empirical comparison using the WMT 2006 data set, and verify our best models and results using the larger WMT 2012 data set.</p><p>The metric used for evaluation is case insensi- tive BLEU score ( <ref type="bibr" target="#b36">Papineni et al. 2002)</ref>. We also perform a significance test using the Wilcoxon signed rank test. Differences are considered statis- tically significant when the p-value is less than 0.05. <ref type="table">Table 1</ref> shows the results measured in BLEU eval- uated on the WMT 2006 data set, where Row 1 is the baseline system. Rows 2 to 4 are the systems enhanced by integrating different versions of the CPTM. Rows 5 to 7 present the results of previous models. Row 8 is our best system. <ref type="table">Table 2</ref> shows the main results on the WMT 2012 data set. CPTM is the model described in Sections 4. As illustrated in <ref type="figure">Figure 2</ref>, the number of the nodes in the input layer is the vocabulary size í µí±. Both the hidden layer and the output layer have 100 nodes <ref type="bibr">6</ref> . That is, í µí° 1 is a í µí± × 100 matrix and í µí° 2 a 100 × 100 matrix. The result shows that CPTM leads to a substantial improvement over the baseline system with a statistically significant margin of 1.0 BLEU points as in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results of the CPTM</head><p>We have developed a set of variants of CPTM to investigate two design choices we made in de- veloping the CPTM: (1) whether to use a linear <ref type="bibr">6</ref> We can achieve slightly better results using more nodes in the hidden and output layers, say 500 nodes. But the model projection or a multi-layer nonlinear projection; and (2) whether to compute the phrase similarity using word-word similarities as suggested by e.g., the lexical weighting model ( <ref type="bibr" target="#b22">Koehn et al. 2003</ref>). We compare these variants on the WMT 2006 data set, as shown in <ref type="table">Table 1</ref>.</p><p>CPTML (Row 3 in <ref type="table">Table 1</ref>) uses a linear neural network to project a word vector of a phrase í µí°± to a feature vector í µí°²: í µí°² ≡ í µí¼(í µí°±) = í µí° T í µí°±, where í µí° is a í µí± × 100 projection matrix. The translation score of a source phrase f and a target phrase e is measured as the similarity of their feature vectors. We choose cosine similarity because it works bet- ter than dot product for linear projection. CPTMW (Row 4 in <ref type="table">Table 1</ref>) computes the phrase similarity using word-word similarity scores. This follows the common smoothing strategy of ad- dressing the data sparseness problem in modeling phrase translations, such as the lexical weighting model ( <ref type="bibr" target="#b22">Koehn et al. 2003</ref>) and the word factored n-gram translation model ( <ref type="bibr" target="#b45">Son et al. 2012</ref>). Let í µí±¤ denote a word, and í µí± and í µí± the source and target phrases, respectively. We define  <ref type="figure">(2 + 7)</ref> 34.39 αβ <ref type="table">Table 1</ref>: BLEU results for the English to French task using translation models and systems built on the WMT 2006 data set. The superscripts α and β indicate statistically significant difference (p &lt; 0.05) from Baseline and CPTM, respec- tively.</p><p>where sim í µí¼ (í µí±¤, í µí±) (or sim í µí¼ (í µí±¤, í µí±) ) is the word- phrase similarity, and is defined as a smooth ap- proximation of the maximum function where í µí¼ is the tuned smoothing parameter.</p><p>Similar to CPTM, CPTMW also uses a nonlin- ear projection to map each word (not a phrase vec- tor as in CPTM) to a feature vector.</p><p>Two observations can be made by comparing CPTM in Row 2 to its variants in <ref type="table">Table 1</ref>. First of all, it is more effective to model the phrase trans- lation directly than decomposing it into word- word translations in the CPTMs. Second, we see that the nonlinear projection is able to generate more effective features, leading to better results than the linear projection. We also compare the best version of the CPTM i.e., CPTM, with three related models proposed previously. We start the discussion with the re- sults on the WMT 2006 data set in <ref type="table">Table 1</ref>.</p><p>Rows 5 and 6 in <ref type="table">Table 1</ref> are two state-of-the- art latent semantic models that are originally trained on clicked query-document pairs (i.e., clickthrough data extracted from search logs) for query-document matching ( <ref type="bibr" target="#b13">Gao et al. 2011</ref>). To adopt these models for SMT, we view source-tar- get sentence pairs as clicked query-document pairs, and trained both models using the same methods as in <ref type="bibr" target="#b13">Gao et al. (2011)</ref> on the parallel bi- lingual training data described earlier. Specifi- cally, BTLMPR is an extension to PLSA, and is the best performer among different versions of the Bi-Lingual Topic Model (BLTM) described in <ref type="bibr" target="#b13">Gao et al. (2011)</ref>. BLTM with Posterior Regular- ization (BLTMPR) is trained on parallel training data using the EM algorithm with a constraint en- forcing a source sentence and its paralleled target sentence to not only share the same prior topic dis- tribution, but to also have similar fractions of words assigned to each topic. We incorporated the model into the log-linear model for SMT (1) as 7 <ref type="bibr" target="#b12">Gao and He (2013)</ref> reported results of MRF models with different feature sets. We picked the MRF using phrase fea- tures only (MRFP) for comparison since we are mainly inter- ested in phrase representation.</p><p>follows. First of all, the topic distribution of a source sentence í µí°¹ í µí± , denoted by í µí±(í µí± §|í µí°¹ í µí± ) , is in- duced from the learned topic-word distributions using EM. Then, each translation candidate í µí°¸inµí°¸in the N-best list GEN(í µí°¹ í µí± ) is scored as í µí±(í µí°¸|í µí°¹ í µí± ) = ∏ ∑ í µí±(í µí±¤|í µí± §)í µí±(í µí± §|í µí°¹ í µí± )</p><p>í µí± § í µí±¤∈í µí°¸í µí°¸í µí±(í µí°¹ í µí± |í µí°¸) can be similarly computed. Finally, the logarithms of the two probabilities are incorpo- rated into the log-linear model of <ref type="formula">(1)</ref> as two addi- tional features. DPM is the Discriminative Projec- tion Model described in <ref type="bibr" target="#b13">Gao et al. (2011)</ref>, which is an extension of LSA. DPM uses a matrix to pro- ject a word vector of a sentence to a feature vector. The projection matrix is learned on parallel train- ing data using the S2Net algorithm ( <ref type="bibr" target="#b51">Yih et al. 2011</ref>). DPM can be incorporated into the log-lin- ear model for SMT (1) by introducing a new fea- ture ℎ í µí±+1 for each phrase pair, which is defined as the cosine similarity of the phrases in the pro- ject space.</p><p>As we see from <ref type="table">Table 1</ref>, both latent semantic models, although leading to some slight improve- ment over Baseline, are much less effective than CPTM.</p><p>Finally, we compare the CPTM with the Mar- kov Random Field model using phrase features <ref type="table">(MRFP in Tables 1 and 2</ref>), proposed by <ref type="bibr" target="#b12">Gao and He (2013)</ref>   <ref type="table">Table 2</ref>: BLEU results for the French to English task using translation models and systems built on the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p &lt; 0.05) from Baseline and MRFp, respectively. same expected BLEU based objective function, CPTM and MRFp model the translation relation- ship between two phrases from different angles. MRFp estimates one translation score for each phrase pair explicitly without parameter sharing, while in CPTM, all phrases share the same neural network that projects raw phrases to the continu- ous space, providing a more smoothed estimation of the translation score for each phrase pair.</p><p>The results in <ref type="table">Tables 1 and 2</ref> show that CPTM outperforms MRFP on most of the test sets across the two WMT data sets, but the difference be- tween them is often not significant. Our interpre- tation is that although CPTM provides a better smoothed estimation for low-frequent phrase pairs, which otherwise suffer the data sparsity is- sue, MRFp provides a more precise estimation for those high-frequent phrase pairs. That is, CPTM and MRFp capture complementary information for translation. We thus combine CPTM and MRFP <ref type="table">(Comb in Tables 1 and 2)</ref> by incorporating two features, each for one model, into the log-lin- ear model of SMT (1). We observe that for both translation tasks, accuracy improves by up to 0.8 BLEU over MRFP alone (e.g., on the news2008 test set in <ref type="table">Table 2</ref>). The results confirm that CPTM captures complementary translation infor- mation to MRFp. Overall, we improve accuracy by up to 1.3 BLEU over the baseline on both WMT data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>The work presented in this paper makes two major contributions. First, we develop a novel phrase translation model for SMT, where joint represen- tations are exploited of a phrase in the source lan- guage and of its translation in the target language, and where the translation score of the pair of source-target phrases are represented as the dis- tance between their feature vectors in a low-di- mensional, continuous space. The space is derived from the representations generated using a multi- layer neural network. Second, we present a new learning method to train the weights in the multi- layer neural network for the end-to-end BLEU metric directly. The training method is based on L-BFGS. We describe in detail how the gradient in closed form, as required for efficient optimiza- tion, is derived. The objective function, which takes the form of the expected BLEU computed from N-best lists, is very different from the usual objective functions used in most existing architec- tures of neural networks, e.g., cross entropy <ref type="bibr" target="#b16">(Hinton et al. 2012</ref>) or mean square error ). We hence have provided details in the der- ivation of the gradient, which can serve as an ex- ample to guide the derivation of neural network learning with other non-standard objective func- tions in the future.</p><p>Our evaluation on two WMT data sets show that incorporating the continuous-space phrase translation model into the log-linear framework significantly improves the accuracy of a state-of- the-art phrase-based SMT system, leading to a gain up to 1.3 BLEU. Careful implementation of the L-BFGS optimization based on the BLEU- centric objective function, together with the asso- ciated closed-form gradient, is a key to the suc- cess.</p><p>A natural extension of this work is to expand the model and learning algorithm from shallow to deep neural networks. The deep models are ex- pected to produce more powerful and flexible se- mantic representations (e.g., <ref type="bibr" target="#b47">Tur et al., 2012)</ref>, and thus greater performance gain than what is pre- sented in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The architecture of the CPTM, where the mapping from a phrase to its continuous representation is shown in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>í</head><label></label><figDesc>µí±(í µí°¸|í µí°¹ í µí± ) = exp(í µí»¾í µí» T í µí°¡(í µí°¹ í µí± ,í µí°¸,í µí°´)) ∑ exp(í µí»¾í µí» T í µí°¡(í µí°¹ í µí± ,í µí°¸′µí°¸′ ,í µí°´)) í µí°¸′µí°¸′ ∈GEN(í µí°¹ í µí± )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>í</head><label></label><figDesc>µí¼ℒ(í µí») í µí¼í µí» = ∑ í µí¼ℒ(í µí») í µí¼sim í µí» (í µí°± í µí± , í µí°± í µí± ) í µí¼sim í µí» (í µí°± í µí± , í µí°± í µí± ) í µí¼í µí» (í µí±,í µí± ) = ∑ −í µí»¿ (í µí±,í µí±) í µí¼sim í µí» (í µí°± í µí± , í µí°± í µí± ) í µí¼í µí» (í µí±,í µí± )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>í</head><label></label><figDesc>µí»¿ (í µí±,í µí±) = − í µí¼ℒ(í µí») í µí¼sim í µí» (í µí°± í µí± ,í µí°± í µí± )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>µí°¸)í µí±(í µí°¸|í µí°¹ í µí± )í µí¼ í µí±+1 í µí±(í µí±, í µí±; í µí°´) (í µí°¸,í µí°´)∈í µí°ºí µí°¸í µí±(í µí°¹ í µí± )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>where G(í µí») = ∑ sBleu(í µí°¸, í µí°¸íµí°¸í µí± ) exp(í µí» T í µí°¡(í µí°¹ í µí± , í µí°¸, í µí°´))</head><label></label><figDesc></figDesc><table>í µí°¸Z 

µí°¸Z(í µí») = ∑ exp(í µí» T í µí°¡(í µí°¹ í µí± , í µí°¸, í µí°´)) 

í µí°Çombining 

µí°Çombining (8) and (11), we have 

í µí»¿ (í µí±,í µí±) = 
í µí¼xBleu(í µí») 
í µí¼sim í µí» (í µí°± í µí± , í µí°± í µí± ) 
(12) 

= 
1 
Z(í µí») 
( 
í µí¼G(í µí») 
í µí¼sim í µí» (í µí°± í µí± , í µí°± í µí± ) 
− 
í µí¼Z(í µí») 

í µí¼sim í µí» (í µí°± í µí± , í µí°± í µí± ) 
xBleu(í µí»)) 

Because í µí» is only relevant to ℎ í µí±+1 which is de-
fined in (4), we have 

í µí¼í µí» T í µí°¡(í µí°¹ í µí± , í µí°¸, í µí°´) 
í µí¼sim í µí» (í µí°± í µí± , í µí°± í µí± ) 
= í µí¼ í µí±+1 
í µí¼ℎ í µí±+1 (í µí°¹ í µí± , í µí°¸, í µí°´) 
í µí¼sim í µí» (í µí°± í µí± , í µí°± í µí± ) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>26.25 α 25.05 α 21.15 αβ 24.91 α 4 Comb (2 + 3) 24.46 αβ 26.56 αβ 25.52 αβ 21.64 αβ 25.22 α</head><label></label><figDesc></figDesc><table>7 , on both the WMT 2006 and WMT 
2012 datasets. MRFp is a state-of-the-art large 
scale discriminative training model that uses the 
same expected BLEU training criterion, which 
has proven to give superior performance across a 
range of MT tasks recently (He and Deng 2012, 
Setiawan and Zhou 2013, Gao and He 2013). 
Unlike CPTM, MRFp is a linear model that 
simply treats each phrase pair as a single feature. 
Therefore, although both are trained using the 

# Systems 
dev 
news2011 
news2010 
news2008 
newssyscomb2010 
1 Baseline 
23.58 
25.24 
24.35 
20.36 
24.14 
2 MRFP 
24.07 α 
26.00 α 
24.90 
20.84 α 
25.05 α 
3 CPTM 
24.12 α 
</table></figure>

			<note place="foot" n="1"> Niehues et al. (2011) use different translation units in order to integrate the n-gram translation model into the phrasebased approach. However, it is not clear how a continuous</note>

			<note place="foot" n="2"> Our baseline system uses a set of standard features suggested in Koehn et al. (2007), which is also detailed in Section 6.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>We thank Michael Auli for providing a dataset and for helpful discussions. We also thank the four anonymous reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable training of L1-regularized log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fundamental Trends Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janvin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why generative phrase models underperform surface heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable stacking and learning for building deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Diamantaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</author>
		<title level="m">Principle Component Neural Networks: Theory and Applications</title>
		<imprint>
			<publisher>Wiley-Interscience</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic cross-language retrieval using latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Letsche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-97 Spring Symposium Series: Cross-Language Text and Speech Retrieval</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2001" to="2049" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training MRF-based translation models using gradient ascent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clickthrough-based latent semantic models for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum expected bleu training of phrase and lexicon translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discovering Binary Codes for Documents by Learning Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topics in Cognitive Science</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Moses: open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2007</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>demonstration session</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Manual and automatic evaluation of machine translation between European languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="102" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="127" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Data inferred multi-word expressions for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Banchs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>MT Summit X, Phuket, Thailand</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive autoencoders for ITG-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An end-to-end discriminative approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouchard-Cote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLINGACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A phrase-based, joint probability model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Statistical Language Model based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Polylingual topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Wider context by using bilingual language models in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The alignment template approach to statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Translingual Document Representations from Discriminative Projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Expected BLEU training for graphs: bbn system description for WMT system combination task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A-V</forename><surname>Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Smooth bilingual n-gram translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Costa-Jussa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A R</forename><surname>Fonollosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="430" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large, pruned or continuous space language models on a GPU for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT Workshop on the future of language modeling for HLT</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discriminative training of 150 million translation parameters and its application to pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Continuous space translation models with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="29" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Comparison of feed forward and recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J-L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freiberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8430" to="8434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards deeper understanding: deep convex networks for semantic utterance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Inferring a semantic representation of text via cross-language correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinokourov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Large scale image annotation: learning to rank with joint word-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Training phrase translation models with leaving-oneout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning discriminative projections for text similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A novel decision function and the associated decision-feedback learning for speech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Acero</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Combining heterogeneous models for measuring relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
