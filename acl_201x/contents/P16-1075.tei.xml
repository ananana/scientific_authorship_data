<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Constrained Multi-Task Learning for Automated Essay Scoring</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Cummins</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">ALTA Institute Computer Lab University of Cambridge</orgName>
								<orgName type="laboratory" key="lab2">ALTA Institute Computer Lab University of Cambridge</orgName>
								<orgName type="institution">ALTA Institute Computer Lab University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">ALTA Institute Computer Lab University of Cambridge</orgName>
								<orgName type="laboratory" key="lab2">ALTA Institute Computer Lab University of Cambridge</orgName>
								<orgName type="institution">ALTA Institute Computer Lab University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">ALTA Institute Computer Lab University of Cambridge</orgName>
								<orgName type="laboratory" key="lab2">ALTA Institute Computer Lab University of Cambridge</orgName>
								<orgName type="institution">ALTA Institute Computer Lab University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Constrained Multi-Task Learning for Automated Essay Scoring</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="789" to="799"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Supervised machine learning models for automated essay scoring (AES) usually require substantial task-specific training data in order to make accurate predictions for a particular writing task. This limitation hinders their utility, and consequently their deployment in real-world settings. In this paper, we overcome this shortcoming using a constrained multi-task pairwise-preference learning approach that enables the data from multiple tasks to be combined effectively. Furthermore, contrary to some recent research , we show that high performance AES systems can be built with little or no task-specific training data. We perform a detailed study of our approach on a publicly available dataset in scenarios where we have varying amounts of task-specific training data and in scenarios where the number of tasks increases.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated essay scoring (AES) involves the pre- diction of a score (or scores) relating to the quality of an extended piece of written text <ref type="bibr" target="#b24">(Page, 1966)</ref>. With the burden involved in manually grading stu- dent texts and the increase in the number of ESL (English as a second language) learners world- wide, research into AES is increasingly seen as playing a viable role in assessment. Automating the assessment process is not only useful for ed- ucators but also for learners, as it can provide in- stant feedback and encourage iterative refinement of their writing.</p><p>The AES task has usually been addressed using machine learning. Given a set of texts and associ- ated gold scores, machine learning approaches aim to build models that can generalise to unseen in- stances. Regression <ref type="bibr" target="#b25">(Page, 1994;</ref><ref type="bibr" target="#b27">Persing and Ng, 2014;</ref><ref type="bibr" target="#b29">Phandi et al., 2015)</ref>, classification <ref type="bibr" target="#b19">(Larkey, 1998;</ref><ref type="bibr" target="#b31">Rudner and Liang, 2002)</ref>, and preference- ranking 1 approaches <ref type="bibr" target="#b36">(Yannakoudakis et al., 2011)</ref> have all been applied to the task. In general, ma- chine learning models only perform well when the training and test instances are from similar dis- tributions. However, it is usually the case that essays are written in response to prompts which are carefully designed to elicit answers accord- ing to a number of dimensions (e.g. register, topic, and genre). For example, <ref type="table">Table 1</ref> shows ex- tracts from two prompts from a publicly available dataset 2 that aim to elicit different genres of per- suasive/argumentative responses on different top- ics.</p><p>Most previous work on AES has either ignored the differences between essays written in response to different prompts <ref type="bibr" target="#b36">(Yannakoudakis et al., 2011</ref>) with the aim of building general AES systems, or has built prompt-specific models for each prompt independently <ref type="bibr" target="#b5">(Chen and He, 2013;</ref><ref type="bibr" target="#b27">Persing and Ng, 2014</ref>). One of the problems hindering the wide-scale adoption and deployment of AES sys- tems is the dependence on prompt-specific train- ing data, i.e. substantial model retraining is often needed when a new prompt is released. Therefore, systems that can adapt to new writing tasks (i.e. prompts) with relatively few new task-specific training examples are particularly appealing. For example, a system that is trained using only re- sponses from prompt #1 in <ref type="table">Table 1</ref> may not gener- alise well to essays written in response to prompt #2, and vice versa. Even more complications arise when the scoring scale, marking criteria, and/or grade level (i.e. educational stage) vary from task #1 Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends. Write a letter to your local newspaper in which you state your opinion on the effects computers have on people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#2</head><p>Do you believe that certain materials, such as books, music, movies, magazines, etc., should be removed from the shelves if they are found offensive? Support your position with convincing arguments from your own experience, observations, and/or reading. <ref type="table">Table 1</ref>: Two sample writing tasks from the ASAP (Automated Student Assessment Prize) dataset.</p><p>to task. If essays written in response to different tasks are marked on different scoring scales, then the actual scores assigned to essays across tasks are not directly comparable. This effect becomes even more pronounced when prompts are aimed at students in different educational stages. In this paper, we address this problem of prompt adaptation using multi-task learning. In particular, we treat each prompt as a different task and intro- duce a constrained preference-ranking approach that can learn from multiple tasks even when the scoring scale and marking criteria are different across tasks. Our constrained preference-ranking approach significantly increases performance over a strong baseline system when there is limited prompt-specific training data available. Further- more, we perform a detailed study using varying amounts of task-specific training data and varying numbers of tasks. First, we review some related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A number of commercially available systems for AES, have been developed using machine learn- ing techniques. These include PEG (Project Essay Grade) <ref type="bibr" target="#b26">(Page, 2003)</ref>, e-Rater ( ), and Intelligent Essay Assessor (IEA) <ref type="bibr" target="#b18">(Landauer et al., 1998</ref>). Beyond commercial systems, there has been much research into varying as- pects involved in automated assessment, including coherence ( <ref type="bibr" target="#b14">Higgins et al., 2004;</ref><ref type="bibr" target="#b35">Yannakoudakis and Briscoe, 2012)</ref>, prompt-relevance ( <ref type="bibr" target="#b27">Persing and Ng, 2014;</ref><ref type="bibr" target="#b15">Higgins et al., 2006</ref>), argumenta- tion ( <ref type="bibr" target="#b17">Labeke et al., 2013;</ref><ref type="bibr" target="#b32">Somasundaran et al., 2014;</ref><ref type="bibr" target="#b28">Persing and Ng, 2015)</ref>, grammatical error detection and correction ( <ref type="bibr" target="#b30">Rozovskaya and Roth, 2011;</ref><ref type="bibr" target="#b11">Felice et al., 2014)</ref>, and the development of publicly available resources <ref type="bibr" target="#b36">(Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b8">Dahlmeier et al., 2013;</ref><ref type="bibr" target="#b27">Persing and Ng, 2014;</ref>.</p><p>While most of the early commercially available systems use linear-regression models to map essay features to a score, a number of more sophisticated approaches have been developed. Preference- ranking (or pairwise learning-to-rank) has been shown to outperform regression for the AES prob- lem <ref type="bibr" target="#b36">(Yannakoudakis et al., 2011</ref>). However, they did not study prompt-specific models, as their models used training data originating from dif- ferent prompts. We also adopt a preference- ranking approach but explicitly model prompt ef- fects during learning. Algorithms that aim to di- rectly maximise an evaluation metric have also been attempted. A listwise learning-to-rank ap- proach <ref type="bibr" target="#b5">(Chen and He, 2013</ref>) that directly op- timises quadratic-weighted Kappa, a commonly used evaluation measure in AES, has also shown promising results.</p><p>Using training data from natural language tasks to boost performance of related tasks, for which there is limited training data, has received much attention of late <ref type="bibr" target="#b7">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b10">Duh et al., 2010;</ref><ref type="bibr" target="#b6">Cheng et al., 2015)</ref>. However, there have been relatively few attempts to apply transfer learning to automated assessment tasks. Notwith- standing, Napoles and Callison-Burch (2015) use a multi-task approach to model differences in assessors, while <ref type="bibr" target="#b13">Heilman and Madnani (2013)</ref> specifically focus on domain-adaptation for short answer scoring over common scales. Most rel- evant is the work of <ref type="bibr" target="#b29">Phandi et al. (2015)</ref>, who applied domain-adaptation to the AES task using EasyAdapt (EA) <ref type="bibr" target="#b9">(Daume III, 2007)</ref>. They showed that supplementing a Bayesian linear ridge re- gression model (BLRR) with data from one other source domain is beneficial when there is limited target domain data. However, it was shown that simply using the source domain data as extra train- ing data outperformed the EA domain adaptation approach in three out of four cases. One major limitation to their approach was that in many in- stances the source domain and target domain pairs were from different grade levels. This means that any attempt to resolve scores to a common scale is undermined by the fact that the gold scores are not comparable across domains, as the essays were written by students of different educational levels. A further limitation is that multi-domain adapta-tion (whereby one has access to multiple source domains) was not considered.</p><p>The main difference between our work and pre- vious work is that our model incorporates multiple source tasks and introduces a learning mechanism that enables us to combine these tasks even when the scores across tasks are not directly compara- ble. This has not been achieved before. This is non-trivial as it is difficult to see how this can be accomplished using a standard linear-regression approach. Furthermore, we perform the first com- prehensive study of multi-task learning for AES using different training set sizes for a number of different learning scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preference Ranking Model</head><p>In this section, we describe our baseline AES model which is somewhat similar to that devel- oped by <ref type="bibr" target="#b36">Yannakoudakis et al. (2011)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Perceptron Ranking (TAP rank )</head><p>We use a preference-ranking model based on a bi- nary margin-based linear classifier (the Timed Ag- gregate Perceptron or TAP) ( <ref type="bibr" target="#b3">Briscoe et al., 2010)</ref>. In its simplest form this Perceptron uses batch learning to learn a decision boundary for classi- fying an input vector x i as belonging to one of two categories. A timing-variable τ (set to 1.0 by default) controls both the learning rate and the number of epochs during training. A preference- ranking model is then built by learning to clas- sify pairwise difference vectors, i.e. learning a weight vector w such that w(x i − x j ) &gt; δ, when essay i has a higher gold score than essay j, where δ is the one-sided margin 3 <ref type="bibr" target="#b16">(Joachims, 2002;</ref><ref type="bibr" target="#b4">Chapelle and Keerthi, 2010)</ref>. Therefore, instead of directly learning to predict the gold score of an essay vector, the model learns a weight vector w that minimizes the misclassification of difference vectors. Given that the number of pairwise differ- ence vectors in a moderately sized dataset can be extremely large, the training set is reduced by ran- domly sampling difference vectors according to a user-defined probability ( <ref type="bibr" target="#b3">Briscoe et al., 2010)</ref>. In all experiments in our paper we choose this proba- bility such that 5n difference vectors are sampled, where n is the number of training instances (es- says) used. We did not tune any of the hyper- parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">From Rankings to Predicted Scores</head><p>As the weight vector w is optimized for pairwise ranking, a further step is needed to use the rank- ing model for predicting a score. In particular, for each of the n vectors in our training set, a real-scalar value is assigned according to the dot- product of the weight vector and the training in- stance (i.e. w · x i ), essentially giving its distance (or margin) from the zero vector. Then using the training data, we train a one-dimensional linear re- gression model β + to map these assignments to the gold score of each instance.</p><p>Finally, to make a predictionˆypredictionˆ predictionˆy for a test vector, we first calculate its distance from the zero vector using w · x i and map it to the scoring scale using the linear regression modeî y = β(w · x i ) + . For brevity we denote this entire approach (a ranking and a linear regression step) to predicting the final score as TAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Features</head><p>The set of features used for our ranking model is similar to those identified in previous work <ref type="bibr" target="#b36">(Yannakoudakis et al., 2011;</ref><ref type="bibr" target="#b29">Phandi et al., 2015</ref>) and is as follows:</p><p>1. word unigrams, bigrams, and trigrams 2. POS (part-of-speech) counts 3. essay length (as the number of unique words) 4. GRs (grammatical relations) 5. max-word length and min-sentence length 6. the presence of cohesive devices 7. an estimated error rate Each essay is processed by the RASP system ( <ref type="bibr" target="#b2">Briscoe et al., 2006</ref>) with the standard tokeni- sation and sentence boundary detection modules. All n-grams are extracted from the tokenised sen- tences. The grammatical relations (GRs) are ex- tracted from the top parse of each sentence in the essay. The presence of cohesive devices are used as features. In particular, we use four categories (i.e. addition, comparison, contrast and conclu- sion) which are hypothesised to measure the cohe- sion of a text.</p><p>The error rate is estimated based on a language model using ukWaC <ref type="bibr" target="#b12">(Ferraresi et al., 2008</ref>) which contains more than 2 billion English tokens. A tri- gram in an essay will be treated as an error if it  <ref type="table">Table 2</ref>: Details of ASAP dataset and a preliminary evaluation of the performance of our TAP baseline against previous work ( <ref type="bibr" target="#b29">Phandi et al., 2015</ref>). All models used only task-specific data and 5-fold cross- validation. Best result is in bold.</p><p>is not found in the language model. Spelling er- rors are detected using a dictionary lookup, while a rule-based error module ( <ref type="bibr" target="#b0">Andersen et al., 2013</ref>) with rules generated from the Cambridge Learner Corpus (CLC) <ref type="bibr" target="#b23">(Nicholls, 2003)</ref> is used to detect further errors. Finally, the unigrams, bigrams and trigrams are weighted by tf-idf <ref type="bibr" target="#b33">(Sparck Jones, 1972)</ref>, while all other features are weighted by their actual frequency in the essay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data and Preliminary Evaluation</head><p>In order to compare our baseline with previ- ous work, we use the ASAP (Automated Stu- dent Assessment Prize) public dataset. Some details of the essays for the eight tasks in the dataset are described in the <ref type="table">Table 2</ref>. The prompts elicit responses of different genres and of dif- ferent lengths. In particular, it is important to note that the prompts have different scoring scales and are associated with different grade levels <ref type="bibr">(7)</ref><ref type="bibr">(8)</ref><ref type="bibr">(9)</ref><ref type="bibr">(10)</ref>. Furthermore, the gold scores are distributed differently even if resolved to a common 0-60 scale. In order to benchmark our baseline system against previously developed approaches (BLRR and SVM regression ( <ref type="bibr" target="#b29">Phandi et al., 2015)</ref>) which use this data, we learned task-specific models us- ing 5-fold cross-validation within each of the eight ASAP sets and aim to predict the unresolved orig- inal score as per previous work. We present the quadratic weighted kappa (QW-κ) of the sys- tems in <ref type="table">Table 2</ref>. <ref type="bibr">4</ref> Our baseline preference-ranking model (TAP) outperforms previous approaches on task-specific data. It is worth noting that we did not tune either of the hyperparameters of TAP. <ref type="bibr">4</ref> The results for BLRR and SVM regression are taken di- rectly from the original work and it is unlikely that we have used the exact same fold split. Regardless, the consistent in- creases mean that TAP represents a strong baseline system upon which we develop our constrained multi-task approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multi-Task Learning</head><p>For multi-task learning we use EA encoding <ref type="bibr" target="#b9">(Daume III, 2007</ref>) extended over k tasks T j=1..k where each essay x i is associated with one task x i ∈ T j . The transfer-learning algorithm takes a set of input vectors associated with the essays, and for each vector x i ∈ R F maps it via Φ(x i ) to a higher dimensional space Φ(x i ) ∈ R (1+k)·F . The encoding function Φ(x i ) is as follows:</p><formula xml:id="formula_0">Φ(x) = k j=0 f (x, j)<label>(1)</label></formula><p>where is vector concatenation and f (x, j) is as follows:</p><formula xml:id="formula_1">f (x, j) =      x, if j = 0 x, if x ∈ T j 0 F , otherwise (2)</formula><p>Essentially, the encoding makes a task-specific copy of the original feature space of dimen- sionality F to ensure that there is one shared- representation and one task-specific representation for each input vector (with a zero vector for all other tasks). This approach can be seen as a re- encoding of the input vectors and can be used with any vector-based learning algorithm. <ref type="figure">Fig. 1  (left)</ref> shows an example of the extended feature vectors for three tasks T j on different scoring scales. Using only the shared-representation (in blue) as input vectors to a learning algorithm re- sults in a standard approach which does not learn task-specific characteristics. However, using the full representation allows the learning algorithm to capture both general and task-specific charac- teristics jointly. This simple encoding technique is easy to implement and has been shown to be useful for a number of NLP tasks <ref type="bibr" target="#b9">(Daume III, 2007</ref>).</p><formula xml:id="formula_2">x 1 x 1 0 0 x 2 x 2 0 0 x 3 x 3 0 0 x 4 0 0 x 4 x 5 0</formula><note type="other">0 x 5 x 6 0 x 6 0 x 7 0 x 7 0 w 0 w 1 w 2 w 3 01 03 00 45 60 20 10 T1 T2 T3 F 0 F 1 F 2 F 3 original score y i shared representation task-specific representations Φ(x 4 ) Φ(x 5 ) Φ(x 5 ) − Φ(x 4 ) train ranking model w wΦ(x 1 ) wΦ(x 2 ) train linear regressor β 1 wΦ(x 4 ) wΦ(x 5 ) train linear regressor β 2 wΦ(x 6 ) wΦ(x 7 )</note><p>train linear regressor β 3 <ref type="figure">Figure 1</ref>: Example of the constrained multi-task learning approach for three tasks where the shared representation is in blue and the task-specific representations are in orange, red, and green. The original gold scores for each task T j are on different scoring scales. The preference-ranking weight vector w to be learned is shown at the bottom. A one-dimensional linear regression model is learned for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Constrained Preference-Ranking</head><p>Given essays from multiple tasks, it is often the case that the gold scores have different distribu- tions, are not on the same scale, and have been marked using different criteria. Therefore, we in- troduce a modification to TAP (called cTAP rank ) that constrains the creation of pairwise difference vectors when training the weight vector w. In par- ticular, during training we ensure that pairwise dif- ference vectors are not created from pairs of essays originating from different tasks. <ref type="bibr">5</ref> We ensure that the same number of difference vectors are sam- pled during training for both TAP rank and our con- strained version (i.e. both models use the same number of training instances). <ref type="figure">Figure 1</ref> shows an example of the creation of a valid pairwise- difference vector in the multi-task framework. Furthermore, for cTAP rank we train a final lin- ear regression step on each of the task-specific training data separately. Therefore, we predict a score y for essay x i for task T j asˆyasˆ asˆy = β j (w · x i ) + j . This is because for cTAP rank we assume that scores across tasks are not necessar- ily comparable. Therefore, although we utilise in- formation originating from different tasks, the ap- proach never mixes or directly compares instances originating from different tasks. This approach to predicting the final score is denoted cTAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Set-up</head><p>In this section, we outline the different learning scenarios, data folds, and evaluation metrics used in our main experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Learning Approaches</head><p>We use the same features outlined in Section 3.3 to encode feature vectors for our learning ap- proaches. In particular we study three learning approaches denoted and summarised as follows:</p><p>TAP: which uses the TAP rank algorithm with input vectors x i of dimensionality F . MTL-TAP: which uses the TAP rank algorithm with MTL extended input vectors Φ(x i ).</p><p>MTL-cTAP: which uses the cTAP rank algo- rithm with MTL extended input vectors Φ(x i ). <ref type="bibr">6</ref> For TAP and MTL-TAP, we attempt to resolve the essay score to a common scale (0-60) and subsequently train and test using this resolved scale. We then convert the score back to the original prompt-specific scale for evaluation. This is the approach used by the work most similar to ours <ref type="bibr" target="#b29">(Phandi et al., 2015)</ref>. It is worth noting that the resolution of scores to a common scale prior to training is necessary for both TAP and MTL-TAP when using data from multiple ASAP prompts. However, this step is not required for MTL-cTAP as this algorithm learns a ranking function w without directly comparing essays from different sets during training. Furthermore, the final regres-  <ref type="table">Table 3</ref>: Average Spearman ρ of systems over two-folds on the ASAP dataset. The best approach per prompt is in bold. ‡ ( †) means that ρ is statistically greater than Src-TAP (top half) and All-TAP (bottom half) using the Steiger test at the 0.05 level ( ‡ means significant for both folds, † means for one of the folds), while means statistically greater than All-MTL-TAP on both folds ( for one fold).  <ref type="table">Table 4</ref>: Average QW-κ of systems over two-folds on the ASAP dataset. The best approach per prompt is in bold. ‡ ( †) means that κ is statistically (p &lt; 0.05) greater than All-TAP using an approximate randomisation test <ref type="bibr" target="#b37">(Yeh, 2000</ref>) using 50,000 samples. means statistically greater than All-MTL-TAP on both folds ( for one fold). sion step in cTAP only uses original target task scores and therefore predicts scores on the correct scoring scale for the task. We study the three different learning approaches, TAP, MTL-TAP, and MTL-cTAP, in the following scenarios:</p><p>All: where the approach uses data from both the target task and the available source tasks.</p><p>Tgt: where the approach uses data from the target task only.</p><p>Src: where the approach uses data from only the available source tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data Folds</head><p>For our main experiments we divide the essays associated with each of the eight tasks into two folds. For all subsequent experiments, we train us- ing data in one fold (often associated with multiple tasks) and test on data in the remaining fold of the specific target task. We report results for each task separately. These splits allow us to perform stud- ies of all three learning approaches (TAP, MTL- TAP, and MTL-cTAP) using varying amounts of source and target task training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation Metrics</head><p>We use both Spearman's ρ correlation and Quadratic-weighted κ (QW-κ) to evaluate the per- formance of all approaches. Spearman's ρ mea- sures the quality of the ranking of predicted scores produced by the system (i.e. the output from the ranking-preference model). We calculate Spear- man's ρ using the ordinal gold score and the real- valued prediction on the original prompt-specific scoring scale of each prompt. Statistical signifi- cant differences between two correlations sharing one dependent variable (i.e. the gold scores) can be determined using <ref type="bibr" target="#b34">Steiger's (1980)</ref> test.</p><p>QW-κ measures the chance corrected agree- ment between the predicted scores and the gold scores. QW-κ can be viewed as a measure of ac- curacy as it is lower when the predicted scores are further away from the gold scores. This metric measures both the quality of the ranking of scores and the quality of the linear regression step of our approach. These metrics are complementary as they measure different aspects of performance. We calculate QW-κ using the ordinal gold score and the real-valued prediction rounded to the near- est score on the original prompt-specific scale (see <ref type="table">Table 2</ref>).  All-TAP Tgt-TAP All-MTL-TAP All-MTL-cTAP <ref type="figure">Figure 2</ref>: Average QW-κ over two folds for all tasks as size of target task training data increases 7 Results and Discussion <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref> show the performance of a number of models for both ρ and κ respectively. In general, we see that the MTL versions nearly always outperform the baseline TAP when using the same training data. This shows that multi- task learning is superior to simply using the source tasks as extra training data for the AES task. Inter- estingly this has not been shown before. Further- more, the MTL-cTAP approach tends to be sig- nificantly better than the other for many prompts under varying scenarios for both Spearman's ρ and QW-κ. This shows that models that attempt to directly compare essays scores across certain writing-tasks lead to poorer performance.</p><p>When looking at Spearman's ρ in <ref type="table">Table 3</ref> we see that the models that do not use any target task data during training (Src) can achieve a performance which is close to the baseline that only uses all of the available target data (Tgt-TAP). This indicates that our system can rank essays well without any target task data. However, it is worth noting that without any target task training data and lacking any prior information as to the distribution of gold scores for the target task, achieving a consistently high accuracy (i.e. QW-κ) is extremely difficult (if not impossible). Therefore, <ref type="table">Table 4</ref> only shows results for models that make use of target task data.</p><p>For the models trained with data from all eight tasks, we can see that All-MTL-cTAP outperforms both All-TAP and All-MTL-TAP on most of the tasks for both evaluation metrics (ρ and κ). In- terestingly, All-MTL-cTAP also outperforms Tgt- TAP on most of the prompts for both evaluation metrics. This indicates that All-MTL-cTAP man- ages to successfully incorporate useful informa- tion from the source tasks even when there is am- ple target-task data. We next look at scenarios when target-task training data is lacking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Study of Target-Task Training Size</head><p>In real-world scenarios, it is often the case that we lack training data for a new writing task. We now report the results of an experiment that uses vary- ing amounts of target-task training data. In partic- ular, we use all source tasks and initially a small sample of task-specific data for each task (every 128 th target essay) and measure the performance of Tgt-TAP and the All-* models. We then dou- ble the amount of target-task training data used (by using every 64 th essay) and again measure perfor- mance, repeating this process until all target-task data is used. <ref type="figure">Figure 2</ref> shows the performance of Tgt-TAP and the All-* models as target-task data increases.</p><p>In particular, <ref type="figure">Figure 2</ref> shows that All-MTL- cTAP consistently outperforms all approaches in terms of agreement (QW-κ) and is particularly su- perior when there is very little target-task training data. It is worth remembering that All-MTL-cTAP only uses the target-task training instances for the final linear regression step. These results indicate that because the preference-ranking model per- forms so well, only a few target-task training in- stances are needed for the linear-regression step of All-MTL-cTAP. On the other hand, All-MTL-TAP uses all of the training instances in its final linear regression step, and performs significantly worse on a number of prompts. Again this shows the strengths of the constrained multi-task approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Study of Number of Source-tasks</head><p>All previous experiments that used source task data used the entire seven additional tasks. We  now study the performance of the approaches as the number of source tasks changes. In particu- lar, we limit the number of target task training in- stances to 25 and cumulatively add entire source task data in the order in which they occur in Ta- ble 2, starting with the source task appearing di- rectly after the target task. We then measure per- formance at each stage. At the end of the process, each approach has access to all source tasks and the limited target task data. <ref type="figure" target="#fig_1">Figure 3</ref> shows the QW-κ for each prompt as the number of source tasks increases. We can see that All-TAP is the worst performing approach and of- ten decreases as certain tasks are added as training data. All-MTL-cTAP is the best performing ap- proach for nearly all prompts. Furthermore, All- MTL-cTAP is more robust than other approaches, as it rarely decreases in performance as the num- ber of tasks increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Qualitative Analysis</head><p>As an indication of the type of interpretable in- formation contained in the task-specific repre- sentations of the All-MTL-cTAP model, we ex- amined the shared representation and two task- specific representations that relate to the example tasks outlined in <ref type="table">Table 1</ref>. <ref type="table">Table 5</ref> shows the top weighted lexical features (i.e. unigrams, bigrams, or trigrams) (and their respective weights) in dif- ferent parts of the All-MTL-cTAP model.</p><p>In general, we can see that the task-specific lex- ical components of the model capture topical as- pects of the tasks and enable domain adaptation to occur. For example, we can see that books, materi- als, and censorship are highly discriminative lexi- cal features for ranking essays written in response to task #2. The shared representation contains highly weighted lexical features across all tasks and captures vocabulary items useful for ranking in general. While this analysis gives us some in- sight into our model, it is more difficult to interpret the weights of other feature types (e.g. POS, GRs) across different parts of the model. We leave fur- ther analysis of our approach to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion and Conclusion</head><p>Unlike previous work <ref type="bibr" target="#b29">(Phandi et al., 2015</ref>) we have shown, for the first time, that MTL outper- forms an approach of simply using source task data as extra training data. This is because our ap- proach uses information from multiple tasks with- out directly relying on the comparability of gold scores across tasks. Furthermore, it was concluded in previous work that at least some target-task training data is necessary to build high perform- ing AES systems. However, as seen in <ref type="table">Table 3</ref>, high performance rankers (ρ) can be built with- out any target-task data. Nevertheless, it is worth noting that without any target-data, accurately pre- dicting the actual score (high κ) is extremely dif- ficult. Therefore, although some extra informa- tion (i.e. the expected distribution of gold scores) would need to be used to produce accurate scores with a high quality ranker, the ranking is still use- ful for assessment in a number of scenarios (e.g. grading on a curve where the distribution of stu- dent scores is predefined).</p><p>The main approach adopted in this paper is quite similar to using SVM rank <ref type="bibr" target="#b16">(Joachims, 2002)</ref> while encoding the prompt id as the qid. When combined with a multi-task learning technique this allows the preference-ranking algorithm to learn  <ref type="table">Table 5</ref>: Highest weighted lexical features (i.e. unigrams, bigrams, or trigrams) and their weights in both shared and task-specific representations of the All-MTL-cTAP model (associated with results in <ref type="table">Table 4</ref>) for the two example tasks referred to in <ref type="table">Table 1.</ref> both task-specific and shared-representations in a theoretically sound manner (i.e. without making any speculative assumptions about the relative or- derings of essays that were graded on different scales using different marking criteria), and is gen- eral enough to be used in many situations. Ultimately these complementary techniques (multi-task learning and constrained pairwise preference-ranking) allow essay scoring data from any source to be included during training. As shown in Section 7.2, our approach is robust to increases in the number of tasks, meaning that one can freely add extra data when available and expect the approach to use this data appropri- ately. This constrained multi-task preference- ranking approach is likely to be useful for many applications of multi-task learning, when the gold- scores across tasks are not directly comparable.</p><p>Future work will aim to study different dimen- sions of the prompt (e.g. genre, topic) using multi- task learning at a finer level. We also aim to further study the characteristics of the multi-task model in order to determine which features transfer well across tasks. Another avenue of potential research is to use multi-task learning to predict scores for different aspects of text quality (e.g. coherence, grammaticality, topicality).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average QW-κ over two folds as number of source tasks increases (using 25 target task instances)</figDesc></figure>

			<note place="foot" n="1"> also known as pairwise learning-to-rank 2 available at https://www.kaggle.com/c/ asap-aes</note>

			<note place="foot" n="3"> This margin is set to δ = 2.0 by default.</note>

			<note place="foot" n="5"> The same effect can be achieved in SVM rank by encoding the prompt/task using the query id (qid). This constraint is analogous to the way SVM rank is used in information retrieval where document relevance scores returned from different queries are not comparable.</note>

			<note place="foot" n="6"> In the standard learning scenario when only target task data is available, MTL-TAP and MTL-cTAP are identical.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Cambridge English Lan-guage Assessment for supporting this research, and the anonymous reviewers for their useful feed-back. We would also like to thank Ekaterina Kochmar, Helen Yannakoudakis, Marek Rei, and Tamara Polajnar for feedback on early drafts of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Developing and testing a self-assessment and tutoring system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Øistein E Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fiona</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated essay scoring with e-rater R v. 2. The Journal of Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigal</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning and Assessment</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The second release of the RASP system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions</title>
		<meeting>the COLING/ACL 2006 Interactive Presentation Sessions<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="77" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Automated assessment of esol free text examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Øistein</forename><surname>Andersen</surname></persName>
		</author>
		<idno>790</idno>
		<imprint>
			<date type="published" when="2010-02" />
		</imprint>
		<respStmt>
			<orgName>The Computer Lab, University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient algorithms for ranking with svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="201" to="215" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated essay scoring by maximizing human-machine agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1741" to="1752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Open-domain name error detection using a multitask RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner english: The nus corpus of learner english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siew Mei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="256" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, chapter NBest Reranking by Multitask Learning</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, chapter NBest Reranking by Multitask Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="375" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grammatical error correction using hybrid systems and type filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>CoNLL; Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-26" />
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
	<note>Helen Yannakoudakis, and Ekaterina Kochmar</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Introducing and evaluating ukwac, a very large web-derived corpus of english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google</title>
		<meeting>the 4th Web as Corpus Workshop (WAC-4) Can we beat Google</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="47" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ets: domain adaptation and stacking for short answer scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd joint conference on lexical and computational semantics</title>
		<meeting>the 2nd joint conference on lexical and computational semantics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="275" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating multiple aspects of coherence in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying off-topic student essays without topic-specific training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Attali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="159" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Openessayist: extractive summarisation and formative assessment of freetext essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Labeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whitelock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Field</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jte</forename><surname>Pulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Discourse-Centric Learning Analytics</title>
		<meeting>the 1st International Workshop on Discourse-Centric Learning Analytics<address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An introduction to latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas K Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><surname>Foltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discourse processes</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="259" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic essay grading using text categorization techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leah S Larkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatically scoring freshman writing: A preliminary investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Tenth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The conll-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwee Tou Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
	<note>Shared Task)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The cambridge learner corpus: Error coding and analysis for lexicography and elt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Nicholls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Corpus Linguistics</title>
		<meeting>the Corpus Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The imminence of grading essays by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis B Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phi Delta Kappan</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="238" to="243" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Computer grading of student prose, using modern concepts and software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Page</forename><surname>Ellis Batten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of experimental education</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="142" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Project essay grade: Peg. Automated essay scoring: A cross-disciplinary perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Page</forename><surname>Ellis Batten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling prompt adherence in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling argument strength in student essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Persing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="543" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Flexible domain adaptation for automated essay scoring using correlated linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Phandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><forename type="middle">A</forename><surname>Kian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Algorithm selection and model adaptation for esl correction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="924" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Automated essay scoring using bayes&apos; theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahung</forename><surname>Rudner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Technology, Learning and Assessment</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lexical chaining for measuring discourse coherence quality in test-taker essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapna</forename><surname>Somasundaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="950" to="961" />
		</imprint>
		<respStmt>
			<orgName>August. Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Sparck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tests for comparing elements of a correlation matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling coherence in esol learner texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading esol texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Computational Linguistics</title>
		<meeting>the 18th Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="947" to="953" />
		</imprint>
	</monogr>
	<note>COLING &apos;00. Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
