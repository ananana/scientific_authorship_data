<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Constructing Sports News from Live Text Commentary</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">China Key Laboratory of Computational Linguistic (Peking University)</orgName>
								<orgName type="institution" key="instit2">MOE</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Constructing Sports News from Live Text Commentary</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1361" to="1371"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we investigate the possibility to automatically generate sports news from live text commentary scripts. As a preliminary study, we treat this task as a special kind of document summarization based on sentence extraction. We formulate the task in a supervised learning to rank framework, utilizing both traditional sentence features for generic document summarization and novelly designed task-specific features. To tackle the problem of local redundancy, we also propose a probabilistic sentence selection algorithm. Experiments on our collected data from football live commentary scripts and corresponding sports news demonstrate the feasibility of this task. Evaluation results show that our methods are indeed appropriate for this task, outperforming several baseline methods in different aspects.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are a huge number of sports games played each day. It is demanding and challenging to write corresponding news reports instantly after various games. Meanwhile, live text commentary services are available on the web and becoming increas- ingly popular for sports fans who do not have ac- cess to live video streams due to copyright reasons. Some people may also prefer live texts on portable devices. The emergence of live texts has produced huge amount of text commentary data. To the best of our knowledge, there exists few studies about utilizing this rich data source.</p><p>Manually written sports news for match report usually share the same information and vocabulary as live texts for the corresponding sports game. Sports news and commentary texts can be treated as two different sources of descriptions for the same sports events. It is tempting to investigate whether we can utilize the huge amount of live texts to automatically construct sports news, typ- ically in a form of match report. Building such a system will largely relax the burden of sports news editors, making them free from repetitive tedious efforts for writing while producing sports news more efficiently.</p><p>In this work, we study the possibility to con- struct sports news in the form of match reports from given live text commentary scripts. As a con- crete example we collect live text data and corre- sponding news reports for football (called soccer more often in the United States) games and con- duct our study thereby. However, our methods and discussions made in this paper can be trivially adapted to other types of sports games as well.</p><p>As a preliminary study, we treat this task as a special kind of document summarization: ex- tracting sentences from live texts to form a match report as generated news. However, generating sports news from live texts is still challenging due to some unique properties of live text commentary scripts. For almost every minute of the game there are usually several sentences describing various kinds of events. Texts are ordered and organized by the timeline, without apparent highlights for many important events <ref type="bibr">1</ref> . Descriptions are usually in short sentences, which is not helpful for sen- tence scoring and selection in general. The com- mentators may tend to use similar, repeated words describing the same type of key events, which may bring additional challenges to traditional summa- rization methods that are designed to avoid literal repetitions in nature. As a result, naively treating the task as an ordinary document summarization problem can hardly lead to the construction of rea- sonable sports news reports.</p><p>To overcome these difficulties, we explore some specific features of live text commentary scripts and formulate a system based on supervised learn- ing to rank models for this task. In order to tackle the local redundancy issue, we also propose a probabilistic sentence selection strategy.</p><p>We summarize our contributions as follows:</p><p>• We originally study the task of sports news construction from live text commentary and we build datasets for supervised learning and evaluation for this task.</p><p>• We formulate the task in a learning to rank framework, utilizing both traditional features for document summarization and novel task- specific features during supervised learning.</p><p>• We propose a probabilistic sentence selection algorithm to address the issue of local redun- dancy in description.</p><p>• We conduct a series of experiments on a real dataset and the evaluation results verify the performance of our system. Results suggest that constructing sports news from live texts is feasible and our proposed methods can out- perform a few strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Description</head><p>In this work, we treat the task of constructing sports news from live text commentary as a spe- cial kind of document summarization: extracting sentences from live text scripts to form a match report. Formally, given a piece of live text commen- tary containing a collection of candidate sen- tences S = {s 1 , s 2 , . . . , s n } describing a partic- ular sports game G, we need to extract sentences to form a summary of G which are suitable to be formed as sports news. The total length should not exceed a pre-specified length budget B.</p><p>The overall framework of generic document summarization can still be retained for this prelim- inary study. We first rank all candidate sentences according to a sentence scoring scheme and then select a few sentences according to certain criteria to form the final generated news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Collection</head><p>To the best of our knowledge, there does not ex- ist off-the-shelf datasets for evaluating sports news construction. Therefore we have to build a new dataset for this study. We will focus on live text scripts for football (soccer) games as a concrete instance, since football live texts are the easiest to collect. Note that the methods and discussions described in this paper can trivially generalize to other types of sports games.</p><p>Meanwhile, live text commentary services are extremely popular in China, where sports fans in many cases do not have access to live video streams due to copyright reasons. The most influ- ential football live services are Sina Sports Live 2 and 163 Football Live 3 . For evaluation purposes we need to simultaneously collect both live texts and news texts describing the same sports games. Due to the convenience and availability of parallel data collection, we build our dataset from Chinese websites. For most football games, there exist both live text scripts recorded after the games and human-written news reports on both Sina Sports and 163 Football. We crawl live text commentary scripts for 150 football matches on Sina Sports Live. <ref type="figure">Figure 1</ref> displays an example of the format of the live texts, containing the main commentary text along with information of the current timeline and scoreline. For every match, two different corresponding sports news reports are collected from Sina Sports Live and 163 Football Matches Live, respec- tively. These news reports are manually written by professional editors and therefore suitable to be treated as gold-standard news for our task. The av- erage number of sentences in the live texts for one match is around 242, containing around 4,590 Chi- nese characters for that match. The gold-standard news reports contain 1,185 Chinese characters on average, forming around 32 sentences.</p><p>For both the gold-standard news and live text commentary scripts, we split them into sentences and then use a Chinese word segmentation tool <ref type="bibr">4</ref> to segment the sentences into word sequences. For each sentence, we compute its TFIDF vector for calculating literal cosine similarity when used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Constructing Sports News via Sentence Extraction</head><p>We build a system to automatically construct match reports from live text commentary. Since we have described the new challenges for this task, we may design a number of relevant features to address them. In this work, we cast the problem into supervised sentence extraction. Supervised approaches, especially those based on learning to rank (LTR), can better utilize the power of vari- ous task-dependent features (Shen and Li, 2011; <ref type="bibr" target="#b26">Wang et al., 2013</ref>). For a given specific sports game, we extract features from all candidate sen- tences in the corresponding live texts and score the sentences using a learning to rank (LTR) model learned from the training data (Section 3.1). Then we select a few of them according to the ranking scores to form the constructed news (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Data Format</head><p>Supervised sentence scoring models based on LTR require input training data in the format of (x i , y i ) for each candidate sentence s i , where x i is the fea- ture vector and y i is the preference score. The feature vector x is described in Section 3.2. The score y will be defined to reflect the importance, or the tendency to be included in the final news re- port, of the candidate sentence. In this work we first calculate a group of ROUGE-2 F-scores (cf. Section 4.4.1) of the candidate sentence, treating each sentence in the gold-standard news as refer- ence. The score y of the candidate sentence is then set to be the maximum among those ROUGE-2 F- scores. Later we will see that this scores can in- deed serve as good learning targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features</head><p>In this work, we extract both common features which have been widely used for generic docu- ment summarization (Shen and Li, 2011; <ref type="bibr" target="#b26">Wang et al., 2013</ref>) and novel task-specific features aiming at proper sports news generation from live broad- cast script. The features are described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Basic Features</head><p>Position: The position of each candidate sentence. Suppose there are n sentences in a document. For the i-th sentence, its position feature is computed as 1 − i−1 n . Length: The number of words contained in the sentence after stopwords removal.</p><p>Number of stopwords: The Number of stop- words contained in each sentence. Sentences with many stopwords should be treated as less impor- tant candidates.</p><p>Sum of word weights: The sum of TF-IDF weights for each word in a sentence.</p><p>Similarity to the Neighboring Sentences: We calculate the average cosine similarity of a candi- date sentence to its previous N and the next N neighboring sentences. We set N as 1 and 2 here to get two different features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Task-specific Features</head><p>The task we study has some unique properties compared with generic document summarization. For instance, in live text commentary for sports games such as football matches, the scripts not only contain descriptive texts but also the score- line and timeline information. Such information can be utilized to judge the quality of candidate sentences as well. We extract a rich set of new features, which can be grouped into four types:</p><p>Explicit highlight markers: Explicit highlight marker words in a sentence are usually good in- dicators for its importance. Sentences with more marker words are more probable to be extracted and contained in news or reports for the games. For example, words such as "破门 (scores)" and "红牌 (red card)" in a sentence may indicate that the sentence is describing important events and will be more likely to be extracted. We collect a short list of 25 explicit highlight marker words <ref type="bibr">5</ref> . For each marker word we create a binary feature to denote the presence or absence of that markers in each candidate sentence. We also use the number of markers as one feature, with the intuition that containing more marker words typically suggests more important sentences.</p><p>Scoreline features: An audience of sports games typically pays more attention on score- line changes, especially those deadlock-breaking scores that break the game from ties. We use three binary features to describe the scoreline informa- tion of each candidate sentence:</p><p>• An indicator feature on whether there was a change of scoreline when the narrator or commentator was producing that sentence.</p><p>• An indicator feature on whether the distance between the candidate sentence and the pre- vious closest sentence with a change of score- line is less than or equal to 5.</p><p>• An indicator feature showing whether the game was a draw or not at that time.</p><p>To better describe these features we give an exam- ple in <ref type="figure">Figure 2</ref>, where S1-S3 corresponds to the above three binary features, respectively.</p><formula xml:id="formula_0">Text Commentary Timeline Scoreline S1 S2 S3</formula><p>Both sides take advantages of counter attacks.</p><formula xml:id="formula_1">32' 1-1 0 0 0 1-2！ ！ 33' 1-2 1 1 1 Alexis!! 33' 1-2 0 1 1</formula><p>Özil finds the teammate byline followed by a low cross to far post, Alexis sends the ball into the net!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>34'</head><p>1-2 0 1 1</p><p>Leicester players are unhappy. 34' 1-2 0 1 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2: An example of scoreline features</head><p>Timeline features: The timestamp on each sen- tence can reflect the progress of a sports game. We divide a match into five different stages as "未赛 (not started)", "上半场(first half)", "中场 休息(half-time)", "下半场(second half)" and "完 赛(full-time)". Then we use five binary features to represent whether the sentence was describing a specific stage. We also use the specific time-stamp (in integral minutes) of the candidate sentence in the match as an additional feature. Suppose there are n minutes of the match (typically 90 minutes for football), for sentences on the time-stamp of the i-th minute , this feature is computed as i n . Player popularity: Sports fans usually focus more on the performance of the star players or in- form players during the games. We design two features to utilize player information described in a candidate sentence: the number of players con- tained in the sentence and the sum of their popu- larity measurements. In this work the popularity of a player is measured using search engines for news: we use the name of a certain player as in- put query to Baidu News 6 , and use the number of recent news retrieved to measure this player's popularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Selection</head><p>Once we have the trained LTR model, we can immediately construct news reports by selecting sentences with the highest scores. Unfortunately this simple strategy will suffer from redundancy in commentary, since the LTR scores are pre- dicted independently for each sentence and assign- ing high scores for repeated commentary texts de- scribing the same key event. Therefore, special care is needed in sentence selection. In princi- ple, any In this work we propose a probabilistic approach based on determinantal point processes ( <ref type="bibr">Kulesza and Taskar, 2012, DPPs)</ref>. This approach can naturally integrate the predicted scores from the LTR model while trying to avoid certain re- dundancy by producing more diverse extractions <ref type="bibr">7</ref> . We first review some background knowledge on the model. More details can be found in the comprehensive survey ) covering this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Determinantal Point Processes</head><p>Determinantal point processes (DPPs) are distri- butions over subsets that jointly prefer quality of each item and diversity of the whole subset. For- mally, a DPP is a probability measure defined on all possible subsets of a group of items Y = {1, 2, . . . , N }. For every Y ⊆ Y we have:</p><formula xml:id="formula_2">P(Y ) = det(L Y ) det(L + I)</formula><p>where L is a positive semidefinite matrix typi- cally called an L-ensemble. L Y ≡ [L ij ] i,j∈Y de- notes the restriction of L to the entries indexed by elements of Y , and det(L ∅ ) = 1. The term det(L + I) is the normalization constant which has a succinct closed-form and easy to compute. We can define the entries of L as follows:</p><formula xml:id="formula_3">L ij = q i φ i φ j q j = q i · sim(i, j) · q j (1)</formula><p>where we can think of q i ∈ R + as the quality of an item i and φ i ∈ R n with φ i 2 = 1 denotes a normalised feature vector such that sim(i, j) ∈ [−1, 1] measures similarity between item i and item j. This simple definition gives rise to a distri- bution that places most of its mass on sets that are both high quality and diverse. This is intuitive in a geometric sense since determinants are closely re- lated to volumes; in particular, det(L Y ) is propor- tional to the volume spanned by the vectors q i φ i for i ∈ Y . Thus, item sets with both high-quality and diverse items will have the highest probability ( <ref type="figure" target="#fig_1">Figure 3</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Sentence Selection</head><p>In this work we formulate the sentence selection problem as maximum a posteriori (MAP) infer- ence for DPPs, i.e. finding argmax Y log det(L Y ). It is known that MAP inference for DPPs is NP- hard ( <ref type="bibr" target="#b6">Gillenwater et al., 2012</ref>). Therefore we adopt the greedy approximate inference procedure used by <ref type="bibr" target="#b9">Kulesza and Taskar (2011)</ref> which is fast and performs reasonably well in practice. The remaining question is how to define the L- ensemble matrix L, or equivalently how to de- fine itemwise quality q i and pairwise similarity sim(i, j), where each item corresponds to a can- didate sentence. Since we have predicted scores for all candidates with the LTR model, we simply set q i to be the ranking score for sentence i.</p><p>The definition of sim(i, j) is more subtle since it directly address specific types of redundancy. The most straightforward definition is to use literal co- sine similarity. This is used for traditional sum- marization problems ( <ref type="bibr" target="#b9">Kulesza and Taskar, 2011</ref>). However, the problem for constructing sports news from live broadcast script is rather different. A live broadcast script may use literally similar sentences to describe similar types of events hap- pened at different time stamps. Simply removing sentences that are similar in content may become harmful to the preservation of important events <ref type="bibr">8</ref> .</p><p>One typical redundancy that we found in this study is local description redundancy. In live texts, <ref type="bibr">8</ref> Using cosine similarity for all similarity-dependent methods performs poorly in our experiments. Therefore we will not discuss cosine similarity in more details later.</p><p>an important event (such as goals) may be stressed multiple times consecutively by the commentator. Therefore in this study we use local literal sim- ilarity as a first attempt. Formally, the pairwise similarity is defined as:</p><formula xml:id="formula_4">sim(i, j) = 0, if max{|i p − j p |, |i t − j t |} &gt; 1, cos(i, j), otherwise,</formula><p>where the subscripts i p and i t denotes position and timestamp for sentence i, respectively. In other words we treat sentences written consecu- tively within one minute as local descriptions and only calculate literal cosine similarity for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Preparation</head><p>As described earlier in Section 2.2, we evaluate the performance of different systems on our collected dataset. To utilize the dataset more sufficiently and draw more reliable conclusions, we perform cross- validation during evaluation. Specifically, we ran- domly divide the dataset into three parts with equal sizes, i.e. each has 50 pairs of live texts and gold- standard news. Each time we set one of them as the test set and use the remaining two parts for training and validation. We will mainly report the averaged results from all three folds. For unsuper- vised baselines the results are calculated similarly via averaging the performance on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning to Rank</head><p>For predicting ranking scores we use the Random Forest (RF) <ref type="bibr" target="#b2">(Breiman, 2001</ref>) ensemble ranker of LambdaMart ( <ref type="bibr" target="#b27">Wu et al., 2010)</ref>, implemented in RankLib <ref type="bibr">9</ref> . We set the number of iterations to 300 and the sampling rate to 0.3. Using different val- ues did not show real differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared Baseline Methods</head><p>Our system is compared with several baselines, typically traditional summarization approaches: HeadTail: Using head and tail sentences only. Commentators usually describe some basic infor- mation of the two sides at the beginning and sum- marize the scoring events in the end of commen- tary. This baseline resembles the baseline of lead- ing sentences for traditional summarization.</p><p>Centroid: In centroid-based summarization ( <ref type="bibr" target="#b20">Radev et al., 2000</ref>), a pseudo-sentence of the doc- ument called centroid is calculated. The centroid consists of words with TFIDF scores above a pre- defined threshold. The score of each sentence is defined by summing the scores based on different features including cosine similarity of sentences with the centroid, position weight and cosine sim- ilarity with the first sentence.</p><p>LexRank: LexRank (Erkan and Radev, 2004) computes sentence importance based on the con- cept of eigenvector centrality in a graph represen- tation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph repre- sentation of sentences.</p><p>ILP: Integer linear programming (ILP) ap- proaches ( <ref type="bibr" target="#b7">Gillick et al., 2008</ref>) cast document sum- marization as combinatorial optimization. An ILP model selects sentences by maximizing the sum of frequency-induced weights of bigram concepts 10 contained in the summary.</p><p>Highlight: This method is designed to show the effect of using merely the explicit highlight mark- ers described in Section 3.2.2. The importance of a sentence is represented by the number of high- light markers it includes.</p><p>For fair comparisons the length of each con- structed news report is limited to be no more than 1,000 Chinese characters, roughly the same with the average length of the gold-standard news. Note that we do not use the traditional MMR redun- dancy removal algorithm based on literal similar- ity <ref type="bibr" target="#b3">(Carbonell and Goldstein, 1998</ref>) since we find only ignorable differences between using MMR or not for all systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Methods and Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Automatic Evaluation</head><p>Similar to the evaluation for traditional summa- rization tasks, we use the ROUGE metrics ( <ref type="bibr" target="#b13">Lin and Hovy, 2003)</ref> to automatically evaluate the quality of produced summaries given the gold- standard reference news. The ROUGE metrics measure summary quality by counting the preci- sion, recall and F-score of overlapping units, such as n-grams and skip grams, between a candidate summary and the reference summaries.</p><p>We use the ROUGE-1.5.5 toolkit to perform the evaluation. In this paper we report the F-scores of the following metrics in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram- based) and ROUGE-SU4 (based on skip bigrams with a maximum skip distance of 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Pyramid Evaluation</head><p>We also conduct manual pyramid evaluation in this study. Specifically, we use the modified pyra- mid scores as described in ( <ref type="bibr" target="#b19">Passonneau et al., 2005</ref>) to manually evaluate the summaries gener- ated by different methods. We randomly sample 20 games from the data set and manually annotate facts on the gold-standard news. The annotated facts are mostly describing specific events hap- pened during the game, e.g. "伊万被黄牌警告" (Ivanovic is shown the yellow card) and "内马尔 开出角球" (Neymar takes the corner). Each fact is treated as a Summarization Content Unit, (SCU) ( <ref type="bibr" target="#b15">Nenkova and Passonneau, 2004</ref>). The number of occurrences for each SCU in the gold-standard news is regarded as the weight of this SCU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with Baseline Methods</head><p>The average performance on all three folds of dif- ferent methods are displayed in As we can see from the results, our learning to rank approach based on RF achieves signif- icantly (&lt; 0.01 significance level for pairwise- t testing) better results compared with traditional unsupervised summarization approaches 11 . The ILP model, which is believed to be suitable for multi-document summarization, did not perform well in our settings. Head and tail sentences are informative but merely using them lacks specific descriptions for procedural events, therefore not providing competitive results either.</p><p>The comparison between RF and RF+DPP shows the effectiveness of our sentence selection strategy. However, the increase is still limited <ref type="bibr">12</ref> . This may become reasonable later when we dis- cuss more about the errors from our systems.</p><p>Merely using highlight markers to construct news also provides competitive results, but infe- rior to supervised models. This suggests that the highlight marker features are relatively strong in- dicators for good sentences while merely using these features may not be sufficient. <ref type="table">Table 2</ref> shows the average pyramid scores for the systems in comparison. The "Gold-standard" row denotes manually written news report and is listed for reference. We can see our learning to rank systems based on RF constructs news with the highest pyramid scores.</p><p>Method <ref type="table">Table 2</ref>: Average Pyramid scores Overall, the experimental results indicate that our system can generate much better news than the baselines in both automatic and manual eval- uations. We include examples of our constructed news reports in the supplementary materials.</p><note type="other">Pyramid scores HeadTail 0.13657 Centroid 0.30663 LexRank 0.28756 ILP 0.20867 Highlight 0.41121 RF 0.53766 RF+DPP 0.62500 Gold-standard 0.88329</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Feature Validation</head><p>Different groups of features may play different roles in the LTR models. In order to validate the impact of both the traditional features and the novel task-specific features, we conduct experi- ments with different combinations by removing each group of features respectively. <ref type="table">Table 3</ref> shows the results, with "w/o" denotes experiments with- out the corresponding group of features.  <ref type="table">Table 3</ref>: Results of feature validation <ref type="bibr">12</ref> Significance level &lt; 0.05 for pairwise-t testing only for ROUGE-1.</p><p>We can observe that both the traditional features and the novel features contribute useful informa- tion for learning to rank models. Due to the na- ture of the sentence extraction approach, features designed for traditional document summarization are still playing an indispensable role for our task, although they might be important in this work for different reasons. For example, position features are indicative for traditional summarization since sentences appearing in the very beginning or the end are more probable as summarizing sentences. For sports commentary, positions are closely re- lated to timeline in a more coarse fashion. Certain types of key events, for example player substitu- tions and even scores, may tend to happen in cer- tain period in a game rather than uniformly spread out in every minute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Room for Improvements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Upper Bounds</head><p>To get a rough estimate of what is actually achiev- able in terms of the final ROUGE scores, we looked at different "upper bounds" under various scenarios <ref type="table" target="#tab_1">(Table 4)</ref>. We first evaluate one refer- ence news with the other reference news served as the gold-standard result. The results are given in the row labeled reference of <ref type="table" target="#tab_1">Table 4</ref>. This provides a reasonable estimate of human performance.</p><p>Second, in sentence extraction we restrict the constructed news to sentences from the origi- nal commentary texts themselves. We use the greedy algorithm to extract sentences that max- imize ROUGE-2F scores. The resulting perfor- mance is given in the row extract of <ref type="table" target="#tab_1">Table 4</ref>. We observe numerically superior scores compared with reference. This is not strange since we are in- tentionally optimizing ROUGE scores. And also this suggests that the sentence extraction approach for sports news construction is rather reasonable, in terms of information overlap.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>内马尔 为 巴 萨 制造 了 一个 位置 不错 的 定位球</head><p>Neymar wins a free kick in a good position.</p><formula xml:id="formula_5">56 内马尔 ~ ~ ~ Neymar!!!!!!! 56</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>内马尔 的 定位球 直接 打入 了 球门 左上 角 的 死角 ！ ！ ！ 门将 无能为力</head><p>The free kick from Neymar goes directly into the top left corner! The keeper can do nothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>56</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>球 在 飞 向 球门 的 过程 中 下坠 速度 非常 快</head><p>The ball drops quickly and flies to the goal.   <ref type="table" target="#tab_1">Table 4</ref>. This validates that using par- tial ROUGE-2 as the training target for LTR mod- els is somewhat reasonable for this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Error Analysis</head><p>In this preliminary study, we use LTR models and probabilistic sentence selection procedure. While reasonable performance has been achieved, there exist certain types of errors as we found in the con- structed news results.</p><p>Error I: First, sentences in live commentary are mostly short, and sometimes noisy. Some- times an important event has been described us- ing a number of consecutive short sentences. Our LTR models failed to generate high scores for such sentences and therefore will cause some lack of information. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates an example of this type of error in the constructed news re- port. All the sentences are describing a key scor- ing event. However, none of them were selected to construct the news because our LTR model assigns low scores for these short sentences. Meanwhile the second sentence can be treated as noisy.</p><p>Error II: Second, commentators are likely to summarize important events during the game, not at the point when the event happens. Our sentence selection algorithm can only address local redun- dancy, while this issue is more global. <ref type="figure" target="#fig_5">Figure 5</ref> il- lustrates an example of this case in the constructed news report. The only goal of the match is de- scribed during full-time (FT). Our method redun- dantly included this in the final constructed news even it had already selected that event.</p><p>These two issues are highly non-trivial and have not been well addressed in the method we explored in this paper. We leave them for further study in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Readability Assessment</head><p>In this work we only consider sentence extrac- tion. Unlike traditional summarization tasks, sports commentary texts are describing a differ- ent specific action in almost every sentence. De- scriptive coherence becomes a more difficult chal- lenge in this scenario. We conduct manual evalu- ation on systems in comparison along with man- ually written news reports (gold-standard). Three volunteers who are fluent in Chinese were asked to perform manual ratings on three factors: co- herence (Coh.), non-redundancy (NR) and overall readability (Read.). The ratings are in the format of 1-5 numerical scores (not necessarily integral), with higher scores denote better quality. The re- sults are shown in <ref type="table">Table 5</ref>  <ref type="table">Table 5</ref>: Manual readability ratings</p><p>The differences between systems in terms of readability factors are not as large as information coverage suggested by ROUGE metrics and pyra- mid scores. Meanwhile, while we can observe that our approaches outperforms the unsupervised ex- tractive summarization approaches in coherence and readability for certain level, the results also clearly suggest that there still exists large room for improvements in terms of the readability factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions</head><p>The general challenges for the particular task of sports news generation are mostly addressed in those designed features in the learning to rank framework. We utilize the timeline and score- line information, while also keep traditional fea- tures such as sentence length. Experimental re- sults show that our framework indeed outperforms strong traditional summarization baselines, while still having much room for improvement.</p><p>We might also notice that there may exist some issues if merely using automatic metrics to eval- uate the overall quality of the generated news re- ports. The ROUGE metrics are mainly based on ngram overlaps. For sports texts most of the pro- portions are dominated by proper names, certain types of actions or key events, etc. Compared with traditional summarization tasks, it might be eas- ier to achieve high ROUGE scores with an em- phasize on selecting important entities. In our ex- periments, methods with higher ROUGE scores can indeed achieve better coverage of important units such as events, as shown in pyramid scores in <ref type="table">Table 2</ref>. However, we can also observe from <ref type="table">Table 5</ref> that automatic metrics currently cannot reflect readability factors very well. Generally speaking, while big difference in ROUGE may suggest big difference in overall quality, smaller ROUGE differences may not be that indicative enough. Therefore, it is interesting to find alter- native automatic metrics in order to better reflect the general quality for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>To the best of our knowledge, generation of sports news from live text commentary is not a well- studied task in related fields. One related study fo- cused on generating textual summaries for sports events from status updates in Twitter ( <ref type="bibr" target="#b17">Nichols et al., 2012</ref>). There also exists earlier work on generation of sports highlight frames from sports videos, focusing on a very different type of data ( <ref type="bibr" target="#b22">Tjondronegoro et al., 2004</ref>). Bouayad-Agha et al. (2011) and Bouayad-Agha et al. (2012) con- structed an ontology-based knowledge base for the generation of football summaries, using pre- defined extraction templates.</p><p>Our task is closely related to document sum- marization, which has been studied quite inten- sively. Various approaches exist to challenge the document summarization task, including centroid- based methods, link analysis and graph-based algorithms ( <ref type="bibr" target="#b4">Erkan and Radev, 2004;</ref><ref type="bibr" target="#b25">Wan et al., 2007)</ref>, combinatorial optimization techniques such as integer linear programming ( <ref type="bibr" target="#b7">Gillick et al., 2008</ref>) and submodular optimization <ref type="bibr" target="#b12">(Lin and Bilmes, 2010)</ref>. Supervised models including learning to rank models <ref type="bibr" target="#b14">(Metzler and Kanungo, 2008;</ref><ref type="bibr" target="#b21">Shen and Li, 2011;</ref><ref type="bibr" target="#b26">Wang et al., 2013</ref>) and regression ( <ref type="bibr" target="#b18">Ouyang et al., 2007;</ref><ref type="bibr" target="#b5">Galanis and Malakasiotis, 2008;</ref><ref type="bibr" target="#b8">Hong and Nenkova, 2014)</ref> have also been adapted in the scenario of docu- ment summarization.</p><p>Since sports live texts contain timeline informa- tion, summarization paradigms that utilize time- line and temporal information <ref type="bibr" target="#b28">(Yan et al., 2011;</ref><ref type="bibr" target="#b16">Ng et al., 2014;</ref><ref type="bibr" target="#b11">Li et al., 2015</ref>) are also conceptu- ally related. Supervised approaches related to this work have also been applied for timeline summa- rization, including linear regression for important scores <ref type="bibr" target="#b23">(Tran et al., 2013a</ref>) and learning to rank models ( <ref type="bibr" target="#b24">Tran et al., 2013b)</ref>. In this preliminary work we only use the timestamps in the definition of similarity for sentence selection. More crafted usages will be explored in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>In this paper we study a challenging task to au- tomatically construct sports news from live text commentary. Using football live texts as an in- stance, we collect training data jointly from live text commentary services and sports news portals. We develop a system based on learning to rank models, with several novel task-specific features. To generate the final news summary and tackle the local redundancy problem, we also propose a probabilistic sentence selection method. Experi- mental results demostrate that this task is feasible and our proposed methods are appropriate.</p><p>As a preliminary work, we only perform sen- tence extraction in this work. Since sports news and live commentary are in different genres, some post-editing rewritings will make the system gen- erating more natural descriptions for sports news. We would like to extend our system to produce sports news beyond pure sentence extraction.</p><p>Another important direction is to focus on the construction of datasets in larger scale. One fea- sible approach is to use a speech recognition sys- tem on live videos or broadcasts of sports games to collect huge amount of transcripts as our raw data source. Although more data can be eas- ily collected in this case, the noisiness of audio transcripts may bring some additional challenges, therefore worthwhile for further study.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Illustration of the live text format</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) The DPP probability of a set Y depends on the volume spanned by vectors q i φ i for i ∈ Y (b) As length increases, so does volume. (c) As similarity increases, volume decreases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Case I: short and noisy sentences Time Live Text Commentary Script FT 切尔西中场快发任意球，科斯塔打进全场唯一入球！ ！ ！ From a quick free kick from Chelsea, Costa scored the only goal of the game!!! FT 整场比赛切尔西占据了主动，可面对诺维奇的铁桶阵，办法不多 Chelsea dominated the game but found it difficult against Norwich's defense. FT 下半场利用对方的一次疏忽，阿扎尔中场被放倒，威廉快发任意球，科斯塔完成致 命一击 From an error from the opponent, Hazard was fouled. Willian launches a quick free kick and assists Costa for the lethal strike.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Case II: summarizing sentences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>Method 
R-1 
R-2 
R-SU4 
HeadTail 0.30147 0.07779 0.10336 
Centroid 0.32508 0.08113 0.11245 
LexRank 0.31284 0.06159 0.09376 
ILP 
0.32552 0.07285 0.10378 
Highlight 0.34687 0.08748 0.11924 
RF 
0.38559 0.11887 0.14907 
RF+DPP 0.39391 0.11986 0.15097 
Table 1: Comparison results of different methods 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Upper bounds on ROUGE scores 
Third, we use the partial ROUGE-2 values, 
i.e. the targets used to train LTR models (cf. 
Section 3.1) for greedy selection and DPP selec-
tion, with results listed in the row target and tar-</table></figure>

			<note place="foot" n="1"> Some live texts services may use different textual format for scoring events, which is not enough for our more general purposes.</note>

			<note place="foot" n="2"> http://match.sports.sina.com.cn/ 3 http://goal.sports.163.com/</note>

			<note place="foot" n="4"> We use the ICTCLAS toolkit for word segmentation in this work: http://ictclas.nlpir.org/</note>

			<note place="foot" n="5"> We include the full list of marker words in the supplementary materials due to the space limit.</note>

			<note place="foot" n="6"> http://news.baidu.com/</note>

			<note place="foot" n="7"> Many other approaches can also be used to achieve similar effect, such as submodular maximization (Lin and Bilmes, 2010). We leave the comparison with these alternatives for future work study.</note>

			<note place="foot" n="9"> http://sourceforge.net/p/lemur/wiki/RankLib/; In preliminary experiments, we contrasted RF with support vector regression predictor as well as other pairwise and listwise LTR models. We found that RF consistently outperformed others.</note>

			<note place="foot" n="10"> We also tried words rather than bigrams but found slightly worse performance.</note>

			<note place="foot" n="11"> We also conducted experiments on using our proposed features to calculate LexRank, but did not observe real difference compared with normal LexRank. This suggest that the performance gain comes from supervised learning to rank approach, not merely from the features.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by National Natural Sci-ence Foundation of China <ref type="formula">(61331011)</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Content selection from an ontologybased knowledge base for the generation of football summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadjet</forename><surname>Bouayad-Agha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Casamayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
		<meeting>the 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Perspective-oriented generation of football match summaries: Old tasks, new challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadjet</forename><surname>Bouayad-Agha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Casamayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing (TSLP)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Random forests. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 21st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aueb at tac</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TAC 2008 Workshop</title>
		<meeting>the TAC 2008 Workshop</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Near-optimal map inference for determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2735" to="2743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The icsi summarization system at tac</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Text Understanding Conference</title>
		<meeting>the Text Understanding Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving the estimation of word importance for news multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="712" to="721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning determinantal point processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Determinantal point processes for machine learning. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving update summarization via supervised ilp and sentence reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1317" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="912" to="920" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Machine learned sentence selection strategies for querybiased summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapas</forename><surname>Kanungo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR Learning to Rank Workshop</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Evaluating content selection in summarization: The pyramid method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting timelines to enhance multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ping</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="923" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Summarizing sporting events using twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmud</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Drews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM international conference on Intelligent User Interfaces</title>
		<meeting>the 2012 ACM international conference on Intelligent User Interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Developing learning strategies for topic-based summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth ACM conference on Conference on information and knowledge management</title>
		<meeting>the sixteenth ACM conference on Conference on information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Applying the pyramid method in duc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rebecca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Passonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Document Understanding Conference (DUC 05)</title>
		<meeting>the Document Understanding Conference (DUC 05)<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Dragomir R Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malgorzata</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Budzikowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization</title>
		<meeting>the 2000 NAACL-ANLP Workshop on Automatic summarization</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to rank for query-focused multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integrating highlights for more complete sports video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Tjondronegoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ping Phoebe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binh</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="22" to="37" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting relevant news events for timeline summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Giang Binh Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dat Quoc</forename><surname>Alrifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web Companion</title>
		<meeting>the 22nd international conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="91" to="92" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Leveraging learning to rank in an optimization framework for timeline summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Giang Binh Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tuan A Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nattiya</forename><surname>Alrifai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanhabua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR 2013 Workshop on Time-aware Information Access</title>
		<imprint>
			<publisher>TAIA</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Manifold-ranking based topic-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2903" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A sentence compression based framework to query-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1384" to="1394" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adapting boosting for information retrieval measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krysta</forename><forename type="middle">M</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Svore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="254" to="270" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Timeline generation through evolutionary trans-temporal summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congrui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="433" to="443" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
