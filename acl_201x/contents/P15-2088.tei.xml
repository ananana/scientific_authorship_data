<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context-Dependent Translation Selection Using Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
							<email>baotianchina@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Computing Research † Noah&apos;s Ark Lab Center</orgName>
								<orgName type="institution">Harbin Institute of Technology Huawei Technologies Co. Ltd. Shenzhen Graduate School</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
							<email>tu.zhaopeng@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Computing Research † Noah&apos;s Ark Lab Center</orgName>
								<orgName type="institution">Harbin Institute of Technology Huawei Technologies Co. Ltd. Shenzhen Graduate School</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
							<email>lu.zhengdong@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Computing Research † Noah&apos;s Ark Lab Center</orgName>
								<orgName type="institution">Harbin Institute of Technology Huawei Technologies Co. Ltd. Shenzhen Graduate School</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Computing Research † Noah&apos;s Ark Lab Center</orgName>
								<orgName type="institution">Harbin Institute of Technology Huawei Technologies Co. Ltd. Shenzhen Graduate School</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">Qingcai</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Intelligent Computing Research † Noah&apos;s Ark Lab Center</orgName>
								<orgName type="institution">Harbin Institute of Technology Huawei Technologies Co. Ltd. Shenzhen Graduate School</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context-Dependent Translation Selection Using Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="536" to="541"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrases and sentence-level contexts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms ( <ref type="bibr" target="#b11">Koehn et al., 2003)</ref>, which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a contin- uous space ( <ref type="bibr" target="#b7">Gao et al., 2014;</ref><ref type="bibr" target="#b21">Zhang et al., 2014;</ref><ref type="bibr" target="#b2">Cho et al., 2014</ref>). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>* Corresponding author</head><p>The above methods, however, neglect the infor- mation of local contexts, which has been proven to be useful for disambiguating translation candi- dates during decoding ( <ref type="bibr" target="#b14">Marton and Resnik, 2008)</ref>. The matching scores of translation pairs are treated the same, even they are in dif- ferent contexts. Accordingly, the methods fail to adapt to local contexts and lead to precision issues for specific sentences in different contexts.</p><p>To capture useful context information, we pro- pose a convolutional neural network architecture to measure context-dependent semantic similari- ties between phrase pairs in two languages. For each phrase pair, we use the sentence contain- ing the phrase in source language as the context. With the convolutional neural network, we sum- marize the information of a phrase pair and its con- text, and further compute the pair's matching score with a multi-layer perceptron. We discriminately train the model using a curriculum learning strat- egy. We classify the training examples according to the difficulty level of distinguishing the positive candidate from the negative candidate. Then we train the model to learn the semantic information from easy (basic semantic similarities) to difficult (context-dependent semantic similarities).</p><p>Experimental results on a large-scale transla- tion task show that the context-dependent convo- lutional matching (CDCM) model improves the performance by up to 1.4 BLEU points over a strong phrase-based SMT system. Moreover, the CDCM model significantly outperforms its context-independent counterpart, proving that it is necessary to incorporate local contexts into SMT. Contributions. Our key contributions include:</p><p>• we introduce a novel CDCM model to cap- ture context-dependent semantic similarities between phrase pairs (Section 2); • we develop a novel learning algorithm to train the CDCM model using a curriculum learning strategy (Section 3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Context-Dependent Convolutional Matching Model</head><p>The model architecture, shown in <ref type="figure" target="#fig_0">Figure 1</ref>, is a variant of the convolutional architecture of . It consists of two components:</p><p>• convolutional sentence model that summa- rizes the meaning of the source sentence and the target phrase;</p><p>• matching model that compares the two representations with a multi-layer percep- tron <ref type="bibr" target="#b1">(Bengio, 2009</ref>).</p><p>LetêLetˆLetê be a target phrase and f be the source sen- tence that contains the source phrase aligning tô e. We first project f andêandˆandê into feature vectors x and y via the convolutional sentence model, and then compute the matching score s(x, y) by the match- ing model. Finally, the score is introduced into a conventional SMT system as an additional feature. Convolutional sentence model. As shown in <ref type="figure" target="#fig_0">Fig- ure 1</ref>, the model takes as input the embeddings of words (trained beforehand elsewhere) in f andêandˆandê. It then iteratively summarizes the meaning of the input through layers of convolution and pooling, until reaching a fixed length vectorial representa- tion in the final layer.</p><p>In Layer-1, the convolution layer takes sliding windows on f andêandˆandê respectively, and models all the possible compositions of neighbouring words. The convolution involves a filter to produce a new feature for each possible composition. Given a k-sized sliding window i on f orêorˆorê, for example, the jth convolution unit of the composition of the words is generated by:</p><formula xml:id="formula_0">c i (1,j) = g( ˆ c i (0) ) · φ(w (1,j) · ˆ c i (0) + b (1,j) ) (1)</formula><p>where</p><formula xml:id="formula_1">• g(·)</formula><p>is the gate function that determines whether to activate φ(·);</p><p>• φ(·) is a non-linear activation function. In this work, we use ReLu ( <ref type="bibr" target="#b5">Dahl et al., 2013)</ref> as the activation function;</p><p>• w (1,j) is the parameters for the jth convolu- tion unit on Layer-1, with matrix</p><formula xml:id="formula_2">W (1) = [w (1,1) , . . . , w (1,J) ]; • ˆ c i (0)</formula><p>is a vector constructed by concatenating word vectors in the k-sized sliding widow i;</p><formula xml:id="formula_3">• b (1,j) is a bias term, with vector B (1) = [b (1,1) , . . . , b (1,J) ].</formula><p>To distinguish the phrase pair from its con- text, we use one additional dimension in word embeddings: 1 for words in the phrase pair and 0 for the others. After transforming words to their tagged embeddings, the convolutional sen- tence model takes multiple choices of composition using sliding windows in the convolution layer. Note that sliding windows are allowed to cross the boundary of the source phrase to exploit both phrasal and contextual information.</p><p>In Layer-2, we apply a local max-pooling in non-overlapping 1 × 2 windows for every convo- lution unit</p><formula xml:id="formula_4">c (2,j) i = max{c (1,j) 2i , c (1,j) 2i+1 } (2)</formula><p>In Layer-3, we perform convolution on output from Layer-2:</p><formula xml:id="formula_5">c i (3,j) = g( ˆ c i (2) ) · φ(w (3,j) · ˆ c i (2) + b (3,j) ) (3)</formula><p>After more convolution and max-pooling opera- tions, we obtain two feature vectors for the source sentence and the target phrase, respectively. Matching model. The matching score of a source sentence and a target phrase can be measured as the similarity between their feature vectors. Specifically, we use the multi-layer perceptron (MLP), a nonlinear function for similarity, to com- pute their matching score. First we use one layer to combine their feature vectors to get a hidden state h c :</p><formula xml:id="formula_6">h c = φ(w c · [x ¯ f i : y ¯ e j ] + b c )<label>(4)</label></formula><p>Then we get the matching score from the MLP:</p><formula xml:id="formula_7">s(x, y) = M LP (h c )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training</head><p>We employ a discriminative training strategy with a max-margin objective. Suppose we are given the following triples (x, y + , y − ) from the ora- cle, where x, y + , y − are the feature vectors for f , ˆ e + , ˆ e − respectively. We have the ranking-based loss as objective:</p><formula xml:id="formula_8">L Θ (x, y + , y − ) = max(0, 1+s(x, y − )−s(x, y + ))<label>(6)</label></formula><p>where s(x, y) is the matching score function de- fined in Eq. 5, Θ consists of parameters for both the convolutional sentence model and MLP. The model is trained by minimizing the above ob- jective, to encourage the model to assign higher matching scores to positive examples and to as- sign lower scores to negative examples. We use stochastic gradient descent (SGD) to optimize the model parameters Θ. We train the CDCM model with a curriculum strategy to learn the context- dependent semantic similarity at the phrase level from easy (basic semantic similarities between the source and target phrase pair) to difficult (context-dependent semantic similarities for the same source phrase in varying contexts).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Curriculum Training</head><p>Curriculum learning, first proposed by <ref type="bibr" target="#b1">Bengio et al. (2009)</ref> in machine learning, refers to a se- quence of training strategies that start small, learn easier aspects of the task, and then gradually in- crease the difficulty level. It has been shown that the curriculum learning can benefit the non- convex training by giving rise to improved gener- alization and faster convergence. The key point is that the training examples are not randomly pre- sented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones.</p><p>For each positive example (f , ˆ e + ), we have three types of negative examples according to the diffi- culty level of distinguishing the positive example from them:</p><p>• Easy: target phrases randomly chosen from the phrase table;</p><p>• Medium: target phrases extracted from the aligned target sentence for other non-overlap source phrases in the source sentence;</p><p>• Difficult: target phrases extracted from other candidates for the same source phrase.</p><p>We want the CDCM model to learn the following semantic information from easy to difficult:</p><p>• the basic semantic similarity between the source sentence and target phrase from the easy negative examples;</p><p>• the general semantic equivalent between the source and target phrase pair from the medium negative examples;</p><p>• the context-dependent semantic similarities for the same source phrase in varying con- texts from the difficult negative examples.</p><p>Alg. 1 shows the curriculum training algorithm for the CDCM model. We use different portions of the overall training instances for different curricu- lums (lines 2-11). For example, we only use the Algorithm 1 Curriculum training algorithm. Here T denotes the training examples, W the initial word embeddings, η the learning rate in SGD, n the pre-defined number, and t the number of train- ing examples.</p><p>1: procedure CURRICULUM-TRAINING(T , W ) 2:</p><p>N1 ← easy negative(T ) 3:</p><p>N2 ← medium negative(T ) 4:</p><p>N3 ← difficult negative(T ) 5:</p><p>T ← N1 6: CURRICULUM(T , n · t) CUR. easy 7:</p><p>T ← MIX([N1, N2]) 8: CURRICULUM(T , n · t) CUR. medium 9:</p><p>for step ← 1 . . . n do 10:</p><formula xml:id="formula_9">T ← MIX([N1, N2, N 3], step) 11: CURRICULUM(T , t) CUR. difficult 12: procedure CURRICULUM(T , K) 13:</formula><p>iterate until reaching a local minima or K iterations 14:</p><p>calculate LΘ for a random instance in T 15:</p><formula xml:id="formula_10">Θ = Θ − η · ∂L Θ ∂Θ update parameters 16: W = W − η · 0.01 · ∂L Θ ∂W update embeddings 17: procedure MIX(N, s = 0) 18:</formula><p>len ← length of N 19:</p><p>if len &lt; 3 then 20:</p><p>T ← sampling with [0.5, 0.5] from N 21:</p><formula xml:id="formula_11">else 22:</formula><p>T ← sampling with</p><formula xml:id="formula_12">[ 1 s+2 , 1 s+2 , s s+2</formula><p>] from N training instances that consist of positive examples and easy negative examples in the easy curriculum (lines 5-6). For the latter curriculums, we gradu- ally increase the difficulty level of the training in- stances (lines 7-12).</p><p>For each curriculum (lines 12-16), we compute the gradient of the loss objective L Θ and learn Θ using the SGD algorithm. Note that we mean- while update the word embeddings to better cap- ture the semantic equivalence across languages during training. If the loss function L Θ reaches a local minima or the iterations reach the pre- defined number, we terminate this curriculum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations.</p><p>There is a line of work that employs local con- texts over discrete representations of words or phrases. For example, ,  and <ref type="bibr" target="#b14">Marton and Resnik (2008)</ref> em- ployed within-sentence contexts that consist of discrete words to guide rule matching. <ref type="bibr" target="#b19">Wu et al. (2014)</ref> exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the phrase pairs and directly compute phrasal similari- ties with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks <ref type="bibr" target="#b4">(Collobert and Weston, 2008)</ref>.</p><p>In recent years, there has also been growing interest in bilingual phrase representations that group phrases with a similar meaning across dif- ferent languages. Based on that translation equiv- alents share the same semantic meaning, they can supervise each other to learn their semantic phrase embeddings in a continuous space ( <ref type="bibr" target="#b7">Gao et al., 2014;</ref><ref type="bibr" target="#b21">Zhang et al., 2014</ref>). However, these mod- els focused on capturing semantic similarities be- tween phrase pairs in the global contexts, and ne- glected the local contexts, thus ignored the use- ful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. <ref type="bibr" target="#b15">Meng et al. (2015)</ref> and <ref type="bibr" target="#b22">Zhang (2015)</ref> have proposed independently to summary source sen- tences with convolutional neural networks. How- ever, they both extend the neural network joint model (NNJM) of <ref type="bibr" target="#b6">Devlin et al. (2014)</ref> to include the whole source sentence, while we focus on cap- turing context-dependent semantic similarities of translation pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset. <ref type="bibr">1</ref> We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit <ref type="bibr" target="#b18">(Stolcke, 2002</ref>) with modified Kneser-Ney Smoothing ( <ref type="bibr" target="#b10">Kneser and Ney, 1995)</ref>. We use the 2002 NIST MT evaluation test data as the development data, and the <ref type="bibr">2004,</ref><ref type="bibr">2005</ref> NIST MT evaluation test data as the test data. We use minimum error rate train- ing <ref type="bibr" target="#b16">(Och, 2003)</ref> to optimize the feature weights. For evaluation, case-insensitive NIST BLEU <ref type="bibr" target="#b17">(Papineni et al., 2002</ref>) is used to measure translation performance. We perform a significance test using the sign-test approach ( <ref type="bibr" target="#b3">Collins et al., 2005</ref>  <ref type="table">Table 1</ref>: Evaluation of translation quality. CDCM k denotes the CDCM model trained in the kth curriculum in Alg. 1 (i.e., three levels of curriculum training), CICM denotes its context- independent counterpart, and "All" is the com- bined test sets. The superscripts α and β indicate statistically significant difference (p &lt; 0.05) from Baseline and CICM, respectively.</p><p>For training the neural networks, we use 4 con- volution layers for source sentences and 3 convo- lution layers for target phrases. For both of them, 4 pooling layers (pooling size is 2) are used, and all the feature maps are 100. We set the sliding win- dow k = 3, and the learning rate η = 0.02. All the parameters are selected based on the develop- ment data. We train the word embeddings using a bilingual strategy similar to <ref type="bibr" target="#b20">Yang et al. (2013)</ref>, and set the dimension of the word embeddings be 50. To produce high-quality bilingual phrase pairs to train the CDCM model, we perform forced decod- ing on the bilingual training sentences and collect the used phrase pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation of Translation Quality</head><p>We have two baseline systems:</p><p>• Baseline: The baseline system is an open- source system of the phrase-based model - Moses ( <ref type="bibr" target="#b12">Koehn et al., 2007</ref>) with a set of com- mon features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model.</p><p>• CICM (context-independent convolutional matching) model: Following the previous works ( <ref type="bibr" target="#b7">Gao et al., 2014;</ref><ref type="bibr" target="#b21">Zhang et al., 2014;</ref><ref type="bibr" target="#b2">Cho et al., 2014</ref>), we calculate the match- ing degree of a phrase pair without consider- ing any contextual information. Each unique phrase pair serves as a positive example and a randomly selected target phrase from the phrase table is the corresponding negative ex- ample. The matching score is also introduced into Baseline as an additional feature. <ref type="table">Table 1</ref> summaries the results of CDCMs trained from different curriculums. No matter from which curriculum it is trained, the CDCM model significantly improves the translation qual- ity on the overall test data (with gains of 1.0 BLEU points). The best improvement can be up to 1.4 BLEU points on MT04 with the fully trained CDCM. As expected, the translation performance is consistently increased with curriculum grow- ing. This indicates that the CDCM model indeed captures the desirable semantic information by the curriculum learning from easy to difficult.</p><p>Comparing with its context-independent coun- terpart (CICM, Row 2), the CDCM model shows significant improvement on all the test data con- sistently. We contribute this to the incorporation of useful discriminative information embedded in the local context. In addition, the performance of CICM is comparable with that of CDCM 1 . This is intuitive, because both of them try to capture the basic semantic similarity between the source and target phrase pair.</p><p>One of the hypotheses we tested in the course of this research was disproved. We thought it likely that the difficult curriculum (CDCM 3 that distin- guishs the correct translation from other candi- dates for a given context) would contribute most to the improvement, since this circumstance is more consistent with the real decoding procedure. This turned out to be false, as shown in <ref type="table">Table 1</ref>. One possible reason is that the "negative" examples (other candidates for the same source phrase) may share the same semantic meaning with the posi- tive one, thus give a wrong guide in the supervised training. Constructing a reasonable set of nega- tive examples that are more semantically different from the positive one is left for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a context-dependent con- volutional matching model to capture semantic similarities between phrase pairs that are sensitive to contexts. Experimental results show that our ap- proach significantly improves the translation per- formance and obtains improvement of 1.0 BLEU scores on the overall test data.</p><p>Integrating deep architecture into context- dependent translation selection is a promising way to improve machine translation. In the future, we will try to exploit contextual information at the tar- get side (e.g., partial translations).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the CDCM model. The convolutional sentence model (bottom) summarizes the meaning of the tagged sentence and target phrase, and the matching model (top) compares the representations using a multi-layer perceptron. "/" indicates all-zero padding turned off by the gating function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Models 

MT04 
MT05 
All 
Baseline 34.86 
33.18 
34.40 
CICM 
35.82 α 
33.51 α 
34.95 α 
CDCM 1 35.87 α 
33.58 
35.01 α 
CDCM 2 35.97 α 
33.80 α 
35.21 α 
CDCM 3 36.26 αβ 33.94 αβ 35.40 αβ 

</table></figure>

			<note place="foot" n="1"> The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by China National 973 project 2014CB340301. Baotian Hu and Qinghai Chen are supported by National Natural Science Foundation of China 61173075 and 61473101. We thank Junhui Li, and the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning deep architectures for ai. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kučerová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2005</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>George E Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning continuous phrase representations for translation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving statistical machine translation using lexicalized rule selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moses: open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Maximum entropy based rule selection model for syntax-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Soft syntactic constraints for hierarchical phrased-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Encoding source language with convolutional neural network for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Seventh International Conference on Spoken Language Processing</title>
		<meeting>Seventh International Conference on Spoken Language Processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improve statistical machine translation with context-sensitive bilingual semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word Alignment Modeling with Context Dependent Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2013</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bilingually-constrained phrase embeddings for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Local translation prediction with global sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
