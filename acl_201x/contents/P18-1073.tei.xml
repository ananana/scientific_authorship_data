<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group University of the Basque Country (UPV/EHU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group University of the Basque Country (UPV/EHU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group University of the Basque Country (UPV/EHU)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="789" to="798"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>789</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully un-supervised initialization that explicitly exploits the structural similarity of the em-beddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github. com/artetxem/vecmap.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cross-lingual embedding mappings have shown to be an effective way to learn bilingual word em- beddings ( <ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b5">Lazaridou et al., 2015)</ref>. The underlying idea is to independently train the embeddings in different languages us- ing monolingual corpora, and then map them to a shared space through a linear transformation. This allows to learn high-quality cross-lingual rep- resentations without expensive supervision, open- ing new research avenues like unsupervised neural machine translation ( <ref type="bibr" target="#b3">Artetxe et al., 2018b;</ref><ref type="bibr" target="#b4">Lample et al., 2018)</ref>.</p><p>While most embedding mapping methods rely on a small seed dictionary, adversarial training has recently produced exciting results in fully unsu- pervised settings ( <ref type="bibr">Zhang et al., 2017a,b;</ref><ref type="bibr" target="#b4">Conneau et al., 2018</ref>). However, their evaluation has fo- cused on particularly favorable conditions, lim- ited to closely-related languages or comparable Wikipedia corpora. When tested on more realis- tic scenarios, we find that they often fail to pro- duce meaningful results. For instance, none of the existing methods works in the standard English- Finnish dataset from <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref>, obtain- ing translation accuracies below 2% in all cases (see Section 5).</p><p>On another strand of work, <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref> showed that an iterative self-learning method is able to bootstrap a high quality mapping from very small seed dictionaries (as little as 25 pairs of words). However, their analysis reveals that the self-learning method gets stuck in poor local op- tima when the initial solution is not good enough, thus failing for smaller training dictionaries.</p><p>In this paper, we follow this second approach and propose a new unsupervised method to build an initial solution without the need of a seed dic- tionary, based on the observation that, given the similarity matrix of all words in the vocabulary, each word has a different distribution of similar- ity values. Two equivalent words in different lan- guages should have a similar distribution, and we can use this fact to induce the initial set of word pairings (see <ref type="figure">Figure 1</ref>). We combine this initial- ization with a more robust self-learning method, which is able to start from the weak initial solu- tion and iteratively improve the mapping. Coupled together, we provide a fully unsupervised cross- lingual mapping method that is effective in re- alistic settings, converges to a good solution in all cases tested, and sets a new state-of-the-art in bilingual lexicon extraction, even surpassing pre- vious supervised methods. Figure 1: Motivating example for our unsupervised initialization method, showing the similarity distri- butions of three words (corresponding to the smoothed density estimates from the normalized square root of the similarity matrices as defined in Section 3.2). Equivalent translations (two and due) have more similar distributions than non-related words (two and cane -meaning dog). This observation is used to build an initial solution that is later improved through self-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Cross-lingual embedding mapping methods work by independently training word embeddings in two languages, and then mapping them to a shared space using a linear transformation. Most of these methods are supervised, and use a bilingual dictionary of a few thousand entries to learn the mapping. Existing approaches can be classified into regression methods, which map the embeddings in one language using a least- squares objective <ref type="bibr">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Shigeto et al., 2015;</ref><ref type="bibr" target="#b5">Dinu et al., 2015)</ref>, canonical methods, which map the embeddings in both languages to a shared space using canonical correlation analy- sis and extensions of it <ref type="bibr">(Faruqui and Dyer, 2014;</ref><ref type="bibr">Lu et al., 2015)</ref>, orthogonal methods, which map the embeddings in one or both languages under the constraint of the transformation being orthog- onal ( <ref type="bibr" target="#b11">Xing et al., 2015;</ref><ref type="bibr" target="#b0">Artetxe et al., 2016;</ref><ref type="bibr" target="#b14">Zhang et al., 2016;</ref><ref type="bibr" target="#b7">Smith et al., 2017)</ref>, and margin meth- ods, which map the embeddings in one language to maximize the margin between the correct trans- lations and the rest of the candidates ( <ref type="bibr" target="#b5">Lazaridou et al., 2015)</ref>. <ref type="bibr" target="#b2">Artetxe et al. (2018a)</ref> showed that many of them could be generalized as part of a multi-step framework of linear transformations.</p><p>A related research line is to adapt these methods to the semi-supervised scenario, where the train- ing dictionary is much smaller and used as part of a bootstrapping process. While similar ideas where already explored for traditional count-based vec- tor space models <ref type="bibr">(Peirsman and Padó, 2010;</ref><ref type="bibr" target="#b10">Vuli´cVuli´c and Moens, 2013)</ref>, <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref> brought this approach to pre-trained low-dimensional word embeddings, which are more widely used nowa- days. More concretely, they proposed a self- learning approach that alternates the mapping and dictionary induction steps iteratively, obtaining re- sults that are comparable to those of supervised methods when starting with only 25 word pairs. A practical approach for reducing the need of bilingual supervision is to design heuristics to build the seed dictionary. The role of the seed lexicon in learning cross-lingual embedding map- pings is analyzed in depth by <ref type="bibr" target="#b9">Vuli´cVuli´c and Korhonen (2016)</ref>, who propose using document-aligned corpora to extract the training dictionary. A more common approach is to rely on shared words and cognates <ref type="bibr">(Peirsman and Padó, 2010;</ref><ref type="bibr" target="#b7">Smith et al., 2017)</ref>, while <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref> go further and restrict themselves to shared numerals. However, while these approaches are meant to eliminate the need of bilingual data in practice, they also make strong assumptions on the writing systems of lan- guages (e.g. that they all use a common alpha- bet or Arabic numerals). Closer to our work, a recent line of fully unsupervised approaches drops these assumptions completely, and attempts to learn cross-lingual embedding mappings based on distributional information alone. For that pur- pose, existing methods rely on adversarial train- ing. This was first proposed by Miceli Barone (2016), who combine an encoder that maps source language embeddings into the target language, a decoder that reconstructs the source language em- beddings from the mapped embeddings, and a dis- criminator that discriminates between the mapped embeddings and the true target language embed-dings. Despite promising, they conclude that their model "is not competitive with other cross-lingual representation approaches". <ref type="bibr" target="#b12">Zhang et al. (2017a)</ref> use a very similar architecture, but incorporate ad- ditional techniques like noise injection to aid train- ing and report competitive results on bilingual lex- icon extraction. <ref type="bibr" target="#b4">Conneau et al. (2018)</ref> drop the reconstruction component, regularize the mapping to be orthogonal, and incorporate an iterative re- finement process akin to self-learning, reporting very strong results on a large bilingual lexicon extraction dataset. Finally, <ref type="bibr" target="#b13">Zhang et al. (2017b)</ref> adopt the earth mover's distance for training, opti- mized through a Wasserstein generative adversar- ial network followed by an alternating optimiza- tion procedure. However, all this previous work used comparable Wikipedia corpora in most ex- periments and, as shown in Section 5, face diffi- culties in more challenging settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed method</head><p>Let X and Z be the word embedding matrices in two languages, so that their ith row X i * and Z i * denote the embeddings of the ith word in their re- spective vocabularies. Our goal is to learn the lin- ear transformation matrices W X and W Z so the mapped embeddings XW X and ZW Z are in the same cross-lingual space. At the same time, we aim to build a dictionary between both languages, encoded as a sparse matrix D where D ij = 1 if the jth word in the target language is a translation of the ith word in the source language.</p><p>Our proposed method consists of four sequen- tial steps: a pre-processing that normalizes the embeddings ( §3.1), a fully unsupervised initializa- tion scheme that creates an initial solution ( §3.2), a robust self-learning procedure that iteratively im- proves this solution ( §3.3), and a final refinement step that further improves the resulting mapping through symmetric re-weighting ( §3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding normalization</head><p>Our method starts with a pre-processing that length normalizes the embeddings, then mean centers each dimension, and then length normal- izes them again. The first two steps have been shown to be beneficial in previous work <ref type="bibr" target="#b0">(Artetxe et al., 2016)</ref>, while the second length normaliza- tion guarantees the final embeddings to have a unit length. As a result, the dot product of any two embeddings is equivalent to their cosine similarity and directly related to their Euclidean distance 1 , and can be taken as a measure of their similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fully unsupervised initialization</head><p>The underlying difficulty of the mapping problem in its unsupervised variant is that the word embed- ding matrices X and Z are unaligned across both axes: neither the ith vocabulary item X i * and Z i * nor the jth dimension of the embeddings X * j and Z * j are aligned, so there is no direct correspon- dence between both languages. In order to over- come this challenge and build an initial solution, we propose to first construct two alternative repre- sentations X ′ and Z ′ that are aligned across their jth dimension X ′ * j and Z ′ * j , which can later be used to build an initial dictionary that aligns their respective vocabularies.</p><p>Our approach is based on a simple idea: while the axes of the original embeddings X and Z are different in nature, both axes of their correspond- ing similarity matrices M X = XX T and M Z = ZZ T correspond to words, which can be exploited to reduce the mismatch to a single axis. More con- cretely, assuming that the embedding spaces are perfectly isometric, the similarity matrices M X and M Z would be equivalent up to a permutation of their rows and columns, where the permutation in question defines the dictionary across both lan- guages. In practice, the isometry requirement will not hold exactly, but it can be assumed to hold ap- proximately, as the very same problem of map- ping two embedding spaces without supervision would otherwise be hopeless. Based on that, one could try every possible permutation of row and column indices to find the best match between M X and M Z , but the resulting combinatorial explosion makes this approach intractable.</p><p>In order to overcome this problem, we pro- pose to first sort the values in each row of M X and M Z , resulting in matrices sorted(M X ) and sorted(M Z ) 2 . Under the strict isometry condition, equivalent words would get the exact same vec- tor across languages, and thus, given a word and its row in sorted(M X ), one could apply nearest neighbor retrieval over the rows of sorted(M Z ) to find its corresponding translation.</p><p>On a final note, given the singular value de- composition X = U SV T , the similarity matrix is M X = U S 2 U T . As such, its square root √ M X = U SU T is closer in nature to the origi- nal embeddings, and we also find it to work better in practice. We thus compute sorted( √ M X ) and sorted( √ M Z ) and normalize them as described in Section 3.1, yielding the two matrices X ′ and Z ′ that are later used to build the initial solution for self-learning (see Section 3.3).</p><p>In practice, the isometry assumption is strong enough so the above procedure captures some cross-lingual signal. In our English-Italian exper- iments, the average cosine similarity across the gold standard translation pairs is 0.009 for a ran- dom solution, 0.582 for the optimal supervised so- lution, and 0.112 for the mapping resulting from this initialization. While the latter is far from be- ing useful on its own (the accuracy of the resulting dictionary is only 0.52%), it is substantially better than chance, and it works well as an initial solution for the self-learning method described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Robust self-learning</head><p>Previous work has shown that self-learning can learn high-quality bilingual embedding mappings starting with as little as 25 word pairs ( <ref type="bibr" target="#b1">Artetxe et al., 2017)</ref>. In this method, training iterates through the following two steps until convergence:</p><p>1. Compute the optimal orthogonal mapping maximizing the similarities for the current dictionary D:</p><p>arg max</p><formula xml:id="formula_0">W X ,W Z i j D ij ((X i * W X ) · (Z j * W Z ))</formula><p>An optimal solution is given by W X = U and W Z = V , where U SV T = X T DZ is the singular value decomposition of X T DZ.</p><p>2. Compute the optimal dictionary over the sim- ilarity matrix of the mapped embeddings XW X W T Z Z T . This typically uses nearest neighbor retrieval from the source language into the target language, so</p><formula xml:id="formula_1">D ij = 1 if j = argmax k (X i * W X ) · (Z k * W Z ) and D ij = 0 otherwise.</formula><p>The underlying optimization objective is inde- pendent from the initial dictionary, and the algo- rithm is guaranteed to converge to a local opti- mum of it. However, the method does not work if starting from a completely random solution, as it tends to get stuck in poor local optima in that case.</p><p>For that reason, we use the unsupervised initial- ization procedure at Section 3.2 to build an initial solution. However, simply plugging in both meth- ods did not work in our preliminary experiments, as the quality of this initial method is not good enough to avoid poor local optima. For that rea- son, we next propose some key improvements in the dictionary induction step to make self-learning more robust and learn better mappings:</p><p>• Stochastic dictionary induction. In or- der to encourage a wider exploration of the search space, we make the dictionary induc- tion stochastic by randomly keeping some el- ements in the similarity matrix with probabil- ity p and setting the remaining ones to 0. As a consequence, the smaller the value of p is, the more the induced dictionary will vary from iteration to iteration, thus enabling to escape poor local optima. So as to find a fine-grained solution once the algorithm gets into a good region, we increase this value during train- ing akin to simulated annealing, starting with p = 0.1 and doubling this value every time the objective function at step 1 above does not improve more than ǫ = 10 −6 for 50 it- erations.</p><p>• Frequency-based vocabulary cutoff. The size of the similarity matrix grows quadrat- ically with respect to that of the vocabular- ies. This does not only increase the cost of computing it, but it also makes the number of possible solutions grow exponentially 3 , pre- sumably making the optimization problem harder. Given that less frequent words can be expected to be noisier, we propose to restrict the dictionary induction process to the k most frequent words in each language, where we find k = 20, 000 to work well in practice.</p><p>• CSLS retrieval. <ref type="bibr" target="#b5">Dinu et al. (2015)</ref> showed that nearest neighbor suffers from the hub- ness problem. This phenomenon is known to occur as an effect of the curse of dimen- sionality, and causes a few points (known as hubs) to be nearest neighbors of many other points (Radovanovi´cRadovanovi´c et al., 2010a,b). Among the existing solutions to penalize the similar- ity score of hubs, we adopt the Cross-domain Similarity Local Scaling (CSLS) from Con- neau et al. <ref type="bibr">(2018)</ref>. Given two mapped em- beddings x and y, the idea of CSLS is to compute r T (x) and r S (y), the average co- sine similarity of x and y for their k near- est neighbors in the other language, respec- tively. Having done that, the corrected score CSLS(x, y) = 2 cos(x, y) − r T (x) − r S (y). Following the authors, we set k = 10.</p><p>• Bidirectional dictionary induction. When the dictionary is induced from the source into the target language, not all target language words will be present in it, and some will oc- cur multiple times. We argue that this might accentuate the problem of local optima, as re- peated words might act as strong attractors from which it is difficult to escape. In or- der to mitigate this issue and encourage di- versity, we propose inducing the dictionary in both directions and taking their correspond- ing concatenation, so</p><formula xml:id="formula_2">D = D X→Z + D Z→X .</formula><p>In order to build the initial dictionary, we com- pute X ′ and Z ′ as detailed in Section 3.2 and apply the above procedure over them. As the only differ- ence, this first solution does not use the stochastic zeroing in the similarity matrix, as there is no need to encourage diversity (X ′ and Z ′ are only used once), and the threshold for vocabulary cutoff is set to k = 4, 000, so X ′ and Z ′ can fit in memory. Having computed the initial dictionary, X ′ and Z ′ are discarded, and the remaining iterations are per- formed over the original embeddings X and Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Symmetric re-weighting</head><p>As part of their multi-step framework, <ref type="bibr" target="#b2">Artetxe et al. (2018a)</ref> showed that re-weighting the tar- get language embeddings according to the cross- correlation in each component greatly improved the quality of the induced dictionary. Given the singular value decomposition U SV T = X T DZ, this is equivalent to taking W X = U and W Z = V S, where X and Z are previously whitened applying the linear transformations (X T X) − 1 2 and (Z T Z) − 1 2 , and later de-whitened applying</p><formula xml:id="formula_3">U T (X T X) 1 2 U and V T (Z T Z) 1 2 V .</formula><p>However, re-weighting also accentuates the problem of local optima when incorporated into self-learning as, by increasing the relevance of dimensions that best match for the current solu- tion, it discourages to explore other regions of the search space. For that reason, we propose using it as a final step once self-learning has converged to a good solution. Unlike <ref type="bibr" target="#b2">Artetxe et al. (2018a)</ref>, we apply re-weighting symmetrically in both lan- guages, taking W X = U S 1 2 and W Z = V S 1 2 . This approach is neutral in the direction of the mapping, and gives good results as shown in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental settings</head><p>Following common practice, we evaluate our method on bilingual lexicon extraction, which measures the accuracy of the induced dictionary in comparison to a gold standard.</p><p>As discussed before, previous evaluation has focused on favorable conditions. In particular, ex- isting unsupervised methods have almost exclu- sively been tested on Wikipedia corpora, which is comparable rather than monolingual, exposing a strong cross-lingual signal that is not available in strictly unsupervised settings. In addition to that, some datasets comprise unusually small embed- dings, with only 50 dimensions and around 5,000- 10,000 vocabulary items ( <ref type="bibr">Zhang et al., 2017a,b)</ref>. As the only exception, <ref type="bibr" target="#b4">Conneau et al. (2018)</ref> re- port positive results on the English-Italian dataset of <ref type="bibr" target="#b5">Dinu et al. (2015)</ref> in addition to their main experiments, which are carried out in Wikipedia. While this dataset does use strictly monolingual corpora, it still corresponds to a pair of two rela- tively close indo-european languages.</p><p>In order to get a wider picture of how our method compares to previous work in differ- ent conditions, including more challenging set- tings, we carry out our experiments in the widely used dataset of <ref type="bibr" target="#b5">Dinu et al. (2015)</ref> and the subsequent extensions of <ref type="bibr" target="#b1">Artetxe et al. (2017</ref><ref type="bibr" target="#b2">Artetxe et al. ( , 2018a</ref>, which together comprise English-Italian, English-German, English-Finnish and English- Spanish. More concretely, the dataset consists of 300-dimensional CBOW embeddings trained on WacKy crawling corpora (English, Italian, Ger- man), Common Crawl (Finnish) and WMT News Crawl (Spanish). The gold standards were de- rived from dictionaries built from Europarl word alignments and available at OPUS <ref type="bibr" target="#b8">(Tiedemann, 2012)</ref>, split in a test set of 1,500 entries and a training set of 5,000 that we do not use in our experiments. The datasets are freely avail- able. As a non-european agglutinative language, the English-Finnish pair is particularly challeng-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ES-EN</head><p>IT-EN TR-EN best avg s t best avg s t best avg s t <ref type="bibr" target="#b12">Zhang et al. (2017a)</ref>, λ = 1 71.43 68.18 10 13.2 60.38 56.45 10 12.3 0.00 0.00 0 13.0 <ref type="bibr" target="#b12">Zhang et al. (2017a)</ref>, λ = 10 70.24 66.37 10 13.0 57.64 52.60 10 12.6 21.07 17.95 10 13.  <ref type="table">Table 1</ref>: Results of unsupervised methods on the dataset of <ref type="bibr" target="#b12">Zhang et al. (2017a)</ref>. We perform 10 runs for each method and report the best and average accuracies (%), the number of successful runs (those with &gt;5% accuracy) and the average runtime (minutes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN-IT EN-DE EN-FI EN-ES</head><p>best avg s t best avg s t best avg s t best avg s t  <ref type="formula">2018)</ref>   <ref type="bibr">2017, 2018a)</ref>. We perform 10 runs for each method and report the best and average accu- racies (%), the number of successful runs (those with &gt;5% accuracy) and the average runtime (minutes).</p><p>ing due to the linguistic distance between them. For completeness, we also test our method in the Spanish-English, Italian-English and Turkish- English datasets of <ref type="bibr" target="#b12">Zhang et al. (2017a)</ref>, which consist of 50-dimensional CBOW embeddings trained on Wikipedia, as well as gold standard dictionaries 4 from Open Multilingual WordNet (Spanish-English and Italian-English) and Google Translate (Turkish-English). The lower dimen- sionality and comparable corpora make an easier scenario, although it also contains a challenging pair of distant languages (Turkish-English).</p><p>Our method is implemented in Python using NumPy and CuPy. Together with it, we also test the methods of <ref type="bibr" target="#b12">Zhang et al. (2017a)</ref> and <ref type="bibr" target="#b4">Conneau et al. (2018)</ref> using the publicly available imple- mentations from the authors 5 . Given that <ref type="bibr" target="#b12">Zhang et al. (2017a)</ref> report using a different value of their hyperparameter λ for different language pairs (λ = 10 for English-Turkish and λ = 1 for the rest), we test both values in all our experiments to <ref type="bibr">4</ref> The test dictionaries were obtained through personal communication with the authors. The rest of the language pairs were left out due to licensing issues. <ref type="bibr">5</ref> Despite our efforts, <ref type="bibr" target="#b13">Zhang et al. (2017b)</ref> was left out be- cause: 1) it does not create a one-to-one dictionary, thus diffi- culting direct comparison, 2) it depends on expensive propri- etary software 3) its computational cost is orders of magni- tude higher (running the experiments would have taken sev- eral months).</p><p>better understand its effect. In the case of <ref type="bibr" target="#b4">Conneau et al. (2018)</ref>, we test both the default hyperparam- eters in the source code as well as those reported in the paper, with iterative refinement activated in both cases. Given the instability of these methods, we perform 10 runs for each, and report the best and average accuracies, the number of successful runs (those with &gt;5% accuracy) and the average runtime. All the experiments were run in a single Nvidia Titan Xp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and discussion</head><p>We first present the main results ( §5.1), then the comparison to the state-of-the-art ( §5.2), and fi- nally ablation tests to measure the contribution of each component ( §5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main results</head><p>We report the results in the dataset of <ref type="bibr" target="#b12">Zhang et al. (2017a)</ref> at <ref type="table">Table 1</ref>. As it can be seen, the pro- posed method performs at par with that of Con- neau et al. (2018) both in Spanish-English and Italian-English, but gets substantially better re- sults in the more challenging Turkish-English pair. While we are able to reproduce the results re- ported by <ref type="bibr" target="#b12">Zhang et al. (2017a)</ref>, their method gets the worst results of all by a large margin. An- other disadvantage of that model is that different  . The remaining results were reported in the original papers. For methods that do not require supervision, we report the average accuracy across 10 runs. ‡ For meaningful comparison, runs with &lt;5% accuracy are excluded when computing the average, but note that, unlike ours, their method often gives a degenerated solution (see <ref type="table" target="#tab_2">Table 2</ref>).</p><note type="other">Supervision Method EN-IT EN-DE EN-FI EN-ES 5k dict</note><p>language pairs require different hyperparameters: λ = 1 works substantially better for Spanish- English and Italian-English, but only λ = 10 works for Turkish-English.</p><p>The results for the more challenging dataset from <ref type="bibr" target="#b5">Dinu et al. (2015)</ref> and the extensions of <ref type="bibr" target="#b1">Artetxe et al. (2017</ref><ref type="bibr" target="#b2">Artetxe et al. ( , 2018a</ref>) are given in <ref type="table" target="#tab_2">Table  2</ref>. In this case, our proposed method obtains the best results in all metrics for all the four language pairs tested. The method of <ref type="bibr" target="#b12">Zhang et al. (2017a)</ref> does not work at all in this more challenging sce- nario, which is in line with the negative results re- ported by the authors themselves for similar con- ditions (only %2.53 accuracy in their large Gi- gaword dataset). The method of <ref type="bibr" target="#b4">Conneau et al. (2018)</ref> also fails for English-Finnish (only 1.62% in the best run), although it is able to get positive results in some runs for the rest of language pairs. Between the two configurations tested, the default hyperparameters in the code show a more stable behavior.</p><p>These results confirm the robustness of the pro- posed method. While the other systems succeed in some runs and fail in others, our method con- verges to a good solution in all runs without excep- tion and, in fact, it is the only one getting positive results for English-Finnish. In addition to being more robust, our method also obtains substantially better accuracies, surpassing previous methods by at least 1-3 points in all but the easiest pairs. More- over, our method is not sensitive to hyperparame- ters that are difficult to tune without a development set, which is critical in realistic unsupervised con- ditions.</p><p>At the same time, our method is significantly faster than the rest. In relation to that, it is interest- ing that, while previous methods perform a fixed number of iterations and take practically the same time for all the different language pairs, the run- time of our method adapts to the difficulty of the task thanks to the dynamic convergence criterion of our stochastic approach. This way, our method tends to take longer for more challenging language pairs (1.7 vs 0.6 minutes for es-en and tr-en in one dataset, and 12.9 vs 7.3 minutes for en-fi and en-de in the other) and, in fact, our (relative) ex- ecution times correlate surprisingly well with the linguistic distance with English (closest/fastest is German, followed by Italian/Spanish, followed by Turkish/Finnish).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN-IT EN-DE EN-FI EN-ES</head><p>best avg s t best avg s t best avg s t best avg s t  <ref type="table">Table 4</ref>: Ablation test on the dataset of <ref type="bibr" target="#b5">Dinu et al. (2015)</ref> and the extensions of <ref type="bibr" target="#b1">Artetxe et al. (2017</ref><ref type="bibr" target="#b2">Artetxe et al. ( , 2018a</ref>. We perform 10 runs for each method and report the best and average accuracies (%), the number of successful runs (those with &gt;5% accuracy) and the average runtime (minutes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2</head><p>Comparison with the state-of-the-art <ref type="table" target="#tab_4">Table 3</ref> shows the results of the proposed method in comparison to previous systems, including those with different degrees of supervision. We focus on the widely used English-Italian dataset of <ref type="bibr" target="#b5">Dinu et al. (2015)</ref> and its extensions. Despite being fully unsupervised, our method achieves the best results in all language pairs but one, even sur- passing previous supervised approaches. The only exception is English-Finnish, where Artetxe et al. (2018a) gets marginally better results with a dif- ference of 0.3 points, yet ours is the only unsu- pervised system that works for this pair. At the same time, it is remarkable that the proposed sys- tem gets substantially better results than <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref>, the only other system based on self- learning, with the additional advantage of being fully unsupervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation test</head><p>In order to better understand the role of different aspects in the proposed system, we perform an ab- lation test, where we separately analyze the effect of initialization, the different components of our robust self-learning algorithm, and the final sym- metric re-weighting. The obtained results are re- ported in <ref type="table">Table 4</ref>.</p><p>In concordance with previous work, our results show that self-learning does not work with ran- dom initialization. However, the proposed unsu- pervised initialization is able to overcome this is- sue without the need of any additional informa- tion, performing at par with other character-level heuristics that we tested (e.g. shared numerals).</p><p>As for the different self-learning components, we observe that the stochastic dictionary induction is necessary to overcome the problem of poor lo- cal optima for English-Finnish, although it does not make any difference for the rest of easier lan- guage pairs. The frequency-based vocabulary cut- off also has a positive effect, yielding to slightly better accuracies and much faster runtimes. At the same time, CSLS plays a critical role in the sys- tem, as hubness severely accentuates the problem of local optima in its absence. The bidirectional dictionary induction is also beneficial, contribut- ing to the robustness of the system as shown by English-Finnish and yielding to better accuracies in all cases.</p><p>Finally, these results also show that symmet- ric re-weighting contributes positively, bringing an improvement of around 1-2 points without any cost in the execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we show that previous unsupervised mapping methods ( <ref type="bibr" target="#b12">Zhang et al., 2017a;</ref><ref type="bibr" target="#b4">Conneau et al., 2018</ref>) often fail on realistic scenarios involv- ing non-comparable corpora and/or distant lan- guages. In contrast to adversarial methods, we propose to use an initial weak mapping that ex- ploits the structure of the embedding spaces in combination with a robust self-learning approach. The results show that our method succeeds in all cases, providing the best results with respect to all previous work on unsupervised and supervised mappings.</p><p>The ablation analysis shows that our initial so- lution is instrumental for making self-learning work without supervision. In order to make self- learning robust, we also added stochasticity to dictionary induction, used CSLS instead of near- est neighbor, and produced bidirectional dictio- naries. Results also improved using smaller in-termediate vocabularies and re-weighting the fi- nal solution. Our implementation is available as an open source project at https://github. com/artetxem/vecmap.</p><p>In the future, we would like to extend the method from the bilingual to the multilingual sce- nario, and go beyond the word level by incorporat- ing embeddings of longer phrases. <ref type="bibr">Manaal Faruqui and Chris Dyer. 2014</ref>. Improving vec- tor space word representations using multilingual correlation. In Proceedings of the 14th Conference of the European Chapter of the Association for Com- putational Linguistics, pages 462-471, Gothenburg, Sweden. Association for Computational Linguistics. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Results of unsupervised methods on the dataset of Dinu et al. (2015) and the extensions of Artetxe et al. (</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Mikolov et al. (2013) 
34.93  † 35.00  † 
25.91  † 27.73  † 
Faruqui and Dyer (2014) 
38.40 * 
37.13 * 
27.60 * 26.80 * 
Shigeto et al. (2015) 
41.53  † 43.07  † 
31.04  † 33.73  † 
Dinu et al. (2015) 
37.7 
38.93 * 
29.14 * 30.40 * 
Lazaridou et al. (2015) 
40.2 
-
-
-
Xing et al. (2015) 
36.87  † 41.27  † 
28.23  † 31.20  † 
Zhang et al. (2016) 
36.73  † 40.80  † 
28.16  † 31.07  † 
Artetxe et al. (2016) 
39.27 
41.87 * 
30.62 * 31.40 * 
Artetxe et al. (2017) 
39.67 
40.87 
28.72 
-
Smith et al. (2017) 
43.1 
43.33  † 
29.42  † 35.13  † 
Artetxe et al. (2018a) 
45.27 
44.13 
32.94 
36.60 

25 dict. 
Artetxe et al. (2017) 
37.27 
39.60 
28.16 
-

Init. 
Smith et al. (2017), cognates 
39.9 
-
-
-
heurist. 
Artetxe et al. (2017), num. 
39.40 
40.27 
26.47 
-

None 

Zhang et al. (2017a), λ = 1 
0.00 * 
0.00 * 
0.00 * 
0.00 * 
Zhang et al. (2017a), λ = 10 
0.00 * 
0.00 * 
0.01 * 
0.01 * 
Conneau et al. (2018), code  ‡ 
45.15 * 
46.83 * 
0.38 * 35.38 * 
Conneau et al. (2018), paper  ‡ 45.1 
0.01 * 
0.01 * 35.44 * 
Proposed method 
48.13 
48.19 
32.63 
37.33 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy (%) of the proposed method in comparison with previous work. * Results obtained 
with the official implementation from the authors.  † Results obtained with the framework from Artetxe 
et al. (2018a)</table></figure>

			<note place="foot" n="1"> Given two length normalized vectors u and v, u · v = cos(u, v) = 1 − ||u − v|| 2 /2. 2 Note that the values in each row are sorted independently from other rows.</note>

			<note place="foot" n="3"> There are m n possible combinations that go from a source vocabulary of n entries to a target vocabulary of m entries.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially supported by the Spanish MINECO (TUNER TIN2015-65308-C5-1-R, MUSTER PCIN-2015-226 and TADEEP TIN2015-70214-P, cofunded by EU FEDER), the UPV/EHU (excellence research group), and the NVIDIA GPU grant program. Mikel Artetxe en-joys a doctoral grant from the Spanish MECD.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalizing and improving bilingual word embedding mappings with a multi-step framework of linear transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5012" to="5019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>workshop track</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ridge regression, hubness, and zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaro</forename><surname>Shigeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikumi</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuo</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Shimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2015, Proceedings, Part I</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in opus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC-2012)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
	<note>Istanbul, Turkey. European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the role of seed lexicons in learning bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1613" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1006" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Earth mover&apos;s distance minimization for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1934" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ten pairs to tag-multilingual pos tagging via coarse mapping between embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1307" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
