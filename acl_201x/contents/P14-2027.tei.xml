<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalized Character-Level Spelling Error Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noura</forename><surname>Farra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Computational Learning Systems</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadi</forename><surname>Tomeh</surname></persName>
							<email>nadi.tomeh@lipn.univ-paris13.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Computational Learning Systems</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Computational Learning Systems</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Computational Learning Systems</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalized Character-Level Spelling Error Correction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="161" to="167"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
					<note>†LIPN, Université Paris 13, Sorbonne Paris Cité</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a generalized discrimina-tive model for spelling error correction which targets character-level transformations. While operating at the character level, the model makes use of word-level and contextual information. In contrast to previous work, the proposed approach learns to correct a variety of error types without guidance of manually-selected constraints or language-specific features. We apply the model to correct errors in Egyptian Arabic dialect text, achieving 65% reduction in word error rate over the input baseline, and improving over the earlier state-of-the-art system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spelling error correction is a longstanding Natural Language Processing (NLP) problem, and it has recently become especially relevant because of the many potential applications to the large amount of informal and unedited text generated online, including web forums, tweets, blogs, and email. Misspellings in such text can lead to increased sparsity and errors, posing a challenge for many NLP applications such as text summarization, sen- timent analysis and machine translation.</p><p>In this work, we present GSEC, a Generalized character-level Spelling Error Correction model, which uses supervised learning to map input char- acters into output characters in context. The ap- proach has the following characteristics:</p><p>Character-level Corrections are learned at the character-level 1 using a supervised sequence la- beling approach.</p><p>Generalized The input space consists of all characters, and a single classifier is used to learn common error patterns over all the training data, without guidance of specific rules.</p><p>Context-sensitive The model looks beyond the context of the current word, when making a deci- sion at the character-level.</p><p>Discriminative The model provides the free- dom of adding a number of different features, which may or may not be language-specific.</p><p>Language-Independent In this work, we in- tegrate only language-independent features, and therefore do not consider morphological or lin- guistic features. However, we apply the model to correct errors in Egyptian Arabic dialect text, following a conventional orthography standard, CODA <ref type="bibr">(Habash et al., 2012)</ref>.</p><p>Using the described approach, we demonstrate a word-error-rate (WER) reduction of 65% over a do-nothing input baseline, and we improve over a state-of-the-art system <ref type="bibr">(Eskander et al., 2013)</ref> which relies heavily on language-specific and manually-selected constraints. We present a de- tailed analysis of mistakes and demonstrate that the proposed model indeed learns to correct a wider variety of errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most earlier work on automatic error correction addressed spelling errors in English and built mod- els of correct usage on native English data ( <ref type="bibr">Kukich, 1992;</ref><ref type="bibr">Golding and Roth, 1999;</ref><ref type="bibr">Carlson and Fette, 2007;</ref><ref type="bibr">Banko and Brill, 2001</ref>). Ara- bic spelling correction has also received consider- able interest <ref type="bibr">(Ben Othmane Zribi and Ben Ahmed, 2003;</ref><ref type="bibr">Haddad and Yaseen, 2007;</ref><ref type="bibr">Hassan et al., 2008;</ref><ref type="bibr" target="#b9">Shaalan et al., 2010;</ref><ref type="bibr" target="#b0">Alkanhal et al., 2012;</ref><ref type="bibr">Eskander et al., 2013;</ref><ref type="bibr" target="#b12">Zaghouani et al., 2014</ref>).</p><p>Supervised spelling correction approaches trained on paired examples of errors and their cor- rections have recently been applied for non-native English correction <ref type="bibr" target="#b11">(van Delden et al., 2004;</ref><ref type="bibr" target="#b1">Li et al., 2012;</ref><ref type="bibr">Gamon, 2010;</ref><ref type="bibr">Dahlmeier and Ng, 2012;</ref><ref type="bibr" target="#b8">Rozovskaya and Roth, 2011</ref>). Discriminative models have been proposed at the word-level for error correction <ref type="bibr" target="#b1">(Duan et al., 2012</ref>) and for error detection <ref type="bibr">(Habash and Roth, 2011</ref>).</p><p>In addition, there has been growing work on lex- ical normalization of social media data, a some- what related problem to that considered in this pa- per ( <ref type="bibr">Han and Baldwin, 2011;</ref><ref type="bibr">Han et al., 2013;</ref><ref type="bibr" target="#b10">Subramaniam et al., 2009;</ref><ref type="bibr" target="#b2">Ling et al., 2013)</ref>.</p><p>The work of <ref type="bibr">Eskander et al. (2013)</ref> is the most relevant to the present study: it presents a character-edit classification model (CEC) using the same dataset we use in this paper. <ref type="bibr">2 Eskander et al. (2013)</ref> analyzed the data to identify the seven most common types of errors. They devel- oped seven classifiers and applied them to the data in succession. This makes the approach tailored to the specific data set in use and limited to a specific set of errors. In this work, a single model is con- sidered for all types of errors. The model consid- ers every character in the input text for a possible spelling error, as opposed to looking only at cer- tain input characters and contexts in which they appear. Moreover, in contrast to <ref type="bibr">Eskander et al. (2013)</ref>, it looks beyond the boundary of the cur- rent word.</p><p>3 The GSEC Approach</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling Spelling Correction at the Character Level</head><p>We recast the problem of spelling correction into a sequence labeling problem, where for each input character, we predict an action label describing how to transform it to obtain the correct charac- ter. The proposed model therefore transforms a given input sentence e = e 1 , . . . , e n of n char- acters that possibly include errors, to a corrected sentence c of m characters, where corrected char- acters are produced by one of the following four actions applied to each input character e i :</p><p>• ok: e i is passed without transformation.</p><p>• substitute − with(c): e i is substituted with a character c where c could be any character encountered in the training data.</p><p>• delete: e i is deleted.</p><p>• insert(c): A character c is inserted bef ore e i . To address errors occurring at the end 2 Eskander et al. (2013) also considered a slower, more expensive, and more language-specific method using a mor- phological tagger <ref type="bibr">(Habash et al., 2013)</ref> that outperformed the CEC model; however, we do not compare to it in this paper.  of the sentence, we assume the presence of a dummy sentence-final stop character.</p><formula xml:id="formula_0">Input Action Label k substitute-with(c) o ok r insert(r) e ok c ok t ok d delete</formula><p>We use a multi-class SVM classifier to predict the action labels for each input character e i ∈ e. A decoding process is then applied to transform the input characters accordingly to produce the cor- rected sentence. Note that we consider the space character as a character like any other, which gives us the ability to correct word merge errors with space character insertion actions and word split er- rors with space character deletion actions. <ref type="table" target="#tab_0">Table 1</ref> shows an example of the spelling correction pro- cess.</p><p>In this paper, we only model single-edit actions and ignore cases where a character requires mul- tiple edits (henceforth, complex actions), such as multiple insertions or a combination of insertions and substitutions. This choice was motivated by the need to reduce the number of output labels, as many infrequent labels are generated by complex actions. An error analysis of the training data, de- scribed in detail in section 3.2, showed that com- plex errors are relatively infrequent (4% of data). We plan to address these errors in future work.</p><p>Finally, in order to generate the training data in the described form, we require a parallel cor- pus of erroneous and corrected reference text (de- scribed below), which we align at the character level. We use the alignment tool Sclite <ref type="bibr">(Fiscus, 1998)</ref>, which is part of the SCTK Toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Description of Data</head><p>We apply our model to correcting Egyptian Ara- bic dialect text. Since there is no standard dialect orthography adopted by native speakers of Ara- bic dialects, it is common to encounter multiple   <ref type="table" target="#tab_1">Table 2</ref> for corpus statistics. <ref type="table" target="#tab_3">Table 3</ref> presents the distri- bution of correction action labels that correspond to spelling errors in the training data together with examples of these errors. <ref type="bibr">3</ref> We group the ac- tions into: Substitute, Insert, Delete, and Complex, and also list common transformations within each group. We further distinguish between the phe- nomena modeled by our system and by <ref type="bibr">Eskander et al. (2013)</ref>. At least 10% of all generated action labels are not handled by Eskander et al. (2013).</p><note type="other">Action % Errors Example Error ⇒ Reference Substitute 80</note><formula xml:id="formula_1">.9 E Alif A forms (/ / / AÂ/ ˇ A/ ¯ A) 33.3 AHdhm ⇒ ÂHdhm ⇒ E Ya / forms ( y/ý) 26.7 ςly ⇒ ςlý ⇒ E h/ / , h/w /</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Distribution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Features</head><p>Each input character is represented by a feature vector. We include a set of basic features inspired by Eskander et al. (2013) in their CEC system and additional features for further improvement.</p><p>Basic features We use a set of nine basic fea- tures: the given character, the preceding and fol- lowing two characters, and the first two and last two characters in the word. These are the same features used by CEC, except that CEC does not include characters beyond the word boundary, while we consider space characters as well as char- acters from the previous and next words.</p><p>Ngram features We extract sequences of char- acters corresponding to the current character and the following and previous two, three, or four characters. We refer to these sequences as bi- grams, trigrams, or 4-grams, respectively. These are an extension of the basic features and allow the model to look beyond the context of the cur- rent word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Maximum Likelihood Estimate (MLE)</head><p>We implemented another approach for error cor- rection based on a word-level maximum likeli- hood model. The MLE method uses a unigram model which replaces each input word with its most likely correct word based on counts from the training data. The intuition behind MLE is that it can easily correct frequent errors; however, it is quite dependent on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Evaluation</head><p>Setup The training data was extracted to gener- ate the form described in Section 3.1, using the Sclite tool <ref type="bibr">(Fiscus, 1998)</ref> to align the input and reference sentences. A speech effect handling step was applied as a preprocessing step to all models.</p><p>This step removes redundant repetitions of charac- ters in sequence, e.g., ktyyyyyr 'veeeeery'.</p><p>The same speech effect handling was applied by <ref type="bibr">Eskander et al. (2013)</ref>. For classification, we used the SVM implemen- tation in YamCha ( <ref type="bibr">Kudo and Matsumoto, 2001)</ref>, and trained with different variations of the fea- tures described above. Default parameters were selected for training (c=1, quadratic kernel, and context window of +/-2).</p><p>In all results listed below, the baseline corre- sponds to the do-nothing baseline of the input text.</p><p>Metrics Three evaluation metrics are used. The word-error-rate WER metric is computed by sum- ming the total number of word-level substitution errors, insertion errors, and deletion errors in the output, and dividing by the number of words in the reference. The correct-rate Corr metric is com- puted by dividing the number of correct output words by the total number of words in the refer- ence. These two metrics are produced by Sclite <ref type="bibr">(Fiscus, 1998)</ref>, using automatic alignment. Fi- nally, the accuracy Acc metric, used by <ref type="bibr">Eskander et al. (2013)</ref>, is a simple string matching metric which enforces a word alignment that pairs words in the reference to those of the output. It is cal- culated by dividing the number of correct output words by the number of words in the input. This metric assumes no split errors in the data (a word incorrectly split into two words), which is the case in the data we are working with.</p><p>Character-level Model Evaluation The per- formance of the generalized spelling correction model (GSEC) on the dev data is presented in the first half of <ref type="table" target="#tab_5">Table 4</ref>. The results of the Eskan- der et al. (2013) CEC system are also presented for the purpose of comparison. We can see that using a single classifier, the generalized model is able to outperform CEC, which relies on a cascade of classifiers (p = 0.03 for the basic model and p &lt; 0.0001 for the best model, GSEC+4grams). <ref type="bibr">4</ref> Model Combination Evaluation Here we present results on combining GSEC with the MLE component (GSEC+MLE). We combine the two models in cascade: the MLE component is applied to the output of GSEC. To train the MLE model, we use the word pairs obtained from the original training data, rather than from the output of GSEC. We found that this configuration allows <ref type="bibr">4</ref> Significance results are obtained using McNemar's test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Corr%/WER Acc%  us to include a larger sample of word pair errors for learning, because our model corrects many errors, leaving fewer example pairs to train an MLE post-processor. The results are shown in the second half of <ref type="table" target="#tab_5">Table 4</ref>. We first observe that MLE improves the per- formance of both CEC and GSEC. In fact, CEC+MLE and GSEC+MLE perform similarly (p = 0.36, not statistically significant). When adding features that go beyond the word bound- ary, we achieve an improvement over MLE, GSEC+MLE, and CEC+MLE, all of which are mostly restricted within the boundary of the word. The best GSEC model outperforms CEC+MLE (p &lt; 0.0001), achieving a WER of 8.3%, corre- sponding to 65% reduction compared to the base- line. It is worth noting that adding the MLE com- ponent allows Eskander's CEC to recover various types of errors that were not modeled previously. However, the contribution of MLE is limited to words that are in the training data. On the other hand, because GSEC is trained on character trans- formations, it is likely to generalize better to words unseen in the training data. <ref type="table" target="#tab_7">Table 5</ref> presents the re- sults of our best model (GSEC+4grams), and best model+MLE. The latter achieves a 92.1% Acc score. The Acc score reported by <ref type="bibr">Eskander et al. (2013)</ref> for CEC+MLE is 91.3% . The two results are statistically significant (p &lt; 0.0001) with re- spect to CEC and CEC+MLE respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Test Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Corr%  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Error Analysis</head><p>To gain a better understanding of the performance of the models on different types of errors and their interaction with the MLE component, we separate the words in the dev data into: (1) words seen in the training data, or in-vocabulary words (IV), and (2) out-of-vocabulary (OOV) words not seen in the training data. Because the MLE model maps every input word to its most likely gold word seen in the training data, we expect the MLE compo- nent to recover a large portion of errors in the IV category (but not all, since an input word can have multiple correct readings depending on the con- text). On the other hand, the recovery of errors in OOV words indicates how well the character-level model is doing independently of the MLE compo- nent. <ref type="table" target="#tab_9">Table 6</ref> presents the performance, using the Acc metric, on each of these types of words. Here our best model (GSEC+4grams) is considered.  When considering words seen in the training data, CEC and GSEC have the same performance. However, when considering OOV words, GSEC performs significantly better (p &lt; 0.0001), veri- fying our hypothesis that a generalized model re- duces dependency on training data. The data is heavily skewed towards IV words (83%), which explains the generally high performance of MLE.</p><p>We performed a manual error analysis on a sam- ple of 50 word errors from the IV set and found that all of the errors came from gold annotation er- rors and inconsistencies, either in the dev or train. We then divided the character transformations in the OOV words into four groups: (1) characters that were unchanged by the gold (X-X transforma- tions), (2) character transformations modeled by CEC (X-Y CEC), (3) character transformations not modeled by CEC, and which include all phenom- ena that were only partially modeled by CEC (X-Y not CEC), and (4) complex errors. The character- level accuracy on each of these groups is shown in <ref type="table">Table 7</ref>.</p><p>Both CEC and GSEC do much better on the second group of character transformations (that is, X-Y CEC) than on the third group (X-Y not CEC). This is not surprising because the former Type #Chars Example CEC GSEC X-X 16502 m-m, space-space 99.25 99.33</p><formula xml:id="formula_2">X-Y 609 -h, h-, ˇ A-A 80.62 83.09 (CEC) A-ˇ A, y-ý X-Y 161 t-θ , del{w} 31.68 43.48 (not CEC) n-ins{space} Complex 32</formula><p>n-ins{A}{m} 37.5 15.63 <ref type="table">Table 7</ref>: Character-level accuracy on different transforma- tion types for out-of-vocabulary words. For complex trans- formations, the accuracy represents the complex category recognition rate, and not the actual correction accuracy.</p><p>transformations correspond to phenomena that are most common in the training data. For GSEC, they are learned automatically, while for CEC they are selected and modeled explicitly. Despite this fact, GSEC generalizes better to OOV words. As for the third group, both CEC and GSEC per- form more poorly, but GSEC corrects more errors (43.48% vs. 31.68% accuracy). Finally, CEC is better at recognizing complex errors, which, al- though are not modeled explicitly by CEC, can sometimes be corrected as a result of applying multiple classifiers in cascade. Dealing with com- plex errors, though there are few of them in this dataset, is an important direction for future work, and for generalizing to other datasets, e.g., <ref type="bibr" target="#b12">(Zaghouani et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We showed that a generalized character-level spelling error correction model can improve spelling error correction on Egyptian Arabic data. This model learns common spelling error patterns automatically, without guidance of manually se- lected or language-specific constraints. We also demonstrate that the model outperforms existing methods, especially on out-of-vocabulary words.</p><p>In the future, we plan to extend the model to use word-level language models to select between top character predictions in the output. We also plan to apply the model to different datasets and differ- ent languages. Finally, we plan to experiment with more features that can also be tailored to specific languages by using morphological and linguistic information, which was not explored in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Character-level spelling error correction process 

on the input word korectd, with the reference word correct 

Train Dev 
Test 
Sentences 
10.3K 1.67K 1.73K 
Characters 675K 
106K 
103K 
Words 
134K 
21.1K 20.6K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : ARZ Egyptian dialect corpus statistics</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Character-level distribution of correction labels. We model all types of transformations except complex actions, and 

rare Insert labels with counts below a tuned threshold. The Delete label is a single label that comprises all deletion actions. 
Labels modeled by Eskander et al. (2013) are marked with E , and EP for cases modeled partially, for example, the Insert{A} 
would only be applied at certain positions such as the end of the word. 

spellings of the same word. The CODA orthogra-
phy was proposed by Habash et al. (2012) in an 
attempt to standardize dialectal writing, and we 
use it as a reference of correct text for spelling 
correction following the previous work by Eskan-
der et al. (2013). We use the same corpus (la-
beled "ARZ") and experimental setup splits used 
by them. The ARZ corpus was developed by 
the Linguistic Data Consortium (Maamouri et al., 
2012a-e). See </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Model Evaluation. GSEC represents the gener-

alized character-level model. CEC represents the character-
level-edit classification model of Eskander et al. (2013). 
Rows marked with an asterisk (*) are statistically signifi-
cant compared to CEC (for the first half of the table) or 
CEC+MLE (for the second half of the table), with p &lt; 0.05. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Evaluation on test data. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Accuracy of character-level models shown sepa-

rately on out-of-vocabulary and in-vocabulary words. 

</table></figure>

			<note place="foot" n="1"> We use the term &apos;character&apos; strictly in the alphabetic sense, not the logographic sense (as in the Chinese script).</note>

			<note place="foot" n="3"> Arabic transliteration is presented in the Habash-SoudiBuckwalter scheme (Habash et al., 2007). For more information on Arabic orthography in NLP, see (Habash, 2010).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This publication was made possible by grant NPRP-4-1058-1-168 from the Qatar National Re-search Fund (a member of the Qatar Foundation). The statements made herein are solely the respon-sibility of the authors. </p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic Stochastic Arabic Spelling Correction With Emphasis on Space Insertions and Deletions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">I</forename><surname>Alkanhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">A</forename><surname>Al-Badrashiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mansour</forename><forename type="middle">M</forename><surname>Alghamdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulaziz</forename><forename type="middle">O</forename><surname>Alqabbany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2111" to="2122" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A generalized hidden markov model with discriminative training for query spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;12</title>
		<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="611" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Paraphrasing 4 microblog normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Maamouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sondos</forename><surname>Krouna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalila</forename><surname>Tabassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ciul</surname></persName>
		</author>
		<title level="m">Egyptian Arabic Treebank DF Part 1 V2.0. LDC catalog number LDC2012E93</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Egyptian Arabic Treebank DF Part 2 V2.0. LDC catalog number LDC2012E98</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Maamouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sondos</forename><surname>Krouna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalila</forename><surname>Tabassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ciul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Egyptian Arabic Treebank DF Part 3 V2.0. LDC catalog number LDC2012E89</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Maamouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sondos</forename><surname>Krouna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalila</forename><surname>Tabassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ciul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Maamouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sondos</forename><surname>Krouna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalila</forename><surname>Tabassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ciul</surname></persName>
		</author>
		<title level="m">Egyptian Arabic Treebank DF Part 4 V2.0. LDC catalog number LDC2012E99</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Maamouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Kulick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sondos</forename><surname>Krouna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalila</forename><surname>Tabassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ciul</surname></persName>
		</author>
		<title level="m">Egyptian Arabic Treebank DF Part 5 V2.0. LDC catalog number LDC2012E107</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Algorithm selection and model adaptation for esl correction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association of Computational Linguistics (ACL)<address><addrLine>Portland, Oregon, 6</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An approach for analyzing and correcting spelling errors for non-native Arabic learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Shaalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rana</forename><surname>Aref</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aly</forename><surname>Fahmy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Informatics and Systems (INFOS)</title>
		<meeting>Informatics and Systems (INFOS)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of types of text noise and techniques to handle noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Venkata Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shourya</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Tanveer A Faruquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Negi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Workshop on Analytics for Noisy Unstructured Text Data</title>
		<meeting>The Third Workshop on Analytics for Noisy Unstructured Text Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised automatic spelling correction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Sebastian Van Delden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Bracewell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE International Conference on</title>
		<meeting>the 2004 IEEE International Conference on</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="530" to="535" />
		</imprint>
	</monogr>
	<note>Information Reuse and Integration</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large scale Arabic error annotation: Guidelines and framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wajdi</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrang</forename><surname>Mohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ossama</forename><surname>Obeid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadi</forename><surname>Tomeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noura</forename><surname>Farra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Alkuhlani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Oflazer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th edition of the Language Resources and Evaluation Conference</title>
		<meeting>the 9th edition of the Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
