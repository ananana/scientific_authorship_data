<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exponential Reservoir Sampling for Streaming Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">Mathematics and Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Denison University</orgName>
								<orgName type="institution" key="instit3">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Lall</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">Mathematics and Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Denison University</orgName>
								<orgName type="institution" key="instit3">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">Mathematics and Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Denison University</orgName>
								<orgName type="institution" key="instit3">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durme</forename><surname>Hltcoe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">Mathematics and Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Denison University</orgName>
								<orgName type="institution" key="instit3">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exponential Reservoir Sampling for Streaming Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="687" to="692"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We show how rapidly changing textual streams such as Twitter can be modelled in fixed space. Our approach is based upon a randomised algorithm called Exponential Reservoir Sampling, unexplored by this community until now. Using language models over Twitter and Newswire as a testbed, our experimental results based on perplexity support the intuition that recently observed data generally outweighs that seen in the past, but that at times, the past can have valuable signals enabling better modelling of the present.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Work by <ref type="bibr" target="#b10">Talbot and Osborne (2007)</ref>, Van Durme and Lall (2009) and <ref type="bibr" target="#b4">Goyal et al. (2009)</ref> consid- ered the problem of building very large language models via the use of randomized data structures known as sketches. 1 While efficient, these struc- tures still scale linearly in the number of items stored, and do not handle deletions well: if pro- cessing an unbounded stream of text, with new words and phrases being regularly added to the model, then with a fixed amount of space, errors will only increase over time. This was pointed out by <ref type="bibr" target="#b8">Levenberg and Osborne (2009)</ref>, who inves- tigated an alternate approach employing perfect- hashing to allow for deletions over time. Their deletion criterion was task-specific and based on how a machine translation system queried a lan- guage model.</p><p>Here we ask what the appropriate selection criterion is for streaming data based on a non- stationary process, when concerned with an in- trinsic measure such as perplexity. Using Twitter and newswire, we pursue this via a sampling strat- egy: we construct models over sentences based on a sample of previously observed sentences, then measure perplexity of incoming sentences, all on a day by day, rolling basis. Three sampling ap- proaches are considered: A fixed-width sliding window of most recent content, uniformly at ran- dom over the stream and a biased sample that prefers recent history over the past.</p><p>We show experimentally that a moving window is better than uniform sampling, and further that exponential (biased) sampling is best of all. For streaming data, recently encountered data is valu- able, but there is also signal in the previous stream.</p><p>Our sampling methods are based on reser- voir sampling <ref type="bibr" target="#b15">(Vitter, 1985)</ref>, a popularly known method in some areas of computer science, but which has seen little use within computational lin- guistics. <ref type="bibr">2</ref> Standard reservoir sampling is a method for maintaining a uniform sample over a dynamic stream of elements, using constant space. Novel to this community, we consider a variant owing to <ref type="bibr" target="#b0">Aggarwal (2006)</ref> which provides for an exponen- tial bias towards recently observed elements. This exponential reservoir sampling has all of the guar- antees of standard reservoir sampling, but as we show, is a better fit for streaming textual data. Our approach is fully general and can be applied to any streaming task where we need to model the present and can only use fixed space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We address two problems: language changes over time, and the observation that space is a problem, even for compact sketches.</p><p>Statistical language models often assume either a local Markov property (when working with ut- terances, or sentences), or that content is gener- ated fully i.i.d. (such as in document-level topic models). However, language shows observable priming effects, sometimes called triggers, where the occurrence of a given term decreases the sur- prisal of some other term later in the same dis- course ( <ref type="bibr" target="#b7">Lau et al., 1993;</ref><ref type="bibr" target="#b2">Church and Gale, 1995;</ref><ref type="bibr" target="#b1">Beeferman et al., 1997;</ref><ref type="bibr" target="#b3">Church, 2000</ref>). Conven- tional cache and trigger models typically do not deal with new terms and can be seen as adjusting the parameters of a fixed model. Accounting for previously unseen entries in a language model can be naively simple: as they ap- pear in new training data, add them to the model! However in practice we are constrained by avail- able space: how many unique phrases can we store, given the target application environment?</p><p>Our work is concerned with modeling language that might change over time, in accordance with current trending discourse topics, but under a strict space constraint. With a fixed amount of memory available, we cannot allow our list of unique words or phrases to grow over time, even while new top- ics give rise to novel names of people, places, and terms of interest. Thus we need an approach that keeps the size of the model constant, but that is geared to what is being discussed now, as com- pared to some time in the past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reservoir Sampling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Uniform Reservoir Sampling</head><p>The reservoir sampling algorithm <ref type="bibr" target="#b15">(Vitter, 1985)</ref> is the classic method of sampling without replace- ment from a stream in a single pass when the length of the stream is of indeterminate or un- bounded length. Say that the size of the desired sample is k. The algorithm proceeds by retain- ing the first k items of the stream and then sam- pling each subsequent element with probability f (k, n) = k/n, where n is the length of the stream so far. (See Algorithm 1.) It is easy to show via in- duction that, at any time, all the items in the stream so far have equal probability of appearing in the reservoir.</p><p>The algorithm processes the stream in a single pass-that is, once it has processed an item in the stream, it does not revisit that item unless it is stored in the reservoir. Given this restriction, the incredible feature of this algorithm is that it is able to guarantee that the samples in the reservoir are a uniformly random sample with no unintended bi- ases even as the stream evolves. This makes it an excellent candidate for situations when the stream is continuously being updated and it is computa- tionally infeasible to store the entire stream or to make more than a single pass over it. Moreover, it is an extremely efficient algorithm as it requires O(1) time (independent of the reservoir size and stream length) for each item in the stream. with probability f (n, k), eject an ele- ment of the reservoir chosen uniformly at random and insert current item into the reservoir <ref type="bibr">8:</ref> n := n + 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Reservoir Sampling Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Non-uniform Reservoir Sampling</head><p>Here we will consider generalizations of the reser- voir sampling algorithm in which the sample items in the reservoir are more biased towards the present. Put another way, we will continuously decay the probability that an older item will ap- pear in the reservoir. Models produced using such biases put more modelling stress on the present than models produced using data that is selected uniformly from the stream. The goal here is to continuously update the reservoir sample in such a way that the decay of older items is done consis- tently while still maintaining the benefits of reser- voir sampling, including the single pass and mem- ory/time constraints.</p><p>The time-decay scheme we will study in this paper is exponential bias towards newer items in the stream. More precisely, we wish for items that </p><formula xml:id="formula_0">g(a) = c · exp (−a/β),</formula><p>where a is the age of the item, β is a scale param- eter indicating how rapidly older items should be deemphasized, and c is a normalization constant. To give a sense of what these time-decay proba- bilities look like, some exponential distributions are plotted (along with the uniform distribution) in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>Aggarwal (2006) studied this problem and showed that by altering the sampling probability (f (n, k) in Algorithm 1) in the reservoir sampling algorithm, it is possible to achieve different age- related biases in the sample. In particular, he showed that by setting the sampling probability to the constant function f (n, k) = k/β, it is possible to approximately achieve exponential bias in the sample with scale parameter β ( <ref type="bibr" target="#b0">Aggarwal, 2006</ref>). Aggarwal's analysis relies on the parameter β be- ing very large. In the next section we will make the analysis more precise by omitting any such as- sumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>In this section we will derive an expression for the bias introduced by an arbitrary sampling function f in Algorithm 1. We will then use this expression to derive the precise sampling function needed to achieve exponential decay. <ref type="bibr">3</ref> Careful selection of f allows us to achieve anything from zero decay (i.e., uniform sampling of the entire stream) to exponential decay. Once again, note that since we are only changing the sampling function, the one-pass, memory-and time-efficient properties of reservoir sampling are still being preserved.</p><p>In the following analysis, we fix n to be the size of the stream at some fixed time and k to be the size of the reservoir. We assume that the ith el- ement of the stream is sampled with probability f (i, k), for i ≤ n. We can then derive the proba- bility that an element of age a will still be in the reservoir as</p><formula xml:id="formula_1">g(a) = f (n − a, k) n t=n−a+1 1 − f (t, k) k</formula><p>, since it would have been sampled with probability f (n − a, k) and had independent chances of being replaced at times t = n − a + 1, . . . , n with proba- bility f (t, k)/k. For instance, when f (x, k) = k x , the above formula simplifies down to g(a) = k n (i.e., the uniform sampling case).</p><p>For the exponential case, we fix the sampling rate to some constant f (n, k) = p k , and we wish to determine what value to use for p k to achieve a given exponential decay rate g(a) = ce −a/β , where c is the normalization constant (to make g a probability distribution) and β is the scale param- eter of the exponential distribution. Substituting f (n, k) = p k in the above formula and equating with the decay rate, we get that p k (1 − p k /k) a ≡ ce −a/β , which must hold true for all possible val- ues of a. After some algebra, we get that when f (x, k) = p k = k(1 − e −1/β ), the probability that an item with age a is included in the reser- voir is given by the exponential decay rate g(a) = p k e −a/β . Note that, for very large values of β, this probability is approximately equal to p k ≈ k/β (by using the approximation e −x ≈ 1 − x, when |x| is close to zero), as given by Aggarwal, but our formula gives the precise sampling probability and works even for smaller values of β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments use two streams of data to illus- trate exponential sampling: Twitter and a more conventional newswire stream. The Twitter data is interesting as it is very multilingual, bursty (for ex- ample, it talks about memes, breaking news, gos- sip etc) and written by literally millions of differ- ent people. The newswire stream is a lot more well behaved and serves as a control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data, Models and Evaluation</head><p>We used one month of chronologically ordered Twitter data and divided it into 31 equal sized</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stream Interval</head><p>Total (toks) Test (toks) Twitter Dec 2013 3282M 105M Giga 1994 -2010 635.5M 12M <ref type="table">Table 1</ref>: Stream statistics blocks (roughly corresponding with days). We also used the AFP portion of the Giga Word corpus as another source of data that evolves at a slower pace. This data was divided into 50 equal sized blocks. <ref type="table">Table 1</ref> gives statistics about the data. As can be seen, the Twitter data is vastly larger than newswire and arrives at a much faster rate. We considered the following models. Each one (apart from the exact model) was trained using the same amount of data:</p><p>• Static. This model was trained using data from the start of the duration and never var- ied. It is a baseline.</p><p>• Exact. This model was trained using all available data from the start of the stream and acts as an upper bound on performance.</p><p>• Moving Window. This model used all data in a fixed-sized window immediately before the given test point.</p><p>• Uniform. Here, we use uniform reservoir sampling to select the data.</p><p>• Exponential. Lastly, we use exponen- tial reservoir sampling to select the data. This model is parameterised, indicating how strongly biased towards the present the sam- ple will be. The β parameter is a multiplier over the reservoir length. For example, a β value of 1.1 with a sample size of 10 means the value is 11. In general, β always needs to be bigger than the reservoir size.</p><p>We sample over whole sentences (or Tweets) and not ngrams. <ref type="bibr">4</ref> Using ngrams instead would give us a finer-grained control over results, but would come at the expense of greatly complicat- ing the analysis. This is because we would need to reason about not just a set of items but a multiset of items. Note that because the samples are large <ref type="bibr">5</ref>   We test the model on unseen data from all of the next day (or block). Afterwards, we advance to the next day (block) and repeat, potentially incorpo- rating the previously seen test data into the current training data. Evaluation is in terms of perplexity (which is standard for language modelling).</p><p>We used KenLM for building models and eval- uating them <ref type="bibr" target="#b6">(Heafield, 2011)</ref>. Each model was an unpruned trigram, with Kneser-Ney smoothing. Increasing the language model order would not change the results. Here the focus is upon which data is used in a model (that is, which data is added and which data is removed) and not upon making it compact or making retraining efficient. <ref type="table" target="#tab_1">Table 2</ref> shows the effect of varying the β param- eter (using Twitter). The higher the β value, the more uniform the sampling. As can be seen, per- formance improves when sampling becomes more biased. Not shown here, but for Twitter, even smaller β values produce better results and for newswire, results degrade. These differences are small and do not affect any conclusions made here. In practise, this value would be set using a devel- opment set and to simplify the rest of the paper, all other experiments use the same β value (1.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Varying the β Parameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Varying the Amount of Data</head><p>Does the amount of data used in a model affect re- sults? <ref type="table">Table 3</ref> shows the results for Twitter when varying the amount of data in the sample and us- ing exponential sampling (β = 1.1). In paren- theses for each result, we show the corresponding moving window results. As expected, using more data improves results. We see that for each sample size, exponential sampling outperforms our mov- ing window. In the limit, all sampling methods would produce the same results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Day</head><p>Sample Size <ref type="table" target="#tab_1">(Days)  1  2  3  5</ref> 652.5 (661.2) 629.1 (635.8) 624.8 (625.9) 6</p><p>635.4 (651.6) 611.6 (620.8) 604.0 (608.7) 7</p><p>636.0 (647.3) 611.0 (625.2) 603.7 (612.5) 8</p><p>654.8 (672.7) 625.6 (641.6) 614.6 (626.9) 9</p><p>653  <ref type="table">Table 3</ref>: Perplexities for different sample sizes over Twitter. Lower is better.  <ref type="table" target="#tab_3">Table 4</ref>: Perplexities for differently selected sam- ples over Twitter (sample size = five days, β = 1.1). Results in bold are the best sampling results. Lower is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Alternative Sampling Strategies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">GigaWord</head><p>Twitter is a fast moving, rapidly changing multi- lingual stream and it is not surprising that our ex- ponential reservoir sampling proves beneficial. Is it still useful for a more conventional stream that is drawn from a much smaller population of re- porters? We repeated our experiments, using the same rolling training and testing evaluation as be- fore, but this time using newswire for data. <ref type="table">Table 5</ref> shows the perplexities when using the Gigaword stream. We see the same general trends, albeit with less of a difference between exponen- tial sampling and our moving window. Perplexity values are all lower than for Twitter.  <ref type="table">Table 5</ref>: Perplexities for differently selected sam- ples over Gigaword (sample size = 10 blocks, β = 1.1). Lower is better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Why does this work for Twitter?</head><p>Although the perplexity results demonstrate that exponential sampling is on average beneficial, it is useful to analyse the results in more detail. For a large stream size (25 days), we built models us- ing uniform, exponential (β = 1.1) and our moving window sampling methods. Each approach used the same amount of data. For the same test set (four million Tweets), we computed per-Tweet log likelihoods and looked at the difference between the model that best explained each tweet and the second best model (ie the margin). This gives us an indication of how much a given model better explains a given Tweet. Analysing the results, we found that most gains came from short grams and very few came from entire Tweets being reposted (or retweeted). This suggests that the Twitter re- sults follow previously reported observations on how language can be bursty and not from Twitter- specific properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced exponential reservoir sam- pling as an elegant way to model a stream of un- bounded size, yet using fixed space. It naturally al- lows one to take account of recency effects present in many natural streams. We expect that our lan- guage model could improve other Social Media tasks, for example lexical normalisation (Han and Baldwin, 2011) or even event detection <ref type="bibr" target="#b9">(Lin et al., 2011</ref>). The approach is fully general and not just limited to language modelling. Future work should look at other distributions for sampling and consider tasks such as machine translation over Social Media.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Parameters: k: maximum size of reservoir 1: Initialize an empty reservoir (any container data type). 2: n := 1 3: for each item in the stream do 4: if n &lt; k then 5: insert current item into the reservoir 6: else 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Different biases for sampling a stream</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>, variations across samples will be small.</figDesc><table>Day Uniform 
β value 
∞ 
1.1 
1.3 
1.5 
2.0 
5 
619.4 
619.4 619.4 619.4 619.4 
6 
601.0 
601.0 603.8 606.6 611.1 
7 
603.0 
599.4 602.7 605.6 612.1 
8 
614.6 
607.7 611.9 614.3 621.6 
9 
623.3 
611.5 615.0 620.0 628.1 
10 
656.2 
643.1 647.2 650.1 658.0 
12 
646.6 
628.9 633.0 636.5 644.6 
15 
647.7 
628.7 630.4 634.5 641.6 
20 
636.7 
605.3 608.4 610.8 618.4 
25 
631.5 
601.9 603.3 604.4 610.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Perplexities for different β values over 
Twitter (sample size = five days). Lower is better. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 compares</head><label>4</label><figDesc></figDesc><table>the two baselines against the two 
forms of reservoir sampling. For Twitter, we see 
a clear recency effect. The static baseline gets 
worse and worse as it recedes from the current 
test point. Uniform sampling does better, but it 
in turn is beaten by the Moving Window Model. 
However, this in turn is beaten by our exponential 
reservoir sampling. 

Day Static Moving Uniform Exp 
Exact 
5 
619.4 619.4 
619.4 
619.4 619.4 
6 
664.8 599.7 
601.8 
601.0 597.6 
7 
684.4 602.8 
603.0 
599.3 595.6 
8 
710.1 612.0 
614.6 
607.7 603.5 
9 
727.0 617.9 
623.3 
613.0 608.7 
10 
775.6 651.2 
656.2 
642.0 640.5 
12 
776.7 639.0 
646.6 
628.7 627.5 
15 
777.1 638.3 
647.7 
626.7 627.3 
20 
800.9 619.1 
636.7 
604.9 607.3 
25 
801.4 621.7 
631.5 
601.5 597.6 

</table></figure>

			<note place="foot" n="2"> Exceptions include work by Van Durme and Lall (2011) and Van Durme (2012), aimed at different problems than that explored here. 687</note>

			<note place="foot" n="3"> Specifying an arbitrary decay function remains an open problem.</note>

			<note place="foot" n="4"> A consequence is that we do not guarantee that each sample uses exactly the same number of grams. This can be tackled by randomly removing sampled sentences. 5 Each day consists of approximately four million Tweets and we evaluate on a whole day.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On biased reservoir sampling in the presence of stream evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international conference on Very large data bases</title>
		<meeting>the 32nd international conference on Very large data bases</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="607" to="618" />
		</imprint>
	</monogr>
<note type="report_type">VLDB Endowment</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A model of lexical attractions and repulsion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beeferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="373" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poisson mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="163" to="190" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Empirical estimates of adaptation: the chance of two noriegas is closer to p/2 than p 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kenneth W Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th conference on Computational linguistics</title>
		<meeting>the 18th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="180" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Streaming for large scale NLP: Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexical normalisation of short text messages: Makn sens a #twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="368" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">KenLM: faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the EMNLP 2011 Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, Scotland; United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Trigger-based language models: A maximum entropy approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saiim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
<note type="report_type">ICASSP-93.</note>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Streambased randomised language models for smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="756" to="764" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smoothing techniques for adaptive online language models: topic tracking in tweet streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="422" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Randomised language modelling for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Probabilistic Counting with Randomized Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Lall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient online locality sensitive hashing via reservoir counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Lall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="18" to="23" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Streaming analysis of discourse participants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012</title>
		<meeting>the 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Natural Language Processing and Computational Natural Language Learning</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="48" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random sampling with a reservoir</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="37" to="57" />
			<date type="published" when="1985-03" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
