<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continuous Learning in a Hierarchical Multiscale Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hugging Face Inc. 81 Prospect St</orgName>
								<address>
									<postCode>11201</postCode>
									<settlement>Brooklyn</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hugging Face Inc. 81 Prospect St</orgName>
								<address>
									<postCode>11201</postCode>
									<settlement>Brooklyn</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hugging Face Inc. 81 Prospect St</orgName>
								<address>
									<postCode>11201</postCode>
									<settlement>Brooklyn</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Continuous Learning in a Hierarchical Multiscale Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1" to="7"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We reformulate the problem of encoding a multi-scale representation of a sequence in a language model by casting it in a continuous learning framework. We propose a hierarchical multi-scale language model in which short timescale dependencies are encoded in the hidden state of a lower-level recurrent neural network while longer timescale dependencies are encoded in the dynamic of the lower-level network by having a meta-learner update the weights of the lower-level neural network in an online meta-learning fashion. We use elastic weights consolidation as a higher-level to prevent catastrophic forgetting in our continuous learning framework.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language models are a major class of natural lan- guage processing (NLP) models whose develop- ment has lead to major progress in many areas like translation, speech recognition or summarization <ref type="bibr" target="#b30">(Schwenk, 2012;</ref><ref type="bibr" target="#b2">Arisoy et al., 2012;</ref><ref type="bibr">Rush et al., 2015;</ref><ref type="bibr" target="#b24">Nallapati et al., 2016</ref>). Recently, the task of language modeling has been shown to be an ad- equate proxy for learning unsupervised represen- tations of high-quality in tasks like text classifica- tion <ref type="bibr" target="#b15">(Howard and Ruder, 2018)</ref>, sentiment detec- tion ( <ref type="bibr" target="#b26">Radford et al., 2017)</ref> or word vector learning ( <ref type="bibr" target="#b25">Peters et al., 2018)</ref>.</p><p>More generally, language modeling is an exam- ple of online/sequential prediction task, in which a model tries to predict the next observation given a sequence of past observations. The development of better models for sequential prediction is be- lieved to be beneficial for a wide range of applica- tions like model-based planning or reinforcement learning as these models have to encode some form of memory or causal model of the world to accurately predict a future event given past events.</p><p>One of the main issues limiting the performance of language models (LMs) is the problem of cap- turing long-term dependencies within a sequence.</p><p>Neural network based language models <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997;</ref>) learn to implicitly store dependencies in a vector of hidden activities ( <ref type="bibr" target="#b22">Mikolov et al., 2010)</ref>. They can be extended by attention mech- anisms, memories or caches ( <ref type="bibr" target="#b32">Tran et al., 2016;</ref><ref type="bibr" target="#b11">Graves et al., 2014</ref>) to capture long-range connections more explicitly. Unfortunately, the very local context is often so highly informative that LMs typically end up using their memories mostly to store short term context ( <ref type="bibr" target="#b8">Daniluk et al., 2016)</ref>.</p><p>In this work, we study the possibility of com- bining short-term representations, stored in neural activations (hidden state), with medium-term rep- resentations encoded in a set of dynamical weights of the language model. Our work extends a series of recent experiments on networks with dynami- cally evolving weights ( <ref type="bibr" target="#b3">Ba et al., 2016;</ref><ref type="bibr" target="#b13">Ha et al., 2016;</ref><ref type="bibr" target="#b18">Krause et al., 2017;</ref><ref type="bibr" target="#b23">Moniz and Krueger, 2018</ref>) which show improvements in sequential prediction tasks. We build upon these works by formulating the task as a hierarchical online meta- learning task as detailed below.</p><p>The motivation behind this work stems from two observations.</p><p>On the one hand, there is evidence from a phys- iological point of view that time-coherent pro- cesses like working memory can involve differing mechanisms at differing time-scales. Biological neural activations typically have a 10 ms coher- ence timescale, while short-term synaptic plastic- ity can temporarily modulate the dynamic of the neural network it-self on timescales of 100 ms to minutes. Longer time-scales (a few minutes to several hours) see long-term learning kicks in with permanent modifications to neural excitabil- ity ( <ref type="bibr" target="#b33">Tsodyks et al., 1998;</ref><ref type="bibr" target="#b0">Abbott and Regehr, 2004;</ref><ref type="bibr" target="#b5">Barak and Tsodyks, 2007;</ref><ref type="bibr" target="#b3">Ba et al., 2016)</ref>. Interestingly, these psychological observations are paralleled, on the computational side, by a se- ries of recent works on recurrent networks with dynamically evolving weights that show benefits from dynamically updating the weights of a net- work during a sequential task ( <ref type="bibr" target="#b3">Ba et al., 2016;</ref><ref type="bibr" target="#b13">Ha et al., 2016;</ref><ref type="bibr" target="#b18">Krause et al., 2017;</ref><ref type="bibr" target="#b23">Moniz and Krueger, 2018)</ref>.</p><p>In parallel to that, it has also been shown that temporal data with multiple time-scales dependen- cies can naturally be encoded in a hierarchical rep- resentation where higher-level features are chang- ing slowly to store long time-scale dependencies and lower-level features are changing faster to encode short time-scale dependencies and local timing <ref type="bibr" target="#b29">(Schmidhuber, 1992;</ref><ref type="bibr" target="#b9">El Hihi and Bengio, 1995;</ref><ref type="bibr" target="#b17">Koutnk et al., 2014;</ref><ref type="bibr" target="#b7">Chung et al., 2016)</ref>.</p><p>As a consequence, we would like our model to encode information in a multi-scale hierarchical representation where</p><p>1. short time-scale dependencies can be en- coded in fast-updated neural activations (hid- den state),</p><p>2. medium time-scale dependencies can be en- coded in the dynamic of the network by using dynamic weights updated more slowly, and 3. a long time-scale memory can be encoded in a static set of parameters of the model.</p><p>In the present work, we take as dynamic weights the full set of weights of a RNN language model (usually word embeddings plus recurrent, input and output weights of each recurrent layer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dynamical Language Modeling</head><p>Given a sequence of T discrete symbols S = (w 1 , w 2 , . . . , w T ), the language modeling task consists in assigning a probability to the sequence P (S) = p(w 1 , . . . , w T ) which can be written, us- ing the chain-rule, as</p><formula xml:id="formula_0">P (S | Î¸) = T t=1 P (w t | w tâ1 , . . . , w 0 , Î¸)P (w 0 | Î¸).</formula><p>(1) where Î¸ is a set of parameters of the language model.</p><p>In the case of a neural-network-based lan- guage model, the conditional probability P (w t | w tâ1 , . . . , w 0 , Î¸) is typically parametrized using an autoregressive neural network as</p><formula xml:id="formula_1">P (w t | w tâ1 , . . . , w 0 , Î¸) = f Î¸ (w tâ1 , . . . , w 0 )</formula><p>(2) where Î¸ are the parameters of the neural network.</p><p>In a dynamical language modeling framework, the parameters Î¸ of the language model are not tied over the sequence S but are allowed to evolve. Thus, prior to computing the probability of a fu- ture token w t , a set of parameters Î¸ t is estimated from the past parameters and tokens as Î¸ t = argmax Î¸ P (Î¸ | w tâ1 , . . . , w 0 , Î¸ tâ1 . . . Î¸ 0 ) and the updated parameters Î¸ t are used to compute the probability of the next token w t .</p><p>In our hierarchical neural network language model, the updated parameters Î¸ t are estimated by a higher level neural network g parametrized by a set of (static) parameters Ï:</p><formula xml:id="formula_2">Î¸ t = g Ï (w tâ1 , . . . , w 0 , Î¸ tâ1 . . . Î¸ 0 )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Online meta-learning formulation</head><p>The function computed by the higher level net- work g, estimating Î¸ t from an history of parame- ters Î¸ &lt;t and data points w &lt;t , can be seen as an on- line meta-learning task in which a high-level meta- learner network is trained to update the weights of a low-level network from the loss of the low-level network on a previous batch of data. Such a meta-learner can be trained ( <ref type="bibr" target="#b1">Andrychowicz et al., 2016)</ref> to reduce the loss of the low-level network with the idea that it will generalize a gradient descent rule</p><formula xml:id="formula_3">f 0 f 1 f 2 f 3 f 4 h 0 h 1 h 2 h 3 h 4 g g w 0 w 1 w 2 w 3 w 4 w 5 w 1 w 2 w 3 w 4</formula><p>Figure 1: A diagram of the Dynamical Language Model. The lower-level neural network f (short-term memory) is a conventional word-level language model where w 0 , . . . , w 5 are words tokens. The medium- level language model g is a feed-forward or recurrent neural network while the higher-level memory is formed by a static set of consolidated pre-trained weights (see text). The meta-learner g is trained to update the lower-level network f by computing f t , i t ,</p><formula xml:id="formula_4">z t = g Ï (Î¸ tâ1 , L LM t , Î¸ tâ1 L LM t</formula><p>, Î¸ 0 ) and updating the set of weights as</p><formula xml:id="formula_5">Î¸ t = f t Î¸ tâ1 + i t Î¸ tâ1 L LM t + z t Î¸ 0 (6)</formula><p>This hierarchical network could be seen as an ana- log of the hierarchical recurrent neural networks ( <ref type="bibr" target="#b7">Chung et al., 2016)</ref> where the gates f t , i t and z t can be seen as controlling a set of COPY, FLUSH and UPDATE operations: One difference with the work of ( <ref type="bibr" target="#b7">Chung et al., 2016</ref>) is that the memory was confined to the hid- den in the later while the memory of our hierar- chical network is split between the weights of the lower-level network and its hidden-state. The meta-learner can be a feed-forward or a RNN network. In our experiments, simple lin- ear feed-forward networks lead to the lower per- plexities, probably because it was easier to regu- larize and optimize. The meta-learner implements coordinate-sharing as described in <ref type="table">(Andrychow- icz</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Continual learning</head><p>The interaction between the meta-learner and the language model implements a form of continual- learning and the language model thus faces a phenomenon known as catastrophic forgetting <ref type="bibr" target="#b10">(French, 1999</ref>). In our case, this correspond to the lower-level network over-specializing to the lexi- cal field of a particular topic after several updates of the meta-learner (e.g. while processing a long article on a specific topic).</p><p>To mitigate this effect we use a higher-level static memory initialized using "elastic weight consolidation" (EWC) introduced by Kirkpatrick  A squared (resp. underlined) word means the first model has a lower (resp. higher) loss on that word than the second model. We emphasize only words associated with a significant difference in loss by setting a threshold at 10 percent of the maximum absolute loss of each sample.</p><p>et al. <ref type="formula" target="#formula_6">(2017)</ref> to reduce forgetting in multi-task re- inforcement learning. Casting our task in the EWC framework, we define a task A which is the language modeling task (prediction of next token) when no context is stored in the weights of the lower-level network. The solution of task A is a set of weights toward which the model could advantageously come back when the context stored in the weights become irrelevant (for example when switching between paragraphs on different topics). To obtain a set of weights for task A, we train the lower-level net- work (RNN) alone on the training dataset and ob- tain a set of weights that would perform well on average, i.e. when no specific context has been provided by a context-dependent weight update performed by the meta-learner.</p><p>We then define a task B which is a language modeling task when a context has been stored in the weights of the lower-level network by an up- date of the meta-learner. The aim of EWC is to learn task B while retaining some performance on task A.</p><p>Empirical results suggest that many weights configurations result in similar performances <ref type="bibr" target="#b31">(Sussmann, 1992)</ref> and there is thus likely a solu- tion for task B close to a solution for task A. The idea behind EWC is to learn task B while protect- ing the performance in task A by constraining the parameters to stay around the solution found for task A.</p><p>This constraint is implemented as a quadratic penalty, similarly to spring anchoring the param- eters, hence the name elastic. The stiffness of the springs should be greater for parameters that most affect performance in task A. We can for- mally write this constrain by using Bayes rule to express the conditional log probability of the pa- rameters when the training dataset D is split be- tween the training dataset for task A (D A ) and the training dataset for task B (D B ):</p><formula xml:id="formula_6">log p(Î¸ | D) = log p(D B | Î¸)+log p(Î¸ | D A )âlog p(D B )<label>(7)</label></formula><p>The true posterior probability on task A p(Î¸ | D A ) is intractable, so we approximate the posterior as a Gaussian distribution with mean given by the pa- rameters and a diagonal precision given by the di- agonal of the Fisher information matrix F which is equivalent to the second derivative of the loss near a minimum and can be computed from first-order derivatives alone.</p><p>Several works have been devoted to dynamically updating the weights of neural networks during in- ference. A few recent architectures are the Fast- Weights of <ref type="bibr" target="#b3">Ba et al. (2016)</ref>, the Hypernetworks of <ref type="bibr" target="#b13">Ha et al. (2016)</ref> and the Nested LSTM of <ref type="bibr" target="#b23">Moniz and Krueger (2018)</ref>. The weights update rules of theses models use as inputs one or several of (i) a previous hidden state of a RNN network or higher level network and/or (ii) the current or previous inputs to the network. However, these models do not use the predictions of the network on the pre- vious tokens (i.e. the loss and gradient of the loss of the model) as in the present work. The archi- tecture that is most related to the present work is the study on dynamical evaluation of <ref type="bibr" target="#b18">Krause et al. (2017)</ref> in which a loss function similar to the loss function of the present work is obtained empiri- cally and optimized using a large hyper-parameter search on the parameters of the SGD-like rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture and hyper-parameters</head><p>As mentioned in 2.2, a set of pre-trained weights of the RNN language model is first obtained by training the lower-level network f and computing the diagonal of the Fisher matrix around the final weights.</p><p>Then, the meta-learner g is trained in an online meta-learning fashion on the validation dataset (al- ternatively, a sub-set of the training dataset could be used). A training sequence S is split in a se- quence of mini-batches B i , each batch B i contain- ing M inputs tokens (w iÃM , . . . , w iÃM +M ) and M associated targets (w iÃM +1 , . . . , w iÃM +M +1 ).</p><p>In our experiments we varied M between 5 and 20.</p><p>The meta-learner is trained as described in ( <ref type="bibr" target="#b1">Andrychowicz et al., 2016;</ref><ref type="bibr" target="#b19">Li and Malik, 2016)</ref> by minimizing the sum over the sequence of LM losses:</p><formula xml:id="formula_7">L meta = i&gt;0 L LM i</formula><p>. The meta-learner is trained by truncated back-propagation through time and is unrolled over at least 40 steps as the re- ward from the medium-term memory is relatively sparse ( <ref type="bibr" target="#b19">Li and Malik, 2016)</ref>.</p><p>To be able to unroll the model over a suffi- cient number of steps while using a state-of-the- art language model with over than 30 millions pa- rameters, we use a memory-efficient version of back propagation through time based on gradi- ent checkpointing as described by <ref type="bibr">Grusly et al. (2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments</head><p>We performed a series of experiments on the Wikitext-2 dataset ( <ref type="bibr" target="#b21">Merity et al., 2016</ref>) using an AWD-LSTM language model ( <ref type="bibr" target="#b20">Merity et al., 2017)</ref> and a feed-forward and RNN meta-learner.</p><p>The test perplexity are similar to perplexi- ties obtained using dynamical evaluation ( <ref type="bibr" target="#b18">Krause et al., 2017)</ref>, reaching 46.9 with a linear feed- forward meta-learner when starting from a one- level language model with test perplexity of 64.8.</p><p>In our experiments, the perplexity could not be improved by using a RNN meta-learner or a deeper meta-learner. We hypothesis that this may be caused by several reasons. First, storing a hid- den state in the meta-learner might be less im- portant in an online meta-learning setup than it is in a standard meta-learning setup <ref type="bibr" target="#b1">(Andrychowicz et al., 2016)</ref> as the target distribution of the weights is non-stationary. Second, the size of the hidden state cannot be increased significantly without reducing the number of steps along which the meta-learner is unrolled during meta-training which may be detrimental.</p><p>Some quantitative experiments are shown on <ref type="figure" target="#fig_3">Figure 2</ref> using a linear feed-forward network to illustrate the effect of the various layers in the hi- erarchical model. The curves shows differences in batch perplexity between model variants.</p><p>The top curve compares a one-level model (lan- guage model) with a two-levels model (language model + meta-learner). The meta-learner is able to learn medium-term representations to progres- sively reduce perplexity along articles (see e.g. ar- ticles C and E). Right sample 1 (resp. 2) details sentences at the begging (resp. middle) of arti- cle E related to a warship called "Ironclad". The addition of the meta-learner reduces the loss on a number of expression related to the warship like "ironclad" or "steel armor".</p><p>Bottom curve compares a three-levels model (language model + meta-learner + long-term memory) with the two-levels model. The local loss is reduced at topics changes and beginning of new topics (see e.g. articles B, D and F). The right sample 3 can be contrasted with sample 1 to illustrate how the hierarchical model is able to better recover a good parameter space following a change in topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>Lower-level / short time-scale: a RNN-based language model f encoding representations in the activations of a hidden state, 2. Middle-level / medium time-scale: a meta- learner g updating the set of weights of the language model to store medium-term repre- sentations, and 3. Higher-level / long time-scale: a static long- term memory of the dynamic of the RNN- based language model (see below).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>COPY (f t ): part of the state copied from the previous state Î¸ tâ1 , 2. UPDATE (i t ): part of the state updated by the loss gradients on the previous batch, and 3. FLUSH (z t ): part of the state reset from a static long term memory Î¸ 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>et al., 2016; Ravi and Larochelle, 2016) and takes as input the loss L LM t and loss-gradients Î¸ tâ1 L LM t over a previous batch B i (a sequence of M tokens w 0 , . . . , w M as illustrated on fig- ure 1). The size M of the batch adjusts the trade-off between the noise of the loss/gradients and updating frequency of the medium-term mem- ory, smaller batches leading to faster updates with higher noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Medium and long-term memory effects on a sample of Wikitext-2 test set with a sequence of Wikipedia articles (letters A â H). (Left) Instantaneous perplexity gain: difference in batch perplexity between models. Higher values means the first model has locally a lower perplexity than the second model. (Top curve) Comparing a two-levels model (LM + meta-learner) with a one-level model (LM). (Bottom curve) Comparing a three-levels model (LM + meta-learner + long-term memory) with a twolevels model. (Right) Token loss difference on three batch samples indicated on the left curves. A squared (resp. underlined) word means the first model has a lower (resp. higher) loss on that word than the second model. We emphasize only words associated with a significant difference in loss by setting a threshold at 10 percent of the maximum absolute loss of each sample.</figDesc></figure>

			<note place="foot">Î¸ t = Î¸ tâ1 â Î± t Î¸ tâ1 L t (4) where Î± t is a learning rate at time t and Î¸ tâ1 L LM t is the gradient of the loss L LM t of the language model on the t-th dataset with respect to previous parameters Î¸ tâ1. Ravi and Larochelle (2016) made the observation that such a gradient descent rule bears similarities with the update rule for LSTM cell-states c t = f t c tâ1 + i t Ë c t (5) when c t â Î¸ t , i t â Î± t andËcandË andËc t â ââ Î¸ tâ1 L t We extend this analogy to the case of a multiscale hierarchical recurrent model illustrated on figure 1 and composed of:</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synaptic computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><forename type="middle">G</forename><surname>Regehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">431</biblScope>
			<biblScope unit="issue">7010</biblScope>
			<biblScope unit="page" from="796" to="803" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04474[cs].ArXiv:1606.04474</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebru</forename><surname>Arisoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACLHLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACLHLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Using Fast Weights to Attend to the Recent Past</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06258</idno>
		<idno>ArXiv: 1610.06258</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Persistent activity in neural networks with dynamic synapses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Tsodyks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<idno>ArXiv: 1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01704[cs].ArXiv:1609.01704</idno>
		<title level="m">Hierarchical Multiscale Recurrent Neural Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Frustratingly Short Attention Spans in Neural Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Daniluk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical Recurrent Neural Networks for Long-term Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">El</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Neural Information Processing Systems, NIPS&apos;95</title>
		<meeting>the 8th International Conference on Neural Information Processing Systems, NIPS&apos;95<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="493" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>French</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audrnas</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03401[cs].ArXiv:1606.03401</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Memory-Efficient Backpropagation Through Time.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106[cs].ArXiv:1609.09106</idno>
		<title level="m">HyperNetworks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<idno>ArXiv: 1801.06146</idno>
		<title level="m">Finetuned Language Models for Text Classification</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Clockwork RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>II-1863-II-1871</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning<address><addrLine>Beijing, China. JMLR.org</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07432[cs].ArXiv:1709.07432</idno>
		<title level="m">Dynamic Evaluation of Neural Sequence Models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to Optimize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01885</idno>
		<idno>ArXiv: 1606.01885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182[cs].ArXiv:1708.02182</idno>
		<title level="m">Regularizing and Optimizing LSTM Language Models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843[cs].ArXiv:1609.07843</idno>
		<title level="m">Pointer Sentinel Mixture Models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Nested LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Ruben</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antony</forename><surname>Moniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10308[cs].ArXiv:1801.10308</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365[cs].ArXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning to Generate Reviews and Discovering Sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01444[cs].ArXiv:1704.01444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Optimization as a Model for Few-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Complex, Extended Sequences Using the Principle of History Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Posters</title>
		<meeting>COLING 2012: Posters</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1071" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Uniqueness of the weights for minimal feedforward nets with a given inputoutput map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hctor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sussmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="593" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01272[cs].ArXiv:1601.01272</idno>
		<title level="m">Recurrent Memory Networks for Language Modeling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Tsodyks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Pawelzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Markram</surname></persName>
		</author>
		<title level="m">Neural Networks with Dynamic Synapses</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
