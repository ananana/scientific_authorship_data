<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A System Demonstration of a Framework for Computer Assisted Pronunciation Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015. c 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renlong</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">German Research Center for Artificial Intelligence</orgName>
								<orgName type="laboratory">Language Technology Lab Alt-Moabit 91c</orgName>
								<address>
									<postCode>10559</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">German Research Center for Artificial Intelligence</orgName>
								<orgName type="laboratory">Language Technology Lab Alt-Moabit 91c</orgName>
								<address>
									<postCode>10559</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A System Demonstration of a Framework for Computer Assisted Pronunciation Training</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL-IJCNLP 2015 System Demonstrations</title>
						<meeting>ACL-IJCNLP 2015 System Demonstrations <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1" to="6"/>
							<date type="published">July 26-31, 2015. c 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we demonstrate a system implementation of a framework for computer assisted pronunciation training for second language learner (L2). This framework supports an iterative improvement of the automatic pronunciation error recognition and classification by allowing integration of annotated error data. The annotated error data is acquired via an annotation tool for linguists. This paper will give a detailed description of the annotation tool and explains the error types. Furthermore, it will present the automatic error recognition method and the methods for automatic visual and audio feedback. This system demonstrates a novel approach to interactive and individualized learning for pronunciation training.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Second language (L2) acquisition is a much greater challenge for human cognition than first language (L1) acquisition during the critical pe- riod. Especially by older children and adults, L2 is usually not acquired solely through imitation in daily communication but by dedicated language education. The teaching of second and further lan- guages normally combines transmission of knowl- edge with practicing of skills. One major hurdle is the interference of L1 proficiency with L2.</p><p>Modern language learning courses are no longer exclusively based on books or face-to-face lec- tures. A clear trend is web-based, interactive, mul- timedia and personalized learning. Learners want to be flexible as to times and places for learn- ing: home, trains, vacation, etc. Both face-to- face teaching and 7/24 personal online language learning services are very expensive. There is a growing economic pressure to employ computer- assisted methods for improving language learn- ing in quality, efficiency and scalability. The current philosophy of Computer Assisted Lan- guage Learning (CALL) puts a strong emphasis on learner-centered materials that enable learners to work on their own. CALL tries to support two im- portant features in language learning: interactive learning and individualized learning.</p><p>Natural language processing (NLP) technolo- gies play a growing important role for CALL <ref type="bibr" target="#b6">(Hubbard, 2009)</ref>. They can help to identify errors in student input and provide feedback so that the learners can be aware of them <ref type="bibr" target="#b5">(Heift, 2013;</ref><ref type="bibr">Nagata, 1993</ref>). Furthermore, they can help to build models of the achieved proficiency of the learners and provide materials and tasks appropriate.</p><p>In this paper, we demonstrate an implementa- tion of a framework of computer assisted pronun- ciation training for L2. The current version, which is a further development of the Sprinter system ( , automatically recognises and classi- fies the pronunciation errors of learners and pro- vides visual and audio feedback with respect to the error types. The framework contains two sub- systems: 1) the annotation tool which provides linguists an easy environment for annotating pro- nunciation errors in learners' speech data; 2) the speech verification tool which identifies learners' prosody and pronunciation errors and helps to cor- rect them.</p><p>The remainder of the paper is as follows: Sec- tion 2 describes the system architecture; Section 3 and 4 explain how each subsystem works; Section 5 evaluates the speech verification tool; A brief conclusion and future plan is mentioned at last in Section 6. <ref type="figure">Figure 1</ref> depicts the framework and the interaction between the annotation tool and the speech veri- fication tool. As presented below, the speech ver- 1 ification tool is initialised with a language model trained with audio data from 10 learners. In case that learners' audio data can be collected and an- notated in the online system, the error database can be updated on the fly and the language model can be updated dynamically, hence speech verification will improve itself iteratively until enough pronun- ciation errors are gathered and no annotation will be needed. Comparing to existing methods on our HMM classifier has the advantage that it is trained from finely annotated L2 speech error data, hence can recognise error types that are specifi- cally defined in the annotations. Moreover, the re- sults not only show the differences between gold standard and learner data, but also contain infor- mation of corrective feedback. Comparing to a general Goodness of Pronunciation (GOP) score or simply showing differences in waveform, our feedback pinpoints different types of error down to phoneme level and show learners how to pro- nounce them correctly via various means. The same annotating-training-verifying process can be adapted to all languages as far as our open source components support them, thus, with almost no extra efforts. Since people with the same L1 back- ground tend to make the same pronunciation errors while learning a second language (Witt, 2012), we choose a specific L1-L2 pair in our experiment, namely, Germans who learn English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Annotation Tool</head><p>To provide annotators an easy and efficient in- terface, we have further developed MAT (Ai and Charfuelan, 2014), with which pronunciation er- rors can be annotated via simple mouse clicks and single phoneme inputs from keyboard. During the annotation, errors are automatically stored in an error database, which then updates the speech ver- ification tool. We also add checking mechanisms to validate the annotations and ask annotators to deal with conflicts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Target Pronunciation Errors</head><p>Four types of phoneme-level pronunciation errors can be annotated in the ways as shown in <ref type="figure" target="#fig_1">Figure  2</ref>. They are:</p><p>• Deletion: A phoneme is removed from learner's utterance, e.g. to omit /g/ in "Eng- land". Annotators only need to check the checkbox in column deletion.</p><p>• Insertion: A phoneme is inserted after an- other one, e.g. learner tries to pronounce a silent letter, like 'b' in "dumb". In this case the annotator should check insertion and type the inserted phoneme in column spoken.</p><p>• Substitution: A phoneme is replaced by an- other one, e.g. replacing /z/ with /s/ in "was". The annotator should check substitution and type the actually pronounced phoneme in spoken.</p><p>• Distortion: A phoneme is not pronounced fully correct, yet is not so wrong that it be- comes another phoneme, e.g. pronouncing /u/ with tongue slightly backward. In this case the annotator should also select a hint from the drop down list in hint column to indicate how the error phoneme can be cor- rected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotations</head><p>As depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, annotations can be con- ducted easily and directly with this tool. If an an- notator finds a pronunciation error, he/she can sim- ply check the checkbox at line: phoneme and col- umn: error type from the table, and provide extra information of the actually spoken phoneme and also hints of how to pronounce correctly. To make the annotation more convenient and efficient, sev- eral functions are built in the tool:</p><p>• Phoneme sequences are generated via MARY phonemizer. Phonemes are listed in the first column of the table and also in the header of the waveform panel. Clicking on a phoneme in the header will highlight the row of the corresponding phoneme in the table, and vice versa. The word, which the clicked phoneme belongs to, is also highlighted in the middle panel where the sentence is shown. Syllables in words are also retrieved from text analysis and displayed in the first column.</p><p>• Phoneme boundaries are recognized via forced alignment performed with HTK. An- notators can play any single phoneme, sylla- ble or word by double-clicking them in the first column, or play any part of the sentence by choosing a range in the waveform panel with mouse and hit space on keyboard. In this way annotators can focus on error phonemes without bothering to listen to the whole sen- tence.</p><p>• There is already a list of hints on articulation provided. If a correction cannot be found in the list, annotators can click hints button and add the correction to the list. After the annotation for a single audio file is done, the annotator clicks Commit to submit the annota- tions. The Commit function will check the anno- tation additionally:</p><p>• If insertion is marked, the inserted phoneme should be also written in spoken.</p><p>• If substitution is marked, the phoneme, which the original phoneme is substituted to, should also be written in spoken.</p><p>• If distortion is marked, a comment should be provided by annotators to indicate in which way this phoneme is distorted.</p><p>• Only one kind of error can be annotated per phoneme. If the annotations are valid, the pronunciation errors and their diphone or triphone informations </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Speech Verification Tool</head><p>The speech verification tool identifies the pronun- ciation errors in learners' speech and provides feedback on correction, and also analyze the dif- ferences in pitch and duration between gold stan- dard and learners' speech and display them in comprehensive ways. The goal is to help learners speak second language error-free and with rhythm and intonation like native speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pronunciation Error Detection</head><p>Pronunciation error detection is realized by per- forming phoneme recognition with HTK and com- paring the correct and recognized phoneme se- quence. Errors can be retrieved from the differ- ences between the sequences. The sentence read by learner is processed by MARY phonemizer to generate the correct phoneme sequence. Possible pronunciation errors are then fetched from the er- ror database based on the phonemes and their di- phone and triphone information in the sequences. A grammar for phoneme recognition is then com- posed from the errors and the phoneme sequence.  Errors are handled differently as they appear in the grammar:</p><p>• Deletion means a phoneme can be optional in the grammar.</p><p>• Insertion means an extra phoneme, i.e. the inserted one, can be optional in the grammar.</p><p>• Substitution means multiple phonemes can appear at this position: the correct one and the substituted ones.</p><p>• Distortion also means multiple phonemes can appear at the same position: the correct and their distorted alias, which are represented with phoneme plus number. For example, the phoneme /A/ can be distorted in two ways: either tongue is placed backward or tongue starts too forward. Since /A/ is represented as /A/ in MARY, its two distorted versions are /A1/ and /A2/. If a given sentence with its MARY phoneme sequence is: </p><formula xml:id="formula_0">(sil A l b (i |i1) I n l (V |A |O) n {d} @ n f (O |O2) r (D |z) @ h @U l j (I |I1) {(r |A)} sil)</formula><p>will be generated, because the following pronun- ciation errors have been learned:</p><p>1. /i/ in be can be distorted. 2. /2/ in London can be substituted with /A/ or /O/. 3. /d/ in London can be removed by learners while pronouncing. 4. /O:/ in for can be distorted. 5. /D/ in the can be replaced with /z/. 6. /i:/ in year can be distorted. 7. /@/ in year can be substituted with /A/, and can also be deleted by learners. If the predefined errors appear in learners' speech, they will be identified by comparing the different phonemes in the correct phoneme se- quence generated with MARY phonemizer and the recognized sequence with HTK. Moreover, cor- rective feedback can also be created by fetching the hint, which annotators provide, from the error database. E.g. if /A1/ instead of /A/ is recognized for word are, we can provide the hint for this dis- torted /A/: Tongue needs to be slightly further for- ward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prosody Differences</head><p>Prosody differences between gold standard and learners' speech are calculated and shown to learn- ers, not only visually, but also with audio feed- back. The prosody from gold standard is applied to learners' speech, so learners can hear their own voice with native prosody, in this way it might be easier for them to mimic the gold standard prosody <ref type="bibr" target="#b4">(Flege, 1995</ref>  Duration differences can be calculated directly from the results of forced alignment. To gain more accurate alignment, the language model is trained with gold standard data and a selected set of learn- ers' data. Before displaying the differences to learners as in <ref type="figure" target="#fig_7">Figure 6</ref>, the durations in learners' speech are normalized so that only words with large duration differences are shown.</p><p>Pitch differences are calculated with Snack Sound Toolkit 1 . Dynamic time warping is ap- plied to the gold standard against speech data from learners, so that differences in pitch can be shown per phoneme synchronously. Due to the fact that each person has a distinct baseline in pitch, differ- ences in pitch value are not considered as differ- ences, we show only differences in pitch variation.</p><p>We use FD-PSOLA combined with DTW ( <ref type="bibr" target="#b7">Latsch and Netto, 2011</ref>) to perform prosody transplantation and generate audio feedback that learners can perceive. Despite the high perfor- mance of the method, there are still cases that prosody transplantation yields faulty results, e.g. the synthesized speech sounds very artificial or there are significant gaps between words or even phonemes. Therefore a scale factor calculator runs before the transplantation to determine if it makes sense to transplant the prosody, and will only do it and provide it as feedback when the scale factors do not exceed the threshold. <ref type="figure" target="#fig_7">Figure 6</ref> shows a screenshot of the speech verifi- cation tool. Learners can choose sentences they want to practice from the list in the left, and click Record and speak to the microphone. Af- ter the audio is recorded, they can click Check to display the verification results. Pronunciation errors are marked in the first panel with colors other than green. Red, pink, yellow and pur- ple are used for substitution, distortion, deletion and insertion. The second panel shows the dif- ferences in pitches, significant and medium dif- ferences are rendered with red and yellow. The panel below shows the differences in durations. Hints of correction or improvement are shown as text if learners click on the colored phonemes or words. If prosody transplantation is feasible, the button Transplanted is enabled and will play audio with learners' voice and transplanted gold stan- dard prosody upon click. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">User Interface</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Experiments are conducted both objectively and subjectively, in order to evaluate the performance of error detection and also how our feedback can help learners improve their second language skills</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Objective Evaluation</head><p>To train the language model for pronunciation er- ror detection, we let 1506 sentences read by native Britons. Given these sentences, 96 sentences are carefully selected, which cover almost all typical pronunciation errors made by Germans who learn English, and have been read by 10 Germans at dif- ferent English skill levels. To evaluate our error detection system, we let 4 additional German peo- ple read these 96 sentences. The recordings were sent to both annotators and the error detection sys- tem. The results from error detection are trans- formed to MARYXML format and compared with the annotations.  The results show a very high precision in recog- nizing deletion and insertion. The recalls are also very good considering that there are new deletion phenomena in testers' speech that are not involved in old training data. Although a large amount of substitution errors appear in the test data, they have been detected accurately. This proves that training a language model considering specific L1 background is important for correct error recogni- tion. For example in our case, most typical sub- stitution errors made by Germans are well trained, like pronouncing /D/ like /z/ in the, or /z/ like /s/ in was.</p><p>Detecting distortion errors seems a more dif- ficult task for the system. Although a good re- call is achieved, the precision is not satisfying. In CALL, this is perhaps not helpful because learn- ers will be discouraged if they make correct pro- nunciation but are told wrong by the system. More speech data is required for training the model. Our annotators are experienced linguists but they may still holds different criterion on judging distortion. Having more linguists working on the annotation should also help to improve the accuracy of error detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Subjective Evaluation</head><p>To evaluate whether and how our feedback can help learners improve their second language, we designed a progressive experiment. 4 learners are chosen to read 30 sentences from the list. They can listen to gold standard as many times as they need, and record the speech when they are ready. If there are pronunciation errors or prosody differ- ences, they can view hints or listen to gold stan- dard, and then try again. If there are still prosody differences, they can then hear the transplanted speech, as many times as they need, and then try to record again. Insertions and deletions are easily corrected by learners once they have been pointed out. We examined the substitution errors that were not corrected, and found most of them are be-  tween phoneme /ae/ and /e/, and also /@Ú/ and /O/. The differences between the phoneme pairs were not easily perceived by learners, and they need to be taught systematically how to pronounce these phonemes. It was difficult for learners to correct distortions. Besides providing tutorial on how to pronounce error phonemes correctly, our system also needs to be modified so that it doesn't handle distortion so strictly.  The results on prosody differences show that most of these differences can be perceived and ad- justed by learners, if they are given enough infor- mation. Almost all the remaining differences are from long sentences. Learners couldn't pay atten- tion to all differences in one attempt. We believe that learners should be able to read all sentences in native intonation and rhythm if they try another several times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we presents a framework for com- puter assisted pronunciation training. Its anno- tation tool substantially simplifies the acquisition of speech error data, while its speech verifica- tion tool automatically detects pronunciation er- rors and prosody differences in learners' speech and provide corrective feedback. Objective and subjective evaluations show that the system not only performs accurately in error detection but also helps learners realize and correct their errors.</p><p>In the future, we attempt to integrate more feed- back content such as video tutorial or articulation animation for teaching pronunciation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Feedback</head><label></label><figDesc>Figure 1: Overall Framework Architecture pronunciation error detection, including building classifiers with Linear Discriminant Analysis or Decision Tree (Truong et al., 2004), or using Support Vector Machine (SVM) classifier based on applying transformation on Mel Frequency Cepstral coefficients (MFCC) of learners' audio data (Picard et al., 2010; Ananthakrishnan et al., 2011), our HMM classifier has the advantage that it is trained from finely annotated L2 speech error data, hence can recognise error types that are specifically defined in the annotations. Moreover, the results not only show the differences between gold standard and learner data, but also contain information of corrective feedback. Comparing to a general Goodness of Pronunciation (GOP) score or simply showing differences in waveform, our feedback pinpoints different types of error down to phoneme level and show learners how to pronounce them correctly via various means. The same annotating-training-verifying process can be adapted to all languages as far as our open source components support them, thus, with almost no extra efforts. Since people with the same L1 background tend to make the same pronunciation errors while learning a second language (Witt, 2012), we choose a specific L1-L2 pair in our experiment, namely, Germans who learn English.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Screenshot of the Annotation Tool</figDesc><graphic url="image-1.png" coords="2,307.28,62.81,226.76,143.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Components in annotation tool</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Workflow of Pronunciation Error Detection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Workflow of Prosody Comparison and Transplantation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Screenshot of the Speech Verification Tool</figDesc><graphic url="image-3.png" coords="5,72.00,292.83,226.77,136.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Audio Data from 
Learner and Gold 
Standard 

Text 

Snack 
Extracted Pitch 
Contours 

MaryTTS NLP Tools 

Speech Model 
HTK 

MaryXML 

Forced Alignment 
Results of both 
Learners and 
Gold Standard 

Scale Factor 
Calculator 

Visualized 
Comparison of 
Pitch and Duration 

FD-PSOLA 
Processor 

Prosody 
Transplantation 

Perform Prosody Transplantation 
only when it's safe to do so. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Amount of detected and corrected pronunciation errors from test speech data.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Amount of detected and corrected prosody differ-

ences from test speech data. 

</table></figure>

			<note place="foot" n="1"> http://www.speech.kth.se/snack/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgment</head><p>This research was partially supported by the German Federal Ministry of Education and Re-search (BMBF) through the projects Deependance (01IW11003) and ALL SIDES (01IW14002).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mat: a tool for l2 pronunciation errors annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renlong</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Charfuelan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Language Resources and Evaluation</title>
		<meeting>the 9th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">European Language Resources Association</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sprinter: Language technologies for interactive and multimedia language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renlong</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Charfuelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Kasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Klwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Gasber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Gienandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC-2014). European Language Resources Association</title>
		<meeting>the 9th International Conference on Language Resources and Evaluation (LREC-2014). European Language Resources Association</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using an ensemble of classifiers for mispronunciation feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopal</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preben</forename><surname>Wik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olov</forename><surname>Engwall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherif</forename><surname>Abdou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SLaTE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="49" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Second language speech learning: Theory, findings, and problems. Speech Perception and Linguistic Experience: Theoretical and Methodological Issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James E Flege</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="233" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learner control and error correction in icall: Browsers, peekers, and adamants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trude</forename><surname>Heift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Calico Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="313" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Computer Assisted Language Learning: Critical Concepts in Linguistics, volume I-IV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hubbard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Routledge</publisher>
			<pubPlace>London &amp; New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pitch-synchronous time alignment of speech signals for prosody transplantation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Latsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Netto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intelligent computer feedback for second language instruction. The Modern Language</title>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="330" to="339" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detection of specific mispronunciations using audiovisual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopal</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preben</forename><surname>Wik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olov</forename><surname>Engwall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherif</forename><surname>Abdou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="7" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic pronunciation error detection: an acoustic-phonetic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khiet</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambra</forename><surname>Neri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catia</forename><surname>Cucchiarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmer</forename><surname>Strik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STIL/ICALL Symposium</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic error detection in pronunciation training: where we are and where we need to go</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Witt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Automatic Detection of Errors in Pronunciation Training</title>
		<meeting><address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
