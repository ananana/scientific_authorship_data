<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Summarization of Multi-Document Topic Hierarchies using Submodular Mixtures</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><forename type="middle">B</forename><surname>Bairi</surname></persName>
							<email>bairi@cse.iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">IITB-Monash Research Academy IIT Bombay Mumbai</orgName>
								<address>
									<postCode>40076</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Iyer</surname></persName>
							<email>rkiyer@u.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA-98175</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
							<email>ganesh@cse.iitb.ac.in</email>
							<affiliation key="aff2">
								<orgName type="institution">IIT Bombay Mumbai</orgName>
								<address>
									<postCode>40076</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
							<email>bilmes@uw.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA-98175</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Summarization of Multi-Document Topic Hierarchies using Submodular Mixtures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="553" to="563"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the problem of summarizing DAG-structured topic hierarchies over a given set of documents. Example applications include automatically generating Wikipedia disambiguation pages for a set of articles, and generating candidate multi-labels for preparing machine learning datasets (e.g., for text classification, functional genomics, and image classification). Unlike previous work, which focuses on clustering the set of documents using the topic hierarchy as features, we directly pose the problem as a submodular optimization problem on a topic hierarchy using the documents as features. Desirable properties of the chosen topics include document coverage, specificity, topic diversity, and topic homogeneity, each of which, we show, is naturally modeled by a submodular function. Other information, provided say by unsupervised approaches such as LDA and its variants, can also be utilized by defining a submodular function that expresses coherence between the chosen topics and this information. We use a large-margin framework to learn convex mixtures over the set of submodular components. We empirically evaluate our method on the problem of automatically generating Wikipedia disambiguation pages using human generated clusterings as ground truth. We find that our framework improves upon several baselines according to a variety of standard evaluation metrics including the Jaccard Index, F1 score and NMI, and moreover, can be scaled to extremely large scale problems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Several real world machine learning applications involve hierarchy based categorization of topics for a set of objects. Objects could be, e.g., a set of documents for text classification, a set of genes in functional genomics, or a set of images in computer vision. One can often define a natural topic hierarchy to categorize these objects. For example, in text and image classification problems, each document or image is assigned a hierarchy of labels -a baseball page is assigned the labels "baseball" and "sports." Moreover, many of these applications, naturally have an existing topic hierarchy generated on the entire set of objects ( <ref type="bibr" target="#b32">Rousu et al., 2006</ref>; <ref type="bibr" target="#b0">Barutcuoglu et al., 2006</ref>; ling Zhang and hua <ref type="bibr" target="#b21">Zhou, 2007;</ref><ref type="bibr" target="#b33">Silla and Freitas, 2011;</ref><ref type="bibr">Tsoumakas et al., 2010)</ref>.</p><p>Given a DAG-structured topic hierarchy and a subset of objects, we investigate the problem of finding a subset of DAG-structured topics that are induced by that subset (of objects). This problem arises naturally in several real world applications. For example, consider the problem of identifying appropriate label sets for a collection of articles. Several existing text collection datasets such as 20 Newsgroup 1 , Reuters-21578 2 work with a prede- fined set of topics. We observe that these topic names are highly abstract <ref type="bibr">3</ref> for the articles catego- rized under them. On the other hand, techniques proposed by systems such as Wikipedia Miner <ref type="bibr" target="#b27">(Milne, 2009)</ref> and TAGME <ref type="bibr" target="#b10">(Ferragina and Scaiella, 2010)</ref> generate several labels for each article in the dataset that are highly specific to the article. Col- lating all labels from all articles to create a label  <ref type="figure">Figure 1</ref>: Topic Summarization overview. On the left, we show many documents related to Apple. In the middle, a Wikipedia category hierarchy shown as a topic DAG, links these documents at the leaf level. On the right, we show the output of our summarization process, which creates a set of summary topics (Plants, Technology, Companies, Films, Music and Places in this example) with the input documents classified under them.</p><p>set for the dataset can result in a large number of labels and become unmanageable. Our proposed techniques can summarize such large sets of labels into a smaller and more meaningful label sets using a DAG-structured topic hierarchy. This also holds for image classification problems and datasets like ImageNet ( <ref type="bibr" target="#b8">Deng et al., 2009</ref>). We use the term summarize to highlight the fact that the smaller la- bel set semantically covers the larger label set. For example, the topics Physics, Chemistry, and Math- ematics can be summarized into a topic Science.</p><p>A particularly important application of our work (and the one we use for our evaluations in Section 4) is the following: Given a collection of articles span- ning different topics, but with similar titles, auto- matically generate a disambiguation page for those titles using the Wikipedia category hierarchy 4 as a topic DAG. Disambiguation pages <ref type="bibr">5</ref> on Wikipedia are used to resolve conflicts in article titles that oc- cur when a title is naturally associated with multi- ple articles on distinct topics. Each disambiguation page organizes articles into several groups, where the articles in each group pertain only to a specific topic. Disambiguations may be seen as paths in a hierarchy leading to different articles that arguably could have the same title. For example, the title Apple 6 can refer to a plant, a company, a film, a television show, a place, a technology, an album, a record label, and a newspaper daily. The problem then, is to organize the articles into multiple groups where each group contains articles of similar nature (topics) and has an appropriately discerned group heading. <ref type="figure">Figure 1</ref> describes the topic summariza- tion process for creation of the disambiguation page for "Apple".</p><p>All the above mentioned problems can be mod- eled as the problem of finding the most representa- tive subset of topic nodes from a DAG-Structured topic hierarchy. We argue that many formulations of this problem are natural instances of submodular maximization, and provide a learning framework to create submodular mixtures to solve this prob- lem. A set function f (.) is said to be submodular if for any element v and sets A ⊆ B ⊆ V \ {v}, where V represents the ground set of elements,</p><formula xml:id="formula_0">f (A ∪ {v}) − f (A) ≥ f (B ∪ {v}) − f (B)</formula><p>. This is called the diminishing returns property and states, informally, that adding an element to a smaller set increases the function value more than adding that element to a larger set. Submodular func- tions naturally model notions of coverage and di- versity in applications, and therefore, a number of machine learning problems can be modeled as forms of submodular optimization ( <ref type="bibr" target="#b13">Kempe et al., 2003;</ref><ref type="bibr" target="#b15">Krause and Guestrin, 2005;</ref><ref type="bibr" target="#b29">Narasimhan and Bilmes, 2004;</ref><ref type="bibr" target="#b11">Iyer et al., 2013;</ref><ref type="bibr" target="#b19">Lin and Bilmes, 2012;</ref><ref type="bibr" target="#b17">Lin and Bilmes, 2010)</ref>. In this paper, we investigate structured prediction methods for learn-ing weighted mixtures of submodular functions to summarize topics for a collection of objects us- ing DAG-structured topic hierarchies. Throughout this paper we use the terms "topic" and "category" interchangeably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>To the best of our knowledge, the specific problem we consider here is new. Previous work on identi- fying topics can be broadly categorized into one of the following types: a) cluster the objects and then identify names for the clusters; or b) dynamically identify topics (including hierarchical) for a set of objects. LDA ( <ref type="bibr" target="#b3">Blei et al., 2003</ref>) clusters the docu- ments and simultaneously produces a set of topics into which the documents are clustered. In LDA, each document may be viewed as a mixture of var- ious topics and the topic distribution is assumed to have a Dirichlet prior. LDA associates a group of high probability words to each identified topic. A name can be assigned to a topic by manually inspecting the words or using additional algorithms like ( <ref type="bibr" target="#b25">Mei et al., 2007;</ref><ref type="bibr" target="#b22">Maiya et al., 2013</ref>). LDA does not make use of existing topic hierarchies and correlation between topics. The Correlated Topic Model ( <ref type="bibr" target="#b2">Blei and Lafferty, 2006</ref>) induces a correla- tion structure between topics by using the logistic normal distribution instead of the Dirichlet. An- other extension is the hierarchical LDA ( <ref type="bibr" target="#b4">Blei et al., 2004</ref>), where topics are joined together in a hierarchy by using the nested Chinese restaurant process. Nonparametric extensions of LDA include the Hierarchical Dirichlet Process ( <ref type="bibr" target="#b35">Teh et al., 2006</ref>) mixture model, which allows the number of top- ics to be unbounded and learnt from data and the Nested Chinese Restaurant Process which allows topics to be arranged in a hierarchy whose structure is learnt from data. In each of these approaches, unlike our proposed approach, an existing topic hierarchy is not used, nor is any additional object- topic information leveraged.</p><p>The pachinko allocation model (PAM)( <ref type="bibr" target="#b16">Li and McCallum, 2006</ref>) captures arbitrary, nested, and possibly sparse correlations between topics using a DAG. The leaves of the DAG represent individual words in the vocabulary, while each interior node represents a correlation among its children, which may be words or other interior nodes (topics). PAM learns the probability distributions of words in a topic, subtopics in a topic, and topics in a document. We cannot, however, generate a subset of topics from a large existing topic DAG that can act as summary topics, using PAM.</p><p>HSLDA <ref type="bibr" target="#b31">(Perotte et al., 2011</ref>) introduces a hierar- chically supervised LDA model to infer hierarchi- cal labels for a document. It assumes an existing label hierarchy in the form of a tree. The model infers one or more labels such that, if a label l is inferred as relevant to a document, then all the la- bels from l to the root of the tree are also inferred as relevant to the document. Our approach differs from HSLDA since: (1) we use the label hierarchy to infer a set of labels for a group of documents; <ref type="formula" target="#formula_4">(2)</ref> we do not enforce the label hierarchy to be a tree as it can be a DAG; and <ref type="formula">(3)</ref> generalizing HSLDA to use a DAG structured hierarchy and infer labels for a group of documents (e.g., combining into one big document) also may not help in solving our problem. HSLDA will apply all the relevant labels to the documents as per the classifier that it learns for every label. Moreover, the "root" label is al- ways applied and it is very likely that many labels near the top level of the label hierarchy are also classified as relevant to the group of documents. <ref type="bibr">Wei and James (Bi and Kwok, 2011</ref>) present a hierarchical multi-label classification algorithm that can be used on both tree and DAG structured hierarchies. They formulate a search for the opti- mal consistent multi-label as the finding of the best subgraph in a tree/DAG. In our approach, we as- sume, individual documents are already associated with one or more topics and we find a consistent label set for a group of documents using the DAG structured topic hierarchy. <ref type="bibr" target="#b24">Medelyan et al. (Medelyan et al., 2008</ref>) and <ref type="bibr" target="#b10">Ferragina et al. (Ferragina and Scaiella, 2010</ref>) de- tect topics for a document using Wikipedia article names and category names as the topic vocabulary. These systems are able to extract signals from a text document and identify Wikipedia articles and/or categories that optimally match the document and assign those article/category names as topics for the document. When run on a large collection of docu- ments, these approaches generate enormous num- bers of topics, a problem our proposed approach addresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Our Contributions</head><p>While most prior work discussed above focuses on the underlying set of documents, (e.g., by clustering documents), we focus directly on the topics. In particular, we formulate the problem as subset selection on the set of topics within a DAG while simultaneously considering the documents to be categorized. Our method can scale to the colossal size of the DAG (1 million topics and 3 million correlation links between topics in Wikipedia). Moreover, our approach can naturally incorporate outputs from many of the aforementioned algorithms. Our approach is based on submodular maximization and mixture learning, which has been successfully used in applications such as document summarization <ref type="bibr" target="#b20">(Lin, 2012)</ref> and image summarization <ref type="bibr" target="#b36">(Tschiatschek et al., 2014</ref>), but has never been applied to topic identification tasks or, more generally, DAG summarization.</p><p>We introduce a family of submodular functions to identify an appropriate set of topics from a DAG structured hierarchy of topics for a group of docu- ments. We characterize this topic appropriateness through a set of desirable properties such as cov- erage, diversity, specificity, clarity, and relevance. Each of the submodular function components we consider are monotone, thereby ensuring a near op- timal performance obtainable via a simple greedy algorithm for optimization. <ref type="bibr">7</ref> . We also show how our technique naturally embodies outputs of other algorithms such as LDA, clustering, and classifi- cations. Finally, we utilize a large margin formu- lation for learning mixtures of these submodular functions, and show how we can optimally learn them from training data.</p><p>Our approach demonstrates how to utilize the features collectively in the document space and the topic space to infer a set of topics. From an em- pirical perspective, we introduce and evaluate our approach on a dataset of around 8000 disambigua- tions that was extracted from Wikipedia and subse- quently cleaned using the methods described in the experimentation section. We show that our learn- ing framework outperforms many of the baselines, and is practical enough to be used on large corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>Let G (V, E) be the DAG structured topic hierarchy with V topics. These topics are observed to have a parent child (isa) relationship forming a DAG. Let D be the set of documents that are associated with one or more of these topics. The middle portion of <ref type="figure">Figure 1</ref> depicts a topic hierarchy with associ- ated documents. The association links between the documents and topics can be hard or soft. In case of a hard link, a document is attached to a set of topics. Examples include multi-labeled documents. In case of a soft link, a document is associated with a topic with some degree of confidence (or prob- ability). Furthermore, if a document is attached to a topic t, we assume that all the ancestor top- ics of t are also relevant for that document. This assumption has been employed in earlier works ( <ref type="bibr" target="#b4">Blei et al., 2004;</ref><ref type="bibr" target="#b1">Bi and Kwok, 2011;</ref><ref type="bibr" target="#b32">Rousu et al., 2006</ref>) as well. Given a budget of K, our objec- tive is to choose a set of K topics from V , which best describe the documents in D. The notion of best describing topics is characterized through a set of desirable properties -coverage, diversity, speci- ficity, clarity, relevance and fidelity -that K topics have to satisfy. The submodular functions that we introduce in the next section ensure these proper- ties are satisfied. Formally, we solve the following discrete optimization problem:</p><formula xml:id="formula_1">S * ∈ argmax S⊆V :|S|≤K i w i f i (S)<label>(1)</label></formula><p>where, f i are monotone submodular mixture com- ponents and w i ≥ 0 are the weights associated with those mixture components. Set S * is the summary topics scored best.</p><p>It is easy to find massive (i.e., size in the order of million) DAG structured topic hierarchies in prac- tice. Wikipedia's category hierarchy consists of more than 1M categories (topics) arranged hierar- chically. In fact, they form a cyclic graph <ref type="bibr" target="#b40">(Zesch and Gurevych, 2007)</ref>. However, we can convert it to a DAG by eliminating the cycles as described in the supplementary material. <ref type="bibr">YAGO (Suchanek et al., 2007</ref>) and Freebase ( <ref type="bibr" target="#b5">Bollacker et al., 2008)</ref> are other instances of massive topic hierarchies. The association of the documents with the existing topic hierarchy is also well studied. Systems such as WikipediaMiner <ref type="bibr" target="#b27">(Milne, 2009)</ref>, TAGME <ref type="bibr" target="#b10">(Ferragina and Scaiella, 2010)</ref> and several annotation systems such as ( <ref type="bibr" target="#b9">Dill et al., 2003;</ref><ref type="bibr" target="#b26">Mihalcea and Csomai, 2007;</ref><ref type="bibr" target="#b7">Bunescu and Pasca, 2006</ref>) attach topics from Wikipedia (and other catalogs) to the documents by establishing the hard or soft links mentioned above.</p><p>Our goal is the following: Given a (ground set) collection V of topics organized in a pre-existing hierarchical DAG structure, and a collection D of documents, chose a size K ∈ Z + representative subset of topics. Our approach is distinct from earlier work (e.g., <ref type="bibr" target="#b12">(Kanungo et al., 2002;</ref><ref type="bibr" target="#b3">Blei et al., 2003)</ref>) where typically only a set of documents is classified and categorized in some way. We next provide a few definitions needed later in the paper.</p><p>Definition 1: Transitive Cover Γ): A topic t is said to cover a set of documents Γ(t), called the transitive cover of the topic t, if for all documents i ∈ Γ(t), either i is associated directly with topic t or with any of the descendant topics of t in the topic DAG. A natural extension of this definition to a set of topics T is defined as Γ(T ) = ∪ t∈T Γ(t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2: Truncated Transitive Cover (Γ α ):</head><p>This is a transitive cover of topic t, but with the limitation that the path length between a docu- ment and the topic t is not more than α. Hence, |Γ α (t)| ≤ |Γ(t)|.</p><p>While our problem is closely related to cluster- ing approaches, which consider the set of docu- ments directly, there are some crucial differences. In particular, we focus on producing a clustering of documents where clusters are encouraged to honor a pre-defined DAG structured topic hierarchy. Ex- isting agglomerative clustering algorithms focusing on the coverage of documents may not produce the desired clustering. To understand this, consider six documents d1, d2 . . . d6 to be grouped into three clusters. There may be multiple ways to do this de- pending upon multiple aggregation paths present in the topic DAG: ((d1, d2), (d3, d4), (d5, d6)) or ((d1, d2, d3), (d4, d5), (d6)) or ((d1, d2, d3, d4), (d5), (d6)) or something else. Hence, we need more stringent measures to prefer one clustering over the others. Our work addresses this with a variety of quality criteria (coverage, diversity, specificity, clarity, relevance and fidelity, which are explained later in this paper) that are organically derived from well established submodular functions. And, most importantly, we learn the right mixture of these qualities to be enforced from the data itself. Fur- thermore, our approach also generalizes these clus- tering approaches, since one of the components in our mixture of submodular functions is defined via these unsupervised approaches, and maps a given clustering to a set of topics in the hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Submodular Components and Learning</head><p>Summarization is the task of extracting information from a source that is both small in size but still representative. Our problem is different from traditional summarization tasks since we have an underlying DAG as a topic hierarchy that we wish to summarize in response to a subset of documents. Thus, a critical part of our problem is to take the graph structure into account while creating the summaries. Below, we identify properties we wish our summaries to posses. Coverage: A summary set of topics should cover most of the documents. A document is said to be covered by a topic if there exists a path from the topic, going through intermediary descendant topics, to the document, i.e., the document is within the transitive cover of the topic.</p><p>Diversity: Summaries should be as diverse as possible, i.e., each summary topic should cover a unique set of documents. When a document is covered by more than one topic, that document is redundantly covered, e.g., "Finance" and "Banking" would be unlikely members of the same summary.</p><p>Summary qualities also involve "quality" notions, including:</p><p>Specificity/Clarity/Relevance/Coherence: These quality measures help us choose a set of topics that are neither too abstract nor overly specific. They ensure that the topics are clear and relevant to the documents that they represent. When additional information such as clustering (from LDA or other sources) and tagging (manual) documents is available, these quality criteria encourage the chosen topics to show resemblance (coherence) to those clustering/tagging in terms of transitive cover of documents they produce.</p><p>In the below, we define a variety of submodular functions that capture the above properties, and we then describe a large margin learning framework for learning convex mixtures of such components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Submodular Components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Coverage Based Functions</head><p>Coverage components capture "coverage" of a set of documents.</p><p>Weighted Set Cover Function: Given a set of categories, S ⊆ V , define Γ(S) as the set of docu- ments covered -for each topic s ∈ S, Γ(s) ⊆ D represents the documents covered by topic s and Γ(S) = ∪ s∈S Γ(s). The weighted set cover func- tion, defined as f (S) = d∈Γ(S) w d = w(Γ(S)), assigns weights to the documents based on their relative importance (e.g., in Wikipedia disambiguation, the different documents could be ranked based on their priority).</p><p>Feature-based Functions: This class of function represents coverage in feature space. Given a set of categories S ⊆ V , and a set of features U , define m u (S) as the score associated with the set of categories S for feature u ∈ U . The feature set could represent, for example, the documents, in which case m u (S) represents the number of times document u is covered by the set S. U could also represent more complicated features. For example, in the context of Wikipedia disambiguation, U could represent TFIDF features over the documents. Feature based are then defined as f (S) = u∈U ψ(m u (S)), where ψ is a concave (e.g., the square root) function. This function class has been successfully used in several applications ( <ref type="bibr" target="#b14">Kirchhoff and Bilmes, 2014;</ref><ref type="bibr" target="#b38">Wei et al., 2014a;</ref><ref type="bibr" target="#b39">Wei et al., 2014b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Similarity based Functions</head><p>Similarity functions are defined through a simi- larity matrix S = {s ij } i,j∈V . Given categories i, j ∈ V , similarity s ij in our case can be defined as s ij = |Γ(i)∩Γ(j)|, i.e the number of documents commonly covered by both i and j.</p><p>Facility Location: The facility location func- tion, defined as f (S) = i∈V max j∈S s ij , is a natural model for k-medoids and exemplar based clustering, and has been used in several summariza- tion problems <ref type="bibr" target="#b36">(Tschiatschek et al., 2014;</ref><ref type="bibr" target="#b38">Wei et al., 2014a)</ref>.</p><p>Penalty based diversity: A similarity ma- trix may be used to express a form of coverage of a set S but that is then penalized with a re- dundancy term, as in the following difference: <ref type="bibr" target="#b18">and Bilmes, 2011)</ref>). Here λ ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. This function is submodular, but is not in general monotone, and has been used in document summarization ( <ref type="bibr" target="#b18">Lin and Bilmes, 2011)</ref>, as a dispersion function ( <ref type="bibr" target="#b6">Borodin et al., 2012)</ref>, and in image summarization <ref type="bibr" target="#b36">(Tschiatschek et al., 2014</ref>).</p><formula xml:id="formula_2">f (S) = i∈V,j∈S s ij − λ i∈S j∈S, s i,j (Lin</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Quality Control (QC) Functions</head><p>QC functions ensure a quality criteria is met by a set S of topics. We define the quality score of the set S as F q (S) = s∈S f q (s), where f q (s) is the quality score of topic s for quality q. Therefore, F q (S) is a modular function in S. We investigate three types of quality control functions: Topic Specificity, Topic Clarity, and Topic Relevance.</p><p>Topic Specificity: The farther a topic is from the root of the DAG, the more specific it becomes. Topics higher up in the hierarchy are abstract and less specific. We therefore prefer topics low in the DAG, but lower topics also have less coverage. We define f specificity (s) = s h where s h is the height of topic s in the DAG. The root topic has height zero and the "height" increases as we move down the DAG in <ref type="figure">Figure 1</ref>.</p><p>Topic Clarity: Topic clarity is the fraction of descendant topics that cover one or more docu- ments. If a topic has many descendant topics that do not cover any documents, it has less clarity. For-</p><formula xml:id="formula_3">mally, f clarity (s) = t∈descendants(s) Γ(t)&gt;0 |descendants(s)|</formula><p>, where is the indicator function.</p><p>Topic Relevance: A topic is considered to be better related to a document if the number of hops needed to reach the document from that topic is lower. Given any set A ⊆ D of document, and any topic s ∈ V , we can define f relevance (s|A) = argmin α {α : A ⊆ Γ α (s)}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QC Functions As Barrier Modular Mixtures:</head><p>We introduce a modular function for every QC function as follows f α specificity (s) = 1 if the height of topic s is at least α 0 otherwise for every possible value of α. This creates a sub- modular mixture with as many components as the number of possible values of α. In our experiments with Wikipedia, we had α varying from 1 to 120 stepping by 1, adding 120 modular mixture compo- nents. Similarly, we define, f β clarity (s) = 1 if the clarity of topic s is at least β 0 otherwise for every possible (discretized to make it count- ably finite) value of β. And, f γ relevance (s) = f cov (s|Γ γ (s)), where f cov () is the coverage submodular function and s|X indi- cates coverage of a topic s over a set of documents X. All these functions (modular and submodular terms) are added as mixture components in our learning framework to learn suitable weights for them. We then use these weights in our inference procedure to obtain a subset of topics as described in 3.2. We show from our experiments that this approach performs better than all other approaches and baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Fidelity Functions</head><p>A function representing the fidelity of a set S to another reference set R is one that gets a large value when the set S represents the set R. Such a function scores inferred topics high when it resembles a reference set of topics and/or item clusters. The reference set in this case can be produced from other algorithms such as k-means, LDA and its variants or from a manually tagged corpus. Next we describe one such fidelity function.</p><p>Topic Coherence: This function scores a set of topics S high when the transitive cover (Def- inition 1) produced by the topics in S resembles the clusters of documents produced by an external source (k-means, LDA or manual). Given an exter- nal source that clusters the documents, producing T clusters L 1 , L 2 , ..., L T (for T topics), topic coher- ence is defined as: f (S) = t∈T max k∈S w k,t where w k,t = harmonic mean(w p k,t , w r k,t ) and w p k,t = |Γ(k)∩Lt| |Γ(k)| and w r k,t = |Γ(k)∩Lt| |Lt| . Note that, w p k,t ≥ 0 and w r k,t ≥ 0 are the precision on recall of the resemblance and w k,t is the F1 measure. If the transitive cover of topics in S resembles the reference clusters L t exactly, we attain maximum coherence (or fidelity). As the resemblance dimin- ishes, the score decreases. The above function f (S) is monotone submodular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Mixture of Submodular Components:</head><p>Given the different classes of submodular functions above, we construct our submodular scoring func- tions F w (·) as a convex combinations of these dif- ferent submodular functions f 1 , f 2 , . . . , f m , above. In other words,</p><formula xml:id="formula_4">F w (S) = m i=1 w i f i (S),<label>(2)</label></formula><p>where w = (w 1 , . . . , w m ), w i ≥ 0, i w i = 1. The components f i are submodular and assumed to be normalized: i.e., f i (∅) = 0, and f i (V ) = 1 for monotone functions and max A⊆V f i (A) ≤ 1 for non-monotone functions. A simple way to normal- ize a monotone submodular function is to define the component as f i (S)/f i (V ). This ensures that the components are compatible with each other. Obviously, the merit of the scoring function F w (·) depends on the selection of the components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Large Margin Learning</head><p>We optimize the weights w of the scoring func- tion F w (·) in a large-margin structured prediction framework. In this setting, we assume we have training data in the form of pairs of a set of docu- ments, and a human generated summary as a set of topics. For example, in the case of Wikipedia disambiguation, we use the human generated dis- ambiguation pages as the ground truth summary. We represent the set of ground-truth summaries as S = {S 1 , S 2 , · · · , S N }. In large margin training, the weights are optimized such that ground-truth summaries S are separated from competitor sum- maries by a loss-dependent margin:</p><formula xml:id="formula_5">Fw(S) ≥ Fw(S ) + L(S ), ∀S ∈ S, S ∈ Y \ S, (3)</formula><p>where L(·) is the loss function, and where Y is a structured output space (for example Y is the set of summaries that satisfy a certain budget B, i.e., Y = {S ⊆ V : |S | ≤ B}). We assume the loss to be normalized, 0 ≤ L(S ) ≤ 1, ∀S ⊆ V , to ensure that mixture and loss are calibrated. Equation (3) can be stated as</p><formula xml:id="formula_6">F w (S) ≥ max S ∈Y [F w (S ) + L(S )]</formula><p>, ∀S ∈ S which is called loss-augmented inference. We introduce slack variables and minimize the regularized sum of slacks ( <ref type="bibr" target="#b19">Lin and Bilmes, 2012)</ref>:</p><formula xml:id="formula_7">min w≥0,w 1 =1 S∈S max S ∈Y Fw(S ) + L(S ) − Fw(S) + λ 2 w 2 2 , (4)</formula><p>where the non-negative orthant constraint, w ≥ 0, ensures that the final mixture is submodular. Note a 2-norm regularizer is used on top of a 1-norm constraint w 1 = 1 which we interpret as a prior to encourage higher entropy, and thus more diverse mixture distributions. Tractability depends on the choice of the loss function. The parameters w are learnt using stochastic gradient descent as in <ref type="bibr" target="#b36">(Tschiatschek et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Loss Functions</head><p>A natural choice of loss functions for our case can be derived from cluster evaluation metrics. Every inferred topic s induces a subset of documents, namely the transitive cover Γ (s) of s. We compare these clusters with the clusters induced from the true topics in the training set and compute the loss.</p><p>In this paper, we use the Jaccard Index (JI) as a loss function. Let S be the inferred topics and T be the true topics. The Jaccard loss is defined as</p><formula xml:id="formula_8">L jaccard (S, T ) = 1 − 1 k s∈S max t∈T |Γ(s)∩Γ(t)|</formula><p>|Γ(s)∪Γ(t)| , where k = |S| = |T | is the number of topics. When the clustering produced by the inferred and the true topics are similar, Jaccard loss is 0. When they are completely dissimilar, the loss is maxi- mum, i.e., 1. Jaccard loss is a modular function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference Algorithm: Greedy</head><p>Having learnt the weights for the mixture components, the resulting function</p><formula xml:id="formula_9">F w (S) = m i=1 w i f i (S)</formula><p>is a submodular function. In the case when the individual components are them- selves monotone (all our functions in fact are), F w (S) can be optimized by the accelerated greedy algorithm <ref type="bibr" target="#b28">(Minoux, 1978)</ref>. Thanks to submodu- larity, we can obtain near optimal solutions very efficiently. In case the functions are all monotone submodular, we can guarantee that the solution is within 1 − 1/e factor from the optimal solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>To validate our approach, we make use of Wikipedia category structure as a topic DAG and apply our technique to the task of automatic generation of Wikipedia disambiguation pages. We pre-processed the category graph to elimi- nate the cycles in order to make it a DAG. Each Wikipedia disambiguation page is manually created by Wikipedia editors by grouping a collection of Wikipedia articles into several groups. Each group is then assigned a name, which serves as a topic for the group. Typically, a disambiguation page segre- gates around 20-30 articles into 5-6 groups. Our goal is to measure how accurately we can recre- ate the groups for a disambiguation page and label them, given only the collection of articles men- tioned in that disambiguation page (when actual groupings and labels are hidden).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We parsed the contents of Wikipedia disambigua- tion pages and extracted disambiguation page names, article groups and group names. We col- lected about 8000 disambiguation pages that had at least four groups on them. Wikipedia category structure is used as the topic DAG. We eliminated few administrative categories such as "Hidden Cat- egories", "Articles needing cleanup", and the like. The final DAG had about 1M topics and 3M links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>Every group of articles on the Wikipedia disam- biguation page is assigned a name by the editors. Unfortunately, these names may not correspond to the Wikipedia category (topic) names. For exam- ple, one of the groups on the "Matrix" disambigua- tion page has a name "Business and government" and there is no Wikipedia category by that name. However, the group names generated by our (and baseline) method are from the Wikipedia categories (which forms our topic DAG). In addition, there can be multiple relevant names for a group. For example, a group on a disambiguation page may be called "Calculus", but an algorithm may rightly generate "Vector Calculus". Hence we cannot eval- uate the accuracy of an algorithm just by matching the generated group names to those on the disam- biguation page. To alleviate this problem, we adopt cluster-based evaluation metrics. We treat every group of articles generated by an algorithm under a topic for a disambiguation page as a cluster of arti- cles. These are considered as inferred clusters for a disambiguation page. We compare them against the actual grouping of articles on the Wikipedia disam- biguation page by treating those groups as true clus- ters. We can now adopt Jaccard Index, F1-measure, and NMI (Normalized Mutual Information) based cluster evaluation metrics described in <ref type="bibr" target="#b23">(Manning et al., 2008</ref>). For each disambiguation page in the test set, we compute every metric score and then average it over all the disambiguation pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Methods Compared</head><p>We validated our approach by comparing against several baselines described below. We also com- pared two variations of our approach as described next. For each of these cases (baselines and two variations) we generated and compared the metrics (Jaccard Index, F1-measure and NMI) as described in the previous section.</p><p>KM docs : K-Means algorithm run on articles as TF-IDF vectors of words. The number of clus- ters K is set to the number of true clusters on the Wikipedia disambiguation page.</p><p>KMed docs : K-Medoids algorithm with articles as TF-IDF vectors of words. The number of clus- ters are set as in KM docs .</p><p>KMed topics : K-Medoids run on topics as TF- IDF vectors of words. The words for each topic is taken from the articles that are in the transitive cover of the topic.</p><p>LDA docs : LDA algorithm with the number of topics set to the number of true clusters on the Wikipedia disambiguation page. Each article is then grouped under the highest probability topic.</p><p>SMML cov : This is the submodular mixture learning case explained in section 3.1.5. Here we consider a mixture of all the submodular functions governing coverage, diversity, fidelity and QC func- tions. However, we exclude the similarity based functions described in section 3.1.2. Coverage based functions have a time complexity of O (n) whereas similarity based functions are O n 2 . By excluding similarity based functions, we can com- pare the quality of the results with and without O(n 2 ) functions. We learn the mixture weights from the training set and use them during infer- ence on the test set to subset K topics through the submodular maximization (Equation 1). SMML cov+sim : This case is similar to SMML cov except that, we include similarity based submodu- lar mixture components. This makes the inference time complexity O n 2 .</p><p>We do not compare against HSLDA, PAM and few other techniques cited in the related work sec- tions because they do not produce a subset of K summary topics -these are not directly compara- ble with our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Results</head><p>We show that the submodular mixture learning and maximization approaches, i.e., SMML cov and SMML cov+sim outperform other approaches in vari- ous metrics. In all these experiments, we performed 5 fold cross validation to learn the parameters from 80% of the disambiguation pages and evaluated on the rest of the 20%, in each fold.</p><p>In <ref type="figure" target="#fig_2">Figure 2a</ref> we summarize the results of the comparison of the methods mentioned above on Jaccard Index, F1 measure and NMI. Our pro- posed techniques SMML cov and SMML cov+sim out- perform other techniques consistently.</p><p>In <ref type="figure" target="#fig_2">Figures 2b and 2c</ref> we measure the number of test instances (i.e., disambiguation queries) in which each of the algorithms dominate (win) in evaluation metrics. In 60% of the disambiguation queries, SMML cov and SMML cov+sim approaches   functions (SMML cov+sim ), but at a greatly reduced execution time, demonstrating the sufficiency of O (n) functions for our task. On the average, for each disambiguation query, SMML cov took around 40 seconds (over 1M topics and 3M edges DAG) to infer the topics, whereas SMML cov+sim took around 35 minutes. Both these experiments were carried on a machine with 32 GB RAM, Eight-Core AMD Opteron(tm) Processor 2427.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We investigated a problem of summarizing topics over a massive topic DAG such that the summary set of topics produced represents the objects in the collection. This representation is characterized through various classes of submodular (and mono- tone) functions that captured coverage, similarity, diversity, specificity, clarity, relevance and fidelity of the topics. Currently we assume that the number of topics K is given as an input to our algorithm. It would be an interesting future problem to estimate the value of K automatically in our setting. As fu- ture work, we also plan to extend our techniques to produce a hierarchical summary of topics and scale it across heterogeneous collection of objects (from different domains) to bring all of them under the same topic DAG and investigate interesting cases thereon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4</head><label></label><figDesc>http://en.wikipedia.org/wiki/Help:Categories 5 http://en.wikipedia.org/wiki/Wikipedia:Disambiguation 6 http://en.wikipedia.org/wiki/Apple_ (disambiguation)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of techniques</figDesc></figure>

			<note place="foot" n="1"> http://qwone.com/ ˜ jason/20Newsgroups/ 2 http://www.daviddlewis.com/resources/ testcollections/reuters21578/ 3 Topic Concept is more abstract than the topic Science which is more abstract than the topicChemistry</note>

			<note place="foot" n="7"> A simple greedy algorithm (Nemhauser et al., 1978) obtains a 1 − 1/e approximation guarantee for monotone submodular function maximization</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical multi-label prediction of gene function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zafer</forename><surname>Barutcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><forename type="middle">G</forename><surname>Troyanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="830" to="836" />
			<date type="published" when="2006-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-label classification on tree-and DAG-structured hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested chinese restaurant process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">2003</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Max-sum diversification, monotone submodular functions and dynamic updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Borodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuli</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Principles of Database Systems</title>
		<meeting>Principles of Database Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="155" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using encyclopedic knowledge for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06)</title>
		<meeting>the 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-06)<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semtag and seeker: Bootstrapping the semantic web via automated semantic annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Eiron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gruhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anant</forename><surname>Jhingran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapas</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Tomlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on World Wide Web, WWW &apos;03</title>
		<meeting>the 12th International Conference on World Wide Web, WWW &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tagme: On-the-fly annotation of short text fragments (by wikipedia entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Information and Knowledge Management, CIKM &apos;10</title>
		<meeting>the 19th ACM International Conference on Information and Knowledge Management, CIKM &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast semidifferential-based submodular function optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An efficient k-means clustering algorithm: Analysis and implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapas</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><forename type="middle">D</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Senior Member, and Senior Member</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="881" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maximizing the spread of influence through a social network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Submodularity for data selection in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>October</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Near-optimal nonmyopic value of information in graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Uncertainity in Artificial Intelligence. UAI</title>
		<meeting>Uncertainity in Artificial Intelligence. UAI</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pachinko allocation: Dag-structured mixture models of topic correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A class of submodular functions for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Meeting of the Assoc. for Comp. Ling. Human Lang. Technologies (ACL/HLT-2011)</title>
		<meeting><address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning mixtures of submodular shells with application to document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">479490</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Submodularity in Natural Language Processing: Algorithms and Applications</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>University of Washington, Dept. of EE</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ml-knn: A lazy learning approach to multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Ling Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
<note type="report_type">PATTERN RECOGNITION</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploratory analysis of highly heterogeneous document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">P</forename><surname>Maiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Loaizalemos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;13</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1375" to="1383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Topic indexing with Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Olena Medelyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Wikipedia and AI workshop at AAAI-08. AAAI</title>
		<meeting>the Wikipedia and AI workshop at AAAI-08. AAAI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic labeling of multinomial topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;07</title>
		<meeting>the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="490" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wikify!: Linking documents to encyclopedic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Csomai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM &apos;07</title>
		<meeting>the Sixteenth ACM Conference on Conference on Information and Knowledge Management, CIKM &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An open-source toolkit for mining wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. New Zealand Computer Science Research Student Conf</title>
		<meeting>New Zealand Computer Science Research Student Conf</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">2009</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accelerated greedy algorithms for maximizing submodular set functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Minoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization Techniques</title>
		<editor>J. Stoer</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin/Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1978" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="234" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PAClearning bounded tree-width graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence: Proceedings of the Twentieth Conference (UAI-2004)</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2004-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An analysis of approximations for maximizing submodular set functionsi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><forename type="middle">A</forename><surname>George L Nemhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall L</forename><surname>Wolsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="265" to="294" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Hierarchically supervised latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Perotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bartlett</surname></persName>
		</author>
		<editor>John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2609" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kernel-based learning of hierarchical multilabel classification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Rousu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sndor</forename><surname>Szedmk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1601" to="1626" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A survey of hierarchical classification across different application domains. Data Mining and Knowledge Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlosn</forename><surname>Silla</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexa</forename><surname>Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="31" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Yago: A core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web, WWW &apos;07</title>
		<meeting>the 16th International Conference on World Wide Web, WWW &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1566" to="1581" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Mixtures of Submodular Functions for Image Collection Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Tschiatschek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoachen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mining multi-label data</title>
	</analytic>
	<monogr>
		<title level="m">Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas</title>
		<editor>Oded Maimon and Lior Rokach</editor>
		<meeting><address><addrLine>US</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="667" to="685" />
		</imprint>
	</monogr>
	<note>Data Mining and Knowledge Discovery Handbook</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast multi-stage submodular maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Submodular subset selection for large-scale speech training data. Proceedings of ICASSP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Florence, Italy</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Analysis of the wikipedia category graph for nlp applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TextGraphs-2 Workshop (NAACL-HLT)</title>
		<meeting>the TextGraphs-2 Workshop (NAACL-HLT)<address><addrLine>Rochester</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-04" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
