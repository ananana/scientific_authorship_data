<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
						</author>
						<title level="a" type="main">Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="310" to="320"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1029</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the un-labeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper proposes a model for labeled sequence transduction tasks, tasks where we are given an input sequence and a set of labels, from which we are expected to generate an output sequence that reflects the content of the input sequence and desiderata specified by the labels. Several exam- ples of these tasks exist in prior work: using labels to moderate politeness in machine translation re- sults ( <ref type="bibr" target="#b26">Sennrich et al., 2016)</ref>, modifying the output language of a machine translation system <ref type="bibr" target="#b13">(Johnson et al., 2016)</ref>, or controlling the length of a summary in summarization ( <ref type="bibr" target="#b17">Kikuchi et al., 2016</ref>). In particular, however, we are motivated by the task of morphological reinflection (Cotterell et al., <ref type="bibr">1</ref> An implementation of our model are avail- able at https://github.com/violet-zct/ MSVED-morph-reinflection. 2016), which we will use as an example in our de- scription and test bed for our models. In morphologically rich languages, different af- fixes (i.e. prefixes, infixes, suffixes) can be com- bined with the lemma to reflect various syntac- tic and semantic features of a word. The ability to accurately analyze and generate morphologi- cal forms is crucial to creating applications such as machine translation ( <ref type="bibr" target="#b4">Chahuneau et al., 2013;</ref><ref type="bibr" target="#b29">Toutanova et al., 2008)</ref> or information retrieval <ref type="bibr" target="#b7">(Darwish and Oard, 2007</ref>) in these languages. As shown in 1, re-inflection of an inflected form given the target linguistic labels is a challenging sub- task of handling morphology as a whole, in which we take as input an inflected form (in the exam- ple, "playing") and labels representing the desired form ("pos=Verb, tense=Past") and must gen- erate the desired form ("played").</p><p>Approaches to this task include those utilizing hand-crafted linguistic rules and heuristics <ref type="bibr" target="#b27">(Taji et al., 2016)</ref>, as well as learning-based approaches using alignment and extracted transduction rules <ref type="bibr" target="#b9">(Durrett and DeNero, 2013;</ref><ref type="bibr" target="#b0">Alegria and Etxeberria, 2016;</ref><ref type="bibr" target="#b24">Nicolai et al., 2016)</ref>. There have also been methods proposed using neural sequence- to-sequence models <ref type="bibr" target="#b11">(Faruqui et al., 2016;</ref><ref type="bibr" target="#b14">Kann et al., 2016;</ref><ref type="bibr" target="#b25">Ostling, 2016)</ref>, and currently ensem- bles of attentional encoder-decoder models <ref type="bibr">(Kann and Schütze, 2016a,b)</ref> have achieved state-of-art results on this task. One feature of these neu- ral models however, is that they are trained in a largely supervised fashion (top of <ref type="figure" target="#fig_0">Fig. 1</ref>), using data explicitly labeled with the input sequence and labels, along with the output representation. Need- less to say, the ability to obtain this annotated data for many languages is limited. However, we can expect that for most languages we can obtain large amounts of unlabeled surface forms that may al- low for semi-supervised learning over this unla- beled data (entirety of <ref type="figure" target="#fig_0">Fig. 1</ref>). <ref type="bibr">2</ref> In this work, we propose a new frame- work for labeled sequence transduction prob- lems: multi-space variational encoder-decoders (MSVED, §3.3). MSVEDs employ continuous or discrete latent variables belonging to multiple separate probability distributions 3 to explain the observed data. In the example of morphological reinflection, we introduce a vector of continuous random variables that represent the lemma of the source and target words, and also one discrete ran- dom variable for each of the labels, which are on the source or the target side.</p><p>This model has the advantage of both providing a powerful modeling framework for supervised learning, and allowing for learning in an unsuper- vised setting. For labeled data, we maximize the variational lower bound on the marginal log like- lihood of the data and annotated labels. For un- labeled data, we train an auto-encoder to recon- struct a word conditioned on its lemma and mor- phological labels. While these labels are unavail- able, a set of discrete latent variables are associ- ated with each unlabeled word. Afterwards we can perform posterior inference on these latent vari- ables and maximize the variational lower bound on the marginal log likelihood of data.</p><p>Experiments on the SIGMORPHON morpho- logical reinflection task ( ) find that our model beats the state-of-the-art for a sin- gle model in the majority of languages, and is par- ticularly effective in languages with more compli- cated inflectional phenomena. Further, we find that semi-supervised learning allows for signifi- cant further gains. Finally, qualitative evaluation of lemma representations finds that our model is able to learn lemma embeddings that match with human intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Labeled Sequence Transduction</head><p>In this section, we first present some notations re- garding labeled sequence transduction problems in general, then describe a particular instantiation for morphological reinflection. Notation: Labeled sequence transduction prob- lems involve transforming a source sequence x (s) into a target sequence x (t) , with some labels describing the particular variety of transforma- tion to be performed. We use discrete variables</p><formula xml:id="formula_0">y (t) 1 , y (t) 2 , · · · , y (t)</formula><p>K to denote the labels associated with each target sequence, where K is the total number of labels. Let</p><formula xml:id="formula_1">y (t) = [y (t) 1 , y (t) 2 , · · · , y (t) K ] denote a vector of these discrete variables. Each discrete variable y (t)</formula><p>k represents a categorical fea- ture pertaining to the target sequence, and has a set of possible labels. In the later sections, we also use y (t) and y (t) k to denote discrete latent variables corresponding to these labels.</p><p>Given a source sequence x (s) and a set of as- sociated target labels y (t) , our goal is to gener- ate a target sequence x (t) that exhibits the fea- tures specified by y (t) using a probabilistic model p(x (t) |x (s) , y (t) ). The best target sequencê x (t) is then given by:</p><formula xml:id="formula_2">ˆ x (t) = arg max x (t) p(x (t) |x (s) , y (t) ).<label>(1)</label></formula><p>Morphological Reinflection Problem: In mor- phological reinflection, the source sequence x (s) consists of the characters in an inflected word (e.g., "played"), while the associated labels y (t) describe some linguistic features (e.g., y</p><p>(t) pos = Verb, y (t) tense = Past) that we hope to realize in the target. The target sequence x (t) is there- fore the characters of the re-inflected form of the source word (e.g., "played") that satisfy the lin- guistic features specified by y (t) . For this task, each discrete variable y (t) k has a set of possible la- bels (e.g. pos=V, pos=ADJ, etc) and follows a multinomial distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries: Variational Autoencoder</head><p>As mentioned above, our proposed model uses probabilistic latent variables in a model based on neural networks. The variational autoencoder ) is an efficient way to handle (continuous) latent variables in neural models. We describe it briefly here, and inter- ested readers can refer to Doersch (2016) for de- tails. The VAE learns a generative model of the probability p(x|z) of observed data x given a la- tent variable z, and simultaneously uses a recog- nition model q(z|x) at learning time to estimate z for a particular observation x <ref type="figure" target="#fig_1">(Fig. 2(a)</ref>). q(·) and p(·) are modeled using neural networks parameter- ized by φ and θ respectively, and these parameters are learned by maximizing the variational lower bound on the marginal log likelihood of data:</p><formula xml:id="formula_3">(a) VAE y (t) x (t) x (t) x (s) x (s) x x x z y z z z z y (t) y (b) Labeled MSVAE (c) MSVAE (d) Labeled MSVED (e) MSVED</formula><formula xml:id="formula_4">log p θ (x) ≥ E z∼q φ (z|x) [log p θ (x|z)]− KL(q φ (z|x)||p(z))<label>(2)</label></formula><p>The KL-divergence term (a standard feature of variational methods) ensures that the distributions estimated by the recognition model q φ (z|x) do not deviate far from our prior probability p(z) of the values of the latent variables. To optimize the parameters with gradient descent,  introduce a reparameterization trick that allows for training using simple back- propagation w.r.t. the Gaussian latent variables z. Specifically, we can express z as a deterministic variable z = g φ (, x) where is an independent Gaussian noise variable ∼ N (0, 1). The mean µ and the variance σ 2 of z are reparameterized by the differentiable functions w.r.t. φ. Thus, instead of generating z from q φ (z|x), we sample the aux- iliary variable and obtain z = µ φ (x) + σ φ (x) • , which enables gradients to backpropagate through φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-space Variational Autoencoders</head><p>As an intermediate step to our full model, we next describe a generative model for a single se- quence with both continuous and discrete latent variables, the multi-space variational auto-encoder (MSVAE). MSVAEs are a combination of two threads of previous work: deep generative mod- els with both continuous/discrete latent variables for classification problems ( <ref type="bibr" target="#b21">Maaløe et al., 2016)</ref> and VAEs with only continu- ous variables for sequential data <ref type="bibr" target="#b3">(Bowman et al., 2016;</ref><ref type="bibr" target="#b5">Chung et al., 2015;</ref><ref type="bibr" target="#b31">Zhang et al., 2016;</ref><ref type="bibr" target="#b10">Fabius and van Amersfoort, 2014;</ref><ref type="bibr" target="#b2">Bayer and Osendorfer, 2014</ref>). In MSVAEs, we have an ob- served sequence x, continuous latent variables z like the VAE, as well as discrete variables y.</p><p>In the case of the morphology example, x can be interpreted as an inflected word to be generated. y is a vector representing its linguistic labels, either annotated by an annotator in the observed case, or unannotated in the unobserved case. z is a vector of latent continuous variables, e.g. a latent embed- ding of the lemma that captures all the information about x that is not already represented in labels y.</p><p>MSVAE: Because inflected words can be naturally thought of as "lemma+morphological labels", to interpret a word, we resort to discrete and continu- ous latent variables that represent the linguistic la- bels and the lemma respectively. In this case when the labels of the sequence y is not observed, we perform inference over possible linguistic labels and these inferred labels are referenced in gener- ating x.</p><formula xml:id="formula_5">The generative model p θ (x, y, z) = p(z)p π (y)p θ (x|y, z) is defined as: p(z) = N (z|0, I) (3) p π (y) = k Cat(y k |π k ) (4) p θ (x|y, z) = f (x; y, z, θ).<label>(5)</label></formula><p>Like the standard VAE, we assume the prior of the latent variable z is a diagonal Gaussian dis- tribution with zero mean and unit variance. We assume that each variable in y is independent, resulting in a factorized distribution in Eq. 4, where Cat(y k |π k ) is a multinomial distribution with parameters π k . For the purposes of this study, we set these to a uniform distribution π k,j = 1 |π k | . f (x; y, z, θ) calculates the likelihood of x, a function parametrized by deep neural networks. Specifically, we employ an RNN decoder to gener- ate the target word conditioned on the lemma vari- able z and linguistic labels y, detailed in §5.</p><p>When inferring the latent variables from the given data x, we assume the joint distribution of latent variables z and y has a factorized form, i.e. q(z, y|x) = q(z|x)q(y|x) as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>(c).</p><p>The inference model is defined as follows:</p><formula xml:id="formula_6">q φ (z|x) = N (z|µ φ (x), diag(σ 2 φ (x))) (6) q φ (y|x) = k q φ (y k |x) = k Cat(y k |π φ (x))<label>(7)</label></formula><p>where the inference distribution over z is a diago- nal Gaussian distribution with mean and variance parameterized by neural networks. The inference model q(y|x) on labels y has the form of a dis- criminative classifier that generates a set of multi- nomial probability vectors π φ (x) over all labels for each tag y k . We represent each multinomial distribution q(y k |x) with an MLP. The MSVAE is trained by maximizing the fol- lowing variational lower bound U(x) on the objec- tive for unlabeled data:</p><formula xml:id="formula_7">log p θ (x) ≥ E (y,z)∼q φ (y,z|x) log p θ (x, y, z) q φ (y, z|x) = E y∼q φ (y|x) [E z∼q φ (z|x) [log p θ (x|z, y)] − KL(q φ (z|x)||p(z)) + log p π (y) − log q φ (y|x)] = U(x)<label>(8)</label></formula><p>Note that this introduction of discrete variables requires more sophisticated optimization algo- rithms, which we will discuss in §4.1.</p><p>Labeled MSVAE: When y is observed as shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, we maximize the following varia- tional lower bound on the marginal log likelihood of the data and the labels:</p><formula xml:id="formula_8">log p θ (x, y) ≥ E z∼q φ (z|x) log p θ (x, y, z) q φ (z|x) = E z∼q φ (z|x) [log p θ (x|y, z) + log p π (y)] − KL(q φ (z|x)||p(z))<label>(9)</label></formula><p>which is a simple extension to Eq. 2. Note that when labels are not observed, the in- ference model q φ (y|x) has the form of a discrim- inative classifier, thus we can use observed labels as the supervision signal to learn a better classifier. In this case we also minimize the following cross entropy as the classification loss:</p><formula xml:id="formula_9">D(x, y) = E (x,y)∼p l (x,y) [− log q φ (y|x)] (10)</formula><p>where p l (x, y) is the distribution of labeled data. This is a form of multi-task learning, as this addi- tional loss also informs the learning of our repre- sentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-space Variational</head><p>Encoder-Decoders</p><p>Finally, we discuss the full proposed method: the multi-space variational encoder-decoder (MSVED), which generates the target x (t) from the source x (s) and labels y (t) . Again, we discuss two cases of this model: labels of the target sequence are observed and not observed. MSVED: The graphical model for the MSVED is given in <ref type="figure" target="#fig_1">Fig. 2 (e)</ref>. Because the labels of target se- quence are not observed, once again we treat them as discrete latent variables and make inference on the these labels conditioned on the target se- quence. The generative process for the MSVED is very similar to that of the MSVAE with one impor- tant exception: while the standard MSVAE condi- tions the recognition model q(z|x) on x, then gen- erates x itself, the MSVED conditions the recogni- tion model q(z|x (s) ) on the source x (s) , then gen- erates the target x (t) . Because only the recogni- tion model is changed, the generative equations for p θ (x (t) , y (t) , z) are exactly the same as Eqs. 3-5 with x (t) swapped for x and y (t) swapped for y. The variational lower bound on the conditional log likelihood, however, is affected by the recognition model, and thus is computed as:</p><formula xml:id="formula_10">log p θ (x (t) |x (s) ) ≥E (y (t) ,z)∼q φ (y (t) ,z|x (s) ,x (t) ) log p θ (x (t) , y (t) , z|x (s) ) q φ (y (t) , z|x (s) , x (t) ) =E y (t) ∼q φ (y (t) |x (t) ) [E z∼q φ (z|x (s) ) [log p θ (x (t) |y (t) , z)] − KL(q φ (z|x (s) )||p(z)) + log p π (y (t) ) − log q φ (y (t) |x (t) )] = L u (x (t) |x (s) )<label>(11)</label></formula><p>Labeled MSVED: When the complete form of x (s) , y (t) , and x (t) is observed in our training data, the graphical model of the labeled MSVED model is illustrated in <ref type="figure" target="#fig_1">Fig. 2 (d)</ref>. We maximize the vari- ational lower bound on the conditional log likeli- hood of observing x (t) and y (t) as follows:</p><formula xml:id="formula_11">log p θ (x (t) , y (t) |x (s) ) ≥ E z∼q φ (z|x (s) ) log p θ (x (t) , y (t) , z|x (s) ) q φ (z|x (s) ) = E z∼q φ (z|x (s) ) [log p θ (x (t) |y (t) , z) + log p π (y (t) )]− KL(q φ (z|x (s) )||p(z)) = L l (x (t) , y (t) |x (s) )<label>(12)</label></formula><p>4 Learning MSVED Now that we have described our overall model, we discuss details of the learning process that prove useful to its success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning Discrete Latent Variables</head><p>One challenge in training our model is that it is not trivial to perform back-propagation through dis- crete random variables, and thus it is difficult to learn in the models containing discrete tags such as MSVAE or MSVED. <ref type="bibr">4</ref> To alleviate this problem, we use the recently proposed Gumbel-Softmax trick ( <ref type="bibr" target="#b22">Maddison et al., 2014;</ref><ref type="bibr" target="#b12">Gumbel and Lieblein, 1954)</ref> to create a differentiable estimator for cate- gorical variables. The Gumbel-Max trick <ref type="bibr" target="#b12">(Gumbel and Lieblein, 1954</ref>) offers a simple way to draw samples from a categorical distribution with class probabili- ties π 1 , π 2 , · · · by using the argmax operation as follows: one hot(arg max i [g i + log π i ]), where g 1 , g 2 , · · · are i.i.d. samples drawn from the Gum- bel(0,1) distribution. <ref type="bibr">5</ref> When making inferences on the morphological labels y 1 , y 2 , · · · , the Gumbel- Max trick can be approximated by the continuous softmax function with temperature τ to generate a sample vectorˆyvectorˆ vectorˆy i for each label i:</p><formula xml:id="formula_12">ˆ y ij = exp((log(π ij ) + g ij )/τ ) N i k=1 exp((log(π ik ) + g ik )/τ<label>(13)</label></formula><p>where N i is the number of classes of label i. When τ approaches zero, the generated samplê y i be- comes a one-hot vector. When τ &gt; 0, ˆ y i is smooth w.r.t π i . In experiments, we start with a relatively large temperature and decrease it gradually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning Continuous Latent Variables</head><p>MSVED aims at generating the target sequence conditioned on the latent variable z and the tar- get labels y (t) . This requires the encoder to generate an informative representation z encod- ing the content of the x (s) . However, the varia- tional lower bound in our loss function contains the KL-divergence between the approximate pos- terior q φ (z|x) and the prior p(z), which is rel- atively easy to learn compared with learning to generate output from a latent representation. We observe that with the vanilla implementation the KL cost quickly decreases to near zero, setting q φ (z|x) equal to standard normal distribution. In this case, the RNN decoder can easily rely on the true output of last time step during training to de- code the next token, which degenerates into an RNN language model. Hence, the latent variables are ignored by the decoder and cannot encode any useful information. The latent variable z learns an undesirable distribution that coincides with the im- posed prior distribution but has no contribution to the decoder. To force the decoder to use the latent variables, we take the following two approaches which are similar to <ref type="bibr" target="#b3">Bowman et al. (2016)</ref>. KL-Divergence Annealing: We add a coefficient λ to the KL cost and gradually anneal it from zero to a predefined threshold λ m . At the early stage of training, we set λ to be zero and let the model first figure out how to project the representation of the source sequence to a roughly right point in the space and then regularize it with the KL cost. Although we are not optimizing the tight varia- tional lower bound, the model balances well be- tween generation and regularization. This tech- nique can also be seen in <ref type="bibr" target="#b20">(Kočisk`Kočisk`y et al., 2016;</ref><ref type="bibr" target="#b23">Miao and Blunsom, 2016)</ref>. Input Dropout in the Decoder: Besides anneal- ing the KL cost, we also randomly drop out the input token with a probability of β at each time step of the decoder during learning. The previous ground-truth token embedding is replaced with a zero vector when dropped. In this way, the RNN decoder could not fully rely on the ground-truth previous token, which ensures that the decoder uses information encoded in the latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Architecture for Morphological Reinflection</head><p>Training details: For the morphological reinflec- tion task, our supervised training data consists of source x (s) , target x (t) , and target tags y (t) . We test three variants of our model trained using dif- ferent types of data and different loss functions. First, the single-directional supervised model (SD- Sup) is purely supervised: it only decodes the target word from the given source word with the loss function L l (x (t) , y (t) |x (s) ) from Eq. 12. Sec- ond, the bi-directional supervised model (BD- Sup) is trained in both directions: decoding the target word from the source word and decoding the source word from the target word, which cor- responds to the loss function   mize the variational lower bounds and minimize the classification cross-entropy error of 10.</p><formula xml:id="formula_13">L l (x (t) , y (t) |x (s) ) + L u (x (s) |x (t) ) using</formula><formula xml:id="formula_14">⌃(x) µ(x) ✏ ⇠ N (0, 1) z &lt;w&gt; k k ä + y T 1 y T 2 y T 3 y T 4 .... ...... k a l b ⌃(x) µ(x) ✏ ⇠ N (0, 1) z &lt;w&gt; k k a Multinomial Sampling + ......</formula><formula xml:id="formula_15">L(x (s) , x (t) , y (t) , x) = α · U(x) + L u (x (s) |x (t) ) + L l (x (t) , y (t) |x (s) ) − D(x (t) , y (t) )<label>(14)</label></formula><p>The weight α controls the relative weight between the loss from unlabeled data and labeled data. We use Monte Carlo methods to estimate the expectation over the posterior distribution q(z|x) and q(y|x) inside the objective function 14. Specifically, we draw Gumbel noise and Gaussian noise one at a time to compute the latent variables y and z.</p><p>The overall model architecture is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. Each character and each label is associ- ated with a continuous vector. We employ Gated Recurrent Units (GRUs) for the encoder and de- coder. Let − → h t and ← − h t denote the hidden state of the forward and backward encoder RNN at time step t. u is the hidden representation of x (s) concate- nating the last hidden state from both directions i.e.</p><p>[</p><formula xml:id="formula_16">− → h T ; ← − h T ]</formula><p>where T is the word length. u is used as the input for the inference model on z. We represent µ(u) and σ 2 (u) as MLPs and sample z from N (µ(u), diag(σ 2 (u))), using z = µ + σ • , where ∼ N (0, I). Similarly, we can obtain the hidden representation of x (t) and use this as input to the inference model on each label y (t) i which is also an MLP following a softmax layer to generate the categorical probabilities of target labels.</p><p>In decoding, we use 3 types of information in calculating the probability of the next character : (1) the current decoder state, (2) a tag context vec- tor using attention ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) over the tag embeddings, and (3) the latent variable z. The intuition behind this design is that we would like the model to constantly consider the lemma represented by z, and also reference the tag cor- responding to the current morpheme being gen- erated at this point. We do not marginalize over the latent variable z however, instead we use the mode µ of z as the latent representation for z. We use beam search with a beam size of 8 to perform search over the character vocabulary at each de- coding time step. Other experimental setups: All hyperparame- ters are tuned on the validation set, and include the following: For KL cost annealing, λ m is set to be 0.2 for all language settings. For character drop-out at the decoder, we empirically set β to be 0.4 for all languages. We set the dimension of character embeddings to be 300, tag label embed- dings to be 200, RNN hidden state to be 256, and latent variable z to be 150. We set α the weight for the unsupervised loss to be 0.8. We train the model with Adadelta <ref type="bibr" target="#b30">(Zeiler, 2012)</ref> and use early- stop with a patience of 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Background: SIGMORPHON 2016</head><p>SIGMORPHON 2016 is a shared task on mor- phological inflection over 10 different morpholog- ically rich languages. There are a total of three tasks, the most difficult of which is task 3, which requires the system to output the reinflection of an inflected word. <ref type="bibr">6</ref> The training data format in task 3 is in triples: (source word, target labels, target word). In the test phase, the system is asked to generate the target word given a source word and the target labels. There are a total of three tracks for each task, divided based the amount of super- vised data that can be used to solve the problem, among which track 2 has the strictest limitation of only using data for the corresponding task. As this is an ideal testbed for our method, which can learn from unlabeled data, we choose track 2 and task 3 to test our our model's ability to exploit this data.</p><p>As a baseline, we compare our results with the MED system ( <ref type="bibr" target="#b15">Kann and Schütze, 2016a</ref>) which achieved state-of-the-art results in the shared task. This system used an encoder-decoder model with attention on the concatenated source word and tar- get labels. Its best result is obtained from an en- semble of five RNN encoder-decoders (Ensem- ble). To make a fair comparison with our mod- els, which don't use ensembling, we also calcu- lated single model results (Single).</p><p>All models are trained using the labeled training data provided for task 3. For our semi-supervised model (Semi-sup), we also leverage unlabeled data from the training and validation data for tasks 1 and 2 to train variational auto-encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results and Analysis</head><p>From the results in Tab. 1, we can glean a number of observations. First, comparing the results of our full Semi-sup model, we can see that for all lan- guages except Spanish, it achieves accuracies bet- ter than the single MED system, often by a large margin. Even compared to the MED ensembled model, our single-model system is quite compet- itive, achieving higher accuracies for Hungarian,   Navajo, Maltese, and Arabic, as well as achieving average accuracies that are state-of-the-art.</p><p>Next, comparing the different varieties of our proposed models, we can see that the semi- supervised model consistently outperforms the bidirectional model for all languages. And simi- larly, the bidirectional model consistently outper- forms the single direction model. From these re- sults, we can conclude that the unlabeled data is beneficial to learn useful latent variables that can be used to decode the corresponding word.</p><p>Examining the linguistic characteristics of the models in which our model performs well pro- vides even more interesting insights.  estimate how often the inflection process involves prefix changes, stem-internal changes or suffix changes, the results of which are shown in Tab. 2. Among the many languages, the inflection processes of Arabic, Maltese and Navajo are relatively diverse, and contain a large amount of all three forms of inflection. By ex- amining the experimental results together with the morphological inflection process of different lan- guages, we found that among all the languages, Navajo, Maltese and Arabic obtain the largest gains in performance compared with the ensem-a l -i m¯ a r ¯ a t i y y ¯ a t u This strongly demonstrates that our model is ag- nostic to different morphological inflection forms whereas the conventional encoder-decoder with attention on the source input tends to perform bet- ter on suffixing-oriented morphological inflection. We hypothesize that for languages that the inflec- tion mostly comes from suffixing, transduction is relatively easy because the source and target words share the same prefix and the decoder can copy the prefix of the source word via attention. However, for languages in which different inflections of a lemma go through different morphological pro- cesses, the inflected word and the target word may differ greatly and thus it is crucial to first analyze the lemma of the inflected word before generat- ing the corresponding the reinflection form based on the target labels. This is precisely what our model does by extracting the lemma representa- tion z learned by the variational inference model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis on Tag Attention</head><p>To analyze how the decoder attends to the linguis- tic labels associated with the target word, we ran- domly pick two words from the Arabic and Navajo test set and plot the attention weight in <ref type="figure" target="#fig_5">Fig. 5</ref>. The Arabic word "al-'im¯ ar¯ atiyy¯ atu" is an adjec- tive which means "Emirati", and its source word in the test data is "'im¯ ar¯ atiyyin" <ref type="bibr">7</ref> . Both of these are declensions of "'im¯ ar¯ atiyy". The source word is <ref type="bibr">7</ref> https://en.wiktionary.org/wiki/%D8% A5%D9%85%D8%A7%D8%B1%D8%A7%D8%AA%D9%8A singular, masculine, genitive and indefinite, while the required inflection is plural, feminine, nomi- native and definite. We can see from the left heat map that the attention weights are turned on at sev- eral positions of the word when generating corre- sponding inflections. For example, "al-" in Ara- bic is the definite article that marks definite nouns. The same phenomenon can also be observed in the Navajo example, as well as other languages, but due to space limitation, we don't provide detailed analysis here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Visualization of Latent Lemmas</head><p>To investigate the learned latent representations, in this section we visualize the z vectors, ex- amining whether the latent space groups together words with the same lemma. Each sample in SIG- MORPHON 2016 contains source word and tar- get words which share the same lemma. We run a heuristic process to assign pairs of words to groups that likely share a lemma by grouping together word pairs for which at least one of the words in each pair shares a surface form. This process is not error free -errors may occur in the case where multiple lemmas share the same surface form -but in general the groupings will generally reflect lem- mas except in these rare erroneous cases, so we dub each of these groups a pseudo-lemma.</p><p>In <ref type="figure" target="#fig_6">Fig. 6</ref>, we randomly pick 1500 words from Maltese and visualize the continuous latent vec- tors of these words. We compute the latent vec- tors as µ φ (x) in the variational posterior inference (Eq. 6) without adding the variance. As expected, words that belong to the same pseudo-lemma (in the same color) are projected into adjacent points in the two-dimensional space. This demonstrates that the continuous latent variable captures the canonical form of a set of words and demonstrates the effectiveness of the proposed representation.  <ref type="table">Table 3</ref>: Randomly picked output examples on the test data. Within each block, the first, second and third lines are outputs that ours is correct and MED's is wrong, ours is wrong and MED's is correct, both are wrong respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Analyzing Effects of Size of Unlabeled Data</head><p>From Tab. 1, we can see that semi-supervised learning always performs better than supervised learning without unlabeled data. In this section, we investigate to what extent the size of unlabeled data can help with performance. We process a German corpus from a 2017 Wikipedia dump and obtain more than 100,000 German words. These words are ranked in order of occurrence frequency in Wikipedia. The data contains a certain amount of noise since we did not apply any special pro- cessing. We shuffle all unlabeled data from both the Wikipedia and the data provided in the shared task used in previous experiments, and increase the number of unlabeled words used in learning by 10,000 each time, and finally use all the un- labeled data (more than 150,000 words) to train the model. <ref type="figure">Fig. 7</ref> shows that the performance on the test data improves as the amount of unla- beled data increases, which implies that the un- supervised learning continues to help improve the model's ability to model the latent lemma repre- sentation even as we scale to a noisy, real, and rela- tively large-scale dataset. Note that the growth rate of the performance grows slower as more data is added, because although the number of unlabeled data is increasing, the model has seen most word patterns in a relatively small vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Case Study on Reinflected Words</head><p>In Tab. 3, we examine some model outputs on the test data from the MED system and our model re- spectively. It can be seen that most errors of MED and our models can be ascribed to either over-copy or under-copy of characters. In particular, from the complete outputs we observe that our model tends to be more aggressive in its changes, resulting in it performing more complicated transformations, both successfully (such as Maltese "ndammhomli" to "tindammhiex") and unsuccessfully ("tqo˙ z˙ zx" to "qa˙ z˙ zejtx"). In contrast, the attentional encoder- decoder model is more conservative in its changes, likely because it is less effective in learning an ab- stracted representation for the lemma, and instead copies characters directly from the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this work, we propose a multi-space variational encoder-decoder framework for labeled sequence transduction problem. The MSVED performs well in the task of morphological reinflection, outper- forming the state of the art, and further improv- ing with the addition of external unlabeled data. Future work will adapt this framework to other sequence transduction scenarios such as machine translation, dialogue generation, question answer- ing, where continuous and discrete latent variables can be abstracted to guide sequence generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Standard supervised labeled sequence transduction, and our proposed semi-supervised method.</figDesc><graphic url="image-2.png" coords="1,401.17,261.92,105.00,105.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical models of (a) VAE, (b) labeled MSVAE, (c) MSVAE, (d) labeled MSVED, and (e) MSVED. White circles are latent variables and shaded circles are observed variables. Dashed lines indicate the inference process while the solid lines indicate the generative process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model architecture for labeled and unlabeled data. For the encoder-decoder model, only one direction from the source to target is given. The classification model is not illustrated in the diagram.</figDesc><graphic url="image-6.png" coords="6,74.45,371.96,213.27,97.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance on test data w.r.t. the percentage of suffixing inflection. Points with the same x-axis value correspond to the same language results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Two examples of attention weights on target linguistic labels: Arabic (Left) and Navajo (Right). When a tag equals None, it means the word does not have this tag.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of latent variables z for Maltese with 35 pseudo-lemma groups in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Percentage of inflected word forms that have mod-
ified each part of the lemma (Cotterell et al., 2016) (some 
words can be inflected zero or multiple times, thus sums may 
not add to 100%). 

</table></figure>

			<note place="foot" n="2"> Faruqui et al. (2016) have attempted a limited form of semi-supervised learning by re-ranking with a standard ngram language model, but this is not integrated with the learning process for the neural model and gains are limited. 3 Analogous to multi-space hidden Markov models (Tokuda et al., 2002)</note>

			<note place="foot" n="4"> Kingma et al. (2014) solve this problem by marginalizing over all labels, but this is infeasible in our case where we have an exponential number of label combinations. 5 The Gumbel (0,1) distribution can be sampled by first drawing u ∼ Uniform(0,1) and computing g = − log(− log(u)).</note>

			<note place="foot" n="6"> Task 1 is inflection of a lemma word and task 2 is reinflection but also provides the source word labels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Jiatao Gu, Xuezhe Ma, Zihang Dai and Pengcheng Yin for their helpful discus-sions. This work has been supported in part by an Amazon Academic Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ehu at the sigmorphon 2016 shared task. a simple proposal: Grapheme-to-phoneme for inflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iñaki</forename><surname>Alegria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izaskun</forename><surname>Etxeberria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Meeting of SIGMORPHON</title>
		<meeting>the 2016 Meeting of SIGMORPHON</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7610</idno>
		<title level="m">Learning stochastic recurrent networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Proceedings of CoNLL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Translating into morphologically rich languages with synthetic phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The sigmorphon 2016 shared taskmorphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hulden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adapting morphology for arabic information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Arabic Computational Morphology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="245" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05908</idno>
		<title level="m">Tutorial on variational autoencoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised learning of complete morphological paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1185" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Variational recurrent auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otto</forename><surname>Fabius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joost R Van Amersfoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6581</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Morphological inflection generation using character sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="634" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Statistical theory of extreme values and some practical applications: a series of lectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>Gumbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>Lieblein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1954" />
			<publisher>US Government Printing Office Washington</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04558</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural multi-source morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06027</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Med: The lmu system for the sigmorphon 2016 shared task on morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</title>
		<meeting>the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Singlemodel encoder-decoder with explicit morphological representation for reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic parsing with semi-supervised sequential autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">`</forename><surname>Kočisk`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auxiliary deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Søren Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A* sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3086" to="3094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Morphological reinflection via discriminative string transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Nicolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Hauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Arnaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Meeting of SIGMORPHON</title>
		<meeting>the 2016 Meeting of SIGMORPHON</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Morphological reinflection with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ostling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology page 23</title>
		<meeting>the 14th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology page 23</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Controlling politeness in neural machine translation via side constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of The North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the 2016 Conference of The North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The columbia university-new york university abu dhabi sigmorphon 2016 morphological reinflection shared task submission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Taji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Eskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Meeting of SIGMORPHON</title>
		<meeting>the 2016 Meeting of SIGMORPHON</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-space probability distribution hmm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiichi</forename><surname>Tokuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Masuko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noboru</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takao</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE TRANSACTIONS on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="464" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Applying morphology generation models to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achim</forename><surname>Ruopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="514" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Variational neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
