<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An End-to-end Approach for Handling Unknown Slot Values in Dialogue State Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 1448</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Mobvoi AI Lab</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An End-to-end Approach for Handling Unknown Slot Values in Dialogue State Tracking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1448" to="1457"/>
							<date type="published">July 15-20, 2018. 2018. 1448</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We highlight a practical yet rarely discussed problem in dialogue state tracking (DST), namely handling unknown slot values. Previous approaches generally assume predefined candidate lists and thus are not designed to output unknown values , especially when the spoken language understanding (SLU) module is absent as in many end-to-end (E2E) systems. We describe in this paper an E2E architecture based on the pointer network (PtrNet) that can effectively extract unknown slot values while still obtains state-of-the-art accuracy on the standard DSTC2 benchmark. We also provide extensive empirical evidence to show that tracking unknown values can be challenging and our approach can bring significant improvement with the help of an effective feature dropout technique.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A dialogue state tracker is a core component in most of today's spoken dialogue systems (SDS). The goal of dialogue state tracking (DST) is to monitor the user's intentional states during the course of the conversation, and provide a compact representation, often called the dialogue states, for the dialogue manager (DM) to decide the next ac- tion to take.</p><p>In task-oriented dialogues, or slot-filling dia- logues in the simplistic form, the dialogue agent is tasked with helping the user achieve simple goals such as finding a restaurant or booking a train ticket. As the name itself suggests, a slot-filling dialogue is composed of a predefined set of slots that need to be filled through the conversation. The dialogue states in this case are therefore the values of these slot variables, which are essentially the search constraints the DM has to maintain in order to perform the database lookup.</p><p>Traditionally in the research community, as ex- emplified in the dialogue state tracking challenge (DSTC) ( , which has be- come a standard evaluation framework for DST research, the dialogues are usually constrained by a fixed domain ontology, which essentially de- scribes in detail all the possible values that each predefined slot can take. Having access to such an ontology can simplify the tracking problem in many ways, however, in many of the SDS appli- cations we have built in the industry, such an on- tology was not obtainable. Oftentimes, the back- end databases are only exposed through an exter- nal API, which is owned and maintained by our partners. It is usually not possible to gain access to their data or enumerate all possible slot val- ues in their knowledge base. Even if such lists or dictionaries exist, they can be very large in size and highly dynamic (e.g. new songs added, new restaurants opened etc.). It is therefore not ami- able to many of the previously introduced DST approaches, which generally rely on classification over a fixed ontology or scoring each slot value pairs separately by enumerating the candidate list.</p><p>In this paper, we will therefore focus on this par- ticular aspect of the DST problem which has rarely been discussed in the community -namely how to perform state tracking in the absence of a compre- hensive domain ontology and how to handle un- known slot values effectively.</p><p>It is worth noting that end-to-end (E2E) mod- eling for task-oriented dialogue systems has be- come a popular trend <ref type="bibr" target="#b25">(Williams and Zweig, 2016;</ref><ref type="bibr" target="#b27">Zhao and Eskenazi, 2016;</ref>, although most of them focus on E2E policy learning and language gener- ation, and still rely on explicit dialogue states in their models. While fully E2E approaches which completely obviate explicit DST have been at- tempted <ref type="bibr" target="#b0">(Bordes and Weston, 2016;</ref><ref type="bibr">Eric and Manning, 2017a,b;</ref><ref type="bibr" target="#b1">Dhingra et al., 2017)</ref>, their gener- ality and scalability in real world applications re- mains to be seen. In reality, a dedicated DST com- ponent remains a central piece to most dialogue systems, even in most of the proclaimed E2E mod- els. E2E approaches for DST, i.e. joint modeling of SLU and DST has also been presented in the lit- erature ( <ref type="bibr">Henderson et al., 2014b,c;</ref><ref type="bibr" target="#b14">Mrksic et al., 2015;</ref><ref type="bibr" target="#b28">Zilka and Jurcicek, 2015;</ref><ref type="bibr" target="#b16">Perez and Liu, 2017;</ref>. In these methods, the conventional practice of having a separate spoken language understanding (SLU) module is replaced by various E2E architectures that couple SLU and DST altogether. They are sometimes called word based state tracking as the dialogue states are de- rived directly from word sequences as opposed to SLU outputs. In the absence of SLU to gener- ate value candidates, most E2E trackers today can only operate with fixed value sets. To address this limitation, we introduce an E2E tracker that al- lows us to effectively handle unknown value sets. The proposed solution is based on the recently in- troduced pointer network (PtrNet) ( <ref type="bibr" target="#b20">Vinyals et al., 2015)</ref>, which essentially performs state tracking in an extractive fashion similar to the sequence label- ing techniques commonly utilized for slot tagging in SLU ( <ref type="bibr" target="#b19">Tur and Mori, 2011</ref>).</p><p>Our proposed technique is similar in spirit as the recent work in ( <ref type="bibr" target="#b17">Rastogi et al., 2018)</ref>, which also targets the problem of unbounded and dy- namic value sets. They introduce a sophisticated candidate generation strategy followed by a neural network based scoring mechanism for each can- didate. Despite the similarity in the motivation, their system relies on SLU to generate value candi- dates, resulting in an extra module to maintain and potential error propagation as commonly faced by pipelined systems.</p><p>The contributions of this paper are three-folds: Firstly, we target a very practical yet rarely investi- gated problem in DST, namely handling unknown slot values in the absence of a predefined ontol- ogy. Secondly, we describe a novel E2E architec- ture without SLU based on the PtrNet to perform state tracking. Thirdly, we also introduce an effec- tive dropout technique for training the proposed model which drastically improves the recall rate of unknown slot values.</p><p>The rest of the paper is structured as follows: We give a brief review of related work in the field in Section 2 and point out its limitations. The Ptr- Net and its proposed application in DST are de- scribed in Section 3. In Section 4, we demon- strate some caveats regarding the use of PtrNet and propose an additional classification module as a complementary component. The targeted dropout technique, which can be essential for generaliza- tion on some datasets, is described in Section 5. Experimental setup and results are presented in Section 6, followed by conclusions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dialogue State Tracking</head><p>In DSTC tasks, the dialogue states are defined as a set of search constraints (i.e. informable slots or goals) the user specified through the dialogue and a set of attribute questions regarding the search results (i.e. requestable slots or requests). The DST component is expected to track the values of the aforementioned slots taking into account the current user utterance as well as the entire dialogue context. As mentioned in the previous section, the values each slot variable can take are specified beforehand through an ontology. This is a hidden assumption that previous techniques usually rely upon implic- itly and also what motivates our work in this paper.</p><p>Discriminative DST While generative models aiming at modeling the joint distribution of di- alogue states and miscellaneous evidences have been a popular modeling choice for DST for many years, the scalability issue resulting from large state spaces has limited the broader application of this family of models, despite the success of vari- ous approximation techniques.</p><p>The discriminative methods, on the other hand, directly model the posterior distribution of dialogue states given the evidences accumulated through the conversation history. Models such as maximum entropy ( <ref type="bibr" target="#b13">Metallinou et al., 2013)</ref> and particularly the more recent deep learning based models ( <ref type="bibr">Henderson et al., 2014b,c;</ref><ref type="bibr" target="#b28">Zilka and Jurcicek, 2015;</ref><ref type="bibr" target="#b14">Mrksic et al., 2015</ref><ref type="bibr" target="#b16">Perez and Liu, 2017)</ref> have demonstrated state-of-the-art results on public benchmarks. Such techniques often involve a multi-class classification step at the end (e.g. in the form of a softmax layer) which for each slot predicts the corresponding value based on the dialogue history. Sometimes the multi-class classification is replaced by a binary prediction that decides whether a particular slot value pair was expressed by the user, and the list of candidates comes from either a fixed ontology or the SLU output.</p><p>E2E DST Previous work has also investigated joint modeling strategies merging SLU and DST altogether. In this line of work, the SLU module is removed from the standard SDS architecture, re- sulting in reduced development cost and alleviat- ing the error propagation problem commonly af- fecting cascaded systems.</p><p>In the absence of SLU providing fine-grained semantic features, the E2E approaches these days typically rely on variants of neural networks such as recurrent neural networks (RNN) or memory networks ( <ref type="bibr" target="#b23">Weston et al., 2014</ref>) to automatically learn features from the raw dialogue history. The deep learning based techniques cited in the previous subsection generally fall into this category.</p><p>Current Limitations In short, most of the previ- ous DST approaches, particularly E2E ones, are not designed to handle slot values that are not known to the tracker.</p><p>As we have described in the introduction, the assumption that a predefined ontology exists for the dialogue and one can enumerate all possible values for each slot is often not valid in real world scenarios. Such an assumption has implicitly in- fluenced many design choices of previous sys- tems. The methods based on classification or scor- ing each slot value pair separately can be very dif- ficult to apply when the set of slot values is not enumerable, either due to its size or its constantly changing nature, especially in E2E models where there is no SLU module to generate an enumerable candidate list for the tracker.</p><p>It is important to point out the difference be- tween unseen states and unknown states, as pre- vious work has tried to address the problem of unseen slot values, i.e. values that were not ob- served during training. E2E approaches in par- ticular, frequently employ a featurization strategy called delexicalization, which replaces slots and values mentioned in the dialogue text with generic labels. Such a conversion allows the models to generalize much better to new values that are in- frequent or unseen in the training data. However, such slot values are still expected to be known to the tracker, either through a predefined value set or provided by SLU, otherwise the delexicalization cannot be performed, nor can the classifier prop- erly output such values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pointer Network</head><p>In this section, we briefly introduce the Ptr- Net ( <ref type="bibr" target="#b20">Vinyals et al., 2015)</ref>, which is the main basis of the proposed technique, and how the DST prob- lem can be reformulated to take advantage of the flexibility enabled by such a model.</p><p>In the PtrNet architecture, similar as other sequence-to-sequence (seq2seq) models, there is an encoder which takes the input and iteratively produces a sequence of hidden states correspond- ing to the feature vector at each input position. There is also a decoder which generates outputs with the help of the weighted encoded states where the weights are computed through attention. Here, instead of using softmax to predict the distribution over a set of predefined candidates, the decoder directly normalizes the attention score at each po- sition and obtains an output distribution over the input sequence. The index of the maximum prob- ability is the pointed position, and the correspond- ing element is selected as decoder output, which is then fed into next decoding step. Both the en- coder and decoder are based on various RNN mod- els, capable of dealing with sequences of variable length.</p><p>The PtrNet specifically targets the problems where the output corresponds to positions in the input sequence, and it is widely used for seq2seq tasks where some kind of copying from the input is needed. Among its various applications, ma- chine comprehension (a form of question answer- ing), such as in ( <ref type="bibr" target="#b21">Wang and Jiang, 2016)</ref>, is the closest to how we apply the model to DST.</p><p>The output of DST, same as in machine compre- hension, is a word segment in the input sequence most of the time, thus can be naturally formu- lated as a pointing problem. Instead of generating longer output sequences, the decoder only has to predict the starting index and the ending index in order to identify the word segment.</p><p>More specifically, words are mapped to embed-dings and the dialogue history w 0 , w 1 , ..., w t up to the current turn t is bidirectionally encoded using LSTM models. To differentiate words spoken by the user versus by the system, the word embed- dings are further augmented with speaker role in- formation. Other features, such as the entity type of each word, can also be fed into the encoder si- multaneously in order to extract richer information from the dialogue context. The encoded state at each position can then be denoted as h i , which is the concatenation of for- ward state and backward state</p><formula xml:id="formula_0">([h f i , h b i ])</formula><p>. The final forward state h f t is used as the initial hidden state of the decoder. We use a special symbol denoting the type of slot (e.g. &lt;food&gt;) as the first decoder input, which is also mapped to a trainable embed- ding E type . Therefore, the starting index s 0 of the slot value is computed as the following, where u 0 i is the attention score of the i th word in the input against the decoder state d 0 .</p><formula xml:id="formula_1">d 0 = LST M (h f t , E type ) u 0 i = v T tanh(W h h i + W d d 0 ) a 0 i = exp(u 0 i )/ t j=0 exp(u 0 j ) s 0 = arg max i a 0 i</formula><p>The attention scores at the second decoding step are computed similarly as below, where E w s 0 is the embedding of the word at the selected start- ing position, and the ending position s 1 can be ob- tained in the same way as s 0 .</p><formula xml:id="formula_2">d 1 = LST M (d 0 , E w s 0 ) u 1 i = v T tanh(W h h i + W d d 1 )</formula><p>Note that there is no guarantee that s 1 &gt; s 0 , al- though most of the time the model is able to iden- tify consistent patterns in the data and therefore output reasonable word segments. When s 1 &lt; s 0 , it is often a good indication that the answer does not exist in the input (such as the none slot in DSTC2). 1 Depending on the nature of the task, it is certainly possible to set a constraint at the sec- ond decoding step, forcing s 1 to be larger than s 0 .</p><p>One can clearly see how the described model can handle unknown slot values -as long as they are mentioned explicitly during the dialogue, we <ref type="bibr">1</ref> It is the backoff strategy we take in our experiments on DSTC2. have a chance of finding them. Compared with previous approaches, which all require some kind of candidate lists, the proposed technique takes a different perspective on DST: For most slots in di- alogue systems, tracking up-to-date values in a di- alogue is not very different from tagging slots in a user query. While sequence labeling models such as conditional random field (CRF) has proven to be a great fit for slot tagging, the same formula- tion may as well be used for DST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Rephrasing and Non-pointable Values</head><p>Our PtrNet based architecture works by directly pinpointing in the conversation history the slot value that the user expressed in its surface form. The model is totally unaware of the different ways of referring to the same entity. Therefore, the derived dialogue states may not have canonical forms that are consistent with the values in the backend database, making it more difficult to re- trieve the correct results. A good example from the DSTC2 dataset is the price slot which can take the reference value "moderate", in the actual di- alogues however, they are frequently expressed as "moderately priced", causing problems for search- ing the database and also computing accuracy.</p><p>While such a problem can be easily remedied by an extra canonicalization step (setting dialogue states to standard forms) before performing the To give an example, in DSTC tasks, the special none value is given when the user has not specified any constraint for the slot. While this information can be easily inferred from the dialogue, it is not possible to point to any specific word segment in the sentence as the corresponding slot value. The same problem also exists for the dontcare value in DSTC, which implies that the user can accept any values for a slot constraint.</p><p>To address this issue, we add a classification component into our neural network architecture to handle non-pointable values. For each turn of the dialogue, the classifier makes a multi-class deci- sion on whether the target slot should take any of the non-pointable values (e.g. dontcare or none) or it should be processed by the PtrNet.</p><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the final forward state out of the dialogue encoder is used as the feature vector for the classification layer, which is trained with cross entropy loss and jointly with the PtrNet.</p><p>The best choice of the set of values to be han- dled by the classifier may not be obvious. In most cases both the classifier and the PtrNet are capable of extracting the correct slot value, although they both offer unique advantages over the other. Ta- ble 1 briefly summarizes the pros and cons of each model.</p><p>The proposed combined architecture, taking the best of both worlds, is similar to the pointer- generator model introduced in (See et al., 2017) for abstractive text summarization. In their ap- proach the PtrNet is also augmented with a classi- fication based word generator, and the model can choose to generate words from a predefined vo- cabulary or copy words from the input. Other classify-and-copy mechanisms have also been ex- plored in ( <ref type="bibr" target="#b4">Gu et al., 2016;</ref><ref type="bibr" target="#b5">Gulcehre et al., 2016;</ref><ref type="bibr" target="#b2">Eric and Manning, 2017a)</ref>, and demonstrated im- proved performance on various seq2seq tasks such as summarization and E2E dialogue generation. <ref type="bibr">2</ref> As we have shown in this paper, DST can also be formulated to incorporate such copying mecha- nisms, allowing itself to handle unknown slot val- ues as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Targeted Feature Dropout</head><p>Feature dropout is an effective technique to pre- vent feature co-adaption and improve model gen- eralization ( <ref type="bibr" target="#b9">Hinton et al., 2012)</ref>. It is most widely used for neural network based models but may as well be utilized for other feature based models. Targeted feature dropout however, was introduced in ( <ref type="bibr" target="#b26">Xu and Sarikaya, 2014</ref>) to address a very spe- cific co-adaptation problem in slot filling, namely insufficient training of word context features.</p><p>For slot filling, this problem often occurs when 1) the dictionary (a precompiled list of possible slot values) covers the majority of the slot values in the training data, or 2) most slot values repeat frequently resulting in insufficient tail representa- tions. In both cases, the contextual features tend to get severely under-trained and as a result the model is not able to generalize to unknown slot values that are neither in the dictionary nor ob- served in training.</p><p>The way our architecture works essentially ex- tracts slot values in the same way as in slot fill- ing, although the goal is to identify slots consider- ing the entire dialogue context rather than a (usu- ally) single user query. The same problem can also happen for DST if training data are not examined carefully. As an example, the DSTC2 task comes with a fixed ontology, it is not originally designed to track unknown slots (see the OOV rate in Ta- ble 2). Taking a closer look at the data, as shown in the histogram in <ref type="figure" target="#fig_1">Figure 2</ref>, the majority of the food type slot appears more than 10 times in the training data. As a result, the model oftentimes only learns to memorize these frequent slot val- ues, and not the contextual patterns which can be more crucial for extracting slot values not known in advance.</p><p>To alleviate the generalization issue, we adapt the targeted dropout trick to work with our neural network based architecture. Instead of randomly disabling unigram and dictionary features for CRF models as done in the original work, we randomly set to zero the input word embeddings that corre- spond to the slot values in the dialogue utterances. For example, the italian food type in DSTC2 ap- pears almost 500 times in the training data. During training, every time "italian" gets mentioned in the dialogue as the labeled user goal, we turn off the word embedding of "italian" in the model input with some probability, forcing the model to learn from the context to identify the slot value. Dic- tionary features are not used in our experiments, otherwise they can be turned off similarly.</p><p>As we will show later in the results, this proves to be a particularly effective yet simple trick for improving generalization to unknown slot values, without sacrificing accuracy for the known and ob- served ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We conduct our experiments on the DSTC2 dataset <ref type="figure" target="#fig_0">(Henderson et al., 2014a)</ref>, and on the bAbI dialogue dataset as used in <ref type="bibr" target="#b0">(Bordes and Weston, 2016)</ref>.</p><p>The DSTC2 dataset is the standard DST bench- mark comprised of real dialogues between human and dialogue systems. We are mainly interested in tracking user goals, whereas the other two compo- nents of the dialogue state, namely search methods and requested slots, are not concerned with un- known slot values, and thus are not the focus in this paper. Meanwhile, the non-pointable values, none and dontcare, constitute a significant portion in DSTC2. Overall almost 60% of the user goals <ref type="table">#food types in train  74  48  #train instances  11677  8546  #test instances  9890  9890  OOV food types in test (%)  0</ref> 30.4 are labeled as either none or dontcare, the two pre- dominant non-pointable values, it is therefore par- ticularly suitable for evaluating our proposed hy- brid architecture. An important part of our experimental evalua- tion is to demonstrate our ability to identify un- known slots. Although it happens frequently in real world situations, the original DSTC2 dataset does not suffer from this particular problem -on the test data, there are no unknown values that we have not observed in training for all of the three slot types. To conduct our investigation, we pick the food type slot to simulate unknown values. Specifically, we randomly select about 35% of the food types in the training set (26 out of 74) as un- known and discard all the training instances where the correct food type is one of the 26 unknown types that we selected. The statistics of the result- ing dataset is shown in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original New</head><p>On the other hand, the bAbI dialogue dataset is initially designed for evaluating E2E goal oriented dialogue systems and has not been used specifi- cally for DST. The model is expected to predict both the system utterances and the API calls to ac- cess the database. We notice that the parameters of the API calls are essentially the dialogue states at the point of the dialogue, it may as well be used as a dataset for measuring the accuracy of the state tracker. We therefore convert Task 5 of the bAbI dataset, which is the full dialogue combination of Tasks 1-4, into a DST dataset for our experiments.</p><p>Although simulated and with highly regular be- haviors, the nice thing about the bAbI dialogue dataset is that it comes with an out-of-vocabulary (OOV) test set in which the entities (locations and food types) are not seen in any training dialogues. This poses exactly the same problem we are try- ing to address in this paper, namely predicting the API call parameters when they are not only un- seen but also unknown to the system. Many of the previous E2E approaches simplifies the pre- diction problem as a selection among all API calls appeared in the entire dataset, thus bypassing the problem of tracking unknown dialogue states ex- plicitly, although we believe it is not a realistic simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Model and Training Details</head><p>The proposed model is implemented in Tensor- Flow. We use the provided development set to tune the hyper-parameters, track the training progress and select the best performing model for reporting the accuracy on test sets. The joint architecture is trained separately for each slot type by minimizing the sum of the cross entropy loss from the PtrNet and the classifier. Mini-batch SGD with a batch size of 50 and Adam optimizer <ref type="bibr" target="#b10">(Kingma and Ba, 2014</ref>) is used for training.</p><p>Each word is mapped to a randomly initialized 100 dimensional embedding and each dialogue in- stance is represented as a 540 * 100 dimensional vector with zero paddings on the left when neces- sary. Instead of the using the raw word sequences, the system utterances are replaced by the more succinct and consistent dialogue act representa- tions such as "request slot food". One layer of LSTM is used with a state size of 200 (additional layers did not help noticeably). Standard dropout with a keep probability of 0.5 is performed for training at the input and output of the LSTM cells. To keep it simple, targeted dropout is done only once for the entire training set before training be- gins, the dataset is therefore static across epochs.</p><p>To train the PtrNet, the location of the reference slot value in the dialogue needs to be provided. It does not require manual labeling though, and we simply use the last occurrence of the reference slot value in the dialogue history as the reference lo- cation. The occurrence is found via exact string match and the two most frequent spelling varia- tions, "moderate" and "moderately", "center" and "centre" are considered equivalent. If no occur- rence exists in a training instance (due to ASR er- rors or rephrasing), it will not be used for training the PtrNet.</p><p>On the other hand, the classifier serves as a gate- keeper that decides which slot values should be handed over to the PtrNet. On the bAbI dataset, there are zero non-pointable slots, and therefore everything is handled by the PtrNet. On DSTC2, we train the classifier to perform a three-way clas- sification that determines if the slot values is none, dontcare or other. As we have described, other slot values can also become non-pointable in the ac- tual dialogue: Those resulting from different sur- face forms are usually easier to handle, all we need is an extra post-processing step to normalize the value; The ones caused by ASR errors though, are much more challenging. One can argue that a classifier may be better equipped for these cases since it does not require locating the actual values in the word sequence, but unless there are consis- tent misrecognition patterns, they are difficult to handle for either the classifier or the PtrNet.</p><p>The non-pointable values in DSTC2, besides none and dontcare, are predominantly due to recognition errors, and we decide not to do any- thing specific about them -the PtrNet is tasked with processing these misrecognized utterances, and no normalization (except for "moderately" and "center") is performed on the network output for computing the accuracy. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation Setup</head><p>The DSTC2 dataset is a standard benchmark for the task, we therefore compare the joint goal ac- curacy (a turn is considered correct if values are predicted correctly for all slots) of the proposed model with previous reported numbers to show the efficacy of our approach under regular circum- stances, i.e. all slot values are known and observed in training. However, it is not our goal to outper- form all previous DST systems -the main theme is that our technique allows identifying unknown slot values effectively and even if used in the standard setting, our model yields state-of-the-art results.</p><p>Measuring the accuracy on unknown slot val- ues, however, does not have well-established base- lines in the literature. Most previous systems are not concerned with this problem, and many of them are inherently not capable of outputting un- known values. So instead of comparisons with previous techniques, we will focus on demonstrat- ing how this could be a serious problem tracking unknown slot values and how the targeted dropout can improve things drastically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results</head><p>The joint goal accuracy on the standard DSTC2 test set is shown in <ref type="table" target="#tab_3">Table 3</ref>   It is important to emphasize that the PtrNet model is an E2E model without using any SLU output and makes use of only the 1-best ASR hypothesis without any confidence measure for testing. Although more sophisticated DST mod- els sometimes demonstrate better accuracy, our PtrNet model holds various advantages against all baseline models: In comparison with our ap- proach, the delexicalized RNN models <ref type="bibr" target="#b7">(Henderson et al., 2014b</ref>,c) utilize the n-best list and/or the SLU output; The NBT ( ) and MemN2N (Perez and Liu, 2017) models are E2E but both depend on candidate lists as given and hence are not designed to handle unknown (differ- ent from unseen) slot values; The scalable DST model ( <ref type="bibr" target="#b17">Rastogi et al., 2018)</ref>, although address- ing the same problem of unbounded value set, re- lies on SLU to generate value candidates, and also does not perform equally well on the standard test set.</p><p>On the modified DSTC2 dataset with the re- duced training set, the accuracy of the known/seen and unknown food types is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The standard training process with no targeted dropout performs poorly when the food types are not known beforehand, epitomizing the often over- looked challenge of handling unknown slot values. With a small dropout probability of 5%, the accu- racy on unknown values essentially increases by three times (from 11.6% to 34.4%), while the ac- curacy on other values remains roughly the same.</p><p>Similar observations can also be made on the bAbI dataset predicting OOV API parameters (Ta- ble 4). While the dataset is quite artificial and in most cases we can achieve perfect accuracy on the regular test set, the OOV parameter values are not nearly as easy to predict. The targeted dropout </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regular Test</head><p>OOV Test p=0 p=0.1 p=0 p=0.1 food 100 100 86.2 100 location 100 100 74.7 99.6 <ref type="table">Table 4</ref>: Accuracy of predicting regular and OOV food and location parameters in bAbI (Task 5) API calls w/ (p=0.1) and w/o (p=0) targeted dropout.</p><p>however, allows us to bridge the accuracy gap en- tirely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>An E2E dialogue state tracker is introduced based on the pointer network. The model outputs slot values in an extractive fashion similar to the slot filling task in SLU. We also add a jointly trained classification component to combine with the pointer network, forming a hybrid architec- ture that not only achieves state-of-the-art accu- racy on the DSTC2 dataset, but also more im- portantly is able to handle unknown slot values, which is a problem often neglected although par- ticularly valuable in real world situations. A fea- ture dropout trick is also described and proves to be particularly effective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the proposed PtrNet based architecture for DST. The classifier outputs "other" indicating the decision should be made by PtrNet; The decoder (red) in PtrNet is predicting the ending word of the slot value given the predicted starting word via attention against the encoded states (blue).</figDesc><graphic url="image-1.png" coords="4,307.28,62.81,226.65,170.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Histogram of food type slot on DSTC2 training data.</figDesc><graphic url="image-2.png" coords="6,72.00,62.81,232.59,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracy of known/seen and unknown food types on the modified DSTC2 dataset with different dropout probabilities.</figDesc><graphic url="image-3.png" coords="8,307.28,62.81,228.59,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the new modified DSTC2 
dataset with unknown food types. About 27% of 
the training instances are discarded. The test set 
remains the same. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Joint goal accuracy on DSTC2 test set vs. 
various approaches as reported in the literature. 

Net based model against various previous reported 
baselines. 
</table></figure>

			<note place="foot" n="0"> The first author is now with Facebook. Qi contributed to the work during an internship at Mobvoi.</note>

			<note place="foot" n="2"> The copy-augmented model in (Eric and Manning, 2017a) also outputs API call parameters (which are essentially dialogue states) in a seq2seq fashion, including unknown parameters by copying from dialogue history, although the work focuses entirely on dialogue generation.</note>

			<note place="foot" n="3"> Non-pointable values besides none and dontcare constitute 9.7% of food, 7.6% of location and 4.7% of price on the test data, effectively setting an upper bound on the accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the anonymous reviewers for their insightful comments. We also would like to thank Mei-Yuh Hwang for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In CoRR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards end-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yun-Nung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A copyaugmented sequence-to-sequence architecture gives good performance on task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arXiv preprint arXiv:1701.04024v3 [cs.CL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Keyvalue retrieval networks for task-oriented dialogue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arXiv preprint arXiv:1705.05414v2 [cs.CL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The second dialog state tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust dialosg state tracking using delexicalised recurrent neural networks and unsupervised adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Spoken Language Technology</title>
		<meeting>IEEE Spoken Language Technology</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word based dialog state tracking with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580v1[cs.NE</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">End-to-end task-completion neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01008v3[cs.CL</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">In arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">End-to-end optimization of task-oriented dialogue model with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pararth</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arxiv preprint arXiv:1711.10712v2 [cs.CL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative state tracking for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Metallinou</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-domain dialog state tracking using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid</forename><surname>Seaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 53th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural belief tracker: Data-driven dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid</forename><surname>Seaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dialog state tracking, a machine reading approach using memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scalable multi-domain dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In arXiv preprint arXiv:1712.10224v2 [cs.CL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Spoken Language Understanding: Systems for Extracting Semantic Information from Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Pointer networks. In NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In arXiv preprint arXiv:1608.07905v2 [cs.CL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In CoRR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepak Ramachandran, and Alan Black</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Raux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2013 Conference</title>
		<meeting>the SIGDIAL 2013 Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>The dialog state tracking challenge</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">End-toend lstm-based dialog control optimized with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In arxiv preprint arXiv:1606.01269v1 [cs.CL</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Targeted feature dropout for robust slot filling in natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDIAL 2016 Conference</title>
		<meeting>SIGDIAL 2016 Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Incremental lstm-based dialog state tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Zilka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurcicek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
