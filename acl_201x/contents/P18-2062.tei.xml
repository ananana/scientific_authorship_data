<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Orthographic Features for Bilingual Lexicon Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename><surname>Riley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Rochester Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Orthographic Features for Bilingual Lexicon Induction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="390" to="394"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>390</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent embedding-based methods in bilingual lexicon induction show good results, but do not take advantage of orthographic features, such as edit distance , which can be helpful for pairs of related languages. This work extends embedding-based methods to incorporate these features, resulting in significant accuracy gains for related languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years, new methods for bilingual lexicon induction have been proposed that are ap- plicable to low-resource language pairs, for which very little sentence-aligned parallel data is avail- able. Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable.</p><p>One prevalent strategy involves creating multi- lingual word embeddings, where each language's vocabulary is embedded in the same latent space <ref type="bibr" target="#b13">(Vuli´cVuli´c and Moens, 2013;</ref><ref type="bibr" target="#b10">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b0">Artetxe et al., 2016)</ref>; however, many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary.</p><p>More recent work has focused on reducing that constraint. Vuli´c <ref type="bibr" target="#b14">Vuli´c and Moens (2016)</ref> and <ref type="bibr" target="#b12">Vulic and Korhonen (2016)</ref> use document-aligned data to learn bilingual embeddings instead of a seed dictionary. <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref> use a very small, automatically-generated seed lexicon of identi- cal numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces; <ref type="bibr" target="#b15">Zhang et al. (2017)</ref> use an adversarial training method to learn a similar mapping. <ref type="bibr" target="#b7">Lample et al. (2018a)</ref> use a series of techniques to align monolingual embedding spaces in a completely unsupervised way; their method is used by <ref type="bibr" target="#b8">Lample et al. (2018b)</ref> as the initialization for a completely unsupervised machine translation system.</p><p>These recent advances in unsupervised bilin- gual lexicon induction show promise for use in low-resource contexts. However, none of them make use of linguistic features of the languages themselves (with the arguable exception of syn- tactic/semantic information encoded in the word embeddings). This is in contrast to work that predates many of these embedding-based meth- ods that leveraged linguistic features such as edit distance and orthographic similarity: <ref type="bibr" target="#b4">Dyer et al. (2011) and</ref><ref type="bibr" target="#b2">Berg-Kirkpatrick et al. (2010)</ref> inves- tigate using linguistic features for word align- ment, and <ref type="bibr" target="#b6">Haghighi et al. (2008)</ref> use linguis- tic features for unsupervised bilingual lexicon in- duction. These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaroni- maccheroni).</p><p>The addition of linguistic features led to in- creased performance in these earlier models, es- pecially for related languages, yet these features have not been applied to more modern methods. In this work, we extend the modern embedding- based approach of <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref> with ortho- graphic information in order to leverage similari- ties between related languages for increased accu- racy in bilingual lexicon induction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>This work is directly based on the work of <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref>. Following their work, let X ∈ R |Vs|×d and Z ∈ R |Vt|×d be the word embedding matrices of two distinct languages, referred to re- spectively as the source and target, such that each row corresponds to the d-dimensional embedding of a single word. We refer to the ith row of one of these matrices as X i * or Z i * . The vocabularies for each language are V s and V t , respectively. Also let D ∈ {0, 1} |Vs|×|Vt| be a binary matrix repre- senting a dictionary such that D ij = 1 if the ith word in the source language is aligned with the jth word in the target language. We wish to find a mapping matrix W ∈ R d×d that maps source embeddings onto their aligned target embeddings. <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref> define the optimal mapping matrix W * with the following equation,</p><formula xml:id="formula_0">W * = arg min W i j D ij X i * W − Z j * 2</formula><p>which minimizes the sum of the squared Euclidean distances between mapped source embeddings and their aligned target embeddings.</p><p>By normalizing and mean-centering X and Z, and enforcing that W be an orthogonal matrix (W T W = I), the above formulation becomes equivalent to maximizing the dot product between the mapped source embeddings and target embed- dings, such that</p><formula xml:id="formula_1">W * = arg max W Tr(XW Z T D T )</formula><p>where Tr(·) is the trace operator, the sum of all di- agonal entries. The optimal solution to this equa- tion is W * = U V T , where X T DZ = U ΣV T is the singular value decomposition of X T DZ.</p><p>This formulation requires a seed dictionary. To reduce the need for a large seed dictionary, <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref> propose an iterative, self-learning framework that determines W as above, uses it to calculate a new dictionary D, and then iterates un- til convergence. In the dictionary induction step, they set</p><formula xml:id="formula_2">D ij = 1 if j = arg max k (X i * W ) · Z k * and D ij = 0 otherwise.</formula><p>We propose two methods for extending this sys- tem using orthographic information, described in the following two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Orthographic Extension of Word Embeddings</head><p>This method augments the embeddings for all words in both languages before using them in the self-learning framework of <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref>.</p><p>To do this, we append to each word's embedding a vector of length equal to the size of the union of the two languages' alphabets. Each position in this vector corresponds to a single letter, and its value is set to the count of that letter within the spelling of the word. This letter count vector is then scaled by a constant before being appended to the base word embedding. After appending, the resulting augmented vector is normalized to have magnitude 1. Mathematically, let A be an ordered set of char- acters (an alphabet), containing all characters ap- pearing in both language's alphabets:</p><formula xml:id="formula_3">A = A source ∪ A target</formula><p>Let O source and O target be the orthographic extension matrices for each language, containing counts of the characters appearing in each word w i , scaled by a constant factor c e :</p><formula xml:id="formula_4">O ij = c e · count(A j , w i ), O ∈ {O source , O target }</formula><p>Then, we concatenate the embedding matrices and extension matrices:</p><formula xml:id="formula_5">X = [X; O source ], Z = [Z; O target ]</formula><p>Finally, in the normalized embedding matrices X and Z , each row has magnitude 1:</p><formula xml:id="formula_6">X i * = X i * X i * , Z i * = Z i * Z i *</formula><p>These new matrices are used in place of X and Z in the self-learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Orthographic Similarity Adjustment</head><p>This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref>, which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of or- thographic similarity, which is a function of the normalized string edit distance of the two words.</p><p>The normalized edit distance is defined as the Levenshtein distance (L(·, ·)) <ref type="bibr" target="#b9">(Levenshtein, 1966)</ref> divided by the length of the longer word. The Lev- enshtein distance represents the minimum number of insertions, deletions, and substitutions required to transform one word into the other. The normal- ized edit distance function is denoted as NL(·, ·).</p><formula xml:id="formula_7">NL(w 1 , w 2 ) = L(w 1 , w 2 ) max(|w 1 |, |w 2 |)</formula><p>We define the orthographic similarity of two words w 1 and w 2 as log(2.0−NL(w 1 , w 2 )). These  </p><note type="other">0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Development Translation Accuracy (%) Scaling Factor English-German English-Italian English-Finnish (a) Embedding Extension 0 10 20 30 40 50 60 70 80 90 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 Development Translation Accuracy (%) Scaling Factor English-German English-Italian English-</note><formula xml:id="formula_8">S ij = c s ·log(2.0−NL(w i , w j )), w i ∈ V s , w j ∈ V t</formula><p>The vocabulary for each language is 200,000 words, so computing a similarity score for each pair would involve 40 billion edit distance cal- culations. Also, the vast majority of word pairs are orthographically very dissimilar, resulting in a normalized edit distance close to 1 and an ortho- graphic similarity close to 0, having little to no ef- fect on the overall estimated similarity. Therefore, we only calculate the edit distance for a subset of possible word pairs. Thus, the actual orthographic similarity matrix that we use is as follows:</p><formula xml:id="formula_9">S ij = S ij w i , w j ∈ symDelete(V t ,V s ,k) 0 otherwise</formula><p>This subset of word pairs was chosen using an adaptation of the Symmetric Delete spelling correction algorithm described by <ref type="bibr">Garbe (2012)</ref>, which we denote as symDelete(·,·,·). This al- gorithm takes as arguments the target vocabulary, source vocabulary, and a constant k, and identifies all source-target word pairs that are identical af- ter k or fewer deletions from each word; that is, all pairs where each is reachable from the other with no more than k insertions and k deletions. For example, the Italian-English pair moderno- modern will be identified with k = 1, and the pair tollerante-tolerant will be identified with k = 2.</p><p>The algorithm works by computing all strings formed by k or fewer deletions from each target word, stores them in a hash table, then does the same for each source word and generates source- target pairs that share an entry in the hash table.</p><p>The complexity of this algorithm can be expressed as O(|V |l k ), where V = V t ∪ V s is the combined vocabulary and l is the length of the longest word in V . This is linear with respect to the vocabu- lary size, as opposed to the quadratic complexity required for computing the entire matrix. How- ever, the algorithm is sensitive to both word length and the choice of k. In our experiments, we found that ignoring all words of length greater than 30 allowed the algorithm to complete very quickly while skipping less than 0.1% of the data. We also used small values of k (0 &lt; k &lt; 4), and used k = 1 for our final results, finding no significant benefit from using a larger value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We use the datasets used by <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref>, consisting of three language pairs: English- Italian, English-German, and English-Finnish. The English-Italian dataset was introduced in <ref type="bibr" target="#b3">Dinu and Baroni (2014)</ref>; the other datasets were created by <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref>. Each dataset includes monolingual word embeddings (trained with word2vec (Mikolov et al., 2013b)) for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictio- nary consisting only of numeral identity transla- tions (such as 2-2, 3-3, et cetera) as in <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref>. <ref type="bibr">1</ref> However, because the methods pre- sented in this work feature tunable hyperparame- ters, we use a portion of the training set as devel-  <ref type="table">Table 1</ref>: Comparison of methods on test data. Scaling constants c e and c s were selected based on performance on development data over all three language pairs. The last two rows report the results of using both methods together. opment data. <ref type="bibr">2</ref> In all experiments, a single target word is predicted for each source word, and full points are awarded if it is one of the listed correct translations. On average, the number of transla- tions for each source (non-English) word was 1.2 for English-Italian, 1.3 for English-German, and 1.4 for English-Finnish.</p><note type="other">Method English-German English-Italian English-Finnish Artetxe et al</note><formula xml:id="formula_10">Source Word Our Prediction (Language) Incorrect Baseline Prediction (Translation) caesium cäsium (German) isotope (isotope) unevenly ungleichmäßig (German) gleichmäßig (evenly) Ethiopians¨Athiopier Ethiopians¨ Ethiopians¨Athiopier (German) Afrikaner (Africans) autumn autunno (Italian) primavera (spring) Brueghel Bruegel (Italian) Dürer (Dürer) Latvians latvialaiset (Finnish) ukrainalaiset (Ukrainians)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>For our experiments with orthographic extension of word embeddings, each embedding was ex- tended by the size of the union of the alphabets of both languages. The size of this union was 199 for English-Italian, 200 for English-German, and 287 for English-Finnish. These numbers are perhaps unintuitively high. However, the corpora include many other char- acters, including diacritical markings and various symbols (%, [, !, etc.) that are an indication that tokenization of the data could be improved. We did not filter these characters in this work.</p><p>For our experiments with orthographic similar- ity adjustment, the heuristic identified approxi- mately 2 million word pairs for each language pair out of a possible 40 billion, resulting in significant computation savings. and c s = 1 as our hyperparameters. The local op- tima were not identical for all three languages, but we felt that these values struck the best compro- mise among them. <ref type="table">Table 1</ref> compares our methods against the sys- tem of <ref type="bibr" target="#b1">Artetxe et al. (2017)</ref>, using scaling factors selected based on development data results. Be- cause approximately 20% of source-target pairs in the dictionary were identical, we also extended all systems to guess the identity translation if the source word appeared in the target vocabulary. This improved accuracy in most cases, with some exceptions for English-Italian. We also experi- mented with both methods together, and found that this was the best of the settings that did not in- clude the identity translation component; with the identity component included, however, the embed- ding extension method alone was best for English- Finnish. The fact that Finnish is the only language here that is not in the Indo-European family (and has fewer words borrowed from English or its an- cestors) may explain why the performance trends for English-Finnish were different than those of the other two language pairs.</p><p>In addition to identifying orthographically sim- ilar words, the extension method is capable of learning a mapping between source and target let- ters, which could partially explain its improved performance over our edit distance method. <ref type="table" target="#tab_1">Table 2</ref> shows some correct translations from our system that were missed by the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this work, we presented two techniques (which can be combined) for improving embedding-based bilingual lexicon induction for related languages using orthographic information and no parallel data, allowing their use with low-resource lan- guage pairs. These methods increased accuracy in our experiments, with both the combined and em- bedding extension methods providing significant gains over the baseline system.</p><p>In the future, we want to extend this work to related languages with different alphabets (experi- menting with transliteration or phonetic transcrip- tion) and to extend other unsupervised bilingual lexicon induction systems, such as that of <ref type="bibr" target="#b7">Lample et al. (2018a)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance on development data vs. scaling factors c e and c s. The lowest tested value for both was 10 −6 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 shows</head><label>1</label><figDesc>Figure 1 shows the results on the development data. Based on these results, we selected c e = 1 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples of pairs correctly identified by our embedding extension method that were incorrectly 
translated by the system of Artetxe et al. (2017). Our system can disambiguate semantic clusters created 
by word2vec. 

</table></figure>

			<note place="foot" n="1"> https://github.com/artetxem/vecmap</note>

			<note place="foot" n="2"> We use all source-target pairs containing one of 1,000 randomly-selected target words.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP16)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP16)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-10)</title>
		<meeting>the 2010 Meeting of the North American chapter of the Association for Computational Linguistics (NAACL-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno>abs/1412.6568</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised word alignment with arbitrary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-11)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="409" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<ptr target="http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/.Accessed" />
		<title level="m">2012. 1000x faster spelling correction algorithm</title>
		<imprint>
			<biblScope unit="page" from="2018" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08)</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="771" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Word translation without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Binary codes capable of correcting deletions, insertions, and reversals. Cybernetics and Control Theory</title>
		<imprint>
			<date type="published" when="1965" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="845" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the role of seed lexicons in learning bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL-16)<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP-13)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP-13)<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1613" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilingual distributed word representations from documentaligned comparable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="953" to="994" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL-17)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
