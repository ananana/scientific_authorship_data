<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Hypernymy Detection with an Integrated Path-based and Distributional Method</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Ilan University Ramat-Gan</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Ilan University Ramat-Gan</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
							<email>dagan@cs.biu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Ilan University Ramat-Gan</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Hypernymy Detection with an Integrated Path-based and Distributional Method</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2389" to="2398"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches. Distributional methods, whose supervised variants are the current best performers, and path-based methods , which received less research attention. We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network , that achieves results comparable to distributional methods. We then extend the approach to integrate both path-based and distributional signals, significantly improving upon the state-of-the-art on this task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hypernymy is an important lexical-semantic rela- tion for NLP tasks. For instance, knowing that Tom Cruise is an actor can help a question an- swering system answer the question "which ac- tors are involved in Scientology?". While seman- tic taxonomies, like WordNet <ref type="bibr" target="#b5">(Fellbaum, 1998)</ref>, define hypernymy relations between word types, they are limited in scope and domain. Therefore, automated methods have been developed to deter- mine, for a given term-pair (x, y), whether y is an hypernym of x, based on their occurrences in a large corpus.</p><p>For a couple of decades, this task has been ad- dressed by two types of approaches: distributional, and path-based. In distributional methods, the de- cision whether y is a hypernym of x is based on the distributional representations of these terms. Lately, with the popularity of word embeddings <ref type="bibr" target="#b17">(Mikolov et al., 2013)</ref>, most focus has shifted to- wards supervised distributional methods, in which each (x, y) term-pair is represented using some combination of the terms' embedding vectors.</p><p>In contrast to distributional methods, in which the decision is based on the separate contexts of x and y, path-based methods base the decision on the lexico-syntactic paths connecting the joint oc- currences of x and y in a corpus. <ref type="bibr" target="#b7">Hearst (1992)</ref> identified a small set of frequent paths that indicate hypernymy, e.g. Y such as X. <ref type="bibr" target="#b29">Snow et al. (2004)</ref> represented each (x, y) term-pair as the multiset of dependency paths connecting their co-occurrences in a corpus, and trained a classifier to predict hy- pernymy, based on these features.</p><p>Using individual paths as features results in a huge, sparse feature space. While some paths are rare, they often consist of certain unimportant components. For instance, "Spelt is a species of wheat" and "Fantasy is a genre of fiction" yield two different paths: X be species of Y and X be genre of Y, while both indicating that X is-a Y. A possible solution is to generalize paths by replac- ing words along the path with their part-of-speech tags or with wild cards, as done in the PATTY sys- tem ( <ref type="bibr" target="#b19">Nakashole et al., 2012)</ref>.</p><p>Overall, the state-of-the-art path-based methods perform worse than the distributional ones. This stems from a major limitation of path-based meth- ods: they require that the terms of the pair oc- cur together in the corpus, limiting the recall of these methods. While distributional methods have no such requirement, they are usually less precise in detecting a specific semantic relation like hy- pernymy, and perform best on detecting broad se- mantic similarity between terms. Though these approaches seem complementary, there has been rather little work on integrating them ( <ref type="bibr" target="#b18">Mirkin et al., 2006;</ref><ref type="bibr" target="#b10">Kaji and Kitsuregawa, 2008)</ref>.</p><p>In this paper, we present HypeNET, an inte- grated path-based and distributional method for hypernymy detection. Inspired by recent progress in relation classification, we use a long short- term memory (LSTM) network <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) to encode dependency paths. In order to create enough training data for our net- work, we followed previous methodology of con- structing a dataset based on knowledge resources.</p><p>We first show that our path-based approach, on its own, substantially improves performance over prior path-based methods, yielding performance comparable to state-of-the-art distributional meth- ods. Our analysis suggests that the neural path rep- resentation enables better generalizations. While coarse-grained generalizations, such as replacing a word by its POS tag, capture mostly syntactic sim- ilarities between paths, HypeNET captures also semantic similarities.</p><p>We then show that we can easily integrate dis- tributional signals in the network. The integration results confirm that the distributional and path- based signals indeed provide complementary in- formation, with the combined model yielding an improvement of up to 14 F 1 points over each indi- vidual model. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We introduce the two main approaches for hyper- nymy detection: distributional (Section 2.1), and path-based (Section 2.2). We then discuss the re- cent use of recurrent neural networks in the related task of relation classification (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distributional Methods</head><p>Hypernymy detection is commonly addressed us- ing distributional methods. In these methods, the decision whether y is a hypernym of x is based on the distributional representations of the two terms, i.e., the contexts with which each term occurs sep- arately in the corpus.</p><p>Earlier methods developed unsupervised mea- sures for hypernymy, starting with symmetric sim- ilarity measures (Lin, 1998), and followed by di- rectional measures based on the distributional in- clusion hypothesis <ref type="bibr" target="#b34">(Weeds and Weir, 2003;</ref><ref type="bibr" target="#b12">Kotlerman et al., 2010)</ref>. This hypothesis states that the contexts of a hyponym are expected to be largely included in those of its hypernym. More recent work ( <ref type="bibr" target="#b27">Santus et al., 2014;</ref><ref type="bibr" target="#b25">Rimell, 2014)</ref> introduce new measures, based on the assumption that the most typical linguistic contexts of a hypernym are less informative than those of its hyponyms.</p><p>More recently, the focus of the distributional ap- proach shifted to supervised methods. In these methods, the (x, y) term-pair is represented by a feature vector, and a classifier is trained on these vectors to predict hypernymy. Several methods are used to represent term-pairs as a combination of each term's embeddings vector: concatenation x⊕ y ( <ref type="bibr" target="#b2">Baroni et al., 2012)</ref>, difference y − x ( <ref type="bibr" target="#b26">Roller et al., 2014;</ref><ref type="bibr" target="#b35">Weeds et al., 2014)</ref>, and dot-product x · y. Using neural word embeddings <ref type="bibr" target="#b17">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b23">Pennington et al., 2014</ref>), these methods are easy to apply, and show good results ( <ref type="bibr" target="#b2">Baroni et al., 2012;</ref><ref type="bibr" target="#b26">Roller et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Path-based Methods</head><p>A different approach to detecting hypernymy be- tween a pair of terms (x, y) considers the lexico- syntactic paths that connect the joint occurrences of x and y in a large corpus. Automatic acquisi- tion of hypernyms from free text, based on such paths, was first proposed by <ref type="bibr" target="#b7">Hearst (1992)</ref>, who identified a small set of lexico-syntactic paths that indicate hypernymy relations (e.g. Y such as X, X and other Y).</p><p>In a later work, <ref type="bibr" target="#b29">Snow et al. (2004)</ref> learned to de- tect hypernymy. Rather than searching for specific paths that indicate hypernymy, they represent each (x, y) term-pair as the multiset of all dependency paths that connect x and y in the corpus, and train a logistic regression classifier to predict whether y is a hypernym of x, based on these paths.</p><p>Paths that indicate hypernymy are those that were assigned high weights by the classifier. The paths identified by this method were shown to subsume those found by Hearst (1992), yield- ing improved performance. Variations of <ref type="bibr">Snow et al.'s (2004)</ref> method were later used in tasks such as taxonomy construction <ref type="bibr" target="#b30">(Snow et al., 2006;</ref><ref type="bibr" target="#b13">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b4">Carlson et al., 2010;</ref><ref type="bibr" target="#b24">Riedel et al., 2013)</ref>, analogy identification <ref type="bibr" target="#b32">(Turney, 2006</ref>), and definition extraction ( <ref type="bibr" target="#b3">Borg et al., 2009;</ref><ref type="bibr" target="#b20">Navigli and Velardi, 2010)</ref>.</p><p>A major limitation in relying on lexico- syntactic paths is the sparsity of the feature space. Since similar paths may somewhat vary at the lex- ical level, generalizing such variations into more abstract paths can increase recall. The PATTY al- gorithm <ref type="bibr" target="#b19">(Nakashole et al., 2012</ref>) applied such gen- eralizations for the purpose of acquiring a taxon- omy of term relations from free text. For each path, they added generalized versions in which a subset of words along the path were replaced by either their POS tags, their ontological types or wild-cards. This generalization increased recall while maintaining the same level of precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">RNNs for Relation Classification</head><p>Relation classification is a related task whose goal is to classify the relation that is expressed between two target terms in a given sentence to one of pre- defined relation classes. To illustrate, consider the following sentence, from the SemEval-2010 relation classification task dataset <ref type="bibr" target="#b8">(Hendrickx et al., 2009)</ref>:</p><formula xml:id="formula_0">"The [apples] e 1 are in the [basket] e 2 ".</formula><p>Here, the relation expressed between the target en- tities is Content − Container(e 1 , e 2 ). The shortest dependency paths between the tar- get entities were shown to be informative for this task ( <ref type="bibr" target="#b6">Fundel et al., 2007)</ref>. Recently, deep learning techniques showed good performance in capturing the indicative information in such paths.</p><p>In particular, several papers show improved per- formance using recurrent neural networks (RNN) that process a dependency path edge-by-edge. <ref type="bibr" target="#b36">Xu et al. (2015;</ref><ref type="bibr" target="#b37">2016)</ref> apply a separate long short- term memory (LSTM) network to each sequence of words, POS tags, dependency labels and Word- Net hypernyms along the path. A max-pooling layer on the LSTM outputs is used as the in- put of a network that predicts the classification. Other papers suggest incorporating additional net- work architectures to further improve performance <ref type="bibr" target="#b21">(Nguyen and Grishman, 2015;</ref><ref type="bibr" target="#b16">Liu et al., 2015)</ref>.</p><p>While relation classification and hypernymy de- tection are both concerned with identifying se- mantic relations that hold for pairs of terms, they differ in a major respect. In relation classification the relation should be expressed in the given text, while in hypernymy detection, the goal is to rec- ognize a generic lexical-semantic relation between terms that holds in many contexts. Accordingly, in relation classification a term-pair is represented by a single dependency path, while in hypernymy detection it is represented by the multiset of all de- pendency paths in which they co-occur in the cor- pus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LSTM-based Hypernymy Detection</head><p>We present HypeNET, an LSTM-based method for hypernymy detection. We first focus on im- proving path representation (Section 3.1), and then integrate distributional signals into our network, resulting in a combined method (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Path-based Network</head><p>Similarly to prior work, we represent each depen- dency path as a sequence of edges that leads from x to y in the dependency tree. <ref type="bibr">2</ref> Each edge contains the lemma and part-of-speech tag of the source node, the dependency label, and the edge direction between two subsequent nodes. We denote each edge as lemma/P OS/dep/dir. See <ref type="figure" target="#fig_0">figure 1</ref> for an illustration.</p><p>Rather than treating an entire dependency path as a single feature, we encode the sequence of edges using a long short-term memory (LSTM) network. The vectors obtained for the different paths of a given (x, y) pair are pooled, and the re- sulting vector is used for classification. <ref type="figure" target="#fig_1">Figure 2</ref> depicts the overall network structure, which is de- scribed below.</p><p>Edge Representation We represent each edge by the concatenation of its components' vectors:</p><formula xml:id="formula_1">v e = [ v l , v pos , v dep , v dir ]</formula><p>where v l , v pos , v dep , v dir represent the embedding vectors of the lemma, part-of-speech, dependency label and dependency direction (along the path from x to y), respectively.</p><p>Path Representation For a path p composed of edges e 1 , ..., e k , the edge vectors v e 1 , ..., v e k are fed in order to an LSTM encoder, resulting in a vector o p representing the entire path p. The LSTM architecture is effective at capturing tem- poral patterns in sequences. We expect the train- ing procedure to drive the LSTM encoder to focus on parts of the path that are more informative for the classification task while ignoring others.  Term-Pair Classification Each (x, y) term-pair is represented by the multiset of lexico-syntactic paths that connected x and y in the corpus, de- noted as paths(x, y), while the supervision is given for the term pairs. We represent each (x, y) term-pair as the weighted-average of its path vec- tors, by applying average pooling on its path vec- tors, as follows:</p><formula xml:id="formula_2">X/NOUN/nsubj/&gt; be/VERB/ROOT/-Y/NOUN/attr/&lt; X/NOUN/dobj/&gt; define/VERB/ROOT/- Y/NOUN/pobj/&lt;</formula><formula xml:id="formula_3">v xy = v paths(x,y) = p∈paths(x,y) f p,(x,y) · o p p∈paths(x,y) f p,(x,y) (1)</formula><p>where f p,(x,y) is the frequency of p in paths(x, y). We then feed this path vector to a single-layer net- work that performs binary classification to decide whether y is a hypernym of x.</p><formula xml:id="formula_4">c = sof tmax(W · v xy )<label>(2)</label></formula><p>c is a 2-dimensional vector whose components sum to 1, and we classify a pair as positive if</p><formula xml:id="formula_5">c[1] &gt; 0.5.</formula><p>Implementation Details To train the network, we used PyCNN. <ref type="bibr">3</ref> We minimize the cross en- tropy loss using gradient-based optimization, with mini-batches of size 10 and the Adam update rule ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>). Regularization is applied by a dropout on each of the components' embed- dings. We tuned the hyper-parameters (learning rate and dropout rate) on the validation set (see the appendix for the hyper-parameters values).</p><p>We initialized the lemma embeddings with the pre-trained GloVe word embeddings ( <ref type="bibr" target="#b23">Pennington et al., 2014</ref>), trained on Wikipedia. We tried both <ref type="bibr">3</ref> https://github.com/clab/cnn the 50-dimensional and 100-dimensional embed- ding vectors and selected the ones that yield bet- ter performance on the validation set. <ref type="bibr">4</ref> The other embeddings, as well as out-of-vocabulary lemmas, are initialized randomly. We update all embedding vectors during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integrated Network</head><p>The network presented in Section 3.1 classifies each (x, y) term-pair based on the paths that con- nect x and y in the corpus. Our goal was to im- prove upon previous path-based methods for hy- pernymy detection, and we show in Section 6 that our network indeed outperforms them. Yet, as path-based and distributional methods are con- sidered complementary, we present a simple way to integrate distributional features in the network, yielding improved performance.</p><p>We extended the network to take into account distributional information on each term. In- spired by the supervised distributional concatena- tion method ( <ref type="bibr" target="#b2">Baroni et al., 2012</ref>), we simply con- catenate x and y word embeddings to the (x, y) feature vector, redefining v xy :</p><formula xml:id="formula_6">v xy = [ v wx , v paths(x,y) , v wy ]<label>(3)</label></formula><p>where v wx and v wy are x and y's word embed- dings, respectively, and v paths(x,y) is the averaged path vector defined in equation 1. This way, each (x, y) pair is represented using both the distribu- tional features of x and y, and their path-based features.  <ref type="table">WordNet instance hypernym, hypernym  DBPedia  type  Wikidata  subclass of, instance of  Yago  subclass of   Table 1</ref>: Hypernymy relations in each resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Creating Instances</head><p>Neural networks typically require a large amount of training data, whereas the existing hypernymy datasets, like BLESS (Baroni and Lenci, 2011), are relatively small. Therefore, we followed the common methodology of creating a dataset us- ing distant supervision from knowledge resources All instances in our dataset, both positive and negative, are pairs of terms that are directly re- lated in at least one of the resources. These re- sources contain thousands of relations, some of which indicate hypernymy with varying degrees of certainty. To avoid including questionable relation types, we consider as denoting positive examples only indisputable hypernymy relations <ref type="table">(Table 1)</ref>, which we manually selected from the set of hyper- nymy indicating relations in <ref type="bibr" target="#b28">Shwartz et al. (2015)</ref>.</p><p>Term-pairs related by other relations (including hyponymy), are considered as negative instances. Using related rather than random term-pairs as negative instances tests our method's ability to dis- tinguish between hypernymy and other kinds of semantic relatedness. We maintain a ratio of 1:4 positive to negative pairs in the dataset.</p><p>Like <ref type="bibr" target="#b29">Snow et al. (2004)</ref>, we include only term- pairs that have joint occurrences in the corpus, re- quiring at least two different dependency paths for each pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Random and Lexical Dataset Splits</head><p>As our primary dataset, we perform standard ran- dom splitting, with 70% train, 25% test and 5% validation sets.</p><p>As pointed out by <ref type="bibr">Levy</ref>   to perform "lexical memorization", i.e., instead of learning a relation between the two terms, they mostly learn an independent property of a single term in the pair: whether it is a "prototypical hy- pernym" or not. For instance, if the training set contains term-pairs such as (dog, animal), (cat, animal), and (cow, animal), all annotated as posi- tive examples, the algorithm may learn that animal is a prototypical hypernym, classifying any new (x, animal) pair as positive, regardless of the relation between x and animal.  sug- gested to split the train and test sets such that each will contain a distinct vocabulary ("lexical split"), in order to prevent the model from overfitting by lexical memorization.</p><p>To investigate such behaviors, we present re- sults also for a lexical split of our dataset. In this case, we split the train, test and validation sets such that each contains a distinct vocabulary. We note that this differs from , who split only the train and the test sets, and dedicated a subset of the train for validation. We chose to devi- ate from  because we noticed that when the validation set contains terms from the train set, the model is rewarded for lexical mem- orization when tuning the hyper-parameters, con- sequently yielding suboptimal performance on the lexically-distinct test set. When each set has a dis- tinct vocabulary, the hyper-parameters are tuned to avoid lexical memorization and are likely to perform better on the test set. We tried to keep roughly the same 70/25/5 ratio in our lexical split. <ref type="bibr">5</ref> The sizes of the two datasets are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Indeed, training a model on a lexically split dataset may result in a more general model, that can better handle pairs consisting of two unseen terms during inference. However, we argue that in the common applied scenario, the inference in- volves an unseen pair (x, y), in which x and/or y have already been observed separately. Models trained on a random split may introduce the model with a term's "prior probability" of being a hyper- nym or a hyponym, and this information can be exploited beneficially at inference time. <ref type="table">Table 3</ref>: Example generalizations of X was established as Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>path</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X/NOUN/dobj/&gt; establish/VERB/ROOT/-as/ADP/prep/&lt; Y/NOUN/pobj/&lt; X/NOUN/dobj/&gt; VERB as/ADP/prep/&lt; Y/NOUN/pobj/&lt; X/NOUN/dobj/&gt; * as/ADP/prep/&lt; Y/NOUN/pobj/&lt; X/NOUN/dobj/&gt; establish/VERB/ROOT/-ADP Y/NOUN/pobj/&lt; X/NOUN/dobj/&gt; establish/VERB/ROOT/-* Y/NOUN/pobj/&lt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baselines</head><p>We compare HypeNET with several state-of-the- art methods for hypernymy detection, as described in Section 2: path-based methods (Section 5.1), and distributional methods (Section 5.2). Due to different works using different datasets and cor- pora, we replicated the baselines rather than com- paring to the reported results. We use the Wikipedia dump from May 2015 as the underlying corpus of all the methods, and parse it using spaCy. <ref type="bibr">6</ref> We perform model selection on the validation set to tune the hyper-parameters of each method. <ref type="bibr">7</ref> The best hyper-parameters are re- ported in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Path-based Methods</head><p>Snow We follow the original paper, and extract all shortest paths of four edges or less between terms in a dependency tree. Like <ref type="bibr" target="#b29">Snow et al. (2004)</ref>, we add paths with "satellite edges", i.e., single words not already contained in the depen- dency path, which are connected to either X or Y, allowing paths like such Y as X. The number of distinct paths was 324,578. We apply χ 2 feature selection to keep only the 100,000 most informa- tive paths and train a logistic regression classifier.</p><p>Generalization We also compare our method to a baseline that uses generalized dependency paths. Following PATTY's approach to general- izing paths <ref type="bibr" target="#b19">(Nakashole et al., 2012)</ref>, we replace edges with their part-of-speech tags as well as with wild cards. We generate the powerset of all possi- ble generalizations, including the original paths. See <ref type="table">Table 3</ref> for examples. The number of features after generalization went up to 2,093,220. Simi- larly to the first baseline, we apply feature selec- tion, this time keeping the 1,000,000 most infor- mative paths, and train a logistic regression classi- fier over the generalized paths. <ref type="bibr">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Distributional Methods</head><p>Unsupervised SLQS ( <ref type="bibr" target="#b27">Santus et al., 2014</ref>) is an entropy-based measure for hypernymy detec- tion, reported to outperform previous state-of- the-art unsupervised methods ( <ref type="bibr" target="#b34">Weeds and Weir, 2003;</ref><ref type="bibr" target="#b12">Kotlerman et al., 2010)</ref>. The original paper was evaluated on the BLESS dataset ( <ref type="bibr" target="#b1">Baroni and Lenci, 2011)</ref>, which consists of mostly frequent words. Applying the vanilla settings of SLQS on our dataset, that contains also rare terms, resulted in low performance. Therefore, we received as- sistance from Enrico Santus, who kindly provided the results of SLQS on our dataset after tuning the system as follows.</p><p>The validation set was used to tune the thresh- old for classifying a pair as positive, as well as the maximum number of each term's most associated contexts (N ). In contrast to the original paper, in which the number of each term's contexts is fixed to N , in this adaptation it was set to the minimum between the number of contexts with LMI score above zero and N . In addition, the SLQS scores were not multiplied by the cosine similarity scores between terms, and terms were lemmatized prior to computing the SLQS scores, significantly im- proving recall.</p><p>As our results suggest, while this method is state-of-the-art for unsupervised hypernymy de- tection, it is basically designed for classifying specificity level of related terms, rather than hy- pernymy in particular.</p><p>Supervised To represent term-pairs with distri- butional features, we tried several state-of-the-art methods: concatenation x⊕ y ( <ref type="bibr" target="#b2">Baroni et al., 2012)</ref>, difference y − x ( <ref type="bibr" target="#b26">Roller et al., 2014;</ref><ref type="bibr" target="#b35">Weeds et al., 2014)</ref>, and dot-product x · y. We downloaded sev- eral pre-trained embeddings ( <ref type="bibr" target="#b17">Mikolov et al., 2013;</ref><ref type="bibr" target="#b23">Pennington et al., 2014</ref>) of different sizes, and trained a number of classifiers: logistic regression, SVM, and SVM with RBF kernel, which was re- ported by  to perform best in this setting. We perform model selection on the val- idation set to select the best vectors, method and regularization factor (see the appendix).   <ref type="table">Table 4</ref>: Performance scores of our method compared to the path-based baselines and the state-of-the-art distributional methods for hypernymy detection, on both variations of the dataset -with lexical and random split to train / test / validation. <ref type="table">Table 4</ref> displays performance scores of HypeNET and the baselines. HypeNET Path-based is our path-based recurrent neural network model (Sec- tion 3.1) and HypeNET Integrated is our combined method (Section 3.2). Comparing the path-based methods shows that generalizing paths improves recall while maintaining similar levels of preci- sion, reassessing the behavior found in Nakas- hole et al. <ref type="formula" target="#formula_4">(2012)</ref>. HypeNET Path-based outper- forms both path-based baselines by a significant improvement in recall and with slightly lower pre- cision. The recall boost is due to better path gen- eralization, as demonstrated in Section 7.1. Regarding distributional methods, the unsuper- vised SLQS baseline performed slightly worse on our dataset. The low precision stems from its inability to distinguish between hypernyms and meronyms, which are common in our dataset, causing many false positive pairs such as (zabrze, poland) and (kibbutz, israel). We sampled 50 false positive pairs of each dataset split, and found that 38% of the false positive pairs in the random split and 48% of those in the lexical split were holonym-meronym pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In accordance with previously reported results, the supervised embedding-based method is the best performing baseline on our dataset as well.</p><p>HypeNET Path-based performs slightly better, achieving state-of-the-art results. Adding distri- butional features to our method shows that these two approaches are indeed complementary. On both dataset splits, the performance differences between HypeNET Integrated and HypeNET Path- based, as well as the supervised distributional method, are substantial, and statistically signifi- cant with p-value of 1% (paired t-test).</p><p>We also reassess that indeed supervised distri- butional methods perform worse on a lexical split ( . We further observe a similar reduction when using HypeNET, which is not a re- sult of lexical memorization, but rather stems from over-generalization (Section 7.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Qualitative Analysis of Learned Paths</head><p>We analyze HypeNET's ability to generalize over path structures, by comparing prominent indica- tive paths which were learned by each of the path- based methods. We do so by finding high-scoring paths that contributed to the classification of true- positive pairs in the dataset. In the path-based baselines, these are the highest-weighted features as learned by the logistic regression classifier. In the LSTM-based method, it is less straightforward to identify the most indicative paths. We assess the contribution of a certain path p to classification by regarding it as the only path that appeared for the term-pair, and compute its TRUE label score from the class distribution: sof tmax(W · v xy ) <ref type="bibr">[1]</ref>, set- ting</p><formula xml:id="formula_7">v xy = [ 0, o p , 0]</formula><p>. A notable pattern is that Snow's method learns specific paths, like X is Y from (e.g. Megadeth is an American thrash metal band from Los An- geles). While Snow's method can only rely on verbatim paths, limiting its recall, the generalized version of Snow often makes coarse generaliza- tions, such as X VERB Y from. Clearly, such a path is too general, and almost any verb assigned to it results in a non-indicative path (e.g. X take Y from). Efforts by the learning method to avoid such generalization, again, lower the recall. Hy- peNET provides a better midpoint, making fine- grained generalizations by learning additional se- mantically similar paths such as X become Y from and X remain Y from. See table 5 for additional example paths which illustrate these behaviors.</p><p>We also noticed that while on the random split our model learns a range of specific paths such as X is Y published (learned for e.g. Y=magazine) and X is Y produced (Y=film), in the lexical split it only learns the general X is Y path for these re- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Snow</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X/NOUN/nsubj/&gt; be/VERB/ROOT/-Y/NOUN/attr/&lt; direct/VERB/acl/&gt; Eyeball is a 1975 Italian-Spanish film directed by Umberto Lenzi X/NOUN/nsubj/&gt; be/VERB/ROOT/-Y/NOUN/attr/&lt; publish/VERB/acl/&gt;</head><p>Allure is a U.S. women's beauty magazine published monthly   lations. We note that X is Y is a rather "noisy" path, which may occur in ad-hoc contexts with- out indicating generic hypernymy relations (e.g. chocolate is a big problem in the context of chil- dren's health). While such a model may identify hypernymy relations between unseen terms, based on general paths, it is prone to over-generalization, hurting its performance, as seen in <ref type="table">Table 4</ref>. As discussed in § 4.2, we suspect that this scenario, in which both terms are unseen, is usually not com- mon enough to justify this limiting training setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Snow + Gen</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Error Analysis</head><p>False Positives We categorized the false positive pairs on the random split according to the rela- tion holding between each pair of terms in the re- sources used to construct the dataset. We grouped several semantic relations from different resources to broad categories, e.g. synonym includes also alias and Wikipedia redirection. <ref type="table" target="#tab_6">Table 6</ref> displays the distribution of semantic relations among false positive pairs. More than 20% of the errors stem from confus- ing synonymy with hypernymy, which are known to be difficult to distinguish.</p><p>An additional 30% of the term-pairs are re- versed hypernym-hyponym pairs (y is a hyponym of x). Examining a sample of these pairs suggests that they are usually near-synonyms, i.e., it is not that clear whether one term is truely more general than the other or not. For instance, fiction is an- notated in WordNet as a hypernym of story, while our method classified fiction as its hyponym. A possible future research direction might be to quite simply extend our network to classify term-pairs simultaneously to multiple semantic re- lations, as in <ref type="bibr" target="#b22">Pavlick et al. (2015)</ref>. Such a multi- class model can hopefully better distinguish be- tween these similar semantic relations.</p><p>Another notable category is hypernymy-like re- lations: these are other relations in the resources that could also be considered as hypernymy, but were annotated as negative due to our restrictive selection of only indisputable hypernymy relations from the resources (see Section 4.1). These in- clude instances like (Goethe, occupation, novelist) and <ref type="figure">(Homo, subdivisionRanks, species)</ref>.</p><p>Lastly, other errors made by the model often correspond to term-pairs that co-occur very few times in the corpus, e.g. xebec, a studio produc- ing Anime, was falsely classified as a hyponym of anime.</p><p>False Negatives We sampled 50 term-pairs that were falsely annotated as negative, and analyzed the major (overlapping) types of errors ( <ref type="table">Table 7)</ref>.</p><p>Most of these pairs had only few co-occurrences in the corpus. This is often either due to infre- quent terms (e.g. cbc.ca), or a rare sense of x in which the hypernymy relation holds (e.g. (night, Error Type % 1 low statistics 80% 2 infrequent term 36% 3 rare hyponym sense 16% 4 annotation error 8% <ref type="table">Table 7</ref>: (Overlapping) categories of false negative pairs:</p><p>(1) x and y co-occurred less than 25 times (average co- occurrences for true positive pairs is 99.7). (2) Either x or y is infrequent. <ref type="formula" target="#formula_6">(3)</ref> The hypernymy relation holds for a rare sense of x. (4) (x, y) was incorrectly annotated as positive.</p><p>play) holding for "Night", a dramatic sketch by Harold Pinter). Such a term-pair may have too few hypernymy-indicating paths, leading to clas- sifying it as negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented HypeNET: a neural-networks-based method for hypernymy detection. First, we fo- cused on improving path representation using LSTM, resulting in a path-based model that per- forms significantly better than prior path-based methods, and matches the performance of the pre- viously superior distributional methods. In partic- ular, we demonstrated that the increase in recall is a result of generalizing semantically-similar paths, in contrast to prior methods, which either make no generalizations or over-generalize paths.</p><p>We then extended our network by integrating distributional signals, yielding an improvement of additional 14 F 1 points, and demonstrating that the path-based and the distributional approaches are indeed complementary. Finally, our architecture seems straightfor- wardly applicable for multi-class classification, which, in future work, could be used to classify term-pairs to multiple semantic relations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example dependency tree of the sentence "parrot is a bird", with x=parrot and y=bird, represented in our notation as X/NOUN/nsubj/&lt; be/VERB/ROOT/Y/NOUN/attr/&gt;.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of term-pair classification. Each term-pair is represented by several paths. Each path is a sequence of edges, and each edge consists of four components: lemma, POS, dependency label and dependency direction. Each edge vector is fed in sequence into the LSTM, resulting in a path embedding vector op. The averaged path vector becomes the term-pair's feature vector, used for classification. The dashed vw x , vw y vectors refer to the integrated network described in Section 3.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>Snow et al., 2004; Riedel et al., 2013). Fol- lowing Snow et al. (2004), who constructed their dataset based on WordNet hypernymy, and aiming to create a larger dataset, we extract hypernymy relations from several resources: WordNet (Fell- baum, 1998), DBPedia (Auer et al., 2007), Wiki- data (Vrandeči´Vrandeči´c, 2012) and Yago (Suchanek et al., 2007).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : The number of instances in each dataset.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>X /NOUN/compound/&gt; NOUN * be/VERB/ROOT/- Y/NOUN/attr/&lt; base/VERB/acl/&gt;</head><label>X</label><figDesc></figDesc><table>Calico Light Weapons Inc. (CLWS) is an 
American privately held manufacturing 
company based in Cornelius, Oregon 
X/NOUN/compound/&gt; NOUN Y/NOUN/compound/&lt; 
Weston Town Council 

HypeNET 
Integrated 

X/NOUN/nsubj/&gt; be/VERB/ROOT/-Y/NOUN/attr/&lt; 
(release|direct|produce|write)/VERB/acl/&gt; 
Blinky is a 1923 American comedy 
film directed by Edward Sedgwick 
X/NOUN/compound/&gt; 
(association|co.|company|corporation| 
foundation|group|inc.|international 
|limited|ltd.)/NOUN/nsubj/&gt; 
be/VERB/ROOT/-Y/NOUN/attr/&lt; 
((create|found|headquarter 
|own|specialize)/VERB/acl/&gt;)? 

Retalix Ltd. is a software company 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Examples of indicative paths learned by each method, with corresponding true positive term-pairs from the random split test set. Hypernyms are marked red and hyponyms are marked blue.</figDesc><table>Relation 
% 
synonymy 
21.37% 
hyponymy 
29.45% 
holonymy / meronymy 
9.36% 
hypernymy-like relations 21.03% 
other relations 
18.77% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Distribution of relations holding between each pair of terms in the resources among false positive pairs.</figDesc><table></table></figure>

			<note place="foot" n="1"> Our code and data are available in: https://github.com/vered1986/HypeNET</note>

			<note place="foot" n="2"> Like Snow et al. (2004), we added for each path, additional paths containing single daughters of x or y not already contained in the path, referred by Snow et al. (2004) as &quot;satellite edges&quot;. This enables including paths like Such Y as X, in which the word &quot;such&quot; is not in the path between x and y.</note>

			<note place="foot" n="4"> Higher-dimensional embeddings seem not to improve performance, while hurting the training runtime.</note>

			<note place="foot" n="5"> The lexical split discards many pairs consisting of crossset terms.</note>

			<note place="foot" n="6"> https://spacy.io/ 7 We applied grid search for a range of values, and picked the ones that yield the highest F1 score on the validation set. 8 We also tried keeping the 100,000 most informative paths, but the performance was worse.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Omer Levy for his in-volvement and assistance in the early stage of this project and Enrico Santus for helping us by com-puting the results of SLQS ( <ref type="bibr" target="#b27">Santus et al., 2014</ref>) on our dataset.</p><p>This work was partially supported by an In-tel ICRI-CI grant, the Israel Science Foundation grant 880/12, and the German Research Founda-tion through the German-Israeli Project Coopera-tion (DIP, grant DA 1600/1-1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Best Hyper-parameters</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How we blessed distributional semantic evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</title>
		<meeting>the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entailment above the word level in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc-Quynh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Chieh</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evolutionary algorithms for definition extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Rosner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Pace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Definition Extraction</title>
		<meeting>the 1st Workshop on Definition Extraction</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Toward an architecture for neverending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Estevam R Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Relexrelation extraction using dependency parse trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Fundel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Küffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="365" to="371" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marti A Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="539" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using hidden markov random fields to combine distributional and pattern-based word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Kaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaru</forename><surname>Kitsuregawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Directional distributional similarity for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Kotlerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Zhitomirsky-Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NLE</title>
		<imprint>
			<biblScope unit="page" from="359" to="389" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A semi-supervised method to learn and construct taxonomies using the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Do supervised distributional methods really learn lexical inference relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04646</idno>
		<title level="m">A dependency-based neural network for relation classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Gregory S Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integrating pattern-based and distributional similarity methods for lexical entailment acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Shachar Mirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maayan</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geffet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING and ACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Patty: a taxonomy of relational patterns with semantic types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP and CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning word-class lattices for definition and hypernym extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Combining neural networks and log-linear models to improve relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05926</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adding semantics to data-driven paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charley</forename><surname>Beller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributional lexical entailment by topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="511" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inclusive yet selective: Supervised distributional hypernymy detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Chasing hypernyms in vector spaces with entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Santus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine Schulte Im</forename><surname>Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="38" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to exploit structured resources for lexical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">175</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning syntactic patterns for automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic taxonomy induction from heterogenous evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Similarity of semantic relations. CL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="379" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wikidata: A new platform for collaborative data collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrandeči´vrandeči´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1063" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A general framework for distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMLP</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to distinguish hypernyms and co-hyponyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Reffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2249" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improved relation classification by deep recurrent neural networks with data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03651</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
