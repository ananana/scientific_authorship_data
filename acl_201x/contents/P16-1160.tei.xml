<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
							<email>junyoung.chung@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Montréal</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">Université de Montréal CIFAR Senior Fellow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Montréal</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">Université de Montréal CIFAR Senior Fellow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Montréal</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">Université de Montréal CIFAR Senior Fellow</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1693" to="1703"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation. In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation? To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs-En-Cs, En-De, En-Ru and En-Fi-using the parallel corpora from WMT&apos;15. Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs. Furthermore , the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The existing machine translation systems have re- lied almost exclusively on word-level modelling with explicit segmentation. This is mainly due to the issue of data sparsity which becomes much more severe, especially for n-grams, when a sen- tence is represented as a sequence of characters rather than words, as the length of the sequence grows significantly. In addition to data sparsity, we often have a priori belief that a word, or its segmented-out lexeme, is a basic unit of meaning, making it natural to approach translation as map- ping from a sequence of source-language words to a sequence of target-language words.</p><p>This has continued with the more recently proposed paradigm of neural machine transla- tion, although neural networks do not suffer from character-level modelling and rather suffer from the issues specific to word-level modelling, such as the increased computational complexity from a very large target vocabulary ( <ref type="bibr" target="#b17">Jean et al., 2015;</ref><ref type="bibr" target="#b25">Luong et al., 2015b</ref>). Therefore, in this paper, we ad- dress a question of whether neural machine trans- lation can be done directly on a sequence of char- acters without any explicit word segmentation.</p><p>To answer this question, we focus on represent- ing the target side as a character sequence. We evaluate neural machine translation models with a character-level decoder on four language pairs from WMT'15 to make our evaluation as convinc- ing as possible. We represent the source side as a sequence of subwords extracted using byte-pair encoding from , and vary the target side to be either a sequence of subwords or characters. On the target side, we further design a novel recurrent neural network (RNN), called bi- scale recurrent network, that better handles multi- ple timescales in a sequence, and test it in addition to a naive, stacked recurrent neural network.</p><p>On all of the four language pairs-En-Cs, En-De, En-Ru and En-Fi-, the models with a character- level decoder outperformed the ones with a subword-level decoder. We observed a similar trend with the ensemble of each of these con- figurations, outperforming both the previous best neural and non-neural translation systems on En- Cs, En-De and En-Fi, while achieving a compara- ble result on En-Ru. We find these results to be a strong evidence that neural machine translation can indeed learn to translate at the character-level and that in fact, it benefits from doing so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>Neural machine translation refers to a recently proposed approach to machine translation ( <ref type="bibr" target="#b6">Cho et al., 2014;</ref><ref type="bibr" target="#b36">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. This approach aims at building an end-to- end neural network that takes as input a source sentence X = (x 1 , . . . , x Tx ) and outputs its trans- lation Y = (y 1 , . . . , y Ty ), where x t and y t are re- spectively source and target symbols. This neural network is constructed as a composite of an en- coder network and a decoder network.</p><p>The encoder network encodes the input sen- tence X into its continuous representation. In this paper, we closely follow the neural transla- tion model proposed in <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> and use a bidirectional recurrent neural network, which consists of two recurrent neural networks. The forward network reads the input sentence in a forward direction:</p><formula xml:id="formula_0">− → z t = − → φ (e x (x t ), − → z t−1 ),</formula><p>where e x (x t ) is a continuous embedding of the t-th input symbol, and φ is a recurrent activa- tion function. Similarly, the reverse network reads the sentence in a reverse direction (right to left):</p><formula xml:id="formula_1">← − z t = ← − φ (e x (x t ), ← − z t+1 ).</formula><p>At each loca- tion in the input sentence, we concatenate the hid- den states from the forward and reverse RNNs to form a context set C = {z 1 , . . . , z Tx } , where</p><formula xml:id="formula_2">z t = − → z t ; ← − z t .</formula><p>Then the decoder computes the conditional dis- tribution over all possible translations based on this context set. This is done by first rewrit- ing the conditional probability of a translation: log p(Y |X) = Ty t =1 log p(y t |y &lt;t , X). For each conditional term in the summation, the decoder RNN updates its hidden state by</p><formula xml:id="formula_3">h t = φ(e y (y t −1 ), h t −1 , c t ),<label>(1)</label></formula><p>where e y is the continuous embedding of a target symbol. c t is a context vector computed by a soft- alignment mechanism:</p><formula xml:id="formula_4">c t = f align (e y (y t −1 ), h t −1 , C)).<label>(2)</label></formula><p>The soft-alignment mechanism f align weights each vector in the context set C according to its relevance given what has been translated. The weight of each vector z t is computed by</p><formula xml:id="formula_5">α t,t = 1 Z e fscore(ey(y t −1 ),h t −1 ,zt) ,<label>(3)</label></formula><p>where f score is a parametric function returning an unnormalized score for z t given h t −1 and y t −1 .</p><p>We use a feedforward network with a single hid- den layer in this paper. 1 Z is a normalization con- stant: Z = Tx k=1 e fscore(ey(y t −1 ),h t −1 ,z k ) . This 1 For other possible implementations, see <ref type="bibr" target="#b24">(Luong et al., 2015a</ref>).</p><p>procedure can be understood as computing the alignment probability between the t -th target symbol and t-th source symbol. The hidden state h t , together with the previous target symbol y t −1 and the context vector c t , is fed into a feedforward neural network to result in the conditional distribution:</p><formula xml:id="formula_6">p(y t | y &lt;t , X) ∝ e f y t out (ey(y t −1 ),h t ,c t )</formula><p>. <ref type="formula">(4)</ref> The whole model, consisting of the encoder, decoder and soft-alignment mechanism, is then tuned end-to-end to minimize the negative log- likelihood using stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Towards Character-Level Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Let us revisit how the source and target sen- tences (X and Y ) are represented in neural ma- chine translation. For the source side of any given training corpus, we scan through the whole cor- pus to build a vocabulary V x of unique tokens to which we assign integer indices. A source sen- tence X is then built as a sequence of the indices of such tokens belonging to the sentence, i.e.,</p><formula xml:id="formula_7">X = (x 1 , . . . , x Tx ), where x t ∈ {1, 2, . . . , |V x |}.</formula><p>The target sentence is similarly transformed into a target sequence of integer indices.</p><p>Each token, or its index, is then transformed into a so-called one-hot vector of dimensionality |V x |. All but one elements of this vector are set to 0. The only element whose index corresponds to the token's index is set to 1. This one-hot vector is the one which any neural machine translation model sees. The embedding function, e x or e y , is simply the result of applying a linear transforma- tion (the embedding matrix) to this one-hot vector.</p><p>The important property of this approach based on one-hot vectors is that the neural network is oblivious to the underlying semantics of the to- kens. To the neural network, each and every token in the vocabulary is equal distance away from ev- ery other token. The semantics of those tokens are simply learned (into the embeddings) to maximize the translation quality, or the log-likelihood of the model. This property allows us great freedom in the choice of tokens' unit. Neural networks have been shown to work well with word tokens ( <ref type="bibr" target="#b2">Bengio et al., 2001;</ref><ref type="bibr" target="#b32">Schwenk, 2007;</ref><ref type="bibr" target="#b26">Mikolov et al., 2010)</ref> but also with finer units, such as subwords <ref type="bibr" target="#b4">Botha and Blunsom, 2014;</ref><ref type="bibr">Lu-ong et al., 2013</ref>) as well as symbols resulting from compression/encoding ( <ref type="bibr" target="#b5">Chitnis and DeNero, 2015)</ref>. Although there have been a number of previous research reporting the use of neural net- works with characters (see, e.g., <ref type="bibr" target="#b27">Mikolov et al. (2012)</ref> and <ref type="bibr" target="#b31">Santos and Zadrozny (2014)</ref>), the dom- inant approach has been to preprocess the text into a sequence of symbols, each associated with a se- quence of characters, after which the neural net- work is presented with those symbols rather than with characters.</p><p>More recently in the context of neural machine translation, two research groups have proposed to directly use characters.  proposed to represent each word not as a single integer index as before, but as a sequence of characters, and use a convolutional network followed by a highway network ( <ref type="bibr" target="#b34">Srivastava et al., 2015)</ref> to extract a con- tinuous representation of the word. This approach, which effectively replaces the embedding func- tion e x , was adopted by Costa-Jussà and Fonollosa (2016) for neural machine translation. Similarly, <ref type="bibr" target="#b22">Ling et al. (2015b)</ref> use a bidirectional recurrent neural network to replace the embedding functions e x and e y to respectively encode a character se- quence to and from the corresponding continuous word representation. A similar, but slightly differ- ent approach was proposed by <ref type="bibr" target="#b20">Lee et al. (2015)</ref>, where they explicitly mark each character with its relative location in a word (e.g., "B"eginning and "I"ntermediate).</p><p>Despite the fact that these recent approaches work at the level of characters, it is less satisfying that they all rely on knowing how to segment char- acters into words. Although it is generally easy for languages like English, this is not always the case. This word segmentation procedure can be as simple as tokenization followed by some punc- tuation normalization, but also can be as compli- cated as morpheme segmentation requiring a sep- arate model to be trained in advance <ref type="bibr" target="#b9">(Creutz and Lagus, 2005;</ref><ref type="bibr" target="#b16">Huang and Zhao, 2007)</ref>. Further- more, these segmentation 2 steps are often tuned or designed separately from the ultimate objective of translation quality, potentially contributing to a suboptimal quality.</p><p>Based on this observation and analysis, in this paper, we ask ourselves and the readers a question which should have been asked much earlier: Is it possible to do character-level translation without any explicit segmentation?</p><p>3.2 Why Word-Level Translation?</p><p>(1) Word as a Basic Unit of Meaning A word can be understood in two different senses. In the abstract sense, a word is a basic unit of mean- ing (lexeme), and in the other sense, can be un- derstood as a "concrete word as used in a sen- tence." <ref type="bibr" target="#b3">(Booij, 2012)</ref>. A word in the former sense turns into that in the latter sense via a process of morphology, including inflection, compound- ing and derivation. These three processes do al- ter the meaning of the lexeme, but often it stays close to the original meaning. Because of this view of words as basic units of meaning (either in the form of lexemes or derived form) from lin- guistics, much of previous work in natural lan- guage processing has focused on using words as basic units of which a sentence is encoded as a sequence. Also, the potential difficulty in finding a mapping between a word's character sequence and meaning 3 has likely contributed to this trend toward word-level modelling.</p><p>(2) Data Sparsity There is a further technical reason why much of previous research on ma- chine translation has considered words as a ba- sic unit. This is mainly due to the fact that ma- jor components in the existing translation systems, such as language models and phrase tables, are a count-based estimator of probabilities. In other words, a probability of a subsequence of sym- bols, or pairs of symbols, is estimated by count- ing the number of its occurrences in a training corpus. This approach severely suffers from the issue of data sparsity, which is due to a large state space which grows exponentially w.r.t. the length of subsequences while growing only lin- early w.r.t. the corpus size. This poses a great chal- lenge to character-level modelling, as any subse- quence will be on average 4-5 times longer when characters, instead of words, are used. Indeed, <ref type="bibr" target="#b38">Vilar et al. (2007)</ref> reported worse performance when the character sequence was directly used by a phrase-based machine translation system. More recently, <ref type="bibr" target="#b28">Neubig et al. (2013)</ref> proposed a method to improve character-level translation with phrase- based translation systems, however, with only a limited success.</p><p>(3) Vanishing Gradient Specifically to neural machine translation, a major reason behind the wide adoption of word-level modelling is due to the difficulty in modelling long-term dependen- cies with recurrent neural networks ( <ref type="bibr" target="#b1">Bengio et al., 1994;</ref><ref type="bibr" target="#b15">Hochreiter, 1998)</ref>. As the lengths of the sentences on both sides grow when they are repre- sented in characters, it is easy to believe that there will be more long-term dependencies that must be captured by the recurrent neural network for suc- cessful translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Why Character-Level Translation?</head><p>Why not Word-Level Translation? The most pressing issue with word-level processing is that we do not have a perfect word segmentation al- gorithm for any one language. A perfect segmen- tation algorithm needs to be able to segment any given sentence into a sequence of lexemes and morphemes. This problem is however a difficult problem on its own and often requires decades of research (see, e.g., <ref type="bibr" target="#b9">Creutz and Lagus (2005)</ref> for Finnish and other morphologically rich languages and Huang and Zhao (2007) for Chinese). There- fore, many opt to using either a rule-based tok- enization approach or a suboptimal, but still avail- able, learning based segmentation algorithm.</p><p>The outcome of this naive, sub-optimal segmen- tation is that the vocabulary is often filled with many similar words that share a lexeme but have different morphology. For instance, if we apply a simple tokenization script to an English corpus, "run", "runs", "ran" and "running" are all separate entries in the vocabulary, while they clearly share the same lexeme "run". This prevents any ma- chine translation system, in particular neural ma- chine translation, from modelling these morpho- logical variants efficiently.</p><p>More specifically in the case of neural machine translation, each of these morphological variants- "run", "runs", "ran" and "running"-will be as- signed a d-dimensional word vector, leading to four independent vectors, while it is clear that if we can segment those variants into a lexeme and other morphemes, we can model them more effi- ciently. For instance, we can have a d-dimensional vector for the lexeme "run" and much smaller vectors for "s" and"ing". Each of those variants will be then a composite of the lexeme vector (shared across these variants) and morpheme vec- tors (shared across words sharing the same suffix, for example) <ref type="bibr" target="#b4">(Botha and Blunsom, 2014</ref>). This makes use of distributed representation, which generally yields better generalization, but seems to require an optimal segmentation, which is un- fortunately almost never available.</p><p>In addition to inefficiency in modelling, there are two additional negative consequences from us- ing (unsegmented) words. First, the translation system cannot generalize well to novel words, which are often mapped to a token reserved for an unknown word. This effectively ignores any meaning or structure of the word to be incorpo- rated when translating. Second, even when a lex- eme is common and frequently observed in the training corpus, its morphological variant may not be. This implies that the model sees this specific, rare morphological variant much less and will not be able to translate it well. However, if this rare morphological variant shares a large part of its spelling with other more common words, it is de- sirable for a machine translation system to exploit those common words when translating those rare variants.</p><p>Why Character-Level Translation? All of these issues can be addressed to certain extent by directly modelling characters. Although the issue of data sparsity arises in character-level transla- tion, it is elegantly addressed by using a paramet- ric approach based on recurrent neural networks instead of a non-parametric count-based approach. Furthermore, in recent years, we have learned how to build and train a recurrent neural network that can well capture long-term dependencies by using more sophisticated activation functions, such as long short-term memory (LSTM) units (Hochre- iter and Schmidhuber, 1997) and gated recurrent units ( <ref type="bibr" target="#b6">Cho et al., 2014</ref>).  and <ref type="bibr" target="#b21">Ling et al. (2015a)</ref> re- cently showed that by having a neural network that converts a character sequence into a word vector, we avoid the issues from having many morpho- logical variants appearing as separate entities in a vocabulary. This is made possible by sharing the character-to-word neural network across all the unique tokens. A similar approach was applied to machine translation by <ref type="bibr" target="#b22">Ling et al. (2015b)</ref>.</p><p>These recent approaches, however, still rely on the availability of a good, if not optimal, segmen- tation algorithm. <ref type="bibr" target="#b22">Ling et al. (2015b)</ref> indeed states that " <ref type="bibr">[m]</ref>uch of the prior information regarding morphology, cognates and rare word translation among others, should be incorporated".</p><p>It however becomes unnecessary to consider these prior information, if we use a neural net- work, be it recurrent, convolution or their combi- nation, directly on the unsegmented character se- quence. The possibility of using a sequence of un- segmented characters has been studied over many years in the field of deep learning. For instance, <ref type="bibr" target="#b27">Mikolov et al. (2012)</ref>  <ref type="bibr">Gillick et al. (2015)</ref> further showed that it is possible to train a recurrent neural network on unicode bytes, instead of characters or words, to perform part-of-speech tagging and named entity recognition.</p><note type="other">and Sutskever et al. (2011) trained a recurrent neural network language model (RNN-LM) on character sequences. The latter showed that it is possible to generate sensible text sequences by simply sampling a character at a time from this model. More recently, Zhang et al. (2015) and Xiao and Cho (2016) successfully applied a convolutional net and a convolutional- recurrent net respectively to character-level docu- ment classification without any explicit segmenta- tion.</note><p>These previous works suggest the possibility of applying neural networks for the task of machine translation, which is often considered a substan- tially more difficult problem compared to docu- ment classification and language modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Challenges and Questions</head><p>There are two overlapping sets of challenges for the source and target sides. On the source side, it is unclear how to build a neural network that learns a highly nonlinear mapping from a spelling to the meaning of a sentence.</p><p>On the target side, there are two challenges. The first challenge is the same one from the source side, as the decoder neural network needs to sum- marize what has been translated. In addition to this, the character-level modelling on the target side is more challenging, as the decoder network must be able to generate a long, coherent sequence of characters. This is a great challenge, as the size of the state space grows exponentially w.r.t. the number of symbols, and in the case of characters, it is often 300-1000 symbols long.</p><p>All these challenges should first be framed as questions; whether the current recurrent neural networks, which are already widely used in neu- ral machine translation, are able to address these challenges as they are. In this paper, we aim at an- (a) Gating units (b) One-step processing <ref type="figure" target="#fig_2">Figure 1</ref>: Bi-scale recurrent neural network swering these questions empirically and focus on the challenges on the target side (as the target side shows both of the challenges).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Character-Level Translation</head><p>In this paper, we try to answer the questions posed earlier by testing two different types of recurrent neural networks on the target side (decoder). First, we test an existing recurrent neural net- work with gated recurrent units (GRUs). We call this decoder a base decoder.</p><p>Second, we build a novel two-layer recurrent neural network, inspired by the gated-feedback network from <ref type="bibr" target="#b7">Chung et al. (2015)</ref>, called a bi- scale recurrent neural network. We design this network to facilitate capturing two timescales, mo- tivated by the fact that characters and words may work at two separate timescales.</p><p>We choose to test these two alternatives for the following purposes. Experiments with the base decoder will clearly answer whether the existing neural network is enough to handle character-level decoding, which has not been properly answered in the context of machine translation. The alterna- tive, the bi-scale decoder, is tested in order to see whether it is possible to design a better decoder, if the answer to the first question is positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bi-Scale Recurrent Neural Network</head><p>In this proposed bi-scale recurrent neural network, there are two sets of hidden units, h 1 and h 2 . They contain the same number of units, i.e., dim(h 1 ) = dim(h 2 ). The first set h 1 models a fast-changing timescale (thereby, a faster layer), and h 2 a slower timescale (thereby, a slower layer). For each hid- den unit, there is an associated gating unit, to which we refer by g 1 and g 2 . For the descrip- tion below, we use y t −1 and c t for the previous target symbol and the context vector (see Eq. <ref type="formula" target="#formula_4">(2)</ref>), respectively.</p><p>Let us start with the faster layer. The faster layer outputs two sets of activations, a normal output h 1 t and its gated versioň h 1 t . The activation of the faster layer is computed by</p><formula xml:id="formula_8">h 1 t = tanh W h 1 e y (y t −1 ); ˇ h 1 t −1 ; ˆ h 2 t −1 ; c t ,</formula><p>wherě h 1 t −1 andˆhandˆ andˆh 2 t −1 are the gated activations of the faster and slower layers respectively. These gated activations are computed byˇh</p><formula xml:id="formula_9">byˇ byˇh 1 t = (1 − g 1 t ) h 1 t , ˆ h 2 t = g 1 t h 2 t .</formula><p>In other words, the faster layer's activation is based on the adaptive combination of the faster and slower layers' activations from the previous time step. Whenever the faster layer determines that it needs to reset, i.e., g 1 t −1 ≈ 1, the next activation will be determined based more on the slower layer's activation.</p><p>The faster layer's gating unit is computed by</p><formula xml:id="formula_10">g 1 t = σ W g 1 e y (y t −1 ); ˇ h 1 t −1 ; ˆ h 2 t −1 ; c t ,</formula><p>where σ is a sigmoid function. The slower layer also outputs two sets of acti- vations, a normal output h 2 t and its gated versioňversioň h 2 t . These activations are computed as follows:</p><formula xml:id="formula_11">h 2 t = (1 − g 1 t ) h 2 t −1 + g 1 t ˜ h 2 t , ˇ h 2 t = (1 − g 2 t ) h 2 t ,</formula><p>where˜hwhere˜ where˜h 2 t is a candidate activation. The slower layer's gating unit g 2 t is computed by</p><formula xml:id="formula_12">g 2 t =σ W g 2 (g 1 t h 1 t ); ˇ h 2 t −1 ; c t .</formula><p>This adaptive leaky integration based on the gat- ing unit from the faster layer has a consequence that the slower layer updates its activation only when the faster layer resets. This puts a soft con- straint that the faster layer runs at a faster rate by preventing the slower layer from updating while the faster layer is processing a current chunk.</p><p>The candidate activation is then computed by˜h</p><formula xml:id="formula_13">by˜ by˜h 2 t = tanh W h 2 (g 1 t h 1 t ); ˇ h 2 t −1 ; c t .<label>(5)</label></formula><p>ˇ h 2 t −1 indicates the reset activation from the pre- vious time step, similarly to what happened in the faster layer, and c t is the input from the context.</p><p>According to g 1 t h 1 t in Eq. <ref type="formula" target="#formula_13">(5)</ref>, the faster layer influences the slower layer, only when the faster layer has finished processing the current chunk and is about to reset itself (g 1 t ≈ 1). In other words, the slower layer does not receive any in- put from the faster layer, until the faster layer has quickly processed the current chunk, thereby run- ning at a slower rate than the faster layer does.</p><p>At each time step, the final output of the pro- posed bi-scale recurrent neural network is the con- catenation of the output vectors of the faster and slower layers, i.e., h 1 ; h 2 . This concatenated vector is used to compute the probability distribu- tion over all the symbols in the vocabulary, as in Eq. (4). See <ref type="figure" target="#fig_2">Fig. 1</ref> for graphical illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Settings</head><p>For evaluation, we represent a source sentence as a sequence of subword symbols extracted by byte- pair encoding (BPE, ) and a target sentence either as a sequence of BPE-based symbols or as a sequence of characters.</p><p>Corpora and Preprocessing We use all avail- able parallel corpora for four language pairs from WMT'15: En-Cs, En-De, En-Ru and En-Fi. They consist of 12.1M, 4.5M, 2.3M and 2M sentence pairs, respectively. We tokenize each corpus using a tokenization script included in Moses. <ref type="bibr">4</ref> We only use the sentence pairs, when the source side is up to 50 subword symbols long and the target side is either up to 100 subword symbols or 500 charac- ters. We do not use any monolingual corpus. For all the pairs other than En-Fi, we use newstest-2013 as a development set, and newstest- 2014 (Test 1 ) and newstest-2015 (Test 2 ) as test sets. For En-Fi, we use newsdev-2015 and newstest- 2015 as development and test sets, respectively.    </p><note type="other">e p t h Attention M o d e l Development Test1 Test2 Trgt h 1 h 2 Single Ens Single Ens Single Ens En</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models and Training</head><p>We test three models set- tings: (1) BPE→BPE, (2) BPE→Char (base) and (3) BPE→Char (bi-scale). The latter two differ by the type of recurrent neural network we use. We use GRUs for the encoder in all the settings. We used GRUs for the decoders in the first two set- tings, <ref type="formula" target="#formula_3">(1)</ref> and <ref type="formula" target="#formula_4">(2)</ref>, while the proposed bi-scale re- current network was used in the last setting, (3). The encoder has 512 hidden units for each direc- tion (forward and reverse), and the decoder has 1024 hidden units per layer.</p><p>We train each model using stochastic gradient descent with Adam ( <ref type="bibr" target="#b19">Kingma and Ba, 2014</ref>). Each update is computed using a minibatch of 128 sen- tence pairs. The norm of the gradient is clipped with a threshold 1 ( <ref type="bibr" target="#b29">Pascanu et al., 2013</ref>).</p><p>Decoding and Evaluation We use beamsearch to approximately find the most likely translation given a source sentence. The beam widths are 5 and 15 respectively for the subword-level and character-level decoders. They were chosen based on the translation quality on the development set.</p><p>The translations are evaluated using BLEU. <ref type="bibr">5</ref> Multilayer Decoder and Soft-Alignment Mech- anism When the decoder is a multilayer re- current neural network (including a stacked net- work as well as the proposed bi-scale network), the decoder outputs multiple hidden vectors- h 1 , . . . , h L for L layers, at a time. This allows an extra degree of freedom in the soft-alignment mechanism (f score in Eq. <ref type="formula" target="#formula_5">(3)</ref>). We evaluate using alternatives, including (1) using only h L (slower layer) and (2) using all of them (concatenated).</p><p>Ensembles We also evaluate an ensemble of neural machine translation models and compare its performance against the state-of-the-art phrase- based translation systems on all four language pairs. We decode from an ensemble by taking the average of the output probabilities at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Quantitative Analysis</head><p>Slower Layer for Alignment On En-De, we test which layer of the decoder should be used for computing soft-alignments. In the case of subword-level decoder, we observed no difference between choosing any of the two layers of the de- coder against using the concatenation of all the layers <ref type="figure" target="#fig_2">(Table 1 (a-b)</ref>) On the other hand, with the character-level decoder, we noticed an improve- ment when only the slower layer (h 2 ) was used for the soft-alignment mechanism (Table 1 (c-g)). This suggests that the soft-alignment mechanism benefits by aligning a larger chunk in the target with a subword unit in the source, and we use only the slower layer for all the other language pairs.</p><p>Single Models In <ref type="table">Table 1</ref>, we present a com- prehensive report of the translation qualities of (1) subword-level decoder, (2) character-level base decoder and (3) character-level bi-scale decoder, for all the language pairs. We see that the both types of character-level decoder outperform the subword-level decoder for En-Cs and En-Fi quite significantly. On En-De, the character-level base decoder outperforms both the subword-level de- coder and the character-level bi-scale decoder, validating the effectiveness of the character-level modelling. On En-Ru, among the single mod- els, the character-level decoders outperform the subword-level decoder, but in general, we observe that all the three alternatives work comparable to each other.</p><p>These results clearly suggest that it is indeed possible to do character-level translation without explicit segmentation. In fact, what we observed is that character-level translation often surpasses the translation quality of word-level translation. Of course, we note once again that our experiment is restricted to using an unsegmented character se- quence at the decoder only, and a further explo- ration toward replacing the source sentence with an unsegmented character sequence is needed.</p><p>Ensembles Each ensemble was built using eight independent models. The first observation we make is that in all the language pairs, neural ma- chine translation performs comparably to, or often better than, the state-of-the-art non-neural transla- tion system. Furthermore, the character-level de- coders outperform the subword-level decoder in all the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Qualitative Analysis</head><p>(1) Can the character-level decoder generate a long, coherent sentence? The translation in characters is dramatically longer than that in words, likely making it more difficult for a recur- rent neural network to generate a coherent sen- tence in characters. This belief turned out to be false. As shown in <ref type="figure" target="#fig_0">Fig. 2 (left)</ref>, there is no sig- nificant difference between the subword-level and character-level decoders, even though the lengths of the generated translations are generally 5-10 times longer in characters.</p><p>(2) Does the character-level decoder help with rare words? One advantage of character-level modelling is that it can model the composition of any character sequence, thereby better modelling rare morphological variants. We empirically con- firm this by observing the growing gap in the aver- age negative log-probability of words between the subword-level and character-level decoders as the frequency of the words decreases. This is shown in <ref type="figure" target="#fig_0">Fig. 2 (right)</ref> and explains one potential cause behind the success of character-level decoding in our experiments (we define diff(x, y) = x − y).</p><p>(3) Can the character-level decoder soft-align between a source word and a target charac- ter? In <ref type="figure" target="#fig_3">Fig. 3 (left)</ref>, we show an example soft- alignment of a source sentence, "Two sets of light so close to one another". It is clear that the character-level translation model well captured the alignment between the source subwords and tar- get characters. We observe that the character- level decoder correctly aligns to "lights" and "sets of" when generating a German compound word "Lichtersets" (see <ref type="figure" target="#fig_3">Fig. 3 (right)</ref> for the zoomed- in version). This type of behaviour happens simi- larly between "one another" and "einander". Of course, this does not mean that there exists an alignment between a source word and a target character. Rather, this suggests that the internal state of the character-level decoder, the base or bi- scale, well captures the meaningful chunk of char- acters, allowing the model to map it to a larger chunk (subword) in the source.</p><p>(4) How fast is the decoding speed of the character-level decoder? We evaluate the de- coding speed of subword-level base, character- level base and character-level bi-scale decoders on newstest2013 corpus (En-De) with a single Titan X GPU. The subword-level base decoder gener- ates 31.9 words per second, and the character-level base decoder and character-level bi-scale decoder generate 27.5 words per second and 25.6 words per second, respectively. Note that this is evalu- ated in an online setting, performing consecutive translation, where only one sentence is translated at a time. Translating in a batch setting could dif- fer from these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we addressed a fundamental ques- tion on whether a recently proposed neural ma- chine translation system can directly handle trans- lation at the level of characters without any word segmentation. We focused on the target side, in which a decoder was asked to generate one char- acter at a time, while soft-aligning between a tar- get character and a source subword. Our extensive experiments, on four language pairs-En-Cs, En- De, En-Ru and En-Fi-strongly suggest that it is indeed possible for neural machine translation to translate at the level of characters, and that it actu- ally benefits from doing so.</p><p>Our result has one limitation that we used sub- word symbols in the source side. However, this has allowed us a more fine-grained analysis, but in the future, a setting where the source side is also represented as a character sequence must be inves- tigated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (left) The BLEU scores on En-Cs w.r.t. the length of source sentences. (right) The difference of word negative log-probabilities between the subword-level decoder and either of the character-level base or bi-scale decoder.</figDesc><graphic url="image-27.png" coords="6,307.28,62.81,106.94,89.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>BLEU scores of the subword-level, character-level base and character-level bi-scale decoders for both single models and ensembles. The best scores among the single models per language pair are bold-faced, and those among the ensembles are underlined. When available, we report the median value, and the minimum and maximum values as a subscript and a superscript, respectively. ( * ) http: //matrix.statmt.org/ as of 11 March 2016 (constrained only). (1) Freitag et al. (2014). (2, 6) Williams et al. (2015). (3, 5) Durrani et al. (2014). (4) Haddow et al. (2015). (7) Rubino et al. (2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Alignment matrix of a test example from En-De using the BPE→Char (bi-scale) model.</figDesc><graphic url="image-29.png" coords="8,52.19,62.81,311.76,93.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> From here on, the term segmentation broadly refers to any method that splits a given character sequence into a sequence of subword symbols.</note>

			<note place="foot" n="3"> For instance, &quot;quit&quot;, &quot;quite&quot; and &quot;quiet&quot; are one editdistance away from each other but have distinct meanings.</note>

			<note place="foot" n="4"> Although tokenization is not necessary for characterlevel modelling, we tokenize the all target side corpora to make comparison against word-level modelling easier.</note>

			<note place="foot" n="5"> We used the multi-bleu.perl script from Moses.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the developers of Theano ( <ref type="bibr" target="#b37">Team et al., 2016)</ref>. We acknowledge the support of the following agencies for research funding and computing support: NSERC, Calcul Québec, Compute Canada, the Canada Research Chairs, CIFAR and Samsung. KC thanks the sup-port by Facebook, Google (Google Faculty Award 2016) and NVIDIA (GPU Center of <ref type="bibr">Excellence 2015</ref><ref type="bibr">Excellence-2016</ref>. JC thanks Orhan Firat for his con-structive feedbacks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="932" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The grammar of words: An introduction to linguistic morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Booij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variablelength word encodings for neural translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chitnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2088" to="2093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marta R Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonollosa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00810</idno>
		<title level="m">Character-based neural machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Helsinki University of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Edinburgh&apos;s phrase-based machine translation systems for wmt-14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the ACL 2014 Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<imprint>
			<pubPlace>Maria Nadejde, Philip Williams, Philipp Koehn</pubPlace>
		</imprint>
	</monogr>
	<note>et al. 2014. Eu-bridge mt: Combined machine translation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Brunk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00103</idno>
		<title level="m">Oriol Vinyals, and Amarnag Subramanya. 2015. Multilingual language processing from bytes</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The edinburgh/jhu phrase-based machine translation systems for wmt 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="126" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Fuzziness and Knowledge-Based Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chinese word segmentation: A decade review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8" to="20" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06615</idno>
		<title level="m">Character-aware neural language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Naver machine translation system for wat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoung-Gyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Seok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ki</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Asian Translation (WAT2015)</title>
		<meeting>the 2nd Workshop on Asian Translation (WAT2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02096</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04586</idno>
		<title level="m">Character-based neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8206</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haison</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Substring-based machine translation. Machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="139" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6026</idno>
		<title level="m">How to construct deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abu-matran at wmt 2015 translation task: Morphological segmentation and web crawling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miquel</forename><surname>Espla-Gomis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Ortiz</forename><surname>Ljubešic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rojas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="184" to="191" />
		</imprint>
	</monogr>
	<note>Vassilis Papavassiliou, Prokopis Prokopidis, and Antonio Toral</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continuous space language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="518" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2368" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML&apos;11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>The Theano Development Team</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoly</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<title level="m">Theano: A python framework for fast computation of mathematical expressions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Can we translate letters?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Edinburgh&apos;s syntax-based systems at wmt 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Efficient character-level document classification by combining convolution and recurrent layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.00367</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
