<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Metrics for Evaluation of Word-Level Machine Translation Quality Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Metrics for Evaluation of Word-Level Machine Translation Quality Estimation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="585" to="590"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The aim of this paper is to investigate suitable evaluation strategies for the task of word-level quality estimation of machine translation. We suggest various metrics to replace F 1-score for the &quot;BAD&quot; class, which is currently used as main metric. We compare the metrics&apos; performance on real system outputs and synthetically generated datasets and suggest a reliable alternative to the F 1-BAD score-the multiplication of F 1-scores for different classes. Other metrics have lower discriminative power and are biased by unfair labellings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Quality estimation (QE) of machine translation (MT) is a task of determining the quality of an au- tomatically translated text without any oracle (ref- erence) translation. This task has lately been re- ceiving significant attention: from confidence es- timation (i.e. estimation of how confident a partic- ular MT system is on a word or a phrase <ref type="bibr" target="#b3">(Gandrabur and Foster, 2003)</ref>) it evolved to system- independent QE and is performed at the word level ( <ref type="bibr" target="#b5">Luong et al., 2014</ref>), sentence level ( <ref type="bibr" target="#b8">Shah et al., 2013</ref>) and document level ( ).</p><p>The emergence of a large variety of approaches to QE led to need for reliable ways to com- pare them. The evaluation metrics that have been used to compare the performance of systems participating in QE shared tasks <ref type="bibr">1</ref> have received some criticisms. <ref type="bibr" target="#b4">Graham (2015)</ref> shows that Pear- son correlation better suits for the evaluation of sentence-level QE systems than mean absolute er- ror (MAE), often used for this purpose. Pearson correlation evaluates how well a system captures the regularities in the data, whereas MAE essen- tially measures the difference between the true and the predicted scores and in many cases can be min- imised by always predicting the average score as given by the training set labels.</p><p>Word-level QE is commonly framed as a bi- nary task, i.e., the classification of every translated word as "OK" or "BAD". This task has been eval- uated in terms of F 1 -score for the "BAD" class, a metric that favours 'pessimistic' systems -i.e. systems that tend to assign the "BAD" label to most words. A trivial baseline strategy that assigns the label "BAD" to all words can thus receive a high score while being completely uninformative ( <ref type="bibr" target="#b1">Bojar et al., 2014)</ref>. However, no analysis of the word-level metrics' performance has been done and no alternative metrics have been proposed that are more reliable than the F 1 -BAD score.</p><p>In this paper we compare existing evaluation metrics for word-level QE, suggest a number of al- ternatives, and show that one of these alternatives leads to more objective and reliable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Metrics</head><p>One of the reasons word-level QE is a challeng- ing problem is the fact that "OK" and "BAD" la- bels are not equally important: we are generally more interested in finding incorrect words than in assigning a suitable category to every single word. An ideal metric should be oriented towards the re- call for the "BAD" class. However, the case of F 1 -BAD score shows that this is not the only re- quirement: in order to be useful the metric should not favour pessimistic labellings, i.e., all or most words labelled as "BAD". Below we describe pos- sible alternatives to the F 1 -BAD score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">F 1 -score variants</head><p>Word-level F 1 -scores. Since F 1 -BAD score is too pessimistic, an obvious solution would be to balance it with F 1 -score for the "OK" class. How- ever, the widely used weighted average of F 1 - scores for the two classes is not suitable as it will be dominated by F 1 -OK due to labels imbalance. Any reasonable MT system will nowadays gener- ate texts where most words are correct, so the la- bel distribution is very skewed towards the "OK" class. Therefore, we suggest instead the multi- plication of F 1 -scores for individual classes: it is equal to zero if one of the components is zero, and since both are in the <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> range, the overall result will not exceed the value of any of the multipliers.</p><p>Phrase-level F 1 -scores. One of the features of MT errors is their phrase-level nature. Errors are not independent: one incorrect word can influence the classification of its neighbours. If several ad- jacent words are tagged as "BAD", they are likely to be part of an error which spans over a phrase.</p><p>Therefore, we also evaluate word-level F 1 - scores and alternative metrics which are based on correctly identified erroneous or error-free spans of words. The phrase-level F 1 -score we suggest is similar to the one used for the evaluation of named entity recognition (NER) systems <ref type="bibr" target="#b9">(Tjong Kim Sang and De Meulder, 2003)</ref>. There, pre- cision is the percentage of named entities found by a system that are correct, recall is the percent- age of named entities present in the corpus that are found by a system. For the QE task, instead of named entities we have spans of erroneous (or correct) words. Precision is the percentage of cor- rectly identified spans among all the spans found by a system, recall is the percentage of correctly identified spans among the spans in the test data.</p><p>However, in NER the correct borders of a named entity are of big importance, because fail- ure to identify them results in an incorrect entity. On the other hand, the actual borders of an error span in QE are not as important: the primary goal is to identify the erroneous region in the sentence, the task of finding the exact borders of an error cannot be solved unambiguously even by human annotators ( <ref type="bibr" target="#b10">Wisniewski et al., 2013)</ref>. In order to take into account partially correct phrases (e.g. a 4-word "BAD" phrase where the first word was tagged as "OK" by a system and the remaining words were correctly tagged as "BAD"), we com- pute the number of true positives as the sum of percentages of words with correctly predicted tags for every "OK" phrase. The number of true nega- tives is defined analogously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Other metrics</head><p>Matthews correlation coefficient. MCC (Pow- ers, 2011) was used as a secondary metric in WMT14 word-level QE shared task ( <ref type="bibr" target="#b1">Bojar et al., 2014</ref>). It is determined as follows:</p><formula xml:id="formula_0">M CC = T P × T N + F P × F N (T P + F P )(T P + F N )(T N + F P )(T N + F N )</formula><p>where T P , T N , F P and F N are true positive, true negative, false positive and false negative val- ues, respectively. This coefficient results in values in the [-1, 1] range. If the reference and hypothesis labellings agree on the majority of the examples, the final fig- ure is dominated by the T P × T N quantity, which gets close to the value of the denominator. The more false positives and false negatives the predic- tor produces, the lower the value of the numerator.</p><p>Sequence correlation. The sequence correla- tion score was used as a secondary evaluation met- ric in the QE shared task at WMT15 ( <ref type="bibr" target="#b2">Bojar et al., 2015)</ref>. Analogously to the phrase-level F 1 -score, it is based on the intersection of spans of correct and incorrect words. It also weights the phrases to give them equal importance and penalises the difference in the number of phrases between the reference and the hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Metrics comparison</head><p>One of the most reliable ways of comparing met- rics is to measure their correlation with human judgements. However, for the word-level QE task, asking humans to rate a system labelling or to compare the outputs of two or more QE systems is a very expensive process. A practical way of getting the human judgements is the use of qual- ity labels in downstream human tasks -i.e. tasks where quality labels can be used as additional in- formation and where they can influence human ac- curacy or speed. One such a downstream task can be computer-assisted translation, where the user translates a sentence having automatic translation as a draft, and word-level quality labels can high- light incorrect parts in a sentence. Improvements in productivity could show the degree of useful- ness of the quality labels in this case. However, such an experiment is also very expensive to be performed. Therefore, we consider indirect ways of comparing the metrics' reliability based on pre- labelled gold-standard test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Comparison on real systems</head><p>One of the purposes of system comparison is to identify the best-performing system. Therefore, we expect a good metric to be able to distinguish between systems as well as possible. One of the quality criteria for a metric will thus be the num- ber of significantly different groups of systems the metric can identify. Another criterion to evalu- ate metrics is to compare the real systems' perfor- mance with synthetic datasets for which we know the desirable behaviour of the metrics. If a metric gives the expected scores to all artificially gener- ated datasets, it detects some properties of the data which are relevant to us, so we can expect it to work adequately also on real datasets.</p><p>Here we compare the performance of six met- rics:</p><p>• F 1 -BAD -F 1 -score for the "BAD" class.</p><p>• F 1 -mult -multiplication of F 1 -scores for "BAD" and "OK" classes.</p><p>• phr F 1 -BAD -phrase-level F 1 -score for the "BAD" class.</p><formula xml:id="formula_1">• phr F 1 -mult -multiplication of phrase- level F 1 -scores. • MCC -Matthews Correlation Coefficient.</formula><p>• SeqCor -Sequence Correlation.</p><p>We used these metrics to rank all systems sub- mitted to the WMT15 QE shared task 2 (word- level QE). <ref type="bibr">2</ref> In addition to that, we test the per- formance of the metrics on a number of syntheti- cally created labellings that should be ranked low in comparison to real system labellings:</p><p>• all-bad -all words are tagged as "BAD".</p><p>• all-good -all words are tagged as "OK".</p><p>• optimistic -98% words are tagged as "OK", with only a small number of "BAD" labels generated: this system should have high precision (0.9) and low recall (0.1) for the "BAD" label.</p><p>• pessimistic -90% words are tagged as "BAD": this system should have high recall (0.9) for the "BAD" label, but low recall (0.1) for the "OK" label.</p><p>• random -labels are drawn randomly from the label probability distribution.</p><p>We rank the systems according to all the met- rics and compute the level of significance for every <ref type="bibr">2</ref> Systems that took part in the shared task are listed and described in <ref type="bibr" target="#b2">(Bojar et al., 2015</ref>). pair of systems with randomisation tests <ref type="bibr" target="#b11">(Yeh, 2000</ref>) with Bonferroni correction <ref type="bibr" target="#b0">(Abdi, 2007)</ref>. In order to evaluate the metrics' performance we compute the system distinction coefficient d -the probability of two systems being significantly dif- ferent, which is defined as the ratio between the number of significantly different pairs of systems and all pairs of systems. We also compute d for the top half and for the bottom half of the ranked systems list separately in order to check how well each metric can discriminate between better per- forming and worse performing systems. <ref type="bibr">3</ref> The results are shown in <ref type="table">Table 1</ref>. For every synthetic dataset we show the number of real sys- tem outputs that were rated lower than this dataset, with the rightmost column showing the sum of this figure across all the synthetic sets.</p><p>We can see that three metrics are better at distin- guishing synthetic results from real systems: Se- qCor and both multiplied F 1 -scores. In the case of SeqCor this result is explained by the fact that it favours longer spans of "OK" and "BAD" la- bels and thus penalises arbitrary labellings. The multiplications of F 1 -scores have two components which penalise different labellings and balance each other. This assumption is confirmed by the fact that F 1 -BAD scores become too pessimistic without the "OK" component: they both favour synthetic systems with prevailing "BAD" labels. Phrase-F 1 -BAD ranks these systems the highest: all-bad and pessimistic outperform 16 out of 17 systems according to this metric.</p><p>MCC is, in contrast, too 'optimistic': the opti- mistic dataset is rated higher than most of system outputs. In addition to that, it is not good at distin- guishing different systems: its system distinction coefficient is the lowest among all metric. SeqCor and phrase-F 1 -multiplied, despite identifying ar- tificial datasets, cannot discriminate between real systems: SeqCor fails with the top half systems, phrase-F 1 -multiplied is bad at finding differences in the bottom half of the list.</p><p>Overall, F 1 -multiplied is the only metric that performs well both in the task of distinguishing <ref type="table">Table 1</ref>: Results for all metrics. Numbers in synthetic dataset columns denote the number of system submissions that were rated lower than the corresponding synthetic dataset.</p><formula xml:id="formula_2">d d top d bottom all-bad all-good optimistic pessimistic random total F 1 -BAD 0.79 0.61 0.81 4 - 1 4 1 10 F 1 -mult 0.81 0.57 0.75 - - 2 - 2 4 phr F 1 -BAD 0.86 0.61 0.78 16 - 1 16 - 33 phr F 1 -mult 0.75 0.54 0.47 - - 1 - - 1 MCC 0.63 0.61 0.34 - - 15 - - 15</formula><note type="other">SeqCor 0.77 0.39 0.75 - - 1 1 2 4</note><p>synthetic systems from real ones and in the task of discriminating among real systems, despite the fact that its d scores are not the best. However, F 1 -BAD is not far behind: it has high values for d scores and can identify synthetic datasets quite often.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison on synthetic datasets</head><p>The experiment described above has a notable drawback: we evaluated metrics on the outputs of systems which had been tuned to maximise the F 1 - BAD score. This means that the system rankings produced by other metrics may be unfairly consid- ered inaccurate. Therefore, we suggest a more objective met- ric evaluation procedure which uses only synthetic datasets. We generate datasets with different pro- portion of errors, compute the metrics' values and their statistical significance and then compare the metrics' discriminative power. This procedure is further referred to as repeated sampling, because we sample artificial datasets multiple times.</p><p>Our goal is for the synthetic datasets to simulate real systems' output. We achieve this by using the following procedure for synthetic data generation:</p><p>• Choose the proportion of errors to introduce in the synthetic data.</p><p>• Collect all sequences that contain incorrect labels from the outputs of real systems.</p><p>• Randomly choose the sequences from this set until the overall number of errors reaches the chosen threshold.</p><p>• Take the rest of segments from the gold- standard labelling (so that they contain no er- rors).</p><p>Thus our artificial datasets contain a specific number of errors, and all of them come from real systems. We can generate datasets with very small differences in quality and identify metrics accord- ing to which this difference is more significant. Let us compare the discriminative power of metrics m 1 and m 2 . We choose two error thresh- olds e 1 and e 2 . Then we sample a relatively small number (e.g. 100) of random datasets with e 1 er- rors. Then -100 random datasets with e 2 er- rors. We compute the values for both metrics on the two sets of random samples and for each met- ric we test if the difference between the results for the two sets is significant (we compute the statistic significance using non-paired t-test with Bonfer- roni correction). Since we sampled the synthetic datasets a small number of times it is likely that the metrics will not detect any significant differ- ences between them. In this case we repeat the process with a larger (e.g. 200) number of samples and compare the p-values for two metrics again. By gradually increasing the number of samples at some point we will find that one of the met- rics recognises the differences in scores as statisti- cally significant, while another one does not. This means that this metric has higher discriminative power: it needs less samples to determine that the systems they are different. The procedure is out- lined in Algorithm 1.</p><p>In our experiments in order to make p-values more stable we repeat each sampling round (sam- pling of a set with e i errors 100, 200, etc. times) 1,000 times and use the average of p-values. We used fixed sets of sample numbers: <ref type="bibr">[100,</ref><ref type="bibr">200,</ref><ref type="bibr">500,</ref><ref type="bibr">1000,</ref><ref type="bibr">2000,</ref><ref type="bibr">5000,</ref><ref type="bibr">10,</ref><ref type="bibr">000]</ref>  Since we compare all six metrics on five er- ror thresholds, we have 10 p-values for each met- ric at every sampling round. We analyse the re- sults in the following way: for every difference in the percentage of errors (e.g. thresholds of 30% and 30.01% give 0.01% difference, thresholds of 30% and 30.2% -0.2% difference), we define the minimum number of samplings that a metric 0.01 0.04 0.05 0.1 0.15 0.2 F <ref type="table" target="#tab_1">1 -mult  10000 2000  2000 500  200 100  MCC  10000 2000  2000 500</ref> 200 100 F 1 -BAD 10000 5000 2000 1000 500 200 phr F 1 -mult 10000 5000 5000 1000 500 200 SeqCor 10000 5000 5000 1000 500 500 phr F 1 -BAD 10000 10000 5000 1000 500 500 needs to observe significant differences between datasets which differ in this number of errors. <ref type="table" target="#tab_1">Ta- ble 2</ref> shows the results. Numbers in cells are min- imum numbers of samplings. We do not show er- ror differences greater than 0.2 because all metrics identify them well. All metrics are sorted by dis- criminative power from best to worst, i.e. metrics at the top of the table require less samplings to tell one synthetic dataset from another.</p><p>As in the previous experiment, here the discrim- inative power of the multiplication of F 1 -scores is the highest. Surprisingly, MCC performs equally well. Similarly to the experiment with real sys- tems, the F 1 -BAD metric performs worse than the F 1 -multiply metric, but here their difference is more salient. All phrase-motivated metrics show worse results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Repeated sampling: the minimum number of samplings required to discriminate between sam-
ples with a different proportions of errors. 

Result: m x ∈ {m 1 , m 2 }, where m x -metric 
with the highest discriminative power 
on error thresholds e 1 and e 2 
N ← 100 
α ← significance level 
while p-val m 1 α and p-val m 2 α do 
s 1 ← N random samples with e 1 errors 
s 2 ← N random samples with e 2 errors 
p-val m 1 ← t-test(m 1 (s 1 ), m 1 (s 2 )) 
p-val m 2 ← t-test(m 2 (s 1 ), m 2 (s 2 )) 
if p-val m 1 &lt; α and p-val m 2 α then 
return m 1 
else if p-val m 1 α and p-val m 2 &lt; α 
then 
return m 2 
else 
N ← N + 100 
end 
Algorithm 1: Repeated sampling for metrics m 1 , 
m 2 and error thresholds e 1 , e 2 . 

</table></figure>

			<note place="foot" n="1"> http://statmt.org/wmt15/ quality-estimation-task.html</note>

			<note place="foot" n="3"> d bottom is always greater than dtop in our experiments because better performing systems tend to have closer scores under all metrics and more often are not significantly different from one another. When comparing two metrics, greater d does not imply greater dtop and d bottom : we use Bonferroni correction for which the significance level depends on the number of compared values, so a difference which is significant when comparing eight systems, for example, can become insignificant when comparing 16 systems.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>The aim of this paper was to compare evaluation metrics for word and phrase-level quality estima-tion and find an alternative for F 1-BAD score, which has been used as primary metric in recent research but has a number of drawbacks, in partic-ular tendency to overrate labellings with predomi-nantly"BAD" instances.</p><p>We found that the multiplication of F 1-BAD and F 1-OK scores is more stable against "pes-simistic" labellings and has bigger discrimina-tive power when comparing synthetic datasets. However, other tested metrics, including advanced phrase-based scores, could not outperform</p><p>This work should be seen as a proxy for real user evaluation of word-level QE metrics, which could be done on downstream tasks (e.g. computer-assisted translation).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The bonferroni andšidákandˇandšidák corrections for multiple comparisons. Encyclopedia of measurement and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Abdi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="103" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="46" />
		</imprint>
	</monogr>
	<note>Findings of the 2015 workshop on statistical machine translation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Confidence estimation for translation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simona</forename><surname>Gandrabur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL-2003</title>
		<meeting><address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving evaluation of machine translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1804" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lig system for word level qe task at wmt14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Quang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT-2014</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="335" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="63" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ushef and usaar-ushef participation in the wmt15 qe shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liling</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An investigation on the effectiveness of features for translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT Summit XIV</title>
		<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<meeting>CoNLL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Design and Analysis of a Large Corpus of Post-Edited Translations: Quality Estimation, Failure Analysis and the Variability of Post-Edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT Summit XIV: 14th Machine Translation Summit</title>
		<meeting><address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling2000: the 18th Conference on Computational Linguistics</title>
		<meeting><address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="947" to="953" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
