<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Morphological Inflection Generation with Hard Monotonic Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
						</author>
						<title level="a" type="main">Morphological Inflection Generation with Hard Monotonic Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2004" to="2015"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1183</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft attention (Bahdanau et al., 2015) models for the task, shedding some light on the features such models extract.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morphological inflection generation involves gen- erating a target word (e.g. "härtestem", the German word for "hardest"), given a source word (e.g.</p><p>"hart", the German word for "hard") and the morpho-syntactic attributes of the target (POS=adjective, gender=masculine, type=superlative, etc.).</p><p>The task is important for many down-stream NLP tasks such as machine translation, especially for dealing with data sparsity in morphologically rich languages where a lemma can be inflected into many different word forms. Several studies have shown that translating into lemmas in the tar- get language and then applying inflection gener- ation as a post-processing step is beneficial for phrase-based machine translation ( <ref type="bibr" target="#b32">Minkov et al., 2007;</ref><ref type="bibr" target="#b41">Toutanova et al., 2008;</ref><ref type="bibr" target="#b8">Clifton and Sarkar, 2011;</ref><ref type="bibr">Fraser et al., 2012;</ref><ref type="bibr" target="#b6">Chahuneau et al., 2013)</ref> and more recently for neural machine translation <ref type="bibr" target="#b17">(García-Martínez et al., 2016)</ref>.</p><p>The task was traditionally tackled with hand en- gineered finite state transducers (FST) <ref type="bibr" target="#b28">(Koskenniemi, 1983;</ref><ref type="bibr" target="#b27">Kaplan and Kay, 1994)</ref> which rely on expert knowledge, or using trainable weighted finite state transducers ( <ref type="bibr" target="#b34">Mohri et al., 1997;</ref><ref type="bibr" target="#b14">Eisner, 2002</ref>) which combine expert knowledge with data- driven parameter tuning. Many other machine- learning based methods <ref type="bibr" target="#b42">(Yarowsky and Wicentowski, 2000;</ref><ref type="bibr" target="#b11">Dreyer and Eisner, 2011;</ref><ref type="bibr" target="#b13">Durrett and DeNero, 2013;</ref><ref type="bibr" target="#b20">Hulden et al., 2014;</ref><ref type="bibr" target="#b0">Ahlberg et al., 2015;</ref><ref type="bibr" target="#b35">Nicolai et al., 2015</ref>) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to cre- ate the output sequence.</p><p>More recently, the task was modeled as neu- ral sequence-to-sequence learning over character sequences with impressive results <ref type="bibr" target="#b15">(Faruqui et al., 2016)</ref>. The vanilla encoder-decoder models as used by <ref type="bibr">Faruqui et al. compress</ref> the input sequence to a single, fixed-sized continuous representation. Instead, the soft-attention based sequence to se- quence learning paradigm (  allows directly conditioning on the entire input se- quence representation, and was utilized for mor- phological inflection generation with great success ( <ref type="bibr">Kann and Schütze, 2016b,a)</ref>.</p><p>However, the neural sequence-to-sequence models require large training sets in order to per- form well: their performance on the relatively small CELEX dataset is inferior to the latent vari- able WFST model of <ref type="bibr" target="#b12">Dreyer et al. (2008)</ref>. Inter- estingly, the neural WFST model by <ref type="bibr" target="#b38">Rastogi et al. (2016)</ref> also suffered from the same issue on the CELEX dataset, and surpassed the latent variable model only when given twice as much data to train on.</p><p>We propose a model which handles the above issues by directly modeling an almost monotonic alignment between the input and output charac- ter sequences, which is commonly found in the morphological inflection generation task (e.g. in languages with concatenative morphology). The model consists of an encoder-decoder neural net- work with a dedicated control mechanism: in each step, the model attends to a single input state and either writes a symbol to the output sequence or advances the attention pointer to the next state from the bi-directionally encoded sequence, as de- scribed visually in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>This modeling suits the natural monotonic alignment between the input and output, as the network learns to attend to the relevant inputs be- fore writing the output which they are aligned to. The encoder is a bi-directional RNN, where each character in the input word is represented using a concatenation of a forward RNN and a backward RNN states over the word's characters. The combination of the bi-directional encoder and the controllable hard attention mechanism enables to condition the output on the entire input se- quence. Moreover, since each character represen- tation is aware of the neighboring characters, non- monotone relations are also captured, which is im- portant in cases where segments in the output word are a result of long range dependencies in the in- put word. The recurrent nature of the decoder, to- gether with a dedicated feedback connection that passes the last prediction to the next decoder step explicitly, enables the model to also condition the current output on all the previous outputs at each prediction step.</p><p>The hard attention mechanism allows the net- work to jointly align and transduce while us- ing a focused representation at each step, rather then the weighted sum of representations used in the soft attention model. This makes our model Resolution Preserving ( <ref type="bibr" target="#b23">Kalchbrenner et al., 2016)</ref> while also keeping decoding time linear in the output sequence length rather than multiplicative in the input and output lengths as in the soft- attention model. In contrast to previous sequence- to-sequence work, we do not require the training procedure to also learn the alignment. Instead, we use a simple training procedure which relies on independently learned character-level alignments, from which we derive gold transduction+control sequences. The network can then be trained using straightforward cross-entropy loss.</p><p>To evaluate our model, we perform extensive experiments on three previously studied morpho- logical inflection generation datasets: the CELEX dataset ( <ref type="bibr" target="#b1">Baayen et al., 1993)</ref>, the Wiktionary dataset <ref type="bibr" target="#b13">(Durrett and DeNero, 2013)</ref> and the SIG- MORPHON2016 dataset ( . We show that while our model is on par with or better than the previous neural and non-neural state-of-the-art approaches, it also performs sig- nificantly better with very small training sets, be- ing the first neural model to surpass the perfor- mance of the weighted FST model with latent vari- ables which was specifically tailored for the task by <ref type="bibr" target="#b12">Dreyer et al. (2008)</ref>. Finally, we analyze and compare our model and the soft attention model, showing how they function very similarly with re- spect to the alignments and representations they learn, in spite of our model being much simpler. This analysis also sheds light on the representa- tions such models learn for the morphological in- flection generation task, showing how they encode specific features like a symbol's type and the sym- bol's location in a sequence.</p><p>To summarize, our contributions in this paper are three-fold:</p><p>1. We present a hard attention model for nearly- monotonic sequence to sequence learning, as common in the morphological inflection set- ting.</p><p>2. We evaluate the model on the task of mor- phological inflection generation, establishing a new state of the art on three previously- studied datasets for the task.</p><p>3. We perform an analysis and comparison of our model and the soft-attention model, shed- ding light on the features such models extract for the inflection generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Hard Attention Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivation</head><p>We would like to transduce an input sequence, x 1:n ∈ Σ * x into an output sequence, y 1:m ∈ Σ * y , where Σ x and Σ y are the input and output vo- cabularies, respectively. Imagine a machine with read-only random access to the encoding of the in- put sequence, and a single pointer that determines the current read location. We can then model se- quence transduction as a series of pointer move- ment and write operations. If we assume the align- ment is monotone, the machine can be simpli-fied: the memory can be read in sequential or- der, where the pointer movement is controlled by a single "move forward" operation (step) which we add to the output vocabulary. We implement this behavior using an encoder-decoder neural net- work, with a control mechanism which determines in each step of the decoder whether to write an output symbol or promote the attention pointer the next element of the encoded input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Definition</head><p>In prediction time, we seek the output sequence y 1:m ∈ Σ * y , for which:</p><formula xml:id="formula_0">y 1:m = arg max y ∈Σ * y p(y |x 1:n , f )<label>(1)</label></formula><p>Where x ∈ Σ * x is the input sequence and f = {f 1 , . . . , f l } is a set of attributes influencing the transduction task (in the inflection generation task these would be the desired morpho-syntactic at- tributes of the output sequence). Given a nearly- monotonic alignment between the input and the output, we replace the search for a sequence of let- ters with a sequence of actions s 1:q ∈ Σ * s , where Σ s = Σ y ∪ {step}. This sequence is a series of step and write actions required to go from x 1:n to y 1:m according to the monotonic alignment be- tween them (we will elaborate on the determinis- tic process of getting s 1:q from a monotonic align- ment between x 1:n to y 1:m in section 2.4). In this case we define: 1</p><formula xml:id="formula_1">s 1:q = arg max s ∈Σ * s p(s |x 1:n , f ) = arg max s ∈Σ * s s i ∈s p(s i |s 1 . . . s i−1 , x 1:n , f )<label>(2)</label></formula><p>which we can estimate using a neural network:</p><formula xml:id="formula_2">s 1:q = arg max s ∈Σ * s NN(x 1:n , f, Θ) (3)</formula><p>where the network's parameters Θ are learned us- ing a set of training examples. We will now de- scribe the network architecture. A round tip expresses concatenation of the inputs it receives. The attention is promoted to the next input element once a step action is predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Network Architecture</head><p>Notation We use bold letters for vectors and ma- trices. We treat LSTM as a parameterized func- tion LSTM θ (x 1 . . . x n ) mapping a sequence of input vectors x 1 . . . x n to a an output vector h n . The equations for the LSTM variant we use are detailed in the supplementary material of this pa- per.</p><p>Encoder For every element in the input sequence:</p><formula xml:id="formula_3">x 1:n = x 1 .</formula><p>. . x n , we take the corresponding em- bedding: e x 1 . . . e xn , where: e x i ∈ R E . These embeddings are parameters of the model which will be learned during training. We then feed the embeddings into a bi-directional LSTM en- coder ( <ref type="bibr" target="#b18">Graves and Schmidhuber, 2005</ref>) which re- sults in a sequence of vectors:</p><formula xml:id="formula_4">x 1:n = x 1 . . . x n , where each vector x i ∈ R 2H is a concate- nation of: LSTM forward (e x 1 , e x 2 , . . . e x i ) and LSTM backward (e xn , e x n−1 . . . e x i )</formula><p>, the forward LSTM and the backward LSTM outputs when fed with e x i . Decoder Once the input sequence is encoded, we feed the decoder RNN, LSTM dec , with three in- puts at each step:</p><p>1. The current attended input, x a ∈ R 2H , ini- tialized with the first element of the encoded sequence, x 1 .</p><p>predicted output symbol in the previous de- coder step.</p><p>Those three inputs are concatenated into a single</p><formula xml:id="formula_5">vector z i = [x a , f , s i−1 ] ∈ R 2H+F ·l+E</formula><p>, which is fed into the decoder, providing the decoder output vector: LSTM dec (z 1 . . . z i ) ∈ R H . Finally, to model the distribution over the possible actions, we project the decoder output to a vector of |Σ s | elements, followed by a softmax layer:</p><formula xml:id="formula_6">p(s i = c) = softmax c (W · LSTM dec (z 1 . . . z i ) + b) (4)</formula><p>Control Mechanism When the most probable ac- tion is step, the attention is promoted so x a con- tains the next encoded input representation to be used in the next step of the decoder. The process is demonstrated visually in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training the Model</head><p>For every example: (x 1:n , y 1:m , f ) in the train- ing data, we should produce a sequence of step and write actions s 1:q to be predicted by the de- coder. The sequence is dependent on the align- ment between the input and the output: ideally, the network will attend to all the input characters aligned to an output character before writing it. While recent work in sequence transduction ad- vocate jointly training the alignment and the de- coding mechanisms ( <ref type="bibr" target="#b43">Yu et al., 2016)</ref>, we instead show that in our case it is worthwhile to decouple these stages and learn a hard alignment beforehand, using it to guide the training of the encoder-decoder network and en- abling the use of correct alignments for the at- tention mechanism from the beginning of the net- work training phase. Thus, our training procedure consists of three stages: learning hard alignments, deriving oracle actions from the alignments, and learning a neural transduction model given the or- acle actions. Learning Hard Alignments We use the character alignment model of <ref type="bibr" target="#b40">Sudoh et al. (2013)</ref>, based on a Chinese Restaurant Process which weights single alignments (character-to-character) in proportion to how many times such an alignment has been seen elsewhere out of all possible alignments. The aligner implementation we used produces either 0- to-1, 1-to-0 or 1-to-1 alignments. Deriving Oracle Actions We infer the sequence of actions s 1:q from the alignments by the deter- ministic procedure described in Algorithm 1. An example of an alignment with the resulting oracle action sequence is shown in <ref type="figure">Figure 2</ref>, where a 4 is a 0-to-1 alignment and the rest are 1-to-1 align- ments.</p><p>Figure 2: Top: an alignment between a lemma x 1:n and an inflection y 1:m as predicted by the aligner. Bottom: s 1:q , the sequence of actions to be predicted by the network, as produced by Al- gorithm 1 for the given alignment.</p><p>Algorithm 1 Generates the oracle action sequence s 1:q from the alignment between x 1:n and y 1:m Require: a, the list of either 1-to-1, 1-to-0 or 0- to-1 alignments between x 1:n and y 1:m 1: Initialize s as an empty sequence 2: for each</p><formula xml:id="formula_7">a i = (x a i , y a i ) ∈ a do 3:</formula><p>if a i is a 1-to-0 alignment then if a i+1 is not a 0-to-1 alignment then 8:</p><p>s.append(step) return s This procedure makes sure that all the source input elements aligned to an output element are read (using the step action) before writing it. Learning a Neural Transduction Model The network is trained to mimic the actions of the ora- cle, and at inference time, it will predict the actions according to the input. We train it using a conven- tional cross-entropy loss function per example:</p><formula xml:id="formula_8">L(x 1:n , y 1:m , f, Θ) = − s j ∈s 1:q log softmax s j (d), d = W · LSTM dec (z 1 . . . z i ) + b<label>(5)</label></formula><p>Transition System An alternative view of our model is that of a transition system with AD- VANCE and WRITE(CH) actions, where the oracle is derived from a given hard alignment, the input is encoded using a biRNN, and the next action is determined by an RNN over the previous inputs and actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We perform extensive experiments with three pre- viously studied morphological inflection genera- tion datasets to evaluate our hard attention model in various settings. In all experiments we com- pare our hard attention model to the best per- forming neural and non-neural models which were previously published on those datasets, and to our implementation of the global (soft) attention model presented by <ref type="bibr" target="#b30">Luong et al. (2015)</ref> which we train with identical hyper-parameters as our hard- attention model. The implementation details for our models are described in the supplementary material section of this paper. The source code and data for our models is available on github. <ref type="bibr">2</ref> CELEX Our first evaluation is on a very small dataset, to see if our model indeed avoids the ten- dency to overfit with small training sets. We re- port exact match accuracy on the German inflec- tion generation dataset compiled by <ref type="bibr" target="#b12">Dreyer et al. (2008)</ref> from the CELEX database ( <ref type="bibr" target="#b1">Baayen et al., 1993</ref>). The dataset includes only 500 training examples for each of the four inflection types: 13SIA→13SKE, 2PIE→13PKE, 2PKE→z, and rP→pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively. <ref type="bibr">3</ref> We first compare our model to three competitive models from the literature that reported results on this dataset: the Morphologi- cal Encoder-Decoder (MED) of <ref type="bibr" target="#b25">Kann and Schütze (2016a)</ref>  </p><p>, dividing the data for each inflection type into five folds, each consisting of 500 training, 1000 development and 1000 test examples. We train a separate model for each fold and report ex- act match accuracy, averaged over the five folds.</p><p>Wiktionary To neutralize the negative effect of very small training sets on the performance of the different learning approaches, we also evalu- ate our model on the dataset created by <ref type="bibr" target="#b13">Durrett and DeNero (2013)</ref>, which contains up to 360k training examples per language. It was built by extracting Finnish, German and Spanish inflection tables from Wiktionary, used in order to evalu- ate their system based on string alignments and a semi-CRF sequence classifier with linguistically inspired features, which we use a baseline. We also used the dataset expansion made by <ref type="bibr" target="#b35">Nicolai et al. (2015)</ref> to include French and Dutch inflec- tions as well. Their system also performs an align- and-transduce approach, extracting rules from the aligned training set and applying them in inference time with a proprietary character sequence classi- fier. In addition to those systems we also com- pare to the results of the recent neural approach of <ref type="bibr" target="#b15">Faruqui et al. (2016)</ref>, which did not use an at- tention mechanism, and <ref type="bibr" target="#b43">Yu et al. (2016)</ref>, which coupled the alignment and transduction tasks.</p><p>SIGMORPHON As different languages show different morphological phenomena, we also ex- periment with how our model copes with these various phenomena using the morphological in- flection dataset from the SIGMORPHON2016 shared task ( . Here the training data consists of ten languages, with five morphological system types (detailed in <ref type="table" target="#tab_2">Table 3</ref>): Russian (RU), German (DE), Spanish (ES), Geor- gian (GE), Finnish (FI), Turkish (TU), Arabic (AR), Navajo (NA), Hungarian (HU) and Maltese (MA) with roughly 12,800 training and 1600 de- velopment examples per language. We compare our model to two soft attention baselines on this dataset: MED (Kann and Schütze, 2016b), which was the best participating system in the shared task, and our implementation of the global (soft) attention model presented by <ref type="bibr" target="#b30">Luong et al. (2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In all experiments, for both the hard and soft at- tention models we implemented, we report results using an ensemble of 5 models with different ran- dom initializations by using majority voting on the final sequences the models predicted, as proposed by <ref type="bibr" target="#b25">Kann and Schütze (2016a)</ref>. This was done to perform fair comparison to the models of <ref type="bibr" target="#b25">Kann and Schütze (2016a,</ref>   On the low resource setting (CELEX), our hard attention model significantly outperforms both the recent neural models of <ref type="bibr" target="#b25">Kann and Schütze (2016a)</ref> (MED) and <ref type="bibr" target="#b38">Rastogi et al. (2016)</ref> (NWFST) and the morphologically aware latent variable model of <ref type="bibr">Dreyer et al. (2008) (LAT)</ref>, as detailed in <ref type="table">Table  1</ref>. In addition, it significantly outperforms our implementation of the soft attention model (Soft). It is also, to our knowledge, the first model that surpassed in overall accuracy the latent variable model on this dataset. We attribute our advantage over the soft attention models to the ability of the hard attention control mechanism to harness the monotonic alignments found in the data. The ad- vantage over the FST models may be explained by our conditioning on the entire output history which is not available in those models. <ref type="figure" target="#fig_3">Figure 3</ref> plots the train-set and dev-set accuracies of the soft and hard attention models as a function of the training epoch. While both models perform similarly on the train-set (with the soft attention model fitting it slightly faster), the hard attention model performs significantly better on the dev-set. This shows the soft attention model's tendency to overfit on the small dataset, as it is not enforcing the monotonic assumption of the hard attention model.</p><p>On the large training set experiments (Wik- tionary), our model is the best performing model on German verbs, Finnish nouns/adjectives and Dutch verbs, resulting in the highest reported av- erage accuracy across all inflection types when compared to the four previous neural and non- neural state of the art baselines, as detailed in Ta- ble 2. This shows the robustness of our model also with large amounts of training examples, and the advantage the hard attention mechanism provides over the encoder-decoder approach of <ref type="bibr" target="#b15">Faruqui et al. (2016)</ref> which does not employ an attention mechanism. Our model is also signifi- cantly more accurate than the model of <ref type="bibr" target="#b43">Yu et al. (2016)</ref>, which shows the advantage of using in- dependently learned alignments to guide the net- work's attention from the beginning of the training process. While our soft-attention implementation outperformed the models of <ref type="bibr" target="#b43">Yu et al. (2016)</ref> and <ref type="bibr" target="#b13">Durrett and DeNero (2013)</ref>, it still performed in- feriorly to the hard attention model.</p><p>As can be seen in <ref type="table" target="#tab_2">Table 3</ref> better than both soft-attention baselines for the suffixing+stem-change languages (Russian, Ger- man and Spanish) and is slightly less accurate than our implementation of the soft attention model on the rest of the languages, which is now the best performing model on this dataset to our knowl- edge. We explain this by looking at the languages from a linguistic typology point of view, as de- tailed in . Since Russian, German and Spanish employ a suffixing morphol- ogy with internal stem changes, they are more suit- able for monotonic alignment as the transforma- tions they need to model are the addition of suf- fixes and changing characters in the stem. The rest of the languages in the dataset employ more context sensitive morphological phenomena like vowel harmony and consonant harmony, which re- quire to model long range dependencies in the in- put sequence which better suits the soft attention mechanism. While our implementation of the soft attention model and MED are very similar model- wise, we hypothesize that our soft attention model results are better due to the fact that we trained the model for 100 epochs and picked the best per- forming model on the development set, while the MED system was trained for a fixed amount of 20 epochs (although trained on more data -both train and development sets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>The Learned Alignments In order to see if the alignments predicted by our model fit the mono- tonic alignment structure found in the data, and whether are they more suitable for the task when compared to the alignments found by the soft at- tention model, we examined alignment predictions of the two models on examples from the develop- ment portion of the CELEX dataset, as depicted in <ref type="figure">Figure 4</ref>. First, we notice the alignments found by the soft attention model are also monotonic, supporting our modeling approach for the task. <ref type="figure">Figure 4</ref> (bottom-right) also shows how the hard- attention model performs deletion (legte→lege) by predicting a sequence of two step operations. Another notable morphological transformation is the one-to-many alignment, found in the top exam- ple: flog→fliege, where the model needs to trans- form a character in the input, o, to two characters in the output, ie. This is performed by two consec- utive write operations after the step operation of the relevant character to be replaced. Notice that in this case, the soft attention model performs a different alignment by aligning the character i to o and the character g to the sequence eg, which is not the expected alignment in this case from a linguistic point of view.</p><p>The Learned Representations How does the soft-attention model manage to learn nearly- perfect monotonic alignments? Perhaps the the network learns to encode the sequential position as part of its encoding of an input element? More generally, what information is encoded by the soft and hard alignment encoders? We selected 500 random encoded characters-in-context from input  words in the CELEX development set, where ev- ery encoded representation is a vector in R 200 .</p><p>Since those vectors are outputs from the bi-LSTM encoders of the models, every vector of this form carries information of the specific character with its entire context. We project these encodings into 2-D using SVD and plot them twice, each time using a different coloring scheme. We first color each point according to the character it represents <ref type="figure" target="#fig_5">(Figures 5a, 5b</ref>). In the second coloring scheme <ref type="figure" target="#fig_5">(Figures 5c, 5d</ref>), each point is colored according to the character's sequential position in the word it came from, blue indicating positions near the be- ginning of the word, and red positions near its end. While both models tend to cluster representa- tions for similar characters together <ref type="figure" target="#fig_5">(Figures 5a,  5b)</ref>, the hard attention model tends to have much more isolated character clusters. <ref type="figure" target="#fig_5">Figures 5c, 5d</ref> show that both models also tend to learn represen- tations which are sensitive to the position of the character, although it seems that here the soft at- tention model is more sensitive to this information as its coloring forms a nearly-perfect red-to-blue transition on the X axis. This may be explained by the soft-attention mechanism encouraging the encoder to encode positional information in the input representations, which may help it to pre- dict better attention scores, and to avoid collisions when computing the weighted sum of representa- tions for the context vector. In contrast, our hard- attention model has other means of obtaining the position information in the decoder using the step actions, and for that reason it does not encode it as strongly in the representations of the inputs. This behavior may allow it to perform well even with fewer examples, as the location information is represented more explicitly in the model using the step actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Many previous works on inflection generation used machine learning methods <ref type="bibr" target="#b42">(Yarowsky and Wicentowski, 2000;</ref><ref type="bibr" target="#b11">Dreyer and Eisner, 2011;</ref><ref type="bibr" target="#b13">Durrett and DeNero, 2013;</ref><ref type="bibr" target="#b20">Hulden et al., 2014;</ref><ref type="bibr" target="#b0">Ahlberg et al., 2015;</ref><ref type="bibr" target="#b35">Nicolai et al., 2015</ref>) with assumptions about the set of possible processes needed to create the output word. Our work was mainly inspired by <ref type="bibr" target="#b15">Faruqui et al. (2016)</ref> which trained an independent encoder-decoder neural network for every inflection type in the training data, alleviating the need for feature engineering. <ref type="bibr">Kann and Schütze (2016b,a)</ref> tackled the task with a single soft attention model ( ) for all inflection types, which resulted in the best submission at the SIGMORPHON 2016 shared task ( . In another closely related work, <ref type="bibr" target="#b38">Rastogi et al. (2016)</ref> model the task with a WFST in which the arc weights are learned by optimizing a global loss function over all the possible paths in the state graph, while modeling contextual features with bi-directional LSTMS. This is similar to our approach, where instead of learning to mimic a single greedy align- ment as we do, they sum over all possible align- ments. While not committing to a single greedy alignment could in theory be beneficial, we see in <ref type="table">Table 1</ref> that-at least for the low resource scenario-our greedy approach is more effective in practice. Another recent work ( <ref type="bibr" target="#b24">Kann et al., 2016)</ref> proposed performing neural multi-source morphological reinflection, generating an inflec- tion from several source forms of a word.</p><p>Previous works on neural sequence transduc- tion include the RNN Transducer (Graves, 2012) which uses two independent RNN's over mono- tonically aligned sequences to predict a distribu- tion over the possible output symbols in each step, including a null symbol to model the alignment. <ref type="bibr" target="#b43">Yu et al. (2016)</ref> improved this by replacing the null symbol with a dedicated learned transition proba- bility. Both models are trained using a forward- backward approach, marginalizing over all possi- ble alignments. Our model differs from the above by learning the alignments independently, thus en- abling a dependency between the encoder and de- coder. While providing better results than <ref type="bibr" target="#b43">Yu et al. (2016)</ref>, this also simplifies the model training us- ing a simple cross-entropy loss. A recent work by <ref type="bibr" target="#b37">Raffel et al. (2017)</ref> jointly learns the hard mono- tonic alignments and transduction while maintain- ing the dependency between the encoder and the decoder. <ref type="bibr">Jaitly et al. (2015)</ref> proposed the Neural Transducer model, which is also trained on exter- nal alignments. They divide the input into blocks of a constant size and perform soft attention sepa- rately on each block. <ref type="bibr" target="#b29">Lu et al. (2016)</ref> used a com- bination of an RNN encoder with a CRF layer to model the dependencies in the output sequence. An interesting comparison between "traditional" sequence transduction models <ref type="bibr" target="#b4">(Bisani and Ney, 2008;</ref><ref type="bibr" target="#b22">Jiampojamarn et al., 2010;</ref><ref type="bibr" target="#b36">Novak et al., 2012</ref>) and encoder-decoder neural networks for monotone string transduction tasks was done by <ref type="bibr" target="#b39">Schnober et al. (2016)</ref>, showing that in many cases there is no clear advantage to one approach over the other.</p><p>Regarding task-specific improvements to the at- tention mechanism, a line of work on attention- based speech recognition ( <ref type="bibr" target="#b7">Chorowski et al., 2015;</ref><ref type="bibr" target="#b3">Bahdanau et al., 2016)</ref> proposed adding location awareness by using the previous attention weights when computing the next ones, and preventing the model from attending on too many or too few inputs using "sharpening" and "smoothing" techniques on the attention weight distributions. <ref type="bibr" target="#b9">Cohn et al. (2016)</ref> offered several changes to the attention score computation to encourage well- known modeling biases found in traditional ma- chine translation models like word fertility, po- sition and alignment symmetry. Regarding the utilization of independent alignment models for training attention-based networks, <ref type="bibr" target="#b31">Mi et al. (2016)</ref> showed that the distance between the attention- infused alignments and the ones learned by an in- dependent alignment model can be added to the networks' training objective, resulting in an im- proved translation and alignment quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a hard attention model for mor- phological inflection generation. The model em- ploys an explicit alignment which is used to train a neural network to perform transduction by de- coding with a hard attention mechanism. Our model performs better than previous neural and non-neural approaches on various morphological inflection generation datasets, while staying com- petitive with dedicated models even with very few training examples. It is also computationally ap- pealing as it enables linear time decoding while staying resolution preserving, i.e. not requiring to compress the input sequence to a single fixed- sized vector. Future work may include apply- ing our model to other nearly-monotonic align- and-transduce tasks like abstractive summariza- tion, transliteration or machine translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The hard attention network architecture. A round tip expresses concatenation of the inputs it receives. The attention is promoted to the next input element once a step action is predicted.</figDesc><graphic url="image-1.png" coords="3,307.28,62.81,221.73,169.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>which is based on the soft-attention model of Bahdanau et al. (2015), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM's to weigh its arcs (NWFST), and the model of Dreyer et al. (2008) which uses a weighted FST with latent-variables structured particularly for morphological string transduction tasks (LAT). Following previous reports on this dataset, we use the same data splits as Dreyer et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves for the soft and hard attention models on the first fold of the CELEX dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc>a) Colors indicate which character is encoded. (b) Colors indicate which character is encoded. (c) Colors indicate the character's position. (d) Colors indicate the character's position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: SVD dimension reduction to 2D of 500 character representations in context from the encoder, for both the soft attention (top) and hard attention (bottom) models.</figDesc><graphic url="image-5.png" coords="8,101.91,219.21,185.95,139.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on the Wiktionary datasets 

suffixing+stem changes 
circ. 
suffixing+agg.+v.h. 
c.h. 
templatic 
RU 
DE 
ES 
GE 
FI 
TU 
HU 
NA 
AR 
MA 
Avg. 

MED 

91.46 95.8 
98.84 98.5 
95.47 98.93 96.8 
91.48 99.3 
88.99 95.56 
Soft 
92.18 96.51 98.88 98.88 96.99 99.37 97.01 95.41 99.3 
88.86 96.34 
Hard 92.21 96.58 98.92 98.12 95.91 97.99 96.25 93.01 98.77 88.32 95.61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Results on the SIGMORPHON 2016 morphological inflection dataset. The text above each lan- guage lists the morphological phenomena it includes: circ.=circumfixing, agg.=agglutinative, v.h.=vowel harmony, c.h.=consonant harmony bling technique.</figDesc><table></table></figure>

			<note place="foot" n="1"> We note that our model (Eq. 2) solves a different objective than (Eq 1), as it searches for the best derivation and not the best sequence. In order to accurately solve (1) we would need to marginalize over the different derivations leading to the same sequence, which is computationally challenging. However, as we see in the experiments section, the bestderivation approximation is effective in practice.</note>

			<note place="foot" n="2">. A set of embeddings for the attributes that influence the generation process, concatenated to a single vector: f = [f 1. .. f l ] ∈ R F ·l. 3. s i−1 ∈ R E , which is an embedding for the</note>

			<note place="foot" n="2"> https://github.com/roeeaharoni/ morphological-reinflection 3 The acronyms stand for: 13SIA=1st/3rd person, singular, indefinite, past;13SKE=1st/3rd person, subjunctive, present; 2PIE=2nd person, plural, indefinite, present;13PKE=1st/3rd person, plural, subjunctive, present; 2PKE=2nd person, plural, subjunctive, present; z=infinitive; rP=imperative, plural; pA=past participle.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the Intel Collabora-tive Research Institute for Computational Intelli-gence (ICRI-CI), and The Israeli Science Founda-tion (grant number 1555/15).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head><p>To train our models, we used the train portion of the datasets as-is and evaluated on the test portion the model which performed best on the develop- ment portion of the dataset, without conducting any specific pre-processing steps on the data. We train the models for a maximum of 100 epochs over the training set. To avoid long training time, we trained the model for 20 epochs for datasets larger than 50k examples, and for 5 epochs for datasets larger than 200k examples. The models were implemented using the python bindings of the dynet toolkit. <ref type="bibr">4</ref> We trained the network by optimizing the ex- pected output sequence likelihood using cross- entropy loss as mentioned in equation 5. For op- timization we used ADADELTA <ref type="bibr" target="#b44">(Zeiler, 2012)</ref> without regularization. We updated the weights after every example (i.e. mini-batches of size 1). We used the dynet toolkit implementation of an LSTM network with two layers for all models, each having 100 entries in both the encoder and decoder. The character embeddings were also vec- tors with 100 entries for the CELEX experiments, and with 300 entries for the SIGMORPHON and Wiktionary experiments.</p><p>The morpho-syntactic attribute embeddings were vectors of 20 entries in all experiments. We did not use beam search while decoding for both the hard and soft attention models as it is signif- icantly slower and did not show clear improve- ment in previous experiments we conducted. For the character level alignment process we use the implementation provided by the organizers of the SIGMORPHON2016 shared task. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM Equations</head><p>We used the LSTM variant implemented in the dynet toolkit, which corresponds to the following 4 https://github.com/clab/dynet 5 https://github.com/ryancotterell/ sigmorphon2016 equations:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Paradigm classification in supervised learning of morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malin</forename><surname>Ahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Markus Forsberg, and Mans Hulden</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1024" to="1029" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The {CELEX} lexical data base on {CDROM}</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Piepenbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rijn</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end attentionbased large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Jointsequence models for grapheme-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Bisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="434" to="451" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<idno type="doi">10.1016/j.specom.2008.01.002</idno>
		<ptr target="https://doi.org/10.1016/j.specom.2008.01.002" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translating into morphologically rich languages with synthetic phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1677" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining morpheme-based machine translation with postprocessing morpheme prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1102" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jason Eisner, and Mans Hulden. 2016. The SIGMORPHON 2016 shared taskmorphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Meeting of SIGMORPHON</title>
		<meeting>the 2016 Meeting of SIGMORPHON</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discovering morphological paradigms from plain text using a dirichlet process mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="616" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Latent-variable modeling of string transductions with finite-state methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1080" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised learning of complete morphological paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1185" to="1195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parameter estimation for probabilistic finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Morphological inflection generation using character sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aoife Cahill, and Fabienne Cap. 2012. Modeling inflection and wordformation in smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<biblScope unit="page" from="664" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04621</idno>
		<title level="m">Factored neural machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of morphological paradigms and lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mans Hulden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malin</forename><surname>Forsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="569" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04868</idno>
		<title level="m">Ilya Sutskever, and Samy Bengio. 2015. A neural transducer</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integrating joint n-gram features into a discriminative training framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Sittichai Jiampojamarn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kondrak</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N10-1103" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="697" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
		<title level="m">Neural machine translation in linear time</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Neural multi-source morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Med: The lmu system for the sigmorphon 2016 shared task on morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Singlemodel encoder-decoder with explicit morphological representation for reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Regular models of phonological rule systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="331" to="378" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Two-level morphology: A general computational model of word-form recognition and production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimmo</forename><surname>Koskenniemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Segmental recurrent neural networks for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00223</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Supervised attentions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1249" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2283" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generating complex morphology for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Minkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th</title>
		<meeting>the 45th</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<ptr target="http://www.aclweb.org/anthology/P07-1017" />
		<title level="m">Annual Meeting of the Association of Computational Linguistics</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A rational design for a weighted finite-state transducer library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Implementing Automata</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="144" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inflection generation as discriminative string transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Nicolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">WFST-based graphemeto-phoneme conversion: Open source tools for alignment, model-building and decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><forename type="middle">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuaki</forename><surname>Minematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keikichi</forename><surname>Hirose</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W12-6208" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 10th International Workshop on Finite State Methods and Natural Language Processing. Association for Computational Linguistics<address><addrLine>Donostia-San Sebastin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="45" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00784</idno>
		<title level="m">Online and Linear-Time Attention by Enforcing Monotonic Alignments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Weighting finite-state transductions with neural context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Still not there? comparing traditional sequence-to-sequence models to encoderdecoder neural networks on monotone string translation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Schnober</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik-Lân Do</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1160" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1703" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Noise-aware character alignment for bootstrapping statistical machine transliteration from bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2013</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="204" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Applying morphology generation models to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achim</forename><surname>Ruopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="514" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Minimally supervised morphological analysis by multimodal alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wicentowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online segment to segment neural transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1307" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
