<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Value of Semantic Parse Labeling for Knowledge Base Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Suh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Value of Semantic Parse Labeling for Knowledge Base Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="201" to="206"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We demonstrate the value of collecting semantic parse labels for knowledge base question answering. In particular, (1) unlike previous studies on small-scale datasets, we show that learning from labeled semantic parses significantly improves overall performance, resulting in absolute 5 point gain compared to learning from answers, (2) we show that with an appropriate user interface, one can obtain semantic parses with high accuracy and at a cost comparable or lower than obtaining just answers, and (3) we have created and shared the largest semantic-parse labeled dataset to date in order to advance research in question answering.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic parsing is the mapping of text to a mean- ing representation. Early work on learning to build semantic parsers made use of datasets of questions and their associated semantic parses <ref type="bibr" target="#b12">(Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b13">Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b9">Wong and Mooney, 2007)</ref>. Recent work on semantic parsing for knowledge base question- answering (KBQA) has called into question the value of collecting such semantic parse labels, with most recent KBQA semantic parsing systems being trained using only question-answer pairs in- stead of question-parse pairs. In fact, there is ev- idence that using only question-answer pairs can yield improved performance as compared with ap- proaches based on semantic parse labels ( . It is also widely believed that collect- ing semantic parse labels can be a "difficult, time consuming task" ( <ref type="bibr" target="#b4">Clarke et al., 2010)</ref> even for do- main experts. Furthermore, recent focus has been more on the final task-specific performance of a system (i.e., did it get the right answer for a ques- tion) as opposed to agreement on intermediate rep- resentations ( <ref type="bibr" target="#b2">Berant et al., 2013;</ref><ref type="bibr" target="#b6">Kwiatkowski et al., 2013)</ref>, which allows for KBQA datasets to be built with only the answers to each question.</p><p>In this work, we re-examine the value of se- mantic parse labeling and demonstrate that seman- tic parse labels can provide substantial value for knowledge base question-answering. We focus on the task of question-answering on Freebase, using the WEBQUESTIONS dataset <ref type="bibr" target="#b2">(Berant et al., 2013)</ref>.</p><p>Our first contribution is the construction of the largest semantic parse dataset for KB question- answering to date. In order to evaluate the costs and benefits of gathering semantic parse labels, we created the WEBQUESTIONSSP dataset 1 , which contains semantic parses for the questions from WEBQUESTIONS that are answerable using Free- base. In particular, we provide SPARQL queries for 4,737 questions. The remaining 18.5% of the original WEBQUESTIONS questions are labeled as "not answerable". This is due to a number of factors including the use of a more stringent as- sessment of "answerable", namely that the ques- tion be answerable via SPARQL rather than by returning or extracting information from textual descriptions. Compared to the previous seman- tic parse dataset on Freebase, <ref type="bibr">Free917 (Cai and Yates, 2013)</ref>, our WEBQUESTIONSSP is not only substantially larger, but also provides the semantic parses in SPARQL with standard Freebase entity identifiers, which are directly executable on Free- base.</p><p>Our second contribution is a demonstration that semantic parses can be collected at low cost. We employ a staged labeling paradigm that enables ef- ficient labeling of semantic parses and improves the accuracy, consistency and efficiency of ob-taining answers. In fact, in a simple comparison with using a web browser to extract answers from freebase.com, we show that we can collect se- mantic parse labels at a comparable or even faster rate than simply collecting answers.</p><p>Our third contribution is an empirical demon- stration that we can leverage the semantic parse labels to increase the accuracy of a state-of-the-art question-answering system. We use a system that currently achieves state-of-the-art performance on KBQA and show that augmenting its training with semantic parse labels leads to an absolute 5-point increase in average F 1 .</p><p>Our work demonstrates that semantic parse la- bels can provide additional value over answer la- bels while, with the right labeling tools, being comparable in cost to collect. Besides accuracy gains, semantic parses also have further benefits in yielding answers that are more accurate and con- sistent, as well as being updatable if the knowl- edge base changes (for example, as facts are added or revised).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Collecting Semantic Parses</head><p>In order to verify the benefits of having labeled semantic parses, we completely re-annotated the WEBQUESTIONS dataset <ref type="bibr" target="#b2">(Berant et al., 2013)</ref> such that it contains both semantic parses and the derived answers. We chose to annotate the ques- tions with the full semantic parses in SPARQL, based on the schema and data of the latest and last version of Freebase (2015-08-09).</p><p>Labeling interface Writing SPARQL queries for natural language questions using a text editor is obviously not an efficient way to provide semantic parses even for experts. Therefore, we designed a staged, dialog-like user interface (UI) to improve the labeling efficiency. Our UI breaks the po- tentially complicated structured-labeling task into separate, but inter-dependent sub-tasks. Given a question, the UI first presents entities detected in the questions using an entity linking system ( <ref type="bibr" target="#b10">Yang and Chang, 2015)</ref>, and asks the user to pick an en- tity in the question as the topic entity that could lead to the answers. The user can also suggest a new entity if none of the candidates returned by the entity linking system is correct. Once the en- tity is selected, the system then requests the user to pick the Freebase predicate that represents the relationship between the answers and this topic entity. Finally, additional filters can be added to further constrain the answers. One key advantage of our UI design is that the annotator only needs to focus on one particular sub-task during each stage. All of the choices made by the labeler are used to automatically construct a coherent semantic parse. Note that the user can easily go back and forth to each of these three stages and change the previous choices, before pressing the final submit button.</p><p>Take the question "who voiced meg on fam- ily guy?" for example. The labeler will be pre- sented with two entity choices: Meg Griffin and Family Guy, where the former links "meg" to the character's entity and the latter links to the TV show. Depending on the entity selected, legiti- mate Freebase predicates of the selected entity will be shown, along with the objects (either proper- ties or entities). Suppose the labeler chooses Meg Griffin as the topic entity. He should then pick actor as the main relationship, meaning the an- swer should be the persons who have played this role. To accurately describe the question, the la- beler should add additional filters like the TV se- ries is Family Guy and the performance type is voice in the final stage 2 .</p><p>The design of our UI is inspired by recent work on semantic parsing that has been applied to the WEBQUESTIONS dataset ( <ref type="bibr" target="#b0">Bast and Haussmann, 2015;</ref><ref type="bibr" target="#b8">Reddy et al., 2014;</ref><ref type="bibr" target="#b1">Berant and Liang, 2014;</ref><ref type="bibr" target="#b11">Yih et al., 2015)</ref>, as these approaches use a sim- pler and yet more restricted semantic representa- tion than first-order logic expressions. Following the notion of query graph in ( <ref type="bibr" target="#b11">Yih et al., 2015)</ref>, the semantic parse is anchored to one of the enti- ties in the question as the topic entity and the core component is to represent the relation between the entity and the answer, referred as the inferential chain. Constraints, such as properties of the an- swer or additional conditions the relation needs to hold, are captured as well. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of these annotated semantic parse com- ponents and the corresponding SPARQL query. While it is clear that our UI does not cover compli- cated, highly compositional questions, most ques- tions in WEBQUESTIONS can be covered 3 .</p><p>Labeling process In order to ensure the data quality, we recruit five annotators who are famil- iar with design of Freebase. Our goal is to provide  correct semantic parses for each of the legitimate and unambiguous questions in WEBQUESTIONS. Our labeling instructions (included in the supple- mentary material) follow several key principles. For instance, the annotators should focus on giv- ing the correct semantic parse of a question, based on the assumption that it will result in correct an- swers if the KB is complete and correct.</p><p>Among all the 5,810 questions in WEB- QUESTIONS, there are 1,073 questions that the an- notators cannot provide the complete parses to find the answers, due to issues with the questions or Freebase. For example, some questions are am- biguous and without clear intent (e.g., "where did romans go?"). Others are questions that Freebase is not the appropriate information source (e.g., "where to watch tv online for free in canada?").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Using Semantic Parses</head><p>In order to compare two training paradigms, learn- ing from question-answer pairs and learning from semantic parses, we adopt the Staged Query Graph Generation (STAGG) algorithm <ref type="bibr" target="#b11">(Yih et al., 2015)</ref>, which achieves the highest published an- swer prediction accuracy on the WEBQUESTIONS dataset. STAGG formulates the output semantic parse in a query graph representation that mimics the design of a graph knowledge base. It searches over potential query graphs for a question, iter- atively growing the query graph by sequentially adding a main topic entity, then adding an in- ferential chain and finally adding a set of con- straints. During the search process, each candi- date query graph is judged by a scoring function on how likely the graph is a correct parse, based on features indicating how each individual com- ponent matches the original question, as well as some properties of the whole query graph. Exam- ple features include the score output by the entity linking system, the match score of the inferential chain to the relation described in the question from a deep neural network model, number of nodes in the candidate query graph, and the number of matching words in constraints. For additional de- tails see <ref type="bibr" target="#b11">(Yih et al., 2015)</ref>.</p><p>When question-answer pairs are available, we create a set of query graphs connecting entities in the question to the answers in the training set, as in ( <ref type="bibr" target="#b11">Yih et al., 2015)</ref>. We score the quality of a query graph by using the F 1 score between the an- swer derived from the query graph and the answer in the training set. These scores are then used in a learning-to-rank approach to predict high-quality query graphs.</p><p>In the case that semantic parses are available, we change the score that we use for evaluating the quality of a query graph. In particular, we assign the query graph score to be zero when- ever the query graph is not a subgraph consis- tent with the semantic parse label and to be the F 1 score described above otherwise. The hope is that by leveraging the semantic parse, we can sig- nificantly reduce the number of incorrect query graphs used during training. For instance, the predicate music.artist.track was incor- rectly predicted as the inferential chain for the question "what are the songs that justin bieber write?", where a correct parse should use the re- lation music.composer.compositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Value of Semantic Parses</head><p>In this section, we explore the costs of collect- ing semantic parse labels and the benefits of using them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benefits of Semantic Parses</head><p>Leveraging the new dataset, we study whether a semantic parser learned using full parses instead of just question-answer pairs can answer questions more accurately, using the knowledge base. Be- low, we describe our basic experimental setting and report the main results.</p><p>Experimental setting We followed the same training/testing splits as in the original WEB- QUESTIONS dataset, but only used questions with complete parses and answers for training and eval- uation in our experiments. In the end, 3,098 ques- tions are used for model training and 1,639 ques- tions are used for evaluation <ref type="bibr">4</ref> . Because there can be multiple answers to a question, precision, re- call and F 1 are computed for each individual ques- tion. The average F 1 score is reported as the main evaluation metric. In addition, we also report the true accuracy -a question is considered answered correctly only when the predicted answers exactly match one of the answer sets.</p><p>Results <ref type="table" target="#tab_1">Table 1</ref> shows the results of two differ- ent models: learning from question-answer pairs vs. learning from semantic parses. With the la- beled parses, the average F 1 score is 4.9-point higher (71.7% vs. 66.8%). The stricter metric, complete answer set accuracy, also reflects the same trend, where the accuracy of training with labeled parses is 5.1% higher than using only the answers (63.9% vs. 58.8%). While it is expected that training using the anno- tated parses could result in a better model, it is still interesting to see the performance gap, especially when the evaluation is on the correctness of the an- swers rather than the parses. We examined the out- put answers to the questions where the two models differ. Although the setting of using answers only often guesses the correct relations connecting the topic entity and answers, it can be confused by re- lated, but incorrect relations as well. Similar phe- nomena also occur on constraints, which suggests that subtle differences in the meaning are difficult</p><note type="other">Labeling Methods Ans. Ans. Sem. Parses Annotator MTurkers Experts Experts Avg. time/Question</note><p>Unknown 82 sec 21 sec Labeling Correctness 66% 92% 94% to catch if the semantic parses are automatically generated using only the answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Costs of Semantic Parses</head><p>Our labeling process is very different from that of the original WEBQUESTIONS dataset, where the question is paired with answers found on the Freebase Website by Amazon MTurk workers. To compare these two annotation methods, we sam- pled 50 questions and had one expert label them using two schemes: finding answers using the Freebase Website and labeling the semantic parses using our UI. The time needed, as well as the cor- rectness of the answers are summarized in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Interestingly, in this study we found that it ac- tually took less time to label these questions with semantic parses using our UI, than to label with only answers. There could be several possible ex- planations. First, as many questions in this dataset are actually "simple" and do not need complicated compositional structured semantic parses, our UI can help make the labeling process very efficient. By ranking the possible linked entities and likely relations, the annotators are able to pick the cor- rect component labels fairly easily. In contrast, simple questions may have many legitimate an- swers. Enumerating all of the correct answers can take significantly longer than authoring a semantic parse that computes them.</p><p>When we compare the annotation quality be- tween labeling semantic parses and answers, we find that the correctness 5 of the answers are about the same (92% vs 94%). In the original WEB- QUESTIONS dataset, only 66% of the answers are completely correct. This is largely due to the low accuracy (42.9%) of the 14 questions contain- ing multiple answers. This indicates that to en- sure data quality, more verification is needed when leveraging crowdsourcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Unlike the work of ( <ref type="bibr" target="#b4">Clarke et al., 2010)</ref>, we demonstrate that semantic parses can improve over state-of-the-art knowledge base question answering systems. There are a number of potential differences that are likely to contribute to this finding. Unlike previous work, we compare training with answers and training with semantic parses while making only minimal changes in a state-of-the-art training algorithm. This enables a more direct evaluation of the potential benefits of using semantic parses. Second, and perhaps the more significant difference, is that our evalu- ation is based on Freebase which is significantly larger than the knowledge bases used in the pre- vious work. We suspect that the gains provided by semantic parse labels are due a significant re- duction in the number of paths between candidate entities and answers when we limit to semantically valid paths. However, in domains where the num- ber of potential paths between candidate entities and answers is small, the value of collecting se- mantic parse labels might also be small.</p><p>Semantic parsing labels provide additional ben- efits. For example, collecting semantic parse la- bels relative to a knowledge base can ensure that the answers are more faithful to the knowledge base and better captures which questions are an- swerable by the knowledge base. Moreover, by creating semantic parses using a labeling system based on the target knowledge base, the correct- ness and completeness of answers can be im- proved. This is especially true for question that have large answer sets. Finally, semantic labels are more robust to changes in knowledge base facts because answers can be computed via exe- cution of the semantic representation for the ques- tion. For instance, the answer to "Who does Chris Hemsworth have a baby with?" might change if the knowledge base is updated with new facts about children but the semantic parse would not need to change.</p><p>Notice that besides being used for the full semantic parsing task, our WEBQUESTIONSSP dataset is a good test bed for several important se- mantic tasks as well. For instance, the topic en- tity annotations are beneficial to training and test- ing entity linking systems. The core inferential chains alone are quality annotations for relation extraction and matching. Specific types of con- straints are useful too. For example, the temporal semantic labels are valuable for identifying tem- poral expressions and their time spans. Because our dataset specifically focuses on questions, it complements existing datasets in these individual tasks, as they tend to target at normal corpora of regular sentences.</p><p>While our labeling interface design was aimed at supporting labeling experts, it would be valu- able to enable crowdsourcing workers to provide semantic parse labels. One promising approach is to use a more dialog-driven interface using natu- ral language (similar to ). Such UI design is also crucial for extending our work to handling more complicated questions. For in- stance, allowing users to traverse longer paths in a sequential manner will increase the expressiveness of the output parses, both in the core relation and constraints. Displaying a small knowledge graph centered at the selected entities and relations may help users explore alternative relations more effec- tively as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example semantic parse of the question (a) "who voiced meg on family guy?" The three components in (b) record the labels collected through our dialog-like user interface, and can be mapped deterministically to either the corresponding query graph (c) or the SPARQL query (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>who voiced meg on family guy? Topic Entity: Meg Griffin (m.035szd) Inf. Chain: in-tv-program -actor Constraints: (1) y 0 -series -Family Guy (m.019nnl) (2) y 0 -performance-type -Voice (m.02nsjvf)</head><label></label><figDesc></figDesc><table>Family Guy 

in-tv-program 
actor 

Meg Griffin 

x 
y 0 

Voice 

PREFIX ns: &lt;http://rdf.freebase.com/ns/&gt; 
SELECT ?x 
WHERE { 
ns:m.035szd ns:tv.tv_character.appeared_in_tv_program ?y0 . 
?y0 ns:tv.regular_tv_appearance.actor ?x ; 
ns:tv.regular_tv_appearance.series ns:m.019nnl ; 
ns:tv.regular_tv_appearance.special_performance_type 
ns:m.02nsjvf . 
} 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : The results of two different model train- ing settings: answers only vs. semantic parses.</head><label>1</label><figDesc></figDesc><table>Training Signals Prec. Rec. Avg. F 1 Acc. 
Answers 
67.3 73.1 66.8 
58.8 
Sem. Parses 
70.9 80.3 71.7 
63.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Comparing labeling methods on 50 sampled ques- tions.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Available at http://aka.ms/WebQSP.</note>

			<note place="foot" n="2"> Screenshots are included in the supplementary material. 3 We manually edited the SPARQL queries for about 3.1% of the questions in WEBQUESTIONS that are not expressible by our UI.</note>

			<note place="foot" n="4"> The average F1 score of the original STAGG&apos;s output to these 1,639 questions is 60.3%, evaluated using WEBQUESTIONS. Note that the number is not directly comparable to what we report in Table 1 because many of the labeled answers in WEBQUESTIONS are either incorrect or incomplete.</note>

			<note place="foot" n="5"> We considered a label to be correct only if the derived/labeled answer set is completely accurate.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Andrei Aron for the initial design of the labeling interface.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">More accurate question answering on Freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Bast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmar</forename><surname>Haussmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1431" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Largescale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="433" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Driving semantic parsing from 205 the world&apos;s response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="18" to="27" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Question-answer driven semantic role labeling: Using natural language to annotate natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1545" to="1556" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing without question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="377" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning synchronous grammars for semantic parsing with lambda calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">S-MART: Novel tree-based structured learning algorithms applied to tweet entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
