<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Iterative Correction for Distant Spelling Errors</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Gubanov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Galinskaya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Baytin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Yandex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Leo Tolstoy St</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country>119021 Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Iterative Correction for Distant Spelling Errors</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="168" to="173"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Noisy channel models, widely used in modern spellers, cope with typical mis-spellings, but do not work well with infrequent and difficult spelling errors. In this paper, we have improved the noisy channel approach by iterative stochastic search for the best correction. The proposed algorithm allowed us to avoid local minima problem and improve the F 1 measure by 6.6% on distant spelling errors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A speller is an essential part of any program as- sociated with text input and processing -e-mail system, search engine, browser, form editor etc. To detect and correct spelling errors, the state of the art spelling correction systems use the noisy channel approach ( <ref type="bibr" target="#b5">Kernighan et al., 1990;</ref><ref type="bibr" target="#b7">Mays et al., 1991;</ref><ref type="bibr" target="#b0">Brill and Moore, 2000</ref>). Its models are usually trained on large corpora and provide high effectiveness in correction of typical errors (most of which consist of 1-2 wrong characters per word), but does not work well for complex (multi- character) and infrequent errors.</p><p>In this paper, we improved effectiveness of the noisy channel for the correction of com- plex errors. In most cases, these are cogni- tive errors in loan words (folsvagen → volkswa- gen), names of drugs (vobemzin → wobenzym), names of brands (scatcher → skechers), scientific terms (heksagidron → hexahedron) and last names <ref type="bibr">(Shwartzneger → Schwarzenegger)</ref>. In all these cases, the misspelled word contains many errors and the corresponding error model penalty cannot be compensated by the LM weight of its proper form. As a result, either the misspelled word it- self, or the other (less complicated, more frequent) misspelling of the same word wins the likelihood race.</p><p>To compensate for this defect of the noisy chan- nel, the iterative approach ( <ref type="bibr" target="#b2">Cucerzan and Brill, 2004</ref>) is typically used. The search for the best variant is repeated several times, what allows cor- recting rather complex errors, but does not com- pletely solve the problem of falling into local min- ima. To overcome this issue we suggest to con- sider more correction hypotheses. For this pur- pose we used a method based on the simulated annealing algorithm. We experimentally demon- strate that the proposed method outperforms the baseline noisy channel and iterative spellers.</p><p>Many authors employ machine learning to build rankers that compensate for the drawbacks of the noisy channel model: ( <ref type="bibr" target="#b8">Whitelaw et al., 2009;</ref><ref type="bibr" target="#b4">Gao et al., 2010</ref>). These techniques can be combined with the proposed method by replacing posterior probability of single correction in our method with an estimate obtained via discriminative training method.</p><p>In our work, we focus on isolated word-error correction <ref type="bibr" target="#b6">(Kukich, 1992)</ref>, which, in a sense, is a harder task, than multi-word correction, because there is no context available for misspelled words. For experiments we used single-word queries to a commercial search engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Baseline speller 2.1 Noisy channel spelling correction</head><p>Noisy channel is a probabilistic model that defines posterior probability P (q 0 |q 1 ) of q 0 being the in- tended word, given the observed word q 1 ; for such model, the optimal decision rule µ is the follow- ing:</p><p>µ(q 1 ) = arg max q 0 P (q 0 |q 1 );</p><formula xml:id="formula_0">P (q 0 |q 1 ) ∝ P dist (q 0 → q 1 )P LM (q 0 ),<label>(1)</label></formula><p>where P LM is the source (language) model, and P dist is the error model. Given P (q 0 |q 1 ) defined, to correct the word q 1 we could iterate through all ever-observed words, and choose the one, that maximizes the posterior probability. However, the practical considerations demand that we do not rank the whole list of words, but instead choose between a limited number of hypotheses h 1 , ..., h K :</p><p>1. Given q 1 , generate a set of hypotheses h 1 , ..., h K , such that</p><formula xml:id="formula_1">K k=1 P (q 0 = h k |q 1 ) ≈ 1;<label>(2)</label></formula><p>2. Choose the hypothesis h k that maximizes P (q 0 = h k |q 1 ).</p><p>If hypotheses constitute a major part of the poste- rior probability mass, it is highly unlikely that the intended word is not among them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Baseline speller setup</head><p>In baseline speller we use a substring-based error model P dist (q 0 → q 1 ) described in <ref type="bibr" target="#b0">(Brill and Moore, 2000</ref>), the error model training method and the hypotheses generator are similar to (Duan and Hsu, 2011). For building language (P LM ) and error (P dist ) models, we use words collected from the 6-months query log of a commercial search engine.</p><p>Hypotheses generator is based on A* beam search in a trie of words, and yields K hy- potheses h k , for which the noisy channel scores P dist (h k → q 1 )P LM (h k ) are highest possible. Hypotheses generator has high K-best recall (see Section 4.2) -in 91.8% cases the correct hy- pothesis is found when K = 30, which confirms the assumption about covering almost all posterior probability mass (see Equation 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Improvements for noisy channel spelling correction</head><p>While choosing arg max of the posterior probabil- ity is an optimal decision rule in theory, in practice it might not be optimal, due to limitations of the language and error modeling. For example, vobe- mzin is corrected to more frequent misspelling vobenzin (instead of correct form wobenzym) by the noisy channel, because P dist (vobemzin → wobenzym) is too low (see <ref type="table">Table 1</ref>).</p><p>There have been attempts ( <ref type="bibr" target="#b2">Cucerzan and Brill, 2004</ref>) to apply other rules, which would over- come limitations of language and error models with compensating changes described further.  <ref type="table">Table 1</ref>: Noisy-channel scores for two corrections of vobemzin</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Iterative correction</head><p>Iterative spelling correction with E iterations uses standard noisy-channel to correct the query q re- peatedly E times. It is motivated by the assump- tion, that we are more likely to successfully correct the query if we take several short steps instead of one big step ( <ref type="bibr" target="#b2">Cucerzan and Brill, 2004</ref>) . Iterative correction is hill climbing in the space of possible corrections: on each iteration we make a transition to the best point in the neighbourhood, i.e. to correction, that has maximal posterior prob- ability P (c|q). As any local search method, itera- tive correction is prone to local minima, stopping before reaching the correct word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stochastic iterative correction</head><p>A common method of avoiding local minima in optimization is the simulated annealing algorithm, key ideas from which can be adapted for spelling correction task. In this section we propose such an adaptation. Consider: we do not always transition deterministically to the next best correction, but instead transition randomly to a (potentially any) correction with transition probability being equal to the posterior P (c i |c i−1 ), where c i−1 is the cor- rection we transition from, c i is the correction we transition to, and P (·|·) is defined by Equation 1. Iterative correction then turns into a random walk: we start at word c 0 = q and stop after E ran- dom steps at some word c E , which becomes our answer.</p><p>To turn random walk into deterministic spelling correction algorithm, we de-randomize it, using the following transformation. Described random walk defines, for each word w, a probability P (c E = w|q) of ending up in w after starting a walk from the initial query q. With that probability defined, our correction algorithm is the following: given query q, pick c = arg max c E P (c E |q) as a correction.</p><p>Probability of getting from c 0 = q to some c E = c is a sum, over all possible paths, of prob- abilities of getting from q to c via specific path</p><formula xml:id="formula_2">q = c 0 → c 1 → ... → c E−1 → c E = c: P (c E |c 0 ) = c 1 ∈W ... c E−1 ∈W E i=1 P (c i |c i−1 ),<label>(3)</label></formula><formula xml:id="formula_3">P (c i |c i−1 ) = P dist (c i → c i−1 )P LM (c i ) P observe (c i−1 ) ,<label>(4)</label></formula><p>where W is the set of all possible words, and P observe (w) is the probability of observing w as a query in the noisy-channel model. Example: if we start a random walk from vobe- mzin and make 3 steps, we most probably will end up in the correct form wobenzym with P = 0.361. A few of the most probable random walk paths are shown in <ref type="table">Table 2</ref>. Note, that despite the fact that most probable path does not lead to the cor- rect word, many other paths to wobenzym sum up to 0.361, which is greater than probability of any other word. Also note, that the method works only because multiple misspellings of the same word are presented in our model; for related research see <ref type="bibr" target="#b1">(Choudhury et al., 2007)</ref>.  <ref type="table">Table 2</ref>: Most probable random walk paths start- ing from c 0 = q = vobemzin (the correct form is in bold).</p><formula xml:id="formula_4">c0 → c1 → c2 → c3 P vobemzin→vobenzin→vobenzin→vobenzin 0</formula><p>Also note, that while Equation 3 uses noisy- channel posteriors, the method can use an arbitrary discriminative model, for example the one from ( <ref type="bibr" target="#b4">Gao et al., 2010)</ref>, and benefit from a more accu- rate posterior estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Additional heuristics</head><p>This section describes some common heuristic im- provements, that, where possible, were applied both to the baseline methods and to the proposed algorithm.</p><p>Basic building block of every mentioned algo- rithm is one-step noisy-channel correction. Each basic correction proceeds as described in Sec- tion 2.1: a small number of hypotheses h 1 , ..., h K is generated for the query q, hypotheses are scored, and scores are recomputed into normalized pos- terior probabilities (see Equation 5). Posterior probabilities are then either used to pick the best correction (in baseline and simple iterative cor- rection), or are accumulated to later compute the score defined by Equation 3.</p><formula xml:id="formula_5">score(h i ) = P dist (h i → q) λ P LM (h i ) P (h i |q) = score(h i ) K j=1 score(h j )<label>(5)</label></formula><p>A standard log-linear weighing trick was ap- plied to noisy-channel model components, see e.g. ( <ref type="bibr" target="#b8">Whitelaw et al., 2009)</ref>. λ is the parameter that controls the trade-off between precision and recall (see Section 4.2) by emphasizing the importance of either the high frequency of the correction or its proximity to the query. We have also found, that resulting posterior probabilities emphasize the best hypothesis too much: best hypothesis gets almost all probability mass and other hypotheses get none. To compen- sate for that, posteriors were smoothed by raising each probability to some power γ &lt; 1 and re- normalizing them afterward:</p><formula xml:id="formula_6">P smooth (h i |q) = P (h i |q) γ K j=1 P (h j |q) γ .<label>(6)</label></formula><p>In a sense, γ is like temperature parameter in sim- ulated annealing -it controls the entropy of the walk and the final probability distribution. Unlike in simulated annealing, we fix γ for all iterations of the algorithm. Finally, if posterior probability of the best hy- pothesis was lower than threshold α, then the orig- inal query q was used as the spell-checker output. (Posterior is defined by Equation 6 for the baseline and simple iterative methods and by Equations 3 and 6 for the proposed method). Parameter α con- trols precision/recall trade-off (as well as λ men- tioned above).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>To evaluate the proposed algorithm we have col- lected two datasets. Both datasets were randomly sampled from single-word user queries from the 1-week query log of a commercial search en- gine. We annotated them with the help of pro- fessional analyst. The difference between datasets is that one of them contained only queries with low search performance: for which the number of documents retrieved by the search engine was less than a fixed threshold (we will address it as the "hard" dataset), while the other dataset had no such restrictions (we will call it "common"). Dataset statistics are shown in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Queries  Increased average error model score and er- ror rate of "common" dataset compared to "hard" shows, that we have indeed managed to collect hard-to-correct queries in the "hard" dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results</head><p>First of all, we evaluated the recall of hypothe- ses generator using K-best recall -the number of correct spelling corrections for misspelled queries among K hypotheses divided by the total number of misspelled queries in the test set. Resulting re- call with K = 30 is 91.8% on "hard" and 98.6% on "common".</p><p>Next, three spelling correction methods were tested: noisy channel, iterative correction and our method (stochastic iterative correction).</p><p>For evaluation of spelling correction quality, we use the following metrics:</p><p>• Precision: The number of correct spelling corrections for misspelled words generated by the system divided by the total number of corrections generated by the system;</p><p>• Recall: The number of correct spelling cor- rections for misspelled words generated by the system divided by the total number of misspelled words in the test set;</p><p>For hypotheses generator, K = 30 was fixed: re- call of 91.8% was considered big enough. Pre- cision/recall tradeoff parameters λ and α (they are applicable to each method, including baseline) were iterated by the grid (0.2, 0.25, 0.3, ..., 1.5) × (0, 0.025, 0.05, ..., 1.0), and E (applicable to it- erative and our method) and γ (just our method) were iterated by the grid <ref type="figure">(2, 3, 4</ref>  We were not able to reproduce superior perfor- mance of the iterative method over the noisy chan- nel, reported by ( <ref type="bibr" target="#b2">Cucerzan and Brill, 2004</ref>). Sup- posedly, it is because the iterative method bene- fits primarily from the sequential application of split/join operations altering query decomposition into words; since we are considering only one- word queries, such decomposition does not matter.</p><p>On the "hard" dataset the performance of the noisy channel and the iterative methods is infe- rior to our proposed method, see <ref type="figure" target="#fig_1">Figure 1</ref>. We tested all three methods on the "common" dataset as well to evaluate if our handling of hard cases affects the performance of our approach on the common cases of spelling error. Our method per- forms well on the common cases as well, as <ref type="figure">Fig- ure 2</ref> shows. The performance comparison for the "common" dataset shows comparable perfor- mance for all considered methods.</p><p>Noisy channel and iterative methods' frontiers are considerably inferior to the proposed method on "hard" dataset, which means that our method works better. The results on "common" dataset show, that the proposed method doesn't work worse than baseline. Next, we optimized parameters for each method and each dataset separately to achieve the highest F 1 measure. Results are shown in <ref type="table" target="#tab_5">Tables 4 and 5</ref>. We can see, that, given the proper tuning, our method can work better on any dataset (but it can- not achieve the best performance on both datasets at once). See <ref type="table" target="#tab_5">Tables 4 and 5</ref>    <ref type="table">Table 5</ref>: Best parameters and F 1 on "common" dataset Next, each parameter was separately iterated (by a coarser grid); initial parameters for each method were taken from <ref type="table" target="#tab_5">Table 4</ref>. Such iteration serves two purposes: to show the influence of pa- rameters on algorithm performance, and to show differences between datasets: in such setup pa- rameters are virtually tuned using "hard" dataset and evaluated using "common" dataset. Results are shown in <ref type="table" target="#tab_7">Table 6</ref>.</p><p>The proposed method is able to successfully correct distant spelling errors with edit distance of 3 characters (see <ref type="table">Table 7</ref>).</p><p>However, if our method is applied to shorter and more frequent queries (as opposed to "hard" dataset), it tends to suggest frequent words as false-positive corrections (for example, grid is cor- rected to creed -Assassin's Creed is popular video game). As can be seen in <ref type="table">Table 5</ref>, in order to fix that, algorithm parameters need to be tuned more towards precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and future work</head><p>In this paper we introduced the stochastic itera- tive correction method for spell check corrections. Our experimental evaluation showed that the pro- posed method improved the performance of popu-     <ref type="table">Table 7</ref>: Correction examples for the noisy chan- nel and the proposed method. lar spelling correction approach -the noisy chan- nel model -in the correction of difficult spelling errors. We showed how to eliminate the local min- ima issue of simulated annealing and proposed a technique to make our algorithm deterministic. The experiments conducted on the specialized datasets have shown that our method significantly improves the performance of the correction of hard spelling errors (by 6.6% F 1 ) while maintain- ing good performance on common spelling errors.</p><p>In continuation of the work we are considering to expand the method to correct errors in multi- word queries, extend the method to work with dis- criminative models, and use a query performance prediction method, which tells for a query whether our algorithm needs to be applied.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, 5, 7, 10) × (0.1, 0.15, ...1.0); for each set of parameters, pre- cision and recall were measured on both datasets. Pareto frontiers for precision and recall are shown in Figures 1 and 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Precision/recall Pareto frontiers on "hard" dataset</figDesc><graphic url="image-1.png" coords="4,307.28,62.81,226.77,172.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Evaluation datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Best parameters and F 1 on "hard" dataset 

Method 
λ 
α 
γ 
E 
F1 
Noisy channel 
0.75 0.225 
-
-
62.06 
Iterative 
0.8 
0.275 
-
2 
63.15 
Stochastic iterative 
1.2 
0.4 
0.35 
3 
63.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Per-coordinate iteration of parameters 
from Table 4; per-method maximum is shown in 
italic, per-dataset in bold 

Query 
Noisy channel Proposed method 
akwamarin akvamarin 
aquamarine 
maccartni 
maccartni 
mccartney 
ariflaim 
ariflaim 
oriflame 
epika 
epica 
replica 
grid 
grid 
creed 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved error model for noisy channel spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 38th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="286" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How difficult is it to develop a perfect spellchecker? a cross-linguistic analysis through complex network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markose</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Ganguly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second workshop on TextGraphs: Graph-based algorithms for natural language processing</title>
		<meeting>the second workshop on TextGraphs: Graph-based algorithms for natural language processing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spelling correction as an iterative process that exploits the collective knowledge of web users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online spelling correction for query completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizhong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-June Paul</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large scale ranker-based system for search query spelling correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Micol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="358" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A spelling correction program based on a noisy channel model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">W</forename><surname>Mark D Kernighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William A</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th conference on Computational linguistics</title>
		<meeting>the 13th conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Techniques for automatically correcting words in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kukich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="439" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Context based spelling correction. Information Processing &amp; Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">J</forename><surname>Damerau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Mercer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="517" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using the web for language independent spellchecking and autocorrection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Whitelaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="890" to="899" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
