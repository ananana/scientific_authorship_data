<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dependency Forest based Word Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Otsuki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Japan Science and Technology Agency</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Japan Science and Technology Agency</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Informatics</orgName>
								<orgName type="institution">Kyoto University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dependency Forest based Word Alignment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-Student Research Workshop <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="8" to="14"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A hierarchical word alignment model that searches for k-best partial alignments on target constituent 1-best parse trees has been shown to outperform previous models. However, relying solely on 1-best parses trees might hinder the search for good alignments because 1-best trees are not necessarily the best for word alignment tasks in practice. This paper introduces a dependency forest based word alignment model, which utilizes target dependency forests in an attempt to minimize the impact on limitations attributable to 1-best parse trees. We present how k-best alignments are constructed over target-side dependency forests. Alignment experiments on the Japanese-English language pair show a relative error reduction of 4% of the alignment score compared to a model with 1-best parse trees.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In statistical machine translation (SMT), word alignment plays an essential role in obtaining phrase tables ( <ref type="bibr" target="#b13">Och and Ney, 2004;</ref><ref type="bibr" target="#b8">Koehn et al., 2003)</ref> or syntactic transformation rules <ref type="bibr" target="#b2">(Chiang, 2007;</ref><ref type="bibr" target="#b17">Shen et al., 2008</ref>). IBM models ( <ref type="bibr" target="#b1">Brown et al., 1993)</ref>, which are based on word sequences, have been widely used for obtaining word align- ments because they are fast and their implementa- tion is available as GIZA++. <ref type="bibr">1</ref> Recently, a hierarchical alignment model (whose implementation is known as Nile 2 ) ( <ref type="bibr" target="#b15">Riesa et al., 2011</ref>), which performs better than IBM models, has been proposed. In the hierarchi- cal alignment model, both source and target con-stituency trees are used for incorporating syntactic information as features, and it searches for k-best partial alignments on the target constituent parse trees. It achieved significantly better results than the IBM Model4 in Arabic-English and Chinese- English word alignment tasks, even though the model was trained on only 2,280 and 1,102 par- allel sentences as gold standard alignments. How- ever, their models rely only on 1-best source and target side parse trees, which are not necessarily good for word alignment tasks.</p><p>In SMT, forest-based decoding has been pro- posed for both constituency and dependency parse trees ( <ref type="bibr" target="#b11">Mi et al., 2008;</ref><ref type="bibr" target="#b20">Tu et al., 2010)</ref>. A forest is a compact representation of n-best parse trees. It provides more alternative parse trees to choose from during decoding, leading to significant im- provements in translation quality. In this paper, we borrow this idea to build an alignment model us- ing dependency forests rather than 1-best parses, which makes it possible to provide the model with more alternative parse trees that may be suitable for word alignment tasks. The motivation of using dependency forests instead of constituency forests in our model is that dependency forests are more appropriate for alignments between language pairs with long-distance reordering, such as the one we study in this paper. This is because they are more suitable for capturing the complex semantic rela- tions of words in a sentence <ref type="bibr" target="#b5">(Kahane, 2012)</ref>.</p><p>We conducted alignment experiments on the Japanese-English language pair. Experimental re- sults show a relative error reduction of 4% of the alignment score compared to the model with 1- best parse trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dependency Forest</head><p>We first briefly explain dependency forests that are used in our model before describing the alignment <ref type="figure">Figure 1</ref>: Bottom-up search for alignments over target-side dependency forest (This forest encodes 2-best parse trees for the sentence "he saw a girl with a telescope." The source sentence is " (He) (telescope) (with) (girl) (saw)". There are two interpretations for this sen- tence; either "with a telescope" depends on "saw" or "boy.") construction method. A dependency forest is rep- resented by a hypergraph ⟨V, E⟩, where V is a set of nodes and E is a set of hyperedges.</p><p>A hyperedge e connects nodes in the forest and is defined to be a triple ⟨tails(e), head(e), score⟩, where tails(e) is a set of dependents of e, head(e) is the head of e, and score is the score of e that is usually obtained by heuristics ( <ref type="bibr" target="#b20">Tu et al., 2010)</ref>. For example, e 1 in <ref type="figure">Figure 1</ref> is equal to ⟨(he 0,1 , boy 2,4 , with 4,7 ), saw 0,7 , 1.234⟩. In our model, we use Algorithm 1 to compute hyperedge scores. Edges in a hyperedge are defined to be the ones obtained by connecting each tail with the head (Line 11). Hyperedge score is the sum of all the scores of edges in it (Line 12). The score of an edge is the normalized sum of the scores of all parses which contain the edge (Line 7).</p><p>Every node in a dependency forest corresponds to a word attached with a span, which is a range of word indices covered by the node. Following ( <ref type="bibr" target="#b20">Tu et al., 2010</ref>), a span is represented in the form i, j, which indicates the node covers all the words from i-th to (j − 1)-th word. This requires dependency forests to be projective. Separate nodes are used for a word if the nodes in dependency trees have different spans. For example, in <ref type="figure">Figure 1</ref> there are two nodes for the word "boy" because they have different spans (i.e., <ref type="bibr">(2,</ref><ref type="bibr">4)</ref> and <ref type="formula">(2, 7)</ref>).</p><p>The construction of a dependency forest from</p><formula xml:id="formula_0">Input : n-best dependency parses {T i } n i=1</formula><p>of a sentence Score of</p><formula xml:id="formula_1">T i Score i Output: A forest F of {T i } n i=1 1 F = CreateForestStructure({T i } n i=1 ) 2 edgeScores = {} 3 minScore = M in({Score i } n i=1</formula><p>) 4 for i = 1 to n do Note that the dependency forest obtained from this method does not necessarily encode ex- actly the dependency trees from which they are created. Usually there are more trees that can be extracted from the dependency forests <ref type="bibr" target="#b0">(Boullier et al., 2009</ref>). In our experiment, when we use the term "a n-best dependency forest", we indicate a dependency forest that is created from n-best de- pendency trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Finding Alignments over Forest</head><p>Following the hierarchical alignment model ( <ref type="bibr" target="#b15">Riesa et al., 2011</ref>), our model searches for the best alignment by constructing partial alignments (hypotheses) over target dependency forests in a bottom-up manner as shown in <ref type="figure">Figure 1</ref>. The algorithm for constructing alignments is shown in Algorithm 2. Note that source depen- dency forests are included in the input to the al- gorithm. This is optional but can be included for richer features. Each node in the forest has partial alignments sorted by alignment scores. Because it is computationally expensive to keep all possi- ble partial alignments for each node, we keep a beam size of k. A partial alignment for a node is an alignment matrix for target words that are cov-ered by the node. In <ref type="figure">Figure 1</ref>, each partial align- ment is represented as a black square. Scores of the partial alignments are a linear combination of features. There are two types of features: local and non-local features. A feature f is defined to be lo- cal if and only if it can be factored among the lo- cal productions in a tree, and non-local otherwise <ref type="bibr" target="#b4">(Huang, 2008)</ref>.</p><p>We visit the nodes in the topological order, to guarantee that we visit a node after visiting all its tail nodes (Line 1). For each node, we first gen- erates partial alignments, which are one column alignment matrices for its word. Because of time complexity, we only generates null, single link and double link alignment (Line 5). A single and dou- ble link alignment refer to a column matrix having exactly one and two alignments, respectively, as shown in <ref type="figure">Figure 1</ref>. For each partial alignment, we compute its score using local features (Line 7) and pushed to a priority queue B v (Line 8). These par- tial alignments are represented by black squares in a blue container in <ref type="figure">Figure 1</ref>. Then, we compute partial alignments for the target words covered by the node, by combining tails' partial alignments and one column alignments for its word using non- local features (Line 10 -14), which is represented by the orange arrows in <ref type="figure">Figure 1</ref>. k-best com- bined partial alignments are put in Y v (Line 14). They are represented by black squares in a yellow container in <ref type="figure">Figure 1</ref>. Here, we use cube prun- ing <ref type="bibr" target="#b2">(Chiang, 2007)</ref> to get the approximate k-best combinations. Note that in the search over con- stituency parse trees, one column alignment ma- trices are generated only on the leaf node ( <ref type="bibr" target="#b15">Riesa et al., 2011</ref>), whereas we generate them also on non- leaf nodes in the search over dependency forests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Features</head><p>The features we used include those used in Nile except for the automatically extracted rule and constellation features. This is because these fea- tures are not easily applicable to dependency forests. As shown in our experiments, these fea- tures have a contribution to the alignment score. However, our primary purpose is to show the ef- fect of using forests on alignment quality.</p><p>Several features in Nile such as source-target POS local feature and coordination feature have to be customized for dependency forests, because it is possible that there are multiple nodes that cor- respond to the same word. We decided to consider all nodes corresponding to a word by counting the </p><note type="other">Input : Source and target sentence s, t Dependency forest F s over s Dependency forest F t over t Set of feature functions h Weight vector w Beam size k Output: A k-best list of alignments over s and t 1 for v ∈TopologicalSort(F</note><formula xml:id="formula_2">Push(α v , ⟨Y c 1 , · · · , Y c |c| , B v ⟩) 13 end 14 Y v =CubePruning(α v , k, w, h, v, s, t, F s , F t )</formula><p>15 end Algorithm 2: Construction of alignments frequency of each POS tag of a node, and normal- izing it with the total frequency of POS tags in the forest. For example, suppose there are four nodes which correspond to the same word, whose POS tags are JJ, VBG, JJ, VGZ. In this case the features "src-tgt-pos-feature-JJ=0.5", "src-tgt-pos-feature- VBG=0.25" and "src-tgt-pos-feature-VBZ=0.25" are activated.</p><p>Besides the features used in Nile, our model uses a contiguous alignment local feature and a hyperedge score non-local feature. The contigu- ous alignment feature fires when a target word is aligned to multiple source words, and these words are contiguous on a forest. Preliminary experi- ments showed, however, that none of these fea- tures contributed to the improvement of the align- ment score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>We conducted alignment experiments on the Japanese-English language pair. For dependency parsers, we used KNP ( <ref type="bibr" target="#b7">Kawahara and Kurohashi, 2006</ref>) for Japanese and Berkeley Parser ( <ref type="bibr" target="#b14">Petrov and Klein, 2007)</ref> for English. We converted con- stituent parse trees obtained by Berkeley Parser to dependency parse trees using rules. <ref type="bibr">3</ref> We used 300, 100, 100 sentences from ASPEC-JE 2 for train- ing, development and test data, respectively. <ref type="bibr">4</ref> Our model as well as Nile has a feature called third party alignment feature, which activates for an alignment link that is presented in the alignment of a third party model. The beam size k was set to 128. We used different number of parse trees to create a target forest, e.g., 1, 10, 20, 50, 100 and 200. <ref type="bibr">5</ref> The baseline in this experiment is a model with 1-best parse trees on the target side. For reference, we also experimented on Nile 6 , the Bayesian subtree alignment model (Nakazawa model) ( <ref type="bibr" target="#b12">Nakazawa and Kurohashi, 2011</ref>) and IBM Model4. <ref type="bibr">7</ref> We used Nile without automatically ex- tracted rule features and constellation features to make a fair comparison with our model. <ref type="table">Table 1</ref> shows the alignment results evaluated on precision, recall and F-score for each experimen- tal setting. The first row shows the names of dif- ferent experimental settings. Each number in the row shows the number of n-best parse trees used to create target forests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We can observe that using forests improves the score. However, the improvement does not mono- tonically increase with the number of trees on the target side. When 100-best is used in target side, it achieved the highest error reduction of 4% com- pared to the baseline model. <ref type="bibr">8</ref> We also conducted experiments on different number of beam size k, e.g, 200 and 300, from the insight that a larger number of trees encoded in a forest indicates that more noisy partial align- ments are generated, using the same k as the 1-best model is not sufficient. However, we could not ob- serve significant improvements. <ref type="bibr">3</ref> The conversion program is available at https://github.com/hitochan777/mt-tools/releases/tag/1.0.1 4 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ <ref type="bibr">5</ref> In the experiments, we used 1-best parse trees for the source side. Although our model also allows to use forests on the source side, preliminary experiments showed that using forests on the source side does not improve the alignment score. <ref type="bibr">6</ref> Note that Nile uses 1-best constituency parse tree <ref type="bibr">7</ref> The alignments from Nakazawa model and IBM Model 4 were symmetrized with the grow-diag-final heuristic.</p><p>8 (82.39 − 81.66) / (100 − 81.66) ≈ 4%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We observed the improvement of alignments by using forests. We checked whether good parse trees were chosen when higher F-scores were achieved. It turned out that better parse trees led to higher F-scores, as shown in <ref type="figure">Figure 2a</ref>, but it was not always the case. <ref type="figure">Figure 2a</ref> shows an improved example by us- ing 100-best trees on the target side. In the fig- ure, we can observe that "" and "of" are cor- rectly aligned. We observe that the English 1-best parse tree is incorrect, whereas 100-best model were able to choose a better tree. <ref type="figure">Figure 2b</ref> shows a worsened example by using 200-best trees on the target side. We can see that the 200-best model aligned many words unneces- sarily and the wrong tree is chosen even though the 1-best parse is good. There were many cases in which forests are harmful for alignments. There are two possible reasons. Firstly, most of the fea- tures in our model comes from Nile, but they are not informative enough to choose better parses from forests. Secondly, our model is likely to suf- fer from the data sparseness because using forests generates more noise than 1-best parses.</p><p>For our model to benefit from forests we have to consider the following: Firstly, our model's fea- ture is based on the assumption that source and target forests contain trees with similar structures to each other. However the projectivity of forests prohibits our model from generating (choosing) target trees that are similar to the ones in source forests. Secondly, we observed the cases where no parse in forests captures the correct root and the difference of n-best parses are mainly POS tags of words.</p><p>Our model performs on par with Nile because our model is based on Nile. However, our model outperforms the Nakazawa model and IBM Model4. This is because our model is supervised but these models are unsupervised. The Nakazawa model outperformed IBM Model4 because it uti- lizes dependency trees, which provide richer in- formation.  <ref type="table">Table 1</ref>: Precision, Recall and F-score for ASPEC-JE. The numbers in the first row refer to the number of k-best parse trees used to generate forests. They introduced a way to extract phrase pairs and estimate their probabilities. Their proposed method outperformed the baseline which uses n- best alignments. <ref type="bibr" target="#b21">Venugopal et al. (2008)</ref> used n-best alignments and parses to generate fraction counts used for machine translation downstream estimation. While their approaches are to use n- best alignments already obtained from some align- ment models, our model finds k-best list of align- ments for given sentences. <ref type="bibr" target="#b11">Mi et al. (2008)</ref> and <ref type="bibr" target="#b20">Tu et al. (2010)</ref> used packed constituency forests and dependency forests re- spectively for decoding. The best path that is suit- able for translation is chosen from the forest dur- ing decoding, leading to significant improvement in translation quality. Note that they do not use forests for obtaining word alignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>The approaches for modeling word alignment can be divided into two categories: discrimi- native models <ref type="bibr" target="#b3">(Dyer et al., 2011;</ref><ref type="bibr" target="#b16">Setiawan et al., 2010</ref>) and generative models ( <ref type="bibr" target="#b1">Brown et al., 1993;</ref><ref type="bibr" target="#b12">Nakazawa and Kurohashi, 2011</ref>). Gener- ative models such as the IBM models <ref type="bibr" target="#b1">(Brown et al., 1993)</ref> have the advantage that they do not re- quire golden alignment training data annotated by humans. However, it is difficult to incorporate arbitrary features in these models. On the other hand, discriminative models can incorporate arbi- trary features such as syntactic information, but they generally require gold training data, which is hard to obtain in large scale. For discriminative models, word alignment models using deep neural network have been proposed recently ( <ref type="bibr" target="#b19">Tamura et al., 2014;</ref><ref type="bibr" target="#b18">Songyot and Chiang, 2014;</ref><ref type="bibr" target="#b22">Yang et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we proposed a hierarchical alignment model based on dependency forests, which ad- vanced an alignment model that uses constituency parse trees ( <ref type="bibr" target="#b15">Riesa et al., 2011</ref>) to allow to use more suitable parse trees for word alignment. Experi- mental results on the Japanese-English language pair show a relative error reduction of 4% of the alignment score compared to a model with 1-best parse trees that using forest on the target side.</p><p>Our future work will involve the implementa- tion of missing features, because the automatic translation rule features had a large contribution to the improvement of alignment quality in Nile.</p><p>The experimental results show that Nile, which uses 1-best constituency parses , had almost the same F-score as our proposed method with 100- best parse trees. It will be interesting to see the effect of using forests in Nile.</p><p>Moreover, we are considering to investigate the efficacy of our model with different parsers and language pairs. Finally, we are also considering using training data with richer information such as the one de- scribed in ( <ref type="bibr" target="#b9">Li et al., 2010</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5</head><label></label><figDesc>Score i − = minScore 6 for edge ∈ T i do 7 edgeScores [edge] + = 1 n Score i 8 end 9 end 10 for hyperEdge ∈ F do 11 for edge ∈ hyperEdge do 12 hyperEdge.score+ = edgeScores [edge] 13 end 14 end Algorithm 1: Computation of a hyperedge score dependency trees is done by sharing the common nodes and edges (Line 1). The common nodes are those with the same span and part-of-speech (POS) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>i</head><label></label><figDesc>= word-index-of(v) 5 links = {(0, i)} ∪SingleLinks(i) ∪DoubleLinks(i) 6 for link ∈ links do 7 score = w · h(links, v, s, t, F s , F t ) 8 Push(B v , ⟨score, link⟩, k) 9 end 10 for hyperEdge ∈InHyperEdges(v) do 11 c = hyperEdge.tail 12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>Figure 2: Alignment result: Black boxes represent golden alignments. Triangles represent 1-best model alignments. Circles represent the alignments of proposed model. Black and red arcs represent 1-best parses and chosen parses respectively.</figDesc></figure>

			<note place="foot" n="1"> http://www.statmt.org/moses/giza/GIZA++.html 2 http://jasonriesa.github.io/nile/</note>

			<note place="foot" n="5"> Related Work Studies have been conducted to make use of more alternatives to cope with the unreliability of 1best results. Liu et al. (2009) proposed a structure called weighted alignment matrix, which encodes the distribution over all possible alignments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Graham Neubig for his valuable com-ments during a pre-submission mentoring pro-gram, which had greatly improved the quality of the manuscript. We also thank the anonymous re-viewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constructing parse forests that include exactly the n-best pcfg trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Boullier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies (IWPT&apos;09)</title>
		<meeting>the 11th International Conference on Parsing Technologies (IWPT&apos;09)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-10" />
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised word alignment with arbitrary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="409" to="419" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Why to choose dependency rather than constituency for syntax: a formal point of view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sylvain Kahane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Apresjan</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iomdin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milicevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Polgù Ere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wanner</surname></persName>
		</author>
		<title level="m">Meanings, Texts, and other exciting things: A Festschrift to Commemorate the 80th Anniversary of Professor Igor A. Mel cuk</title>
		<imprint>
			<biblScope unit="page" from="257" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fully-lexicalized probabilistic model for japanese syntactic and case structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="176" to="183" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enriching word alignment with linguistic tags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation Conference</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Weighted alignment matrices for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1017" to="1026" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Forestbased translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian subtree alignment model based on dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="794" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The alignment template approach to statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="449" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-04" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature-rich language-independent syntax-based alignment for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discriminative word alignment with a function word reordering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="534" to="544" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new string-to-dependency machine translation algorithm with a target dependency language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving word alignment using word similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theerawat</forename><surname>Songyot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1840" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for word alignment model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Antenna Measurement Techniques Association</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dependency forest for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Sook</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1092" to="1100" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wider pipelines: N-best alignments and parses in mt training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Antenna Measurement Techniques Association</title>
		<meeting>Antenna Measurement Techniques Association</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Word alignment modeling with context dependent deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Antenna Measurement Techniques Association</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="166" to="175" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
