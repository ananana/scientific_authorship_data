<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">Language Technologies Institute, Carnegie Mellon University ~ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">⇤</forename><surname>}~</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">Language Technologies Institute, Carnegie Mellon University ~ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
							<email>sthomson@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">Language Technologies Institute, Carnegie Mellon University ~ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">⇤</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">Language Technologies Institute, Carnegie Mellon University ~ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">Language Technologies Institute, Carnegie Mellon University ~ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">}</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">Language Technologies Institute, Carnegie Mellon University ~ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">Language Technologies Institute, Carnegie Mellon University ~ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">|</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Engineering</orgName>
								<orgName type="laboratory">Language Technologies Institute, Carnegie Mellon University ~ Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="295" to="305"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>295</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a new model that aims to bridge these two approaches. SoPa combines neural representation learning with weighted finite-state automata (WFSAs) to learn a soft version of traditional surface patterns. We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly , to a restricted form of WFSA. Empirically , on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks <ref type="bibr">(RNNs;</ref><ref type="bibr" target="#b13">Elman, 1990)</ref> and convolutional neural networks (CNNs; <ref type="bibr" target="#b28">LeCun, 1998</ref>) are two of the most useful text repre- sentation learners in NLP <ref type="bibr" target="#b17">(Goldberg, 2016)</ref>. These methods are generally considered to be quite dif- ferent: the former encodes an arbitrarily long se- quence of text, and is highly expressive <ref type="bibr" target="#b51">(Siegelmann and Sontag, 1995)</ref>. The latter is more local, encoding fixed length windows, and accordingly less expressive. In this paper, we seek to bridge the gap between RNNs and CNNs, presenting SoPa (for Soft Patterns), a model that lies in between them.</p><p>SoPa is a neural version of a weighted finite- state automaton (WFSA), with a restricted set of transitions. Linguistically, SoPa is appealing as it ⇤ The first two authors contributed equally. Self-loops allow for repeat- edly inserting words (e.g., "funny"). ✏-transitions allow for dropping words (e.g., "a").</p><p>is able to capture a soft notion of surface patterns (e.g., "what a great X !"; Hearst, 1992), where some words may be dropped, inserted, or replaced with similar words (see <ref type="figure" target="#fig_0">Figure 1</ref>). From a model- ing perspective, SoPa is interesting because WF- SAs are well-studied and come with efficient and flexible inference algorithms <ref type="bibr" target="#b35">(Mohri, 1997;</ref><ref type="bibr" target="#b11">Eisner, 2002</ref>) that SoPa can take advantage of. SoPa defines a set of soft patterns of different lengths, with each pattern represented as a WFSA (Section 3). While the number and lengths of the patterns are hyperparameters, the patterns them- selves are learned end-to-end. SoPa then repre- sents a document with a vector that is the aggre- gate of the scores computed by matching each of the patterns with each span in the document. Be- cause SoPa defines a hidden state that depends on the input token and the previous state, it can be thought of as a simple type of RNN.</p><p>We show that SoPa is an extension of a one- layer CNN (Section 4). Accordingly, one-layer CNNs can be viewed as a collection of linear- chain WFSAs, each of which can only match fixed-length spans, while our extension allows matches of flexible-length. As a simple type of RNN that is more expressive than a CNN, SoPa helps to link CNNs and RNNs.</p><p>To test the utility of SoPa, we experiment with three text classification tasks (Section 5). We compare against four baselines, including both a bidirectional LSTM and a CNN. Our model performs on par with or better than all base- lines on all tasks (Section 6). Moreover, when training with smaller datasets, SoPa is particu- larly useful, outperforming all models by sub- stantial margins. Finally, building on the con- nections discovered in this paper, we offer a new, simple method to interpret SoPa (Section 7). This method applies equally well to CNNs. We release our code at https://github.com/ Noahs-ARK/soft_patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Surface patterns. Patterns <ref type="bibr" target="#b19">(Hearst, 1992)</ref> are particularly useful tool in NLP ( <ref type="bibr" target="#b33">Lin et al., 2003;</ref><ref type="bibr" target="#b14">Etzioni et al., 2005;</ref>). The most basic definition of a pattern is a sequence of words and wildcards (e.g., "X is a Y"), which can either be manually defined or extracted from a corpus using cooccurrence statistics. Patterns can then be matched against a specific text span by re- placing wildcards with concrete words.  introduced a flexible no- tion of patterns, which supports partial matching of the pattern with a given text by skipping some of the words in the pattern, or introducing new words. In their framework, when a sequence of text partially matches a pattern, hard-coded partial scores are assigned to the pattern match. Here, we represent patterns as WFSAs with neural weights, and support these partial matches in a soft manner.</p><p>WFSAs. We review weighted finite-state au- tomata with ✏-transitions before we move on to our special case in Section 3. A WFSA-✏ with d states over a vocabulary V is formally defined as a tu- ple F = h⇡, T, ⌘i, where ⇡ 2 R d is an initial weight vector, T : (V [ {✏}) ! R d⇥d is a transi- tion weight function, and ⌘ 2 R d is a final weight vector. Given a sequence of words in the vocab- ulary x = hx 1 , . . . , x n i, the Forward algorithm ( <ref type="bibr" target="#b1">Baum and Petrie, 1966</ref>) scores x with respect to F . Without ✏-transitions, Forward can be written as a series of matrix multiplications:</p><formula xml:id="formula_0">p 0 span (x) = ⇡ &gt; n Y i=1 T(x i ) ! ⌘<label>(1)</label></formula><p>✏-transitions are followed without consuming a word, so Equation 1 must be updated to reflect the possibility of following any number (zero or more) of ✏-transitions in between consuming each word:</p><formula xml:id="formula_1">p span (x) = ⇡ &gt; T(✏) ⇤ n Y i=1 T(x i )T(✏) ⇤ ! ⌘ (2)</formula><p>where ⇤ is matrix asteration: A ⇤ := P 1 j=0 A j . In our experiments we use a first-order approxima- tion, A ⇤ ⇡ I + A, which corresponds to allow- ing zero or one ✏-transition at a time. When the FSA F is probabilistic, the result of the Forward algorithm can be interpreted as the marginal prob- ability of all paths through F while consuming x (hence the symbol "p").</p><p>The Forward algorithm can be generalized to any semiring <ref type="bibr" target="#b11">(Eisner, 2002</ref>), a fact that we make use of in our experiments and analysis. <ref type="bibr">1</ref> The vanilla version of Forward uses the sum-product semiring: is addition, ⌦ is multiplication. A special case of Forward is the Viterbi algorithm <ref type="bibr" target="#b57">(Viterbi, 1967)</ref>, which sets to the max opera- tor. Viterbi finds the highest scoring path through F while consuming x. Both Forward and Viterbi have runtime O(d 3 + d 2 n), requiring just a sin- gle linear pass through the phrase. Using first- order approximate asteration, this runtime drops to O(d 2 n). <ref type="bibr">2</ref> Finally, we note that Forward scores are for ex- act matches-the entire phrase must be consumed. We show in Section 3.2 how phrase-level scores can be summarized into a document-level score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SoPa: A Weighted Finite-State Automaton RNN</head><p>We introduce SoPa, a WFSA-based RNN, which is designed to represent text as collection of sur- face pattern occurrences. We start by showing how a single pattern can be represented as a WFSA-✏ (Section 3.1). Then we describe how to score a complete document using a pattern (Section 3.2), and how multiple patterns can be used to encode a document (Section 3.3). Finally, we show that SoPa can be seen as a simple variant of an RNN (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Patterns as WFSAs</head><p>We describe how a pattern can be represented as a WFSA-✏. We first assume a single pattern. A pat- tern is a WFSA-✏, but we impose hard constraints on its shape, and its transition weights are given by differentiable functions that have the power to capture concrete words, wildcards, and everything in between. Our model is designed to behave sim- ilarly to flexible hard patterns (see Section 2), but to be learnable directly and "end-to-end" through backpropagation. Importantly, it will still be inter- pretable as simple, almost linear-chain, WFSA-✏. Each pattern has a sequence of d states (in our experiments we use patterns of varying lengths be- tween 2 and 7). Each state i has exactly three pos- sible outgoing transitions: a self-loop, which al- lows the pattern to consume a word without mov- ing states, a main path transition to state i + 1 which allows the pattern to consume one token and move forward one state, and an ✏-transition to state i + 1, which allows the pattern to move forward one state without consuming a token. All other transitions are given score 0. When process- ing a sequence of text with a pattern p, we start with a special START state, and only move for- ward (or stay put), until we reach the special END state. <ref type="bibr">3</ref> A pattern with d states will tend to match token spans of length d 1 (but possibly shorter spans due to ✏-transitions, or longer spans due to self-loops). See <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration.</p><p>Our transition function, T, is a parameterized function that returns a d ⇥ d matrix. For a word x:</p><formula xml:id="formula_2">[T(x)] i,j = 8 &gt; &lt; &gt; : E(u i · v x + a i ), if j = i (self-loop) E(w i · v x + b i ), if j = i + 1 0, otherwise,<label>(3)</label></formula><p>where u i and w i are vectors of parameters, a i and b i are scalar parameters, v x is a fixed pre-trained word vector for x, <ref type="bibr">4</ref> and E is an encoding function, typically the identity function or sigmoid. ✏-tran- sitions are also parameterized, but don't consume a token and depend only on the current state:</p><formula xml:id="formula_3">[T(✏)] i,j = ( E(c i ), if j = i + 1 0, otherwise,<label>(4)</label></formula><p>where c i is a scalar parameter. <ref type="bibr">5</ref> As we have only <ref type="bibr">3</ref> To ensure that we start in the START state and end in the END state, we fix ⇡ = [1, 0, . . . , 0] and ⌘ = [0, . . . , 0, 1]. <ref type="bibr">4</ref> We use GloVe 300d 840B ( <ref type="bibr" target="#b41">Pennington et al., 2014)</ref>. <ref type="bibr">5</ref> Adding ✏-transitions to WFSAs does not increase their three non-zero diagonals in total, the matrix multi- plications in Equation 2 can be implemented using vector operations, and the overall runtimes of For- ward and Viterbi are reduced to O(dn). 6</p><p>Words vs. wildcards. Traditional hard patterns distinguish between words and wildcards. Our model does not explicitly capture the notion of ei- ther, but the transition weight function can be in- terpreted in those terms. Each transition is a logis- tic regression over the next word vector v x . For example, for a main path out of state i, T has two parameters, w i and b i . If w i has large magnitude and is close to the word vector for some word y (e.g., w i ⇡ 100v y ), and b i is a large negative bias (e.g., b i ⇡ ⇡100), then the transition is essentially matching the specific word y. Whereas if w i has small magnitude (w i ⇡ 0) and b i is a large pos- itive bias (e.g., b i ⇡ 100), then the transition is ignoring the current token and matching a wild- card. <ref type="bibr">7</ref> The transition could also be something in between, for instance by focusing on specific di- mensions of a word's meaning encoded in the vec- tor, such as POS or semantic features like animacy or concreteness ( <ref type="bibr" target="#b44">Rubinstein et al., 2015;</ref><ref type="bibr" target="#b55">Tsvetkov et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Scoring Documents</head><p>So far we described how to calculate how well a pattern matches a token span exactly (consuming the whole span). To score a complete document, we prefer a score that aggregates over all matches on subspans of the document (similar to "search" instead of "match" in regular expression parlance). We still assume a single pattern. Either the Forward algorithm can be used to cal- culate the expected count of the pattern in the doc- ument, P 1ijn p span (x i:j ), or Viterbi to calcu- late s doc (x) = max 1ijn s span (x i:j ), the score of the highest-scoring match. In short documents, we expect patterns to typically occur at most once, so in our experiments we choose the Viterbi algo- rithm, i.e., the max-product semiring.</p><p>Implementation details. We give the specific recurrences we use to score documents in a single expressive power, and in fact slightly complicates the For- ward equations. We use them as they require fewer parame- ters, and make the modeling connection between (hard) flex- ible patterns and our (soft) patterns more direct and intuitive. <ref type="bibr">6</ref> Our implementation is optimized to run on GPUs, so the observed runtime is even sublinear in d.</p><p>7 A large bias increases the eagerness to match any word. pass with this model. We define:</p><formula xml:id="formula_4">[maxmul(A, B)] i,j = max k A i,k B k,j . (5)</formula><p>We also define the following for taking zero or one ✏-transitions:</p><formula xml:id="formula_5">eps (h) = maxmul (h, max(I, T(✏))) (6)</formula><p>where max is element-wise max. We maintain a row vector h t at each token: 8</p><formula xml:id="formula_6">h 0 = eps(⇡ &gt; ),<label>(7a)</label></formula><formula xml:id="formula_7">h t+1 = max (eps(maxmul (h t , T(x t+1 ))), h 0 ),<label>(7b)</label></formula><p>and then extract and aggregate END state values:</p><formula xml:id="formula_8">s t = maxmul (h t , ⌘),<label>(8a)</label></formula><formula xml:id="formula_9">s doc = max 1tn s t .<label>(8b)</label></formula><p>[h t ] i represents the score of the best path through the pattern that ends in state i after consuming t tokens. By including h 0 in Equation 7b, we are accounting for spans that start at time t + 1. s t is the maximum of the exact match scores for all spans ending at token t. And s doc is the maximum score of any subspan in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aggregating Multiple Patterns</head><p>We describe how k patterns are aggregated to score a document. These k patterns give k dif- ferent s doc scores for the document, which are stacked into a vector z 2 R k and constitute the final document representation of SoPa. This vec- tor representation can be viewed as a feature vec- tor. In this paper, we feed it into a multilayer per- ceptron (MLP), culminating in a softmax to give a probability distribution over document labels. We minimize cross-entropy, allowing the SoPa and MLP parameters to be learned end-to-end. SoPa uses a total of (2e + 3)dk parameters, where e is the word embedding dimension, d is the number of states and k is the number of patterns. For comparison, an LSTM with a hidden dimen- sion of h has 4((e + 1)h + h 2 ). In Section 6 we show that SoPa consistently uses fewer parameters than a BiLSTM baseline to achieve its best result. <ref type="bibr">8</ref> Here a row vector h of size n can also be viewed as a 1 ⇥ n matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">SoPa as an RNN</head><p>SoPa can be considered an RNN. As shown in Sec- tion 3.2, a single pattern with d states has a hidden state vector of size d. Stacking the k hidden state vectors of k patterns into one vector of size k ⇥ d can be thought of as the hidden state of our model. This hidden state is, like in any other RNN, depen- dent of the input and the previous state. Using self- loops, the hidden state at time point i can in theory depend on the entire history of tokens up to x i (see <ref type="figure">Figure 2</ref> for illustration). We do want to discour- age the model from following too many self-loops, only doing so if it results in a better fit with the remainder of the pattern. To do this we use the sigmoid function as our encoding function E (see Equation 3), which means that all transitions have scores strictly less than 1. This works to keep pat- tern matches close to their intended length. Using other encoders, such as the identity function, can result in different dynamics, potentially encourag- ing rather than discouraging self-loops.</p><p>Although even single-layer RNNs are Turing complete ( <ref type="bibr" target="#b51">Siegelmann and Sontag, 1995</ref>), SoPa's expressive power depends on the semiring. When a WFSA is thought of as a function from finite sequences of tokens to semiring values, it is re- stricted to the class of functions known as rational series <ref type="bibr" target="#b46">(Schützenberger, 1961;</ref><ref type="bibr" target="#b10">Droste and Gastin, 1999;</ref><ref type="bibr" target="#b45">Sakarovitch, 2009)</ref>. <ref type="bibr">9</ref> It is unclear how lim- iting this theoretical restriction is in practice, es- pecially when SoPa is used as a component in a larger network. We defer the investigation of the exact computational properties of SoPa to future work. In the next section, we show that SoPa is an extension of a one-layer CNN, and hence more expressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SoPa as a CNN Extension</head><p>A convolutional neural network (CNN; <ref type="bibr" target="#b28">LeCun, 1998</ref>) moves a fixed-size sliding window over the document, producing a vector representation for each window. These representations are then of- ten summed, averaged, or max-pooled to produce a document-level representation <ref type="bibr" target="#b25">(Kim, 2014;</ref><ref type="bibr" target="#b59">Yin and Schütze, 2015)</ref>. In this section, we show that SoPa is an extension of one-layer, max-pooled CNNs.</p><p>To recover a CNN from a soft pattern with d + 1 states, we first remove self-loops and ✏-transitions,  <ref type="figure">Figure 2</ref>: State activations of two patterns as they score a document. pattern1 (length three) matches on "in years". pattern2 (length five) matches on "funniest and most likeable book", using a self-loop to consume the token "most". Active states in the best match are marked with arrow cursors.</p><p>retaining only the main path transitions. We also use the identity function as our encoder E (Equa- tion 3), and use the max-sum semiring. With only main path transitions, the network will not match any span that is not exactly d tokens long. Using max-sum, spans of length d will be assigned the score:</p><formula xml:id="formula_10">s span (x i:i+d ) = d1 X j=0 w j · v x i+j + b j ,<label>(9a)</label></formula><formula xml:id="formula_11">=w 0:d · v x i:i+d + d1 X j=0 b j , (9b)</formula><p>where</p><formula xml:id="formula_12">w 0:d = [w &gt; 0 ; . . . ; w &gt; d1 ] &gt; , v x i:i+d = [v &gt; x i ; . . . ; v &gt; x i+d1</formula><p>] &gt; . Rearranged this way, we rec- ognize the span score as an affine transformation of the concatenated word vectors v x i:i+d . If we use k patterns, then together their span scores cor- respond to a linear filter with window size d and output dimension k. 10 A single pattern's score for a document is:</p><formula xml:id="formula_13">s doc (x) = max 1ind+1 s span (x i:i+d ).<label>(10)</label></formula><p>The max in Equation 10 is calculated for each pattern independently, corresponding exactly to element-wise max-pooling of the CNN's output layer. Based on the equivalence between this impov- erished version of SoPa and CNNs, we conclude that one-layer CNNs are learning an even more <ref type="bibr">10</ref> This variant of SoPa has d bias parameters, which cor- respond to only a single bias parameter in a CNN. The re- dundant biases may affect optimization but are an otherwise unimportant difference. restricted class of WFSAs (linear-chain WFSAs) that capture only fixed-length patterns.</p><p>One notable difference between SoPa and arbi- trary CNNs is that in general CNNs can use any filter (like an MLP over v x i:i+d , for example). In contrast, in order to efficiently pool over flexible- length spans, SoPa is restricted to operations that follow the semiring laws. <ref type="bibr">11</ref> As a model that is more flexible than a one-layer CNN, but (arguably) less expressive than many RNNs, SoPa lies somewhere on the continuum be- tween these two approaches. Continuing to study the bridge between CNNs and RNNs is an exciting direction for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To evaluate SoPa, we apply it to text classification tasks. Below we describe our datasets and base- lines. More details can be found in Appendix A.</p><p>Datasets. We experiment with three binary clas- sification datasets.</p><p>• SST. The Stanford Sentiment Treebank (Socher et al., 2013) 12 contains roughly 10K movie re- views from Rotten Tomatoes, 13 labeled on a scale of 1-5. We consider the binary task, which considers 1 and 2 as negative, and 4 and 5 as positive (ignoring 3s). It is worth noting that this dataset also contains syntactic phrase level an- notations, providing a sentiment label to parts of sentences. In order to experiment in a realistic setup, we only consider the complete sentences, and ignore syntactic annotations at train or test time. The number of training/development/test sentences in the dataset is 6,920/872/1,821.</p><p>• Amazon.</p><p>The Amazon Review Corpus ( <ref type="bibr" target="#b34">McAuley and Leskovec, 2013)</ref>  <ref type="bibr">14</ref> contains elec- tronics product reviews, a subset of a larger re- view dataset. Each document in the dataset con- tains a review and a summary. Following Yo- gatama et al. <ref type="formula" target="#formula_0">(2015)</ref>, we only use the reviews part, focusing on positive and negative reviews. The number of training/development/test sam- ples is 20K/5K/25K.</p><p>• ROC. The ROC story cloze task ( <ref type="bibr" target="#b38">Mostafazadeh et al., 2016</ref>) is a story understanding task. <ref type="bibr">15</ref> The task is composed of four-sentence story pre- fixes, followed by two competing endings: one that makes the joint five-sentence story coher- ent, and another that makes it incoherent. Fol- lowing <ref type="bibr" target="#b49">Schwartz et al. (2017)</ref>, we treat it as a style detection task: we treat all "right" endings as positive samples and all "wrong" ones as neg- ative, and we ignore the story prefix. We split the development set into train and development (of sizes 3,366 and 374 sentences, respectively), and take the test set as-is (3,742 sentences).</p><p>Reduced training data. In order to test our model's ability to learn from small datasets, we also randomly sample 100, 500, 1,000 and 2,500 SST training instances and 100, 500, 1,000, 2,500, 5,000, and 10,000 Amazon training instances. De- velopment and test sets remain the same.</p><p>Baselines. We compare to four baselines: a BiL- STM, a one-layer CNN, DAN (a simple alterna- tive to RNNs) and a feature-based classifier trained with hard-pattern features.</p><p>• BiLSTM. Bidirectional LSTMs have been suc- cessfully used in the past for text classification tasks ( <ref type="bibr" target="#b64">Zhou et al., 2016)</ref>. We learn a one-layer BiLSTM representation of the document, and feed the average of all hidden states to an MLP.</p><p>• CNN. CNNs are particularly useful for text classification <ref type="bibr" target="#b25">(Kim, 2014)</ref>. We train a one-layer CNN with max-pooling, and feed the resulting representation to an MLP.</p><p>• DAN. We learn a deep averaging network with word dropout <ref type="bibr" target="#b23">(Iyyer et al., 2015</ref>), a simple but strong text-classification baseline.</p><p>• Hard. We train a logistic regression classifier with hard-pattern features. Following , we replace low frequency words with a special wildcard symbol. We learn sequences of 1-6 concrete words, where any number of wild- cards can come between two adjacent words. We consider words occurring with frequency of at least 0.01% of our training set as concrete words, and words occurring in frequency 1% or less as wildcards. <ref type="bibr">16</ref> Number of patterns. SoPa requires specifying the number of patterns to be learned, and their lengths. Preliminary experiments showed that the model doesn't benefit from more than a few dozen patterns. We experiment with several configu- rations of patterns of different lengths, generally considering 0, 10 or 20 patterns of each pattern length between 2-7. The total number of patterns learned ranges between 30-70. <ref type="bibr">17</ref>   <ref type="table">The table also</ref> shows the number of parameters used by each model for each task. Given enough data, mod- els with more parameters should be expected to perform better. However, SoPa performs better or roughly the same as a BiLSTM, which has 3-6 times as many parameters. <ref type="figure" target="#fig_2">Figure 3</ref> shows a comparison of all models on the SST and Amazon datasets with varying train- ing set sizes. SoPa is substantially outperform- ing all baselines, in particular BiLSTM, on small datasets (100 samples). This suggests that SoPa is better fit to learn from small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Ablation analysis. <ref type="table" target="#tab_1">Table 1</ref> also shows an abla- tion of the differences between SoPa and CNN: max-product semiring with sigmoid vs. max-sum semiring with identity, self-loops, and ✏-transi- tions. The last line is equivalent to a CNN with   <ref type="table" target="#tab_1">Table 1</ref>: Test classification accuracy (and the number of parameters used). The bottom part shows our ablation results: SoPa: our full model. SoPa ms 1 : running with max-sum semiring (rather than max-product), with the identity function as our encoder E (see Equation 3). sl: self-loops, ✏: ✏ transitions. The final row is equivalent to a one-layer CNN. multiple window sizes. Interestingly, the most no- table difference between SoPa and CNN is the semiring and encoder function, while ✏ transitions and self-loops have little effect on performance. <ref type="bibr">18</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Interpretability</head><p>We turn to another key aspect of SoPa-its inter- pretability. We start by demonstrating how we in- terpret a single pattern, and then describe how to interpret the decisions made by downstream clas- sifiers that rely on SoPa-in this case, a sentence classifier. Importantly, these visualization tech- niques are equally applicable to CNNs.</p><p>Interpreting a single pattern. In order to visu- alize a pattern, we compute the pattern matching scores with each phrase in our training dataset, and select the k phrases with the highest scores. <ref type="table" target="#tab_4">Ta- ble 2</ref> shows examples of six patterns learned us- ing the best SoPa model on the SST dataset, as <ref type="bibr">18</ref> Although SoPa does make use of them-see Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highest Scoring Phrases</head><p>Patt. <ref type="table" target="#tab_1">1   thoughtful  ,  reverent  portrait  of  and  astonishingly articulate  cast  of  entertaining ,  thought-provoking film  with  gentle  ,  mesmerizing  portrait  of  poignant  and  uplifting  story</ref>   represented by their five highest scoring phrases in the training set. A few interesting trends can be observed from these examples. First, it seems our patterns encode semantically coherent expres- sions. A large portion of them correspond to senti- ment (the five top examples in the table), but others capture different semantics, e.g., time expressions. Second, it seems our patterns are relatively soft, and allow lexical flexibility. While some patterns do seem to fix specific words, e.g., "of" in the first example or "minutes" in the last one, even in those cases some of the top matching spans replace these words with other, similar words ("with" and "half- hour", respectively). Encouraging SoPa to have more concrete words, e.g., by jointly learning the word vectors, might make SoPa useful in other contexts, particularly as a decoder. We defer this direction to future work.</p><p>Finally, SoPa makes limited but non-negligible use of self-loops and epsilon steps. Interestingly, the second example shows that one of the pat-Analyzed Documents it 's dumb , but more importantly , it 's just not scary though moonlight mile is replete with acclaimed actors and actresses and tackles a subject that 's potentially moving , the movie is too predictable and too self-conscious to reach a level of high drama While its careful pace and seemingly opaque story may not satisfy every moviegoer 's appetite, the film 's final scene is soaringly , transparently moving unlike the speedy wham-bam effect of most hollywood of- ferings , character development -and more importantly, character empathy -is at the heart of italian for beginners . the band 's courage in the face of official repression is in- spiring , especially for aging hippies ( this one included ) . <ref type="table">Table 3</ref>: Documents from the SST training data. Phrases with the largest contribution toward a pos- itive sentiment classification are in bold green, and the most negative phrases are in italic orange.</p><p>terns had an ✏-transition at the same place in every phrase. This demonstrates a different function of ✏-transitions than originally designed-they allow a pattern to effectively shorten itself, by learning a high ✏-transition parameter for a certain state.</p><p>Interpreting a document. SoPa provides an in- terpretable representation of a document-a vec- tor of the maximal matching score of each pat- tern with any span in the document. To visual- ize the decisions of our model for a given docu- ment, we can observe the patterns and correspond- ing phrases that score highly within it.</p><p>To understand which of the k patterns con- tributes most to the classification decision, we ap- ply a leave-one-out method. We run the forward method of the MLP layer in SoPa k times, each time zeroing-out the score of a different pattern p. The difference between the resulting score and the original model score is considered p's contri- bution. We then consider the highest contributing patterns, and attach each one with its highest scor- ing phrase in that document. <ref type="table">Table 3</ref> shows exam- ple texts along with their most positive and nega- tive contributing phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Weighted finite-state automata. WFSAs and hidden Markov models <ref type="bibr">19</ref> were once popular in au- tomatic speech recognition <ref type="bibr" target="#b20">(Hetherington, 2004;</ref><ref type="bibr" target="#b37">Moore et al., 2006;</ref><ref type="bibr" target="#b22">Hoffmeister et al., 2012</ref>) <ref type="bibr">19</ref> HMMs are a special case of WFSAs ( <ref type="bibr" target="#b36">Mohri et al., 2002</ref>). and remain popular in morphology <ref type="bibr" target="#b9">(Dreyer, 2011;</ref><ref type="bibr" target="#b6">Cotterell et al., 2015)</ref>. Most closely related to this work, neural networks have been combined with weighted finite-state transducers to do morpholog- ical reinflection <ref type="bibr" target="#b42">(Rastogi et al., 2016</ref>). These prior works learn a single FSA or FST, whereas our model learns a collection of simple but comple- mentary FSAs, together encoding a sequence. We are the first to incorporate neural networks both before WFSAs (in their transition scoring func- tions), and after (in the function that turns their vector of scores into a final prediction), to produce an expressive model that remains interpretable.</p><p>Recurrent neural networks. The ability of RNNs to represent arbitrarily long sequences of embedded tokens has made them attractive to NLP researchers. The most notable variants, the long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU; <ref type="bibr" target="#b4">Cho et al., 2014</ref>), have become ubiqui- tous in NLP algorithms <ref type="bibr" target="#b17">(Goldberg, 2016)</ref>. Re- cently, several works introduced simpler versions of RNNs, such as recurrent additive networks ( <ref type="bibr" target="#b29">Lee et al., 2017)</ref> and Quasi-RNNs ( <ref type="bibr" target="#b2">Bradbury et al., 2017)</ref>. Like SoPa, these models can be seen as points along the bridge between RNNs and CNNs.</p><p>Other works have studied the expressive power of RNNs, in particular in the context of WFSAs or HMMs ( <ref type="bibr" target="#b5">Cleeremans et al., 1989;</ref><ref type="bibr" target="#b16">Giles et al., 1992;</ref><ref type="bibr" target="#b56">Visser et al., 2001;</ref><ref type="bibr" target="#b3">Chen et al., 2018)</ref>. In this work we relate CNNs to WFSAs, showing that a one-layer CNN with max-pooling can be simu- lated by a collection of linear-chain WFSAs.</p><p>Convolutional neural networks. CNNs are prominent feature extractors in NLP, both for gen- erating character-based embeddings <ref type="bibr" target="#b26">(Kim et al., 2016)</ref>, and as sentence encoders for tasks like text classification <ref type="bibr" target="#b59">(Yin and Schütze, 2015)</ref> and machine translation <ref type="bibr" target="#b15">(Gehring et al., 2017)</ref>. Sim- ilarly to SoPa, several recently introduced vari- ants of CNNs support varying window sizes by ei- ther allowing several fixed window sizes <ref type="bibr" target="#b59">(Yin and Schütze, 2015)</ref> or by supporting non-consecutive n-gram matching ( <ref type="bibr" target="#b30">Lei et al., 2015;</ref><ref type="bibr" target="#b39">Nguyen and Grishman, 2016)</ref>.</p><p>Neural networks and patterns. Some works used patterns as part of a neural network. <ref type="bibr" target="#b48">Schwartz et al. (2016)</ref> used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word contexts. <ref type="bibr" target="#b50">Shwartz et al. (2016)</ref> designed an LSTM representation for dependency patterns, us- ing them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs.</p><p>Interpretability. There have been several ef- forts to interpret neural models. The weights of the attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> are often used to display the words that are most sig- nificant for making a prediction. LIME ( <ref type="bibr" target="#b43">Ribeiro et al., 2016</ref>) is another approach for visualizing neural models (not necessarily textual). <ref type="bibr" target="#b61">Yogatama and Smith (2014)</ref> introduced structured sparsity, which encodes linguistic information into the reg- ularization of a model, thus allowing to visualize the contribution of different bag-of-word features.</p><p>Other works jointly learned to encode text and extract the span which best explains the model's prediction ( <ref type="bibr" target="#b58">Yessenalina et al., 2010;</ref><ref type="bibr" target="#b31">Lei et al., 2016)</ref>. <ref type="bibr" target="#b32">Li et al. (2016)</ref> and <ref type="bibr">Kádár et al. (2017)</ref> sug- gested a method that erases pieces of the text in or- der to analyze their effect on a neural model's de- cisions. Finally, several works presented methods to visualize deep CNNs ( <ref type="bibr" target="#b63">Zeiler and Fergus, 2014;</ref><ref type="bibr" target="#b52">Simonyan et al., 2014;</ref><ref type="bibr" target="#b62">Yosinski et al., 2015)</ref>, fo- cusing on visualizing the different layers of the network, mainly in the context of image and video understanding. We believe these two types of research approaches are complementary: invent- ing general purpose visualization tools for exist- ing black-box models on the one hand, and on the other, designing models like SoPa that are inter- pretable by construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We introduced SoPa, a novel model that combines neural representation learning with WFSAs. We showed that SoPa is an extension of a one-layer CNN. It naturally models flexible-length spans with insertion and deletion, and it can be easily customized by swapping in different semirings. SoPa performs on par with or strictly better than four baselines on three text classification tasks, while requiring fewer parameters than the stronger baselines. On smaller training sets, SoPa outper- forms all four baselines. As a simple version of an RNN, which is more expressive than one-layer CNNs, we hope that SoPa will encourage future research on the bridge between these two mecha- nisms. To facilitate such research, we release our implementation at https://github.com/ Noahs-ARK/soft_patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A representation of a surface pattern as a six-state automaton. Self-loops allow for repeatedly inserting words (e.g., "funny"). ✏-transitions allow for dropping words (e.g., "a").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Test accuracy on SST and Amazon with varying number of training instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 shows</head><label>1</label><figDesc></figDesc><table>our main experimental results. In 
two of the cases (SST and ROC), SoPa outper-
forms all models. On Amazon, SoPa performs 
within 0.3 points of CNN and BiLSTM, and out-
performs the other two baselines. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>in</head><label></label><figDesc></figDesc><table>Patt. 2 

's 
✏ 
uninspired 
story 
. 
this 
✏ 
bad 
on 
purpose 
this 
✏ 
leaden 
comedy 
. 
a 
✏ 
half-assed 
film 
. 
is 
✏ 
clumsy ,SL 
the 
writing 

Patt. 3 

mesmerizing portrait 
of 
a 
engrossing 
portrait 
of 
a 
clear-eyed 
portrait 
of 
an 
fascinating portrait 
of 
a 
self-assured portrait 
of 
small 

Patt. 4 

honest 
, 
and 
enjoyable 
soulful 
, scathingSL and 
joyous 
unpretentious , charmingSL , 
quirky 
forceful 
, 
and 
beautifully 
energetic 
, 
and 
surprisingly 

Patt. 5 

is 
deadly 
dull 
a 
numbingly dull 
is 
remarkably dull 
is 
a 
phlegmatic 
an 
utterly 
incompetent 

Patt. 6 

five 
minutes 
four 
minutes 
final 
minutes 
first 
half-hour 
fifteen 
minutes 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Six patterns of different lengths learned 
by SoPa on SST. Each group represents a single 
pattern p, and shows the five phrases in the training 
data that have the highest score for p. Columns 
represent pattern states. Words marked with SL are 
self-loops. ✏ symbols indicate ✏-transitions. All 
other words are from main path transitions. 

</table></figure>

			<note place="foot" n="1"> The semiring parsing view (Goodman, 1999) has produced unexpected connections in the past (Eisner, 2016). We experiment with max-product and max-sum semirings, but note that our model could be easily updated to use any semiring. 2 In our case, we also use a sparse transition matrix (Section 3.1), which further reduces our runtime to O(dn).</note>

			<note place="foot" n="9"> Rational series generalize recognizers of regular languages, which are the special case of the Boolean semiring.</note>

			<note place="foot" n="11"> The max-sum semiring corresponds to a linear filter with max-pooling. Other semirings could potentially model more interesting interactions, but we leave this to future work. 12 https://nlp.stanford.edu/sentiment/ index.html 13 http://www.rottentomatoes.com</note>

			<note place="foot" n="14"> http://riejohnson.com/cnn_data.html 15 http://cs.rochester.edu/nlp/ rocstories/</note>

			<note place="foot" n="16"> Some words may serve as both words and wildcards. See Davidov and Rappoport (2008) for discussion. 17 The number of patterns and their length are hyperparameters tuned on the development data (see Appendix A).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We thank Dallas Card, Elizabeth Clark, Peter Clark, Bhavana Dalvi, Jesse Dodge, Nicholas FitzGerald, Matt Gardner, Yoav Goldberg, Mark Hopkins, Vidur Joshi, Tushar Khot, Kelvin Luu, Mark Neumann, Hao Peng, Matthew E. Peters, Sasha Rush, Ashish Sabharwal, Minjoon Seo, Sofia Serrano, Swabha Swayamdipta, Chenhao Tan, Niket Tandon, Trang Tran, Mark Yatskar, Scott Yih, Vicki Zayats, Rowan Zellers, Luke</head><p>Zettlemoyer, and several anonymous reviewers for their helpful advice and feedback. This work was supported in part by NSF grant IIS-1562364, by the Extreme Science and Engineering Discov-ery Environment (XSEDE), which is supported by NSF grant ACI-1548562, and by the NVIDIA Corporation through the donation of a Tesla GPU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical Inference for Probabilistic Functions of Finite State Markov Chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Petrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1554" to="1563" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quasi-Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recurrent neural networks as weighted language recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorcha</forename><surname>Gilroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finite state automata and simple recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Cleeremans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Servan-Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James L</forename><surname>Mcclelland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="372" to="381" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling word forms using latent underlying morphs and phonology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="433" to="447" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of generic relationships using pattern clusters and its evaluation by automatically generated SAT analogy questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhanced sentiment learning using twitter hashtags and smileys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A Non-parametric Model for the Discovery of Inflectional Paradigms from Plain Text Using Graphical Models over Strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Baltimore, MD, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The KleeneSchützenberger theorem for formal power series in partially commuting variables. Information and Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Droste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Gastin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="47" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parameter estimation for probabilistic finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inside-outside and forwardbackward algorithms are just backprop (tutorial paper)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised named-entity extraction from the web: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anamaria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="134" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning and extracting finite state automata with second-order recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><forename type="middle">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsinghen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Zheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee-Chun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="405" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A primer on neural network models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="345" to="420" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<title level="m">Semiring parsing. Computational Linguistics</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="573" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The MIT finite-state transducer toolkit for speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Hetherington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INTERSPEECH</title>
		<meeting>of INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Hoffmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<title level="m">WFST enabled solutions to ASR problems: Beyond HMM decoding. IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="551" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representation of linguistic form and function in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>´ Akos Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afra</forename><surname>Chrupala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alishahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="761" to="780" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based Learning Applied to Document Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07393</idno>
		<title level="m">Recurrent additive networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Molding CNNs for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rationalizing Neural Predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08220</idno>
		<title level="m">Understanding Neural Networks through Representation Erasure</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Identifying synonyms among distributionally similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of RecSys</title>
		<meeting>of RecSys</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Finite-state transducers in language and speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="269" to="311" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Juicer: A weighted finite-state transducer speech decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Magimai-Doss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jithendra</forename><surname>Vepa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLMI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling Skip-Grams for Event Detection with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Weighting finite-state transductions with neural context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Why should I trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">How well do distributional models capture different types of semantic knowledge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Effi</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rational and recognisable power series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Sakarovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Weighted Automata</title>
		<editor>Manfred Droste, Werner Kuich, and Heiko Vogler</editor>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="105" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the definition of a family of automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Schützenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="270" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Symmetric pattern based word embeddings for improved word similarity prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Symmetric patterns and coordinations: Fast and enhanced representations of verbs and adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The effect of different writing tasks on linguistic style: A case study of the roc story cloze task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc.of CoNLL</title>
		<meeting>.of CoNLL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On the computational power of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">D</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and system sciences</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="150" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR Workshop</title>
		<meeting>of ICLR Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ICWSM-a great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICWSM</title>
		<meeting>of ICWSM</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Evaluation of word vector representations by subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Hidden markov model interpretations of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Maartje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raijmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Molenaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionist Models of Learning, Development and Evolution</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-level structured models for documentlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multichannel Variable-Size Convolution for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bayesian optimization of text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Linguistic structured sparsity in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Mai</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICML Deep Learning Workshop</title>
		<meeting>of the ICML Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Text classification improved by integrating bidirectional LSTM with two-dimensional max pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
