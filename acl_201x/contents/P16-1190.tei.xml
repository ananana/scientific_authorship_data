<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointly Learning to Embed and Predict with Multiple Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Ferreira</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Unbabel Lda</orgName>
								<address>
									<addrLine>Rua Visconde de Santarém, 67-B</addrLine>
									<postCode>1000-286</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Instituto de Telecomunicações</orgName>
								<orgName type="institution" key="instit2">Instituto Superior Técnico</orgName>
								<address>
									<postCode>1049-001</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><forename type="middle">S C</forename><surname>Almeida</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Priberam Labs</orgName>
								<address>
									<addrLine>Alameda D. Afonso Henriques, 41, 2 o</addrLine>
									<postCode>1000-123</postCode>
									<settlement>Lisboa</settlement>
									<country key="PT">Portugal</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jointly Learning to Embed and Predict with Multiple Languages</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2019" to="2028"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a joint formulation for learning task-specific cross-lingual word em-beddings, along with classifiers for that task. Unlike prior work, which first learns the embeddings from parallel data and then plugs them in a supervised learning problem, our approach is one-shot: a single optimization problem combines a co-regularizer for the multilingual embeddings with a task-specific loss. We present theoretical results showing the limitation of Euclidean co-regularizers to increase the embedding dimension, a limitation which does not exist for other co-regularizers (such as the 1-distance). Despite its simplicity, our method achieves state-of-the-art accuracies on the RCV1/RCV2 dataset when transferring from English to German, with training times below 1 minute. On the TED Corpus, we obtain the highest reported scores on 10 out of 11 languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations of text (embeddings) have been the target of much research in natural language processing <ref type="bibr" target="#b6">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b26">Mikolov et al., 2013;</ref><ref type="bibr" target="#b27">Pennington et al., 2014;</ref><ref type="bibr" target="#b20">Levy et al., 2015)</ref>. Word embeddings partially capture semantic and syntactic properties of text in the form of dense real vectors, making them apt for a wide variety of tasks, such as language model- ing ( <ref type="bibr" target="#b2">Bengio et al., 2003)</ref>, sentence tagging ( <ref type="bibr" target="#b34">Turian et al., 2010;</ref><ref type="bibr" target="#b7">Collobert et al., 2011</ref>), sentiment anal- ysis <ref type="bibr" target="#b32">(Socher et al., 2011</ref>), parsing <ref type="bibr" target="#b5">(Chen and Manning, 2014</ref>), and machine translation ( <ref type="bibr" target="#b36">Zou et al., 2013)</ref>.</p><p>At the same time, there has been a consis- tent progress in devising "universal" multilin- gual models via cross-lingual transfer techniques of various kinds <ref type="bibr" target="#b16">(Hwa et al., 2005;</ref><ref type="bibr" target="#b35">Zeman and Resnik, 2008;</ref><ref type="bibr" target="#b25">McDonald et al., 2011;</ref><ref type="bibr" target="#b12">Ganchev and Das, 2013;</ref><ref type="bibr" target="#b24">Martins, 2015)</ref>. This line of re- search seeks ways of using data from resource- rich languages to solve tasks in resource-poor languages. Given the difficulty of handcrafting language-independent features, it is highly appeal- ing to obtain rich, delexicalized, multilingual rep- resentations embedded in a shared space.</p><p>A string of work started with <ref type="bibr" target="#b17">Klementiev et al. (2012)</ref> on learning bilingual embeddings for text classification. <ref type="bibr">Hermann and Blunsom (2014)</ref> pro- posed a noise-contrastive objective to push the embeddings of parallel sentences to be close in space. A bilingual auto-encoder was proposed by <ref type="bibr" target="#b4">Chandar et al. (2014)</ref>, while <ref type="bibr" target="#b11">Faruqui and Dyer (2014)</ref> applied canonical correlation analysis to parallel data to improve monolingual embeddings. Other works optimize a sum of monolingual and cross-lingual terms ( <ref type="bibr" target="#b14">Gouws et al., 2015;</ref><ref type="bibr" target="#b33">Soyer et al., 2015)</ref>, or introduce bilingual variants of skip-gram ( <ref type="bibr" target="#b8">Coulmance et al., 2015)</ref>. Recently,  extended the non-compositional paragraph vectors of <ref type="bibr" target="#b19">Le and Mikolov (2014)</ref> to a bilingual setting, achieving a new state of the art at the cost of more expensive (and non-deterministic) prediction.</p><p>In this paper, we propose an alternative joint formulation that learns embeddings suited to a par- ticular task, together with the corresponding clas- sifier for that task. We do this by minimizing a combination of a supervised loss function and a multilingual regularization term. Our approach leads to a convex optimization problem and makes a bridge between classical co-regularization ap- proaches for semi-supervised learning <ref type="bibr" target="#b31">(Sindhwani et al., 2005;</ref><ref type="bibr" target="#b1">Altun et al., 2005;</ref><ref type="bibr" target="#b13">Ganchev et al., 2008</ref>) and modern representation learning. In addition, we show that Euclidean co-regularizers have serious limitations to learn rich embeddings, when the number of task labels is small. We es- tablish this by proving that the resulting embed- ding matrices have their rank upper bounded by the number of labels. This limitation does not ex- ist for other regularizers (convex or not), such as the 1 -distance and noise-contrastive distances.</p><p>Our experiments in the RCV1/RCV2 dataset yield state-of-the-art accuracy (92.7%) with this simple convex formulation, when transferring from English to German, without the need of neg- ative sampling, extra monolingual data, or non- additive representations. For the reverse direction, our best number (79.3%), while far behind the re- cent para_doc approach ( , is on par with current compositional methods.</p><p>On the TED corpus, we obtained general pur- pose multilingual embeddings for 11 target lan- guages, by considering the (auxiliary) task of reconstructing pre-trained English word vectors. The resulting embeddings led to cross-lingual multi-label classifiers that achieved the highest re- ported scores on 10 out of these 11 languages. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Cross-Lingual Text Classification</head><p>We consider a cross-lingual classification frame- work, where a classifier is trained on a dataset from a source language (such as English) and ap- plied to a target language (such as German). Later, we generalize this setting to multiple target lan- guages and to other tasks besides classification.</p><p>The following data are assumed available:</p><formula xml:id="formula_0">1. A labeled dataset D l := {{x (m) , y (m) )} M m=1</formula><p>, consisting of text documents x in the source lan- guage categorized with a label y ∈ {1, . . . , L}.</p><p>2. An unlabeled parallel corpus</p><formula xml:id="formula_1">D u := {(s (n) , t (n) )} N n=1</formula><p>, containing sentences s in the source language paired with their translations t in the target language (but no information about their categories).</p><p>Let V S and V T be the vocabulary size of the source and target languages, respectively. Throughout, we represent sentences s ∈ R V S and t ∈ R V T as vectors of word counts, and documents x as an average of sentence vectors. We assume that the unlabeled sentences largely outnumber the la- beled documents, N M , and that the number of labels L is relatively small. The goal is to use the data above to learn a classifier h : R V T → {1, . . . , L} for the target language.</p><p>This problem is usually tackled with a two-stage approach: in the first step, bilingual word embed- dings P ∈ R V S ×K and Q ∈ R V T ×K are learned from D u , where each row of these matrices con- tains a Kth dimensional word representation in a shared vector space. In the second step, a standard classifier is trained on D l , using the source embed- dings P ∈ R V S ×K . Since the embeddings are in a shared space, the trained model can be applied directly to classify documents in the target lan- guage. We describe next these two steps in more detail. We assume throughout an additive repre- sentation for sentences and documents (denoted ADD by <ref type="bibr">Hermann and Blunsom (2014)</ref>). These representations can be expressed algebraically as P x, P s, Q t ∈ R K , respectively.</p><p>Step 1: Learning the Embeddings. The cross- lingual embeddings P and Q are trained so that the representations of paired sentences (s, t) ∈ D u have a small (squared) Euclidean distance</p><formula xml:id="formula_2">d 2 (s, t) = 1 2 P s − Q t 2 .<label>(1)</label></formula><p>Since a direct minimization of Eq. 1 leads to a de- generate solution (P = 0, Q = 0), <ref type="bibr">Hermann and Blunsom (2014)</ref> use instead a noise-contrastive large-margin distance obtained via negative sam- pling,</p><formula xml:id="formula_3">d ns (s, t, n) = [m + d 2 (s, t) − d 2 (s, n)] + , (2)</formula><p>where n is a random (unpaired) target sentence, m is a "margin" parameter, and <ref type="bibr">[x]</ref> + := max{0, x}.</p><p>Letting J be the number of negative examples in each sample, they arrive at the following objective function to be minimized:</p><formula xml:id="formula_4">R ns (P , Q) := 1 N N n=1 J j=1 d ns (s (n) , t (n) , n (n,j) ).</formula><p>(3) This minimization can be carried out efficiently with gradient-based methods, such as stochastic gradient descent or AdaGrad ( <ref type="bibr" target="#b9">Duchi et al., 2011</ref>). Note however that the objective function in Eq. 3 is not convex. Therefore, one may land at different local minima, depending on the initialization.</p><p>Step 2: Training the Classifier. Once we have the bilingual embeddings P and Q, we can com- pute the representation P x ∈ R K of each docu- ment x in the labeled dataset D l . Let V ∈ R K×L be a matrix of parameters (weights), with one col- umn v y per label. A linear model is used to make predictions, according to y = argmax y∈{1,...,L} v y P x = argmax y∈{1,...,L} w y x,</p><p>where w y is a column of the matrix W := P V ∈ R V S ×L . In prior work, the perceptron algorithm was used to learn the weights V from the labeled examples in D l ( <ref type="bibr" target="#b17">Klementiev et al., 2012;</ref><ref type="bibr">Hermann and Blunsom, 2014)</ref>. Note that, at test time, it is not necessary to store the full embeddings: if L K, we may simply precompute W := P V (one weight per word and label) if the input is in the source language-or QV , if the input is in the target language-and treat this as a regular bag-of- words linear model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Jointly Learning to Embed and Classify</head><p>Instead of a two-stage approach, we propose to learn the bilingual embeddings and the classifier jointly on D l ∪ D u , as described next.</p><p>Our formulation optimizes a combination of a co-regularization function R, whose goal is to push the embeddings of paired sentences in D u to stay close, and a loss function L, which fits the model to the labeled data in D l .</p><p>The simplest choice for R is a simple Euclidean co-regularization function:</p><formula xml:id="formula_6">R 2 (P , Q) = 1 N N n=1 d 2 (s (n) , t (n) ) (5) = 1 2N N n=1 P s (n) − Q t (n) 2 .</formula><p>An alternative is the 1 -distance:</p><formula xml:id="formula_7">R 1 (P , Q) = 1 N N n=1 P s (n) − Q t (n) 1 . (6)</formula><p>One possible advantage of R 1 (P , Q) over R 2 (P , Q) is that the 1 -distance is more robust to outliers, hence it is less sensitive to differences in the parallel sentences. Note that both functions in Eqs. 5-6 are jointly convex on P and Q, un- like the one in Eq. 3. They are also simpler and do not require negative sampling. While these func- tions have a degenerate behavior in isolation (since they are both minimized by P = 0 and Q = 0), we will see that they become useful when plugged into a joint optimization framework. The next step is to define the loss function L to leverage the labeled data in D l . We consider a log- linear model P (y | x; W ) ∝ exp(w y x), which leads to the following logistic loss function:</p><formula xml:id="formula_8">L LL (W ) = − 1 M M m=1</formula><p>log P (y (m) | x (m) ; W ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(7)</head><p>We impose that W is of the form W = P V for a fixed V ∈ R K×L , whose choice we discuss below.</p><p>Putting the pieces together and adding some ex- tra regularization terms, we formulate our joint ob- jective function as follows:</p><formula xml:id="formula_9">F(P , Q) = µR(P , Q) + L(P V ) + µ S 2 P 2 F + µ T 2 Q 2 F ,<label>(8)</label></formula><p>where µ, µ S , µ T ≥ 0 are regularization constants. By minimizing a combination of L(P V ) and R(P , Q), we expect to obtain embeddings Q * that lead to an accurate classifier h for the target language. Note that P = 0 and Q = 0 is no longer a solution, due to the presence of the loss term L(P V ) in the objective.</p><p>Choice of V . In Eq. 8, we chose to keep V fixed rather than optimize it. The rationale is that there are many more degrees of freedom in the embed- ding matrices P and Q than in V (concretely,</p><formula xml:id="formula_10">O(K(V S + V T )) versus O(KL)</formula><p>, where we are as- suming a small number of labels, L V S + V T ).</p><p>Our assumption is that we have enough degrees of freedom to obtain an accurate model, regardless of the choice of V . These claims will be backed in §4 by a more rigorous theoretical result. Keeping V fixed has another important advantage: it al- lows to minimize F with respect to P and Q only, which makes it a convex optimization problem if we choose R and L to be both convex-e.g., set- ting R ∈ {R 2 , R 1 } and L := L LL .</p><p>Relation to Multi-View Learning. An interest- ing particular case of this formulation arises if K = L and V = I L (the identity matrix). In that case, we have W = P and the embedding matrices P and Q are in fact weights for every pair of word and label, as in standard bag-of-word models. In this case, we may interpret the co- regularizer R(P , Q) in Eq. 8 as a term that pushes the label scores of paired sentences P s (n) and Q t (n) to be similar, while the source-based log- linear model is fit via L(W ). The same idea un- derlies various semi-supervised co-regularization methods that seek agreement between multiple views ( <ref type="bibr" target="#b31">Sindhwani et al., 2005;</ref><ref type="bibr" target="#b1">Altun et al., 2005;</ref><ref type="bibr" target="#b13">Ganchev et al., 2008)</ref>. In fact, we may regard the joint optimization in Eq. 8 as a generalization of those methods, making a bridge between those methods and representation learning.</p><p>Multilingual Embeddings. It is straightforward to extend the framework herein presented to the case where there are multiple target languages (say R of them), and we want to learn one embedding matrix for each, {Q 1 , . . . , Q R }. The simplest way is to consider a sum of pairwise co-regularizers,</p><formula xml:id="formula_11">R (P , {Q 1 , . . . , Q R }) := R r=1 R(P , Q r ). (9)</formula><p>If R is additive over the parallel sentences (which is the case for R 2 , R 1 and R ns ), then this pro- cedure is equivalent to concatenating all the par- allel sentences (regardless of the target language) and adding a language suffix to the words to dis- tinguish them. This reduces directly to a problem in the same form as Eq. 8.</p><p>Pre-Trained Source Embeddings. In practice, it is often the case that pre-trained embeddings for the source language are already available (let ¯ P be the available embedding matrix). It would be foolish not to exploit those resources. In this sce- nario, the goal is to use ¯ P and the dataset D u to obtain "good" embeddings for the target lan- guages (possibly tweaking the source embeddings too, P ≈ ¯ P ). Our joint formulation in Eq. 8 can also be used to address this problem. It suffices to set K = L and V = I L (as in the multi-view learning case discussed above) and to define an auxiliary task that pushes P and ¯ P to be similar. The simplest way is to use a reconstruction loss:</p><formula xml:id="formula_12">L 2 (P , ¯ P ) := 1 2 P − ¯ P 2 F .<label>(10)</label></formula><p>The resulting optimization problem has resem- blances with the retrofitting approach of <ref type="bibr" target="#b10">Faruqui et al. (2015)</ref>, except that the goal here is to ex- tend the embeddings to other languages, instead of pushing monolingual embeddings to agree with a semantic lexicon. We will present some experi- ments in §5.2 using this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Limitations of the Euclidean Co-Regularizer</head><p>One may wonder how much the embedding di- mension K influences the learned classifier. The next proposition shows the (surprising) result that, with the formulation in Eq. 8 with R = R 2 , it makes absolutely no difference to increase K past the number of labels L. Below, T ∈ R V T ×N de- notes the matrix with columns t (1) , . . . , t (N ) .</p><p>Proposition 1. Let R = R 2 and assume T has full row rank. <ref type="bibr">2</ref> Then, for any choice of V ∈ R K×L , possibly with K &gt; L, the following holds:</p><p>1. There is an alternative, low-dimensional, V ∈ R K ×L with K ≤ L such that the classifier ob- tained (for both languages) by optimizing Eq. 8 using V is the same as if using V . 3</p><p>2. This classifier depends on V only via the L-by- L matrix V V .</p><p>3. If P * , Q * are the optimal embeddings obtained with V , then we always have rank(P * ) ≤ L and rank(Q * ) ≤ L regardless of K.</p><p>Proof. See App. A.1 in the supplemental material.</p><p>Let us reflect for a moment on the practical im- pact of Prop. 1. This result shows the limitation of the Euclidean co-regularizer R 2 in a very con- crete manner: when R = R 2 , we only need to consider representations of dimension K ≤ L.</p><p>Note also that a corollary of Prop. 1 arises when V V = I L , i.e., when V is chosen to have orthonormal columns (a sensible choice, since it corresponds to seeking embeddings that leave the label weights "uncorrelated"). Then, the second statement of Prop. 1 tells us that the resulting clas- sifier will be the same as if we had simply set V = I L (the particular case discussed in §3). We will see in §5.1 that, despite this limitation, this classifier is actually a very strong baseline. Of course, if the number of labels L is large enough, this limitation might not be a reason for concern. <ref type="bibr">4</ref> An instance will be presented in §5.2, where we will see that the Euclidean co-regularizer excels.</p><p>Finally, one might wonder whether Prop. 1 ap- plies only to the (Euclidean) 2 norm or if it holds for arbitrary regularizers. In fact, we show in App. A.2 that this limitation applies more gener- ally to Mahalanobis-Frobenius norms, which are essentially Euclidean norms after a linear trans- formation of the vector space. However, it turns out that for general norms such limitation does not exist, as shown below.</p><p>Proposition 2. If R = R 1 in Eq. 8, then the anal- ogous to Proposition 1 does not hold. It also does not hold for the ∞ -norm and the 0 -"norm."</p><p>Proof. See App. A.3 in the supplemental material.</p><p>This result suggests that, for other regulariz- ers R = R 2 , we may eventually obtain bet- ter classifiers by increasing K past L. As such, in the next section, we experiment with R ∈ {R 2 , R 1 , R ns }, where R ns is the (non-convex) noise-contrastive regularizer of Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We report results on two experiments: one on cross-lingual classification on the Reuters RCV1/RCV2 dataset, and another on multi-label classification with multilingual embeddings on the TED Corpus. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Reuters RCV1/RCV2</head><p>We evaluate our framework on the cross-lingual document classification task introduced by <ref type="bibr" target="#b17">Klementiev et al. (2012)</ref>. Following prior work, our dataset D u consists of 500,000 parallel sen- tences from the Europarl v7 English-German cor- pus ( <ref type="bibr" target="#b18">Koehn, 2005</ref>); and our labeled dataset D l consists of English and German documents from the RCV1/RCV2 corpora ( <ref type="bibr" target="#b21">Lewis et al., 2004</ref>), each categorized with one out of L = 4 labels. We used the same split as <ref type="bibr" target="#b17">Klementiev et al. (2012)</ref>: 1,000 documents for training, of which 200 are held out as validation data, and 5,000 for testing. <ref type="bibr">4</ref> For regression tasks (such as the one presented in the last paragraph of 3), instead of the "number of labels," L should be regarded as the number of output variables to regress. <ref type="bibr">5</ref> Our code is available at https: //github.com/dcferreira/ multilingual-joint-embeddings.</p><p>Note that, in this dataset, we are classifying documents based on their bag-of-word representa- tions, and learning word embeddings by bringing the bag-of-word representations of parallel sen- tences to be close together. In this sense, we are bringing together these multiple levels of repre- sentations (document, sentence and word).</p><p>We experimented with the joint formulation in Eq. 8, with L := L LL and R ∈ {R 2 , R 1 , R ns }. We optimized with AdaGrad ( <ref type="bibr" target="#b9">Duchi et al., 2011</ref>) with a stepsize of 1.0, using mini-batches of 100 Reuters RCV1/RCV2 documents and 50,000 Eu- roparl v7 parallel sentences. We found no need to run more than 100 iterations, with most of our runs converging under 50. Our vocabulary has 69,714 and 175,650 words for English and German, re- spectively, when training on the English portion of the Reuters RCV1/RCV2 corpus, and 61,120 and 183,888 words for English and German, when training in the German portion of the corpus. This difference is due to the inclusion of words in the training data into the vocabulary. We do not re- move any words from the vocabulary, for simplic- ity. We used the validation set to tune the hyper- parameters {µ, µ S , µ T } and to choose the iteration number. When using K = L, we chose V = I L ; otherwise, we chose V randomly, sampling its en- tries from a Gaussian N (0, 0.1). <ref type="table">Table 1</ref> shows the results. We include for com- parison the most competitive systems published to date. The first thing to note is that our joint system with Euclidean co-regularization performs very well for this task, despite the theoretical lim- itations shown in §4. Although its embedding size is only K = 4 (one dimension per label), it out- performed all the two-stage systems trained on the same data, in both directions.</p><p>For the EN→DE direction, our joint system with 1 co-regularization achieved state-of-the-art results (92.7%), matching two-stage systems that use extra monolingual data, negative sampling, or non-additive document representations. It is con- ceivable that the better results of R 1 over R 2 come from its higher robustness to differences in the parallel sentences.</p><p>For the DE→EN direction, our best result (79%) was obtained with the noise-contrastive co- regularizer, which outperformed all systems ex- cept para_doc ( . While the accuracy of para_doc is quite impressive, note that it requires 500-dimensional embeddings   (hence many more parameters), was trained on more parallel sentences, and requires more expen- sive (and non-deterministic) computation at test time to compute a document's embedding. Our method has the advantage of being simple and very fast to train: it took less than 1 minute to train the joint-R 1 system for EN→DE, using a single core on an Intel Xeon @2.5 GHz. This can be compared with <ref type="bibr" target="#b17">Klementiev et al. (2012)</ref>, who took 10 days on a single core, or <ref type="bibr" target="#b8">Coulmance et al. (2015)</ref>, who took 10 minutes with 6 cores. 6</p><p>Although our theoretical results suggest that in- creasing K when using the 1 norm may increase the expressiveness of our embeddings, our results do not support this claim (the improvements in DE→EN from K = 4 to K = 40 were tiny). However, it led to a gain of 2.5 points when us- ing negative sampling. For K = 40, this system is much more accurate than <ref type="bibr">Hermann and Blunsom (2014)</ref>, which confirms that learning the embed- dings together with the task is highly beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TED Corpus</head><p>To assess the ability of our framework to han- dle multiple target languages, we ran a second set of experiments on the TED corpus ( <ref type="bibr" target="#b3">Cettolo et al., 2012)</ref>, using the training and test parti- tions created by <ref type="bibr">Hermann and Blunsom (2014)</ref>, downloaded from http://www.clg.ox.ac. uk/tedcorpus. The corpus contains English transcriptions and multilingual, sentence-aligned translations of talks from the TED conference in 12 different languages, with 12,078 parallel docu- ments in the training partition (totalling 1,641,985 parallel sentences). Following their prior work, we used this corpus both as parallel data (D u ) and as the task dataset (D l ). There are L = 15 labels and documents can have multiple labels.</p><p>We experimented with two different strategies:</p><p>• A one-stage system (Joint), which jointly trains the multilingual embeddings and the multi-label classifier (similarly as in §5.1). To cope with multiple target languages, we used a sum of pairwise co-regularizers as described in Eq. 9. For classification, we use multinomial logistic regression, where we select those labels with a posterior probability above 0.18 (tuned on vali-dation data).</p><p>• A two-stage approach (Joint w/ Aux), where we first obtain multilingual embeddings by apply- ing our framework with an auxiliary task with pre-trained English embeddings (as described in Eq. 10 and in the last paragraph of §3), and then use the resulting multilingual representations to train the multi-label classifier. We address this multi-label classification problem with indepen- dent binary logistic regressors (one per label), trained by running 100 iterations of L-BFGS ( <ref type="bibr" target="#b22">Liu and Nocedal, 1989)</ref>. At test time, we se- lect those labels whose posterior probability are above 0.5.</p><p>For the Joint w/ Aux strategy, we used the 300-dimensional GloVe-840B vectors <ref type="bibr" target="#b27">(Pennington et al., 2014</ref>), downloaded from http:// nlp.stanford.edu/projects/glove/. <ref type="table">Table 2</ref> shows the results for cross-lingual clas- sification, where we use English as source and each of the other 11 languages as target. We compare our two strategies above with the strong Machine Translation (MT) baseline used by Her- mann and Blunsom (2014) (which translates the input documents to English with a state-of-the- art MT system) and with their two strongest sys- tems, which build document-level representations from embeddings trained bilingually or multi- lingually (called DOC/ADD single and DOC/ADD joint, respectively). <ref type="bibr">7</ref> Overall, our Joint system with 2 regularization outperforms both <ref type="bibr">Hermann and Blunsom (2014)</ref>'s systems (but not the MT baseline) for 8 out of 11 languages, performing generally better than our 1 -regularized system. However, the clear winner is our 2 -regularized Joint w/ Aux system, which wins over all systems (including the MT baseline) by a substantial mar- gin, for all languages. This shows that pre-trained source embeddings can be extremely helpful in bootstrapping multilingual ones. 8 On the other hand, the performance of the Joint w/ Aux sys- tem with 1 regularization is rather disappointing. Note that the limitations of R 2 shown in §4 are not a concern here, since the auxiliary task has L = 300 dimensions (the dimension of the pre- trained embeddings). A small sample of the mul- tilingual embeddings produced by the winner sys- tem is shown in <ref type="table">Table 4</ref>.</p><p>Finally, we did a last experiment in which we use our multilingual embeddings obtained with Joint w/ Aux to train monolingual systems for each language. This time, we compare with a bag-of- words naïve Bayes system (reported by <ref type="bibr">Hermann and Blunsom (2014)</ref>), a system trained on the Polyglot embeddings from Al-Rfou et al. <ref type="formula" target="#formula_2">(2013)</ref> (which are multilingual, but not in a shared rep- resentation space), and the two systems developed by <ref type="bibr">Hermann and Blunsom (2014)</ref>. The results are shown in <ref type="table">Table 3</ref>. We observe that, with the excep- tion of Turkish, our systems consistently outper- form all the competitors. Comparing the bottom two rows of <ref type="table">Tables 2 and 3</ref> we also observe that, for the 2 -regularized system, there is not much degradation caused by cross-lingual training ver- sus training on the target language directly (in fact, for Spanish, Polish, and Brazilian Portuguese, the former scores are even higher). This suggests that the multilingual embeddings have high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed a new formulation which jointly minimizes a combination of a supervised loss function with a multilingual co-regularization term using unlabeled parallel data. This allows learning task-specific multilingual embeddings to- gether with a classifier for the task. Our method achieved state-of-the-art accuracy on the Reuters RCV1/RCV2 cross-lingual classification task in the English to German direction, while being ex- tremely simple and computationally efficient. Our results in the Reuters RCV1/RCV2 task, obtained using Europarl v7 as parallel data, show that our method has no trouble handling different levels of representations simutaneously (document, sen- tence and word). On the TED Corpus, we obtained the highest reported scores for 10 out of 11 lan- guages, using an auxiliary task with pre-trained English embeddings.   <ref type="table">Table 2</ref>: Cross-lingual experiments on the TED Corpus using English as a source language. Reported are the micro-averaged F 1 scores for a machine translation baseline and the two strongest systems of <ref type="bibr">Hermann and Blunsom (2014)</ref>, our one-stage joint system (Joint), and our two-stage system that trains the multilingual embeddings jointly with the auxiliary task of fitting pre-trained English embeddings (Joint w/ Aux), with both 1 and 2 regularization. Bold indicates the best result for each target language.  <ref type="table">Table 3</ref>: Monolingual experiments on the TED Corpus. Shown are the micro-averaged F 1 scores for a bag-of-words baseline, a system trained on Polyglot embeddings, the two strongest systems of <ref type="bibr">Hermann and Blunsom (2014)</ref>, and our Joint w/ Aux system with 1 and 2 regularization.  <ref type="table">Table 4</ref>: Examples of nearest neighbor words for the multilingual embeddings trained with our Joint w/ Aux system with 2 regularization. Shown for each English word are the 20 closest target words in Euclidean distance, regardless of language. through contracts UID/EEA/50008/2013, through the LearnBig project (PTDC/EEI- SII/7092/2014), and the GoLocal project (grant CMUPERI/TIC/0046/2014).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 :</head><label>1</label><figDesc>Accuracies in the RCV1/RCV2 dataset. Shown for comparison are Klementiev et al. (2012) [KTB12], Hermann and Blunsom (2014) [HB14], Gouws et al. (2015) [GBC15], Soyer et al. (2015) [SSA15], Shi et al. (2015) [SLLS15], and Pham et al. (2015) [PLM15]. Systems marked with ( †) used the full 1.8M parallel sentences in Europarl. The one with ( ‡) used additional target monolingual data from RCV1/RCV2. The bottom rows refer to our joint method, with Euclidean ( 2 ), 1 , and noise- contrastive co-regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Ara</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>. Ger. Spa. Fre.</figDesc><table>Ita. 
Dut. Pol. Br. Pt. Rom. Rus. Tur. 
MT Baseline 
[HB14] 42.9 46.5 51.8 52.6 51.4 50.5 44.5 
47.0 
49.3 
43.2 40.9 
DOC/ADD single 
[HB14] 41.0 42.4 38.3 47.6 48.5 26.4 40.2 
35.4 
41.8 
44.8 45.2 
DOC/ADD joint 
[HB14] 39.2 40.5 44.3 44.7 47.5 45.3 39.4 
40.9 
44.6 
47.6 41.7 
Joint, R 2 , K = 15 
41.8 46.6 46.6 46.0 48.7 52.5 39.5 
40.8 
47.6 
44.9 47.2 
Joint, R 1 , K = 15 
44.0 44.7 49.4 40.1 46.1 49.4 35.7 
43.5 
40.5 
42.2 43.4 
Joint w/ Aux, R 2 , K = 300 46.9 52.0 59.4 54.6 56.0 53.6 51.0 
51.7 
53.9 
52.3 49.5 
Joint w/ Aux, R 1 , K = 300 44.0 40.4 40.4 39.5 38.6 38.1 43.2 
36.6 
35.1 
44.3 44.4 

</table></figure>

			<note place="foot" n="1"> We provide the trained embeddings at http://www. cs.cmu.edu/~afm/projects/multilingual_ embeddings.html.</note>

			<note place="foot" n="2"> This assumption is not too restrictive: it holds if N ≥ VT and if no target sentence can be written as a linear combination of the others (this can be accomplished if we remove redundant parallel sentences). 3 Let P * , Q * and P * , Q * be the optimal embeddings obtained with V and V , respectively. Since we are working with linear classifiers, the two classifiers are the same in the sense that P * V = P * V and Q * V = Q * V .</note>

			<note place="foot" n="6"> Coulmance et al. (2015) reports accuracies of 87.8% (EN→DE) and 78.7% (DE→EN), when using 10,000 training documents from the RCV1/RCV2 corpora.</note>

			<note place="foot" n="7"> Note that, despite the name, the Hermann and Blunsom (2014)&apos;s joint systems are not doing joint training as we are. 8 Note however that, overall, our Joint w/ Aux systems have access to more data than our Joint systems and also than Hermann and Blunsom (2014)&apos;s systems, since the pretrained embeddings were trained on a large amount of English monolingual data. Yet, the amount of target language data is the same.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the three anonymous reviewers.</p><p>This work was partially sup-ported by the European Union under H2020 project SUMMA, grant 688139, and by Fun-dação para a Ciência e Tecnologia (FCT),</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.1662</idno>
		<title level="m">Polyglot: Distributed word representations for multilingual NLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum Margin SemiSupervised Learning for Structured Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 18</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Pascal Vincent, and Christian Janvin</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wit3: Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th Conference of the European Association for Machine Translation</title>
		<meeting>of the 16th Conference of the European Association for Machine Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">An Autoencoder Approach to Learning Bilingual Word Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Empirical Methods for Natural Language Processing</title>
		<meeting>of Empirical Methods for Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Machine Learning</title>
		<meeting>of the International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transgram, Fast Cross-lingual Word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Coulmance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1109" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual Meeting of the NorthAmerican Chapter of the Association for Computational Linguistics</title>
		<meeting>of Annual Meeting of the NorthAmerican Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual Meeting of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>of Annual Meeting of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crosslingual discriminative learning of sequence models with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Empirical Methods in Natural Language Processing</title>
		<meeting>of Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-view learning over structured and non-identical outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>of Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BilBOWA: Fast Bilingual Distributed Representations without Word Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilingual Models for Compositional Distributed Semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL pages</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>ACL pages</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bootstrapping parsers via syntactic projection across parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Kolak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="325" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Computational Linguistics-Proceedings of COLING 2012: Technical Papers</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Europarl: A parallel corpus 2027 for statistical machine translation. MT summit 11</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<title level="m">Distributed Representations of Sentences and Documents. International Conference on Machine Learning-ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RCV1: A New Benchmark Collection for Text Categorization Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the limited memory BFGS method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<title level="m">Bilingual Word Representations with Monolingual Quality in Mind. Workshop on Vector Modeling for NLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transferring coreference resolvers with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-source transfer of delexicalized dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Empirical Methods in Natural Language Processing</title>
		<meeting>of Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR 2013</title>
		<meeting>the International Conference on Learning Representations (ICLR 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Kaare Brandt Petersen and Michael Syskind Pedersen. 2012. The Matrix Cookbook</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<title level="m">Learning Distributed Representations for Multilingual Text Sequences. Workshop on Vector Modeling for NLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Cross-lingual Word Embeddings via Matrix Co-factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A co-regularization approach to semi-supervised learning with multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML workshop on learning with multiple views. Citeseer</title>
		<meeting>ICML workshop on learning with multiple views. Citeseer</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eh</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Leveraging Monolingual Data for Crosslingual Compositional Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations (ICLR)</title>
		<meeting>the 2015 International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>of the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Crosslanguage parser adaptation between related languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Empirical Methods for Natural Language Processing</title>
		<meeting>of Empirical Methods for Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
