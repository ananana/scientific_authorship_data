<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Triangular Architecture for Rare Language Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SKLSDE Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research in Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research in Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research in Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SKLSDE Lab</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Advanced Innovation Center for Big Data and Brain Computing</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Triangular Architecture for Rare Language Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="volume">56</biblScope>
							<biblScope unit="page" from="56" to="65"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural Machine Translation (NMT) performs poor on the low-resource language pair (X, Z), especially when Z is a rare language. By introducing another rich language Y , we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y, Z) (may be small) and (X, Y) (can be rich) to improve the translation performance of low-resource pairs. In this triangular architecture , Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with a unified bidi-rectional EM algorithm under the goal of maximizing the translation likelihood of (X, Y). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, Neural Machine Translation (NMT) <ref type="bibr" target="#b10">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b20">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) has achieved remarkable performance on many translation tasks ( <ref type="bibr" target="#b9">Jean et al., 2015;</ref><ref type="bibr" target="#b18">Sennrich et al., 2016;</ref><ref type="bibr" target="#b16">Sennrich et al., 2017)</ref>. Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b14">Luong et al., 2015)</ref>. During training, NMT systems are optimized to maximize the translation probability of a given language pair * Contribution during internship at MSRA.</p><p>with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space. Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs ( <ref type="bibr" target="#b24">Zoph et al., 2016)</ref>.</p><p>In order to deal with the data sparsity problem for NMT, exploiting monolingual data ( <ref type="bibr" target="#b17">Sennrich et al., 2015;</ref><ref type="bibr" target="#b22">Zhang and Zong, 2016;</ref><ref type="bibr" target="#b5">Cheng et al., 2016;</ref><ref type="bibr" target="#b23">Zhang et al., 2018;</ref>) is the most common method. With monolingual data, the back-translation method <ref type="bibr" target="#b17">(Sennrich et al., 2015)</ref> generates pseudo bilingual sentences with a target- to-source translation model to train the source-to- target one. By extending back-translation, source- to-target and target-to-source translation models can be jointly trained and boost each other ( <ref type="bibr" target="#b5">Cheng et al., 2016;</ref><ref type="bibr" target="#b23">Zhang et al., 2018)</ref>. Similar to joint training ( <ref type="bibr" target="#b5">Cheng et al., 2016;</ref><ref type="bibr" target="#b23">Zhang et al., 2018)</ref>, dual learning ( ) designs a reinforce- ment learning framework to better capitalize on monolingual data and jointly train two models.</p><p>Instead of leveraging monolingual data (X or Z) to enrich the low-resource bilingual pair (X, Z), in this paper, we are motivated to intro- duce another rich language Y , by which addi- tionally acquired bilingual data (Y, Z) and (X, Y ) can be exploited to improve the translation per- formance of (X, Z). This requirement is easy to satisfy, especially when Z is a rare language but X is not. Under this scenario, (X, Y ) can be a rich-resource pair and provide much bilingual data, while (Y, Z) would also be a low-resource pair mostly because Z is rare. For example, in the dataset IWSLT2012, there are only 112.6K bilin- gual sentence pairs of English-Hebrew, since He- brew is a rare language. If French is introduced as the third language, we can have another low- resource bilingual data of French-Hebrew (116.3K sentence pairs), and easily-acquired bilingual data of the rich-resource pair English-French. With the introduced rich language Y , in this paper, we propose a novel triangular architec- ture (TA-NMT) to exploit the additional bilingual data of (Y, Z) and (X, Y ), in order to get better translation performance on the low-resource pair (X, Z), as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In this architec- ture, (Y, Z) is used for training another translation model to score the translation model of (X, Z), while (X, Y ) is used to provide large bilingual data with favorable alignment information.</p><p>Under the motivation to exploit the rich- resource pair (X, Y ), instead of modeling X â‡’ Z directly, our method starts from modeling the translation task X â‡’ Y while taking Z as a la- tent variable. Then, we decompose X â‡’ Y into two phases for training two translation mod- els of low-resource pairs ((X, Z) and (Y, Z)) re- spectively. The first translation model generates a sequence in the hidden space of Z from X, based on which the second one generates the translation in Y . These two models can be optimized jointly with an Expectation Maximization (EM) frame- work with the goal of maximizing the translation probability p(y|x). In this framework, the two models can boost each other by generating pseudo bilingual data for model training with the weights scored from the other. By reversing the transla- tion direction of X â‡’ Y , our method can be used to train another two translation models p(z|y) and p(x|z). Therefore, the four translation mod- els (p(z|x), p(x|z), p(z|y) and p(y|z)) of the rare language Z can be optimized jointly with our pro- posed unified bidirectional EM algorithm.</p><p>Experimental results on the MultiUN and IWSLT2012 datasets demonstrate that our method can achieve significant improvements for rare languages translation. By incorporating back- translation (a method leveraging more monolin- gual data) into our method, TA-NMT can achieve even further improvements.</p><p>Our contributions are listed as follows:</p><p>â€¢ We propose a novel triangular training archi- tecture (TA-NMT) to effectively tackle the data sparsity problem for rare languages in NMT with an EM framework.</p><p>â€¢ Our method can exploit two additional bilin- gual datasets at both the model and data lev- els by introducing another rich language.</p><p>â€¢ Our method is a unified bidirectional EM al- gorithm, in which four translation models on two low-resource pairs are trained jointly and boost each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our method tries to lever- age (X, Y ) (a rich-resource pair) and (Y, Z) to im- prove the translation performance of low-resource pair (X, Z), during which translation models of (X, Z) and (Y, Z) can be improved jointly.</p><p>Instead of directly modeling the translation probabilities of low-resource pairs, we model the rich-resource pair translation X â‡’ Y , with the language Z acting as a bridge to connect X and Y . We decompose X â‡’ Y into two phases for training two translation models. The first model p(z|x) generates the latent translation in Z from the input sentence in X, based on which the sec- ond one p(y|z) generate the final translation in lan- guage Y . Following the standard EM procedure <ref type="bibr" target="#b2">(Borman, 2004</ref>) and Jensen's inequality, we derive the lower bound of p(y|x) over the whole training data D as follows:</p><formula xml:id="formula_0">L(Î˜; D) = (x,y)âˆˆD log p(y|x) = (x,y)âˆˆD log z p(z|x)p(y|z) = (x,y)âˆˆD log z Q(z) p(z|x)p(y|z) Q(z) â‰¥ (x,y)âˆˆD z Q(z) log p(z|x)p(y|z) Q(z) . = L(Q)<label>(1)</label></formula><p>where Î˜ is the model parameters set of p(z|x) and p(y|z), and Q(z) is an arbitrary posterior distri- bution of z. We denote the lower-bound in the last but one line as L(Q). Note that we use an approxi- mation that p(y|x, z) â‰ˆ p(y|z) due to the semantic equivalence of parallel sentences x and y.</p><p>In the following subsections, we will first pro- pose our EM method in subsection 2.1 based on the lower-bound derived above. Next, we will extend our method to two directions and give our unified bidirectional EM training in subsec- tion 2.2. Then, in subsection 2.3, we will discuss more training details of our method and present our algorithm in the form of pseudo codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">EM Training</head><p>To maximize L(Î˜; D), the EM algorithm can be leveraged to maximize its lower bound L(Q). In the E-step, we calculate the expectation of the variable z using current estimate for the model, namely find the posterior distribution Q(z). In the M-step, with the expectation Q(z), we max- imize the lower bound L(Q). Note that condi- tioned on the observed data and current model, the calculation of Q(z) is intractable, so we choose</p><formula xml:id="formula_1">Q(z) = p(z|x) approximately.</formula><p>M-step: In the M-step, we maximize the lower bound L(Q) w.r.t model parameters given Q(z).</p><formula xml:id="formula_2">By substituting Q(z) = p(z|x) into L(Q)</formula><p>, we can get the M-step as follows:</p><formula xml:id="formula_3">Î˜ y|z = arg max Î˜ y|z L(Q) = arg max Î˜ y|z (x,y)âˆˆD z p(z|x) log p(y|z) = arg max Î˜ y|z (x,y)âˆˆD E zâˆ¼p(z|x) log p(y|z) (2) E-step:</formula><p>The approximate choice of Q(z) brings in a gap between L(Q) and L(Î˜; D), which can be minimized in the E-step with Generalized EM method <ref type="bibr" target="#b15">(McLachlan and Krishnan, 2007)</ref>. Ac- cording to <ref type="bibr" target="#b1">Bishop (2006)</ref>, we can write this gap explicitly as follows:</p><formula xml:id="formula_4">L(Î˜; D) âˆ’ L(Q) = z Q(z) log Q(z) p(z|y) = KL(Q(z)||p(z|y)) = KL(p(z|x)||p(z|y))<label>(3)</label></formula><p>where KL(Â·) is the KullbackLeibler divergence, and the approximation that p(z|x, y) â‰ˆ p(z|y) is also used above.</p><p>In the E-step, we minimize the gap between L(Q) and L(Î˜; D) as follows:</p><formula xml:id="formula_5">Î˜ z|x = arg min Î˜ z|x KL(p(z|x)||p(z|y)) (4)</formula><p>To sum it up, the E-step optimizes the model p(z|x) by minimizing the gap between L(Q) and L(Î˜; D) to get a better lower bound L(Q). This lower bound is then maximized in the M-step to optimize the model p(y|z). Given the new model p(y|z), the E-step tries to optimize p(z|x) again to find a new lower bound, with which the M-step is re-performed. This iteration process continues until the models converge, which is guaranteed by the convergence of the EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unified Bidirectional Training</head><p>The model p(z|y) is used as an approximation of p(z|x, y) in the E-step optimization (Equation 3). Due to the low resource property of the language pair (Y, Z), p(z|y) cannot be well trained. To solve this problem, we can jointly optimize p(x|z) and p(z|y) similarly by maximizing the reverse translation probability p(x|y).</p><p>We now give our unified bidirectional general- ized EM procedures as follows:</p><formula xml:id="formula_6">â€¢ Direction of X â‡’ Y E: Optimize Î˜ z|x . arg min Î˜ z|x KL(p(z|x)||p(z|y))<label>(5)</label></formula><p>M: Optimize Î˜ y|z .</p><p>arg max</p><formula xml:id="formula_7">Î˜ y|z (x,y)âˆˆD E zâˆ¼p(z|x) log p(y|z) (6) â€¢ Direction of Y â‡’ X E: Optimize Î˜ z|y .</formula><p>arg min</p><formula xml:id="formula_8">Î˜ z|y KL(p(z|y)||p(z|x))<label>(7)</label></formula><p>M: Optimize Î˜ x|z .</p><p>arg max</p><formula xml:id="formula_9">Î˜ x|z (x,y)âˆˆD E zâˆ¼p(z|y) log p(x|z) (8)</formula><p>Based on the above derivation, the whole archi- tecture of our method can be illustrated in <ref type="figure" target="#fig_1">Fig- ure 2</ref>, where the dash arrows denote the direction of p(y|x), in which p(z|x) and p(y|z) are trained jointly with the help of p(z|y), while the solid ones denote the direction of p(x|y), in which p(z|y) and p(x|z) are trained jointly with the help of p(z|x). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training Details</head><p>A major difficulty in our unified bidirectional training is the exponential search space of the translation candidates, which could be addressed by either sampling <ref type="bibr" target="#b19">(Shen et al., 2015;</ref><ref type="bibr" target="#b5">Cheng et al., 2016)</ref> or mode approximation <ref type="bibr" target="#b11">(Kim and Rush, 2016)</ref>. In our experiments, we leverage the sam- pling method and simply generate the top target sentence for approximation.</p><p>In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows:</p><formula xml:id="formula_10">Î˜ z|x KL(p(z|x)||p(z|y)) = E zâˆ¼p(z|x) log p(z|x) p(z|y) Î˜ z|x log p(z|x) Î˜ z|y KL(p(z|y)||p(z|x)) = E zâˆ¼p(z|y) log p(z|y) p(z|x) Î˜ z|y log p(z|y)<label>(9)</label></formula><p>Similar to reinforcement learning, models p(z|x) and p(z|y) are trained using samples gen- erated by the models themselves. According to our observation, some samples are noisy and detri- mental to the training process. One way to tackle this is to filter out the bad ones using some addi- tional metrics (BLEU, etc.). Nevertheless, in our settings, BLEU scores cannot be calculated dur- ing training due to the absence of the golden tar- gets (z is generated based on x or y from the rich- resource pair (x, y)). Therefore we choose IBM model1 scores to weight the generated translation candidates, with the word translation probabilities calculated based on the given bilingual data (the low-resource pair (x, z) or (y, z)). Additionally, to stabilize the training process, the pseudo samples generated by model p(z|x) or p(z|y) are mixed with true bilingual samples in the same mini-batch with the ratio of 1-1. The whole training procedure is described in the following Algorithm 1, where the 5th and 9th steps are generating pseudo data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training low-resource translation models with the triangular architecture</head><p>Input: Rich-resource bilingual data (x, y); low- resource bilingual data (x, z) and (y, z) Output: Parameters Î˜ z|x , Î˜ y|z , Î˜ z|y and Î˜ x|z 1: Pre-train p(z|x), p(z|y), p(x|z), p(y|z) 2: while not convergence do 3:</p><formula xml:id="formula_11">Sample (x, y), (x * , z * ), (y * , z * ) âˆˆ D 4:</formula><p>X â‡’ Y : Optimize Î˜ z|x and Î˜ y|z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Generate z from p(z |x) and build the training batches Generate z from p(z |y) and build the training batches </p><formula xml:id="formula_12">B 1 = (x, z )âˆª(x * , z * ), B 2 = (y, z ) âˆª (y * , z * ) 6</formula><formula xml:id="formula_13">B 3 = (y, z ) âˆª (y * , z * ), B 4 = (x, z ) âˆª (x * , z * ) 10</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>In order to verify our method, we conduct exper- iments on two multilingual datasets. The one is <ref type="bibr">MultiUN (Eisele and Chen, 2010)</ref>, which is a col- lection of translated documents from the United Nations, and the other is IWSLT2012 ( <ref type="bibr" target="#b3">Cettolo et al., 2012)</ref>, which is a set of multilingual tran- scriptions of TED talks. As is mentioned in sec- tion 1, our method is compatible with methods ex- ploiting monolingual data. So we also find some extra monolingual data of rare languages in both datasets and conduct experiments incorporating back-translation into our method.</p><p>MultiUN: English-French (EN-FR) bilingual data are used as the rich-resource pair (X, Y ). Arabic (AR) and Spanish (ES) are used as two simulated rare languages Z. We randomly choose subsets of bilingual data of (X, Z) and (Y, Z) in the original dataset to simulate low-resource sit- uations, and make sure there is no overlap in Z between chosen data of (X, Z) and (Y, Z).</p><p>IWSLT2012 1 : English-French is used as the rich-resource pair (X, Y ), and two rare languages Z are Hebrew (HE) and Romanian (RO) in our <ref type="table">Table 1</ref>: training data size of each language pair.</p><formula xml:id="formula_14">Pair MultiUN IWSLT2012 Lang Size Lang Size (X, Y ) EN-FR 9.9 M EN-FR 3 7.9 M (X, Z) EN-AR 116 K EN-HE 112.6 K (Y, Z) FR-AR 116 K FR-HE 116.3 K mono Z AR 3 M HE 512.5 K (X,</formula><note type="other">Z) EN-ES 116 K EN-RO 4 467.3 K (Y, Z) FR-ES 116 K FR-RO 111.6 K mono Z ES 3 M RO 885.0 K</note><p>choice. Note that in this dataset, low-resource pairs (X, Z) and (Y, Z) are severely overlapped in Z. In addition, English-French bilingual data from WMT2014 dataset are also used to enrich the rich-resource pair. We also use additional English- Romanian bilingual data from Europarlv7 dataset ( <ref type="bibr" target="#b12">Koehn, 2005</ref>). The monolingual data of Z (HE and RO) are taken from the web 2 .</p><p>In both datasets, all sentences are filtered within the length of 5 to 50 after tokenization. Both the validation and the test sets are 2,000 parallel sen- tences sampled from the bilingual data, with the left as training data. The size of training data of all language pairs are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We compare our method with four baseline sys- tems. The first baseline is the RNNSearch model ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), which is a sequence-to- sequence model with attention mechanism trained with given small-scale bilingual data. The trained translation models are also used as pre-trained models for our subsequent training processes.</p><p>The second baseline is PBSMT ( <ref type="bibr" target="#b13">Koehn et al., 2003)</ref>, which is a phrase-based statistical machine translation system. PBSMT is known to perform well on low-resource language pairs, so we want to compare it with our proposed method. And we use the public available implementation of Moses 5 for training and test in our experiments.</p><p>The third baseline is a teacher-student alike method ( <ref type="bibr" target="#b4">Chen et al., 2017)</ref>. For the sake of brevity, we will denote it as T-S. The process is illus- trated in <ref type="figure" target="#fig_2">Figure 3</ref>. We treat this method as a sec- ond baseline because it can also be regarded as a method exploiting (Y, Z) and (X, Y ) to improve <ref type="bibr">2</ref>   <ref type="table">Table 2</ref>: Resources that different methods use the translation of (X, Z) if we regard (X, Z) as the zero-resource pair and p(x|y) as the teacher model when training p(z|x) and p(x|z).</p><formula xml:id="formula_15">Method Resources PBSMT (X, Z), (Y, Z) RNNSearch (X, Z), (Y, Z) T-S (X, Z), (Y, Z), (X, Y ) BackTrans (X, Z), (Y, Z), (X, Y ), mono Z TA-NMT (X, Z), (Y, Z), (X, Y ) TA-NMT(GI) (X, Z), (Y, Z), (X, Y ), mono Z</formula><p>The fourth baseline is back-translation (Sen- nrich et al., 2015). We will denote it as Back- Trans. More concretely, to train the model p(z|x), we use extra monolingual Z described in <ref type="table">Table 1</ref> to do back-translation; to train the model p(x|z), we use monolingual X taken from (X, Y ). Pro- cedures for training p(z|y) and p(y|z) are simi- lar. This method use extra monolingual data of Z compared with our TA-NMT method. But we can incorporate it into our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overall Results</head><p>Experimental results on both datasets are shown in <ref type="table" target="#tab_4">Table 3</ref> and 4 respectively, in which RNNSearch, PBSMT, T-S and BackTrans are four base- lines. TA-NMT is our proposed method, and TA-NMT(GI) is our method incorporating back- translation as good initialization. For the purpose of clarity and a fair comparison, we list the re- sources that different methods exploit in <ref type="table">Table 2</ref>.</p><p>From <ref type="table" target="#tab_4">Table 3</ref> on MultiUN, the performance of RNNSearch is relatively poor. As is expected, PBSMT performs better than RNNSearch on low- resource pairs by the average of 1.78 BLEU. The T-S method which can doubling the training data   for both (X, Z) and (Y, Z) by generating pseudo data from each other, leads up to 1.1 BLEU points improvement on average over RNNSearch. Com- pared with T-S, our method gains a further im- provement of about 0.9 BLEU on average, because our method can better leverage the rich-resource pair (X, Y ). With extra large monolingual Z in- troduced, BackTrans can improve the performance of p(z|x) and p(z|y) significantly compared with all the methods without monolingual Z. How- ever TA-NMT is comparable with or even bet- ter than BackTrans for p(x|z) and p(y|z) because both of the methods leverage resources from rich- resource pair (X, Y ), but BackTrans does not use the alignment information it provides. Moreover, with back-translation as good initialization, fur- ther improvement is achieved by TA-NMT(GI) of about 0.7 BLEU on average over BackTrans.</p><p>In <ref type="table" target="#tab_5">Table 4</ref>, we can draw the similar conclu- sion. However, different from MultiUN, in the EN-FR-HE group of IWSLT, (X, Z) and (Y, Z) are severely overlapped in Z. Therefore, T-S cannot improve the performance obviously (only about 0.2 BLEU) on RNNSearch because it fails to essentially double training data via the teacher model. As for EN-FR-RO, with the additionally introduced EN-RO data from Europarlv7, which has no overlap in RO with FR-RO, T-S can im- prove the average performance more than the EN- FR-HE group. TA-NMT outperforms T-S by 0.93 BLEU on average. Note that even though Back- Trans uses extra monolingual Z, the improve- ments are not so obvious as the former dataset, the reason for which we will delve into in the next subsection. Again, with back-translation as good initialization, TA-NMT(GI) can get the best result.</p><p>Note that BLEU scores of TA-NMT are lower than BackTrans in the directions of Xâ‡’Z and Yâ‡’Z. The reason is that the resources used by these two methods are different, as shown in <ref type="table">Table  2</ref>. To do back translation in two directions (e.g., Xâ‡’Z and Zâ‡’X), we need monolingual data from both sides (e.g., X and Z), however, in TA-NMT, the monolingual data of Z is not necessary. There- fore, in the translation of Xâ‡’Z or Yâ‡’Z, Back- Trans uses additional monolingual data of Z while TA-NMT does not, that is why BackTrans outper- forms TA-NMT in these directions. Our method can leverage back translation as a good initializa- tion, aka TA-NMT(GI) , and outperforms Back- Trans on all translation directions.</p><p>The average test BLEU scores of different methods in each data group (EN-FR-AR, EN-FR- ES, EN-FR-HE, and EN-FR-RO) are listed in the column Ave of the tables for clear comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Effect of Extra Monolingual Data</head><p>Comparing the results of BackTrans and TA- NMT(GI) on both datasets, we notice the improve- ments of both methods on IWSLT are not as signif- icant as MultiUN. We speculate the reason is the relatively less amount of monolingual Z we use in the experiments on IWSLT as shown in <ref type="table">Table 1</ref>. So we conduct the following experiment to verify the conjecture by changing the scale of monolin- gual Arabic data in the MultiUN dataset, of which the data utilization rates are set to 0%, 10%, 30%, 60% and 100% respectively. Then we compare the performance of BackTrans and TA-NMT(GI) in the EN-FR-AR group. As <ref type="figure" target="#fig_3">Figure 4</ref> shows, the amount of monolingual Z actually has a big effect on the results, which can also verify our conjec- ture above upon the less significant improvement of BackTrans and TA-NMT(GI) on IWSLT. In ad- dition, even with poor "good-initialization", TA- NMT(GI) still get the best results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">EM Training Curves</head><p>To better illustrate the behavior of our method, we print the training curves in both the M-steps and E- steps of TA-NMT and TA-NMT(GI) in <ref type="figure">Figure 5</ref> above. The chosen models printed in this figure are EN2AR and AR2FR on MultiUN, and EN2RO and RO2FR on IWLST.</p><p>From <ref type="figure">Figure 5</ref>, we can see that the two low- resource translation models are improved nearly simultaneously along with the training process, which verifies our point that two weak models could boost each other in our EM framework. No- tice that at the early stage, the performance of all models stagnates for several iterations, especially of TA-NMT. The reason could be that the pseudo bilingual data and the true training data are hetero- geneous, and it may take some time for the mod- els to adapt to a new distribution which both mod- els agree. Compared with TA-NMT, TA-NMT(GI) are more stable, because the models may have <ref type="figure">Figure 5</ref>: BLEU curves on validation sets dur- ing the training processes of TA-NMT and TA- NMT(GI). (Top: EN2AR (the E-step) and AR2FR (the M-step); Bottom: EN2RO (the E-step) and RO2FR (the M-step)) adapted to a mixed distribution of heterogeneous data in the preceding back-translation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Reinforcement Learning Mechanism in Our Method</head><p>As shown in Equation 9, the E-step actually works as a reinforcement learning (RL) mecha- nism. Models p(z|x) and p(z|y) generate samples by themselves and receive rewards to update their parameters. Note that the reward here is described by the log terms in Equation 9, which is derived from our EM algorithm rather than defined arti- ficially. In <ref type="table" target="#tab_6">Table 5</ref>, we do a case study of the EN2ES translation sampled by p(z|x) as well as its time-step rewards during the E-step.</p><p>In the first case, the best translation of "politi- cal" is "polÃ­ticos". When the model p(z|x) gen- erates an inaccurate one "polÃ­ticas", it receives a negative reward (-0.01), with which the model pa- rameters will be updated accordingly. In the sec-Source in concluding , poverty eradication requires political will and commitment .</p><p>Output en (0.66) conclusiÃ³n (0.80) , (0.14) la (0.00) erradicaciÃ³n (1.00) de (0.40) la (0.00) pobreza (0.90) requiere (0.10) voluntad (1.00) y (0.46) compromiso (0.90) polÃ­ticas (-0.01) . (1.00)</p><p>Reference en conclusiÃ³n , la erradicaciÃ³n de la pobreza necesita la voluntad y compromiso polÃ­ticos .  ond case, the output misses important words and is not fluent. Rewards received by the model p(z|x) are zero for nearly all tokens in the output, leading to an invalid updating. In the last case, the output sentence is identical to the human reference. The rewards received are nearly all positive and mean- ingful, thus the RL rule will update the parameters to encourage this translation candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>NMT systems, relying heavily on the availabil- ity of large bilingual data, result in poor transla- tion quality for low-resource pairs ( <ref type="bibr" target="#b24">Zoph et al., 2016)</ref>. This low-resource phenomenon has been observed in much preceding work. A very com- mon approach is exploiting monolingual data of both source and target languages <ref type="bibr" target="#b17">(Sennrich et al., 2015;</ref><ref type="bibr" target="#b22">Zhang and Zong, 2016;</ref><ref type="bibr" target="#b5">Cheng et al., 2016;</ref><ref type="bibr" target="#b23">Zhang et al., 2018;</ref>. As a kind of data augmentation technique, ex- ploiting monolingual data can enrich the training data for low-resource pairs. <ref type="bibr" target="#b17">Sennrich et al. (2015)</ref> propose back-translation, exploits the monolin- gual data of the target side, which is then used to generate pseudo bilingual data via an additional target-to-source translation model. Different from back-translation, <ref type="bibr" target="#b22">Zhang and Zong (2016)</ref> propose two approaches to use source-side monolingual data, of which the first is employing a self-learning algorithm to generate pseudo data, while the sec- ond is using two NMT models to predict the trans- lation and to reorder the source-side monolingual sentences. As an extension to these two meth- ods, <ref type="bibr" target="#b5">Cheng et al. (2016)</ref> and <ref type="bibr" target="#b23">Zhang et al. (2018)</ref> combine two translation directions and propose a training framework to jointly optimize the source- to-target and target-to-source translation models. Similar to joint training,  propose a dual learning framework with a reinforcement learning mechanism to better leverage monolin- gual data and make two translation models pro- mote each other. All of these methods are concen- trated on exploiting either the monolingual data of the source and target language or both of them.</p><p>Our method takes a different angle but is com- patible with existing approaches, we propose a novel triangular architecture to leverage two ad- ditional language pairs by introducing a third rich language. By combining our method with existing approaches such as back-translation, we can make a further improvement.</p><p>Another approach for tackling the low-resource translation problem is multilingual neural machine translation ( <ref type="bibr" target="#b7">Firat et al., 2016)</ref>, where different encoders and decoders for all languages with a shared attention mechanism are trained. This method tends to exploit the network architecture to relate low-resource pairs. Our method is differ- ent from it, which is more like a training method rather than network modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a triangular architec- ture (TA-NMT) to effectively tackle the problem of low-resource pairs translation with a unified bidirectional EM framework. By introducing an- other rich language, our method can better ex- ploit the additional language pairs to enrich the original low-resource pair. Compared with the RNNSearch ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), a teacher- student alike method <ref type="bibr" target="#b4">(Chen et al., 2017</ref>) and the back-translation ( <ref type="bibr" target="#b17">Sennrich et al., 2015)</ref> on the same data level, our method achieves significant improvement on the MutiUN and IWSLT2012 datasets. Note that our method can be com- bined with methods exploiting monolingual data for NMT low-resource problem such as back- translation and make further improvements.</p><p>In the future, we may extend our architecture to other scenarios, such as totally unsupervised train- ing with no bilingual data for the rare language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Triangular architecture for rare language translation. The solid lines mean rich-resource and the dash lines mean low-resource. X, Y and Z are three different languages.</figDesc><graphic url="image-1.png" coords="2,125.20,86.71,109.14,59.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Triangular Learning Architecture for Low-Resource NMT</figDesc><graphic url="image-2.png" coords="4,103.38,62.81,152.79,84.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A teacher-student alike method for low-resource translation. For training p(z|x) and p(x|z), we mix the true pair (y * , z * ) âˆˆ D with the pseudo pair (x , z * ) generated by teacher model p (x |y * ) in the same mini-batch. The training procedure of p(z|y) and p(y|z) is similar.</figDesc><graphic url="image-3.png" coords="5,307.28,383.76,218.27,57.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Test BLEU of the EN-FR-AR group performed by BackTrans and TA-NMT(GI) with different amount of monolingual Arabic data.</figDesc><graphic url="image-4.png" coords="7,81.55,266.35,196.44,142.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Source visit us and get to know and love berlin ! Output visita (0.00) y (0.05) se (0.00) a (0.17) saber (0.00) y (0.04) a (0.01) berlÃ­n (0.00) ! (0.00) Reference visÃ­tanos y llegar a saber y amar a berlÃ­n . Source legislation also provides an important means of recognizing economic , social and cultural rights at the domestic level . Output la (1.00) legislaciÃ³n (0.34) tambin (1.00) constituye (0.60) un (1.00) medio (0.22) importante (0.74) de (0.63) reconocer (0.21) los (0.01) derechos (0.01) econmicos (0.03) , (0.01) sociales (0.02) y (0.01) culturales (1.00) a (0.00) nivel (0.40) nacional (1.00) . (0.03) Reference la legislaciÃ³n tambiÃ©n constituye un medio importante de reconocer los derechos econÃ³micos , iales y culturales a nivel nacional .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>:</head><label></label><figDesc></figDesc><table>E-step: update Î˜ z|x with B 1 (Equation 5) 

7: 

M-step: update Î˜ y|z with B 2 (Equation 6) 

8: 

Y â‡’ X: Optimize Î˜ z|y and Î˜ x|z 

9: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>:</head><label></label><figDesc></figDesc><table>E-step: update Î˜ z|y with B 3 (Equation 7) 

11: 

M-step: update Î˜ x|z with B 4 (Equation 8) 
12: end while 
13: return Î˜ z|x , Î˜ y|z , Î˜ z|y and Î˜ x|z 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : Test BLEU on MultiUN Dataset.</head><label>3</label><figDesc></figDesc><table>Method 
EN2HE HE2EN FR2HE HE2FR 
Ave 
EN2RO RO2EN FR2RO RO2FR 
Ave 
(Xâ‡’Z) 
(Zâ‡’X) (Yâ‡’Z) (Zâ‡’Y) 
(Xâ‡’Z) 
(Zâ‡’X) (Yâ‡’Z) (Zâ‡’Y) 

RNNSearch 
17.94 
28.32 
11.86 
21.67 
19.95 
31.44 
40.63 
17.34 
25.20 
28.65 
PBSMT 
17.39 
28.05 
12.77 
21.87 
20.02 
31.51 
39.98 
18.13 
25.47 
28.77 
T-S 
17.97 
28.42 
12.04 
21.99 
20.11 
31.80 
40.86 
17.94 
25.69 
29.07 
BackTrans 
18.69 
28.55 
12.31 
21.63 
20.20 
32.18 
41.03 
18.19 
25.30 
29.18 
TA-NMT 
19.19 
29.28 
12.76 
22.62 
20.96 
33.65 
41.93 
18.53 
26.35 
30.12 
TA-NMT(GI) 
19.90 
29.94 
13.54 
23.25 
21.66 
34.41 
42.61 
19.30 
26.53 
30.71 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Test BLEU on IWSLT Dataset.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>English to Spanish translation sampled in the E-step as well as its time-step rewards. 

</table></figure>

			<note place="foot" n="1"> https://wit3.fbk.eu/mt.php?release=2012-02-plain</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Zhirui Zhang and Shuangzhi Wu for use-ful discussions. This work is supported in part by NSFC U1636210, 973 Program 2014CB340300, and NSFC 61421003.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The expectation maximization algorithm-a short tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Borman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wit3: Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 16th Conference of the European Association for Machine Translation (EAMT)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page">268</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A teacher-student framework for zeroresource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00753</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semisupervised learning for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04596</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiun: A multilingual corpus from united nation documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eisele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh conference on International Language Resources and Evaluation</title>
		<meeting>the Seventh conference on International Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2868" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01073</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for wmt&apos;15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">SÃ©bastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT@ EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">413</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07947</idno>
		<title level="m">Sequencelevel knowledge distillation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The EM algorithm and extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thriyambakam</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">382</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00726</idno>
		<title level="m">The university of edinburgh&apos;s neural mt systems for wmt17</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Edinburgh neural machine translation systems for wmt 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02891</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02433</idno>
		<title level="m">Minimum risk training for neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploiting source-side monolingual data in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint training for neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Transfer learning for lowresource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02201</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
