<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Data Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Implicit Discourse Relation Detection via a Deep Architecture with Gated Relevance Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1726" to="1735"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word pairs, which are one of the most easily accessible features between two text segments, have been proven to be very useful for detecting the discourse relations held between text segments. However, because of the data sparsity problem, the performance achieved by using word pair features is limited. In this paper, in order to overcome the data sparsity problem, we propose the use of word embeddings to replace the original words. Moreover, we adopt a gated relevance network to capture the semantic interaction between word pairs, and then aggregate those semantic interactions using a pooling layer to select the most informative interactions. Experimental results on Penn Discourse Tree Bank show that the proposed method without using manually designed features can achieve better performance on recognizing the discourse level relations in all of the relations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In a well-written document, no unit of the text is completely isolated, discourse relations describe how two units (e.g. clauses, sentences, and larger multi-clause groupings) of discourse are logically connected. Many downstream NLP applications such as opinion mining, summarization, and event detection, can benefit from those relations.</p><p>The task of automatically identify discourse re- lation is relatively simple when explicit connec- tives such as however and because are given ( <ref type="bibr" target="#b15">Pitler et al., 2009</ref>). However, the identification becomes much more challenging when such connectives are missing. In fact, such implicit discourse relations outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser ( <ref type="bibr" target="#b9">Lin et al., 2014</ref>).</p><p>Most of the existing researches used rich lin- guistic features and supervised learning methods to achieve the task <ref type="bibr" target="#b21">(Soricut and Marcu, 2003;</ref><ref type="bibr" target="#b15">Pitler et al., 2009;</ref><ref type="bibr" target="#b18">Rutherford and Xue, 2014</ref>). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast rela- tion. However, because of the data sparsity prob- lem <ref type="bibr" target="#b11">(McKeown and Biran, 2013)</ref> and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem ( <ref type="bibr" target="#b27">Zhao and Grosky, 2002</ref>), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the re- lation held between them using only word pairs. Consider the following sentence pair with a casual relation as an example: S1: Psyllium's not a good crop. S2: You get a rain at the wrong time and the crop is ruined.</p><p>Intuitively, (good, wrong) and (good, ruined), seem to be the most informative word pairs, and it is likely that they will trigger a contrast rela- tion. Therefore, we can see that another main dis- advantage of using word pairs is the lack of con- textual information, and using n-gram pairs will again suffer from data sparsity problem.</p><p>Recently, the distributed word representa- tions ( <ref type="bibr" target="#b0">Bengio et al., 2006;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013)</ref> have shown an advantage when dealing with data sparsity problem <ref type="bibr" target="#b1">(Braud and Denis, 2015)</ref>, and many deep learning based models are generat- ing substantial interests in text semantic matching and have achieved some significant progresses (Hu  et al., 2014; <ref type="bibr" target="#b17">Qiu and Huang, 2015;</ref><ref type="bibr" target="#b24">Wan et al., 2015</ref>). Inspired by their work, we in this pa- per propose the use of word embeddings to re- place the original words in the text segments to fight against the data sparsity problem. Further more, in order to preserve the contextual infor- mation around the word embeddings, we encode the text segment to its positional representation via a recurrent neural network, specifically, we use a bidirectional LSTM (Hochreiter and Schmidhu- ber, 1997). Then, to overcome the semantic gap, we propose the use of a gated relevance network to capture the semantic interaction between those positional representations. Finally, all the interac- tions generated by the relevance network are fed to a max pooling layer to get the strongest interac- tions. We then aggregate them to predict the dis- course relation through a multi-layer perceptron (MLP). Our model is trained end to end by Back- Propagation and Adagrad.</p><p>The main contribution of this paper can be sum- marized as follows:</p><p>• We use word embeddings to replace the orig- inal words in the text segments to overcome data sparsity problem. In order to preserve the contextual information, we further en- code the text segment to its positional repre- sentation through a recurrent neural network.</p><p>• To deal with the semantic gap problem, we adopt a gated relevance network to capture the semantic interaction between the interme- diate representations of the text segments.</p><p>• Experimental results on PDTB ( <ref type="bibr" target="#b16">Prasad et al., 2008)</ref> show that the proposed method can achieve better performance in recognizing discourse level relations in all of the relations than the previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Method</head><p>The architecture of our proposed method is shown in <ref type="figure" target="#fig_0">figure 1</ref>. In the following of this section, we will illustrate the details of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embedding Layer</head><p>To model the sentences with neural model, we firstly need to transform the one-hot representa- tion of word into the distributed representation. All words of two text segments X and Y will be mapped into low dimensional vector representa- tions, which are taken as input of the network.</p><p>Through this layer, we can filter the words appear in low frequency, and we then map these words to a special OOV (out of vocabulary) word em- bedding. In addition, all the text segments in our experiment are padded to have the same length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentence Modeling with LSTM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long</head><p>Short-Term Memory network (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>) is a type of recurrent neural network (RNN), and specifically addresses the issue of learning long-term dependencies. Given a variable-length sentence S = (x 0 , x 1 , ..., x T ), LSTM processes it by incrementally adding up new content into a single slot of maintained memory, with gates controlling the extent to which new content should be memorized, old content should be erased and current content should be exposed. At position t, the memory c t and the hidden state h t are updated with the following equations:</p><formula xml:id="formula_0">    ˜ c t o t i t f t     =     tanh σ σ σ     T A,b x t h t−1 ,<label>(1)</label></formula><formula xml:id="formula_1">c t = ˜ c t i t + c t−1 f t ,<label>(2)</label></formula><formula xml:id="formula_2">h t = o t tanh (c t ) ,<label>(3)</label></formula><p>where i t , f t , o t , denote the input, forget and out- put gate at time step t respectively, and T A,b is an affine transformation which depends on parame- ters of the network A and b. σ denotes the logis- tic sigmoid function and denotes elementwise multiplication.</p><p>Notice that the LSTM defined above only get context information from the past. However, con- text information from the future could also be cru- cial. To capture the context from both past and the future, we propose to use the bidirectional LSTM ( <ref type="bibr" target="#b19">Schuster and Paliwal, 1997</ref>). Bidirectional LSTM preserves the previous and future context information by two separate LSTMs, one encodes the sentence from start to the end, and the other encodes the sentence from end to the start. There- fore, at each position t of the sentence, we can ob- tain two representations − → h t and ← − h t . It is natural to concatenate them to get the intermediate rep- resentation at position t, i.e.</p><formula xml:id="formula_3">h t = [ − → h t , ← − h t ].</formula><p>A illustration for the bidirectional LSTM are shown in <ref type="figure">Figure 2</ref>.</p><p>Given a sentence S = (x 0 , x 1 , ..., x T ), we can now encode it with a bidirectional LSTM, and re-</p><formula xml:id="formula_4">h t-1 h t h t+1 x t-1 x t x t+1 h t-1 h t h t+1 h t-1 h t h t+1</formula><p>Figure 2: A illustration of bidirectional LSTM.</p><p>place the word w t with h t , we can interpret h t as a representation summarizing the word at position t and its contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gated Relevance Network</head><p>Given two text segments X = x 1 , x 2 , ..., x n and Y = y 1 , y 2 , ..., y m , after the encoding procedure with a bidirectional LSTM, we can get their posi- tional representation Bilinear Model is defined as follows:</p><formula xml:id="formula_5">X h = x h 1 , x h 2 , .</formula><formula xml:id="formula_6">s(h x i , h y j ) = h T x i M h y j ,<label>(4)</label></formula><p>where the only parameter M ∈ R d h ×d h . The bi- linear model is a simple but efficient way to incor- porate the strong linear interactions between two vectors, while the main weakness of it is the lack of ability to deal with nonlinear interaction.</p><p>Single Layer Network is defined as:</p><formula xml:id="formula_7">s(h x i , h y j ) = u T f (V h x i h y j + b),<label>(5)</label></formula><p>where f is a standard nonlinearity applied element-wise, V ∈ R k×2d h , b ∈ R k , and u ∈ R k . The single layer network could capture nonlinear interaction, while at the expense of a weak inter- action between two vectors. Each of the two models have its own advan- tages, and they can not take the place of each other.</p><p>In our work, we propose to incorporate the two models through the gate mechanism, so that the model is more powerful to capture more complex semantic interactions. The incorporated model, namely gated relevance network (GRN), is de- fined as:</p><formula xml:id="formula_8">s(h x i , h y j ) = u T (g h T x i M [1:r] h y j + (1 − g) f (V h x i h y j ) + b),<label>(6)</label></formula><p>where f is a standard nonlinearity applied element-wise, M <ref type="bibr">[1:r]</ref> ∈ R r×d h ×d h is a bilinear tensor and the tensor product h T x i M <ref type="bibr">[1:r]</ref> h y j results in a vector m ∈ R r , where each entry is com- puted by one slice k = 1, 2, ..., r of the tensor:</p><formula xml:id="formula_9">m k = h T x i M k h y j , V ∈ R r×2d h , b ∈ R r</formula><p>, and u ∈ R r , g is a gate expressing how the output is produced by the linear and nonlinear semantic in- teractions between the input, defined as:</p><formula xml:id="formula_10">g = σ(W g h x i h y j + b g ),<label>(7)</label></formula><p>where W g ∈ R r×2d h , b ∈ R r and σ denotes the logistic sigmoid function. The gated relevance network is a little bit simi- lar to the Neural Tensor Network (NTN) proposed by <ref type="bibr" target="#b20">Socher et al. (2013)</ref>:</p><formula xml:id="formula_11">s(h x i , h y j ) = u T f (h T x i M [1:r] h y j +V h x i h y j +b).</formula><p>(8) Compared with NTN, the main advantage of our model is we use a gate to tell how the lin- ear and nonlinear interaction should be combined, while in NTN, the interaction generated by bi- linear model and single layer network are treated equally. Also, NTN feeds the incorporated inter- action to a nonlinearity, while we are not.</p><p>As we can see, for each pair of the intermediate representation, the gated relevance network will produce a semantic interaction score, thus, the en- tire output of two text segments is an interaction score matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Max-Pooling Layer and MLP</head><p>The relation between two text segments is often determined by some strong semantic interactions, therefore, we adopt max-pooling strategy which partitions the score matrix as shown in <ref type="figure" target="#fig_0">Figure 1</ref> into a set of non-overlapping sub-regions, and for each such sub-region, outputs the maximum value. The pooling scores are further reshaped to a vector and fed to a multi-layer perceptron (MLP). More specifically, the vector obtained by the pooling layer is fed into a full connection hidden layer to get a more abstractive representation first, and then connect to the output layer. For the task of classi- fication, the outputs are probabilities of different classes, which is computed by a softmax function after the fully-connected layer. We name the full architecture of our model Bi-LSTM+GRN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Model Training</head><p>Given a text segment pair (X, Y ) and its label l, the training objective is to minimize the cross- entropy of the predicted and the true label distri- butions, defined as:</p><formula xml:id="formula_12">L(X, Y ; l, ˆ l) = − C j=1 l j log( ˆ l j ),<label>(9)</label></formula><p>where l is one-hot representation of the ground- truth label l; ˆ l is the predicted probabilities of la- bels; C is the class number.</p><p>To minimize the objective, we use stochastic gradient descent with the diagonal variant of Ada- Grad ( <ref type="bibr" target="#b4">Duchi et al., 2011</ref>) with minibatches. The parameter update for the i-th parameter θ t,i at time step t is as follows:</p><formula xml:id="formula_13">θ t,i = θ t−1,i − α t τ =1 g 2 τ,i g t,i ,<label>(10)</label></formula><p>where α is the initial learning rate and g τ ∈ R |θ τ,i | is the gradient at time step τ for parameter θ τ,i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>The dataset we used in this work is Penn Dis- course   <ref type="table" target="#tab_2">Table 1</ref>. The negative samples were chosen ran- domly from training sections 2-20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment Protocols</head><p>In this part, we will mainly introduce the exper- iment settings, including baselines and parameter setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Baselines</head><p>The baselines for comparison with our proposed method are listed as follows:</p><p>• LSTM: We use two single LSTM to encode the two text segments, then concatenate them and feed to a MLP to do the relation detec- tion.</p><p>• Bi-LSTM: We use two single bidirectional LSTM to encode the two text segments, then concatenate them and feed to a MLP to do the relation detection.</p><p>• Word+NTN: We use the neural tensor de- fined in (8) to capture the semantic interac- tion scores between every word embedding pair, the rest of the method is the same as our proposed method.</p><p>• LSTM+NTN: We use two single LSTM to generate the positional text segments repre- sentation. The rest of the method is the same as Word-NTN.</p><p>• BLSTM+NTN: We use two single bidirec- tional LSTM to generate the positional text <ref type="table">Table 2</ref>: Hyperparameters for our model in the ex- periment.</p><p>Word Embedding size n w = 50 Initial learning rate ρ = 0.01 Minibatch size m = 32 Pooling Size (p, q) = (3, 3) Number of tensor slice r = 2 segments representation. The rest of the method is the same as Word-NTN.</p><p>• Word+GRN: We use the gated relevance net- work proposed in this paper to capture the se- mantic interaction scores between every word embedding pair of the two text segments. The rest of the method is the same as our model.</p><p>• LSTM+GRN: We use the gated relevance network proposed in this paper to capture the semantic interaction scores between every in- termediate representation pair of the two text segments generated by LSTM. The rest of the method is the same as our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Parameter Setting</head><p>For the initialization of the word embeddings used in our model, we use the 50-dimensional pre-trained embeddings provided by <ref type="bibr" target="#b23">Turian et al. (2010)</ref>, and the embeddings are fixed during train- ing. We only preserve the top 10,000 words ac- cording to its frequency of occurrence in the train- ing data, all the text segments are padded to have the same length of 50, the intermediate represen- tations of LSTM are also set to 50. The other parameters are initialized by randomly sampling from uniform distribution in [-0.1,0.1]. For other hyperparameters of our proposed model, we take those hyperparameters that achieved best performance on the development set, and keep the same parameters for other com- petitors. The final hyper-parameters are show in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Result</head><p>The results on PDTB are show in Table3, from the results, we have several experiment findings.</p><p>First of all, it is easy to notice that LSTM and Bi-LSTM achieve lower performance than all of the methods of using a tensor to capture the se- mantic interactions between word pairs and the in- termediate representation pairs. Because the main <ref type="table">Table 3</ref>: The performances of different approaches on the PDTB.</p><p>Comparison Contingency Expansion Temporal ( <ref type="bibr" target="#b15">Pitler et al., 2009)</ref> 21.96% 47.13% 76.42% 16.76% ( <ref type="bibr" target="#b28">Zhou et al., 2010)</ref> 31.79% 47.16% 70.11% 20.30% ( <ref type="bibr" target="#b13">Park and Cardie, 2012)</ref> 31.32% 49.82% 79.22% 26.57% <ref type="bibr" target="#b18">(Rutherford and Xue, 2014)</ref> 39.70% 54.42% 80.44% 28.69% ( <ref type="bibr" target="#b8">Ji and Eisenstein, 2015)</ref> 35 disadvantage of using LSTM and Bi-LSTM to en- code the text segment into a single representation is that some important local information such as key words can not be fully preserved when com- pressing a long sentence into a single representa- tion. Second, the performance improves a lot when using LSTM and Bi-LSTM to encode the text seg- ments to positional representations instead of us- ing word representations directly. We conclude it is mainly because the following two reasons: for one thing, some words are important only when they are associated with their context, for the other, the intermediate representations are the high-level representation of the sentence at each position, there is no doubt for they can obtain much more semantic information than the words along. In addition, Bi-LSTM also takes the future information of the text segments into considera- tion, resulting in a consistently better performance than LSTM.</p><p>Third, take a comparison to the methods using NTN and the methods using GRN, we can find that the GRN performs consistently better. Such results show that the gate we proposed to combine the information of two aspects is actually useful.</p><p>At last, our proposed model, namely, Bi- LSTM+GRN achieves best performance on all of the relations. It not only shows the interaction be- tween word pairs is useful, but also shows the way we proposed to capture such information is use- ful too. Further more, compared with the previ- ous methods <ref type="bibr" target="#b15">(Pitler et al., 2009;</ref><ref type="bibr" target="#b13">Park and Cardie, 2012;</ref><ref type="bibr" target="#b18">Rutherford and Xue, 2014)</ref>, which used ei- ther a lot of complex textual features and contex- tual information about the two text segments or a larger unannotated corpus to help the prediction, our model only uses the the information of the two text segments themselves, but yet achieves better performance. It demonstrates that our model is powerful in modeling the discourse relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter Sensitivity</head><p>In this section, we evaluate our model through different settings of the proposed gated relevance network, the other hyperparameters are the same as mentioned above and we use a bidirectional LSTM to encode the text segments. The settings are shown as follows:</p><p>• GRN-1 We set the parameters r = 1, M <ref type="bibr">[1]</ref> = I, V = 0, b = 0 and g = 1. The model can be regarded as cosine similarity.  <ref type="table">Table 4</ref>: Comparison of our model with different parameter settings to the gated relevance network. Cmp denotes the comparison relation, Ctg denotes the contingency relation, Exp denotes the expan- sion relation and Tmp denotes the temporal rela- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cmp</head><p>• GRN-2 We set the parameters r = 1, V = 0, b = 0, and g = 1. The model can be regarded as the bilinear model.</p><p>• GRN-3 We set the parameters r = 1, M <ref type="bibr">[1]</ref> = 0, and g = 0. The model can be regarded as a single layer network.</p><p>• GRN-4 We set the parameters r = 1. This model is the full GRN model.</p><p>• GRN-5 We set the parameters r = 2. This model is the full GRN model.</p><p>• GRN-6 We set the parameters r = 3. This model is the full GRN model.</p><p>The results for different parameter settings are shown in <ref type="table">Table 4</ref>. It is obvious that GRN-1 achieves a relatively lower performance, showing that the cosine similarity is not enough to capture the complex semantic interaction. Take a com- parison on GRN-2 and GRN-3, we can see that GRN-2 outperforms GRN-3 on Comparison and Expansion relation, while achieves a lower perfor- mance on the other two relations, moreover, the combination method GRN-4 outperforms both of the methods, demonstrating that the semantic in- teractions captured by the bilinear model and the single layer network are different. Hence, they can not take the place of each other, and it is reason- able to use a gate to combine them.</p><p>Among the methods of using the full GRN model, GRN-5 which has 2 bilinear tensor slices achieves the best performance. We explain this phenomenon on two aspects, on one hand, we can see each slice of the bilinear tensor as being re- sponsible for one type of the relation, a bilinear tensor with 2 slices is more suitable for training a binary classifier than the original bilinear model. On the other hand, increasing the number of slices will increase the complexity of the model, thus making it harder to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Case Study</head><p>In this section, we go back to the example men- tioned above to show see what information be- tween the text segment pairs is captured, and how the positional sentence representations affect the performance of our model.</p><p>The examples is listed below:</p><p>S1: Psyllium's not a good crop. S2: You get a rain at the wrong time and the crop is ruined. In this case, the relation between the sentence pair is Contingency, and the implicit connective annotated by human is "because". The pair is likely to be classified to a wrong contrast rela- tion if we only focus on the informative word pairs (good,wrong) and (good,ruined). It is mainly because their relation is highly depended on the semantic of the whole sentence, and the words should be considered with their context. , we can see that the word pairs which as- sociate with "not" get high scores, scores on the other pairs are relatively arbitrary. It demonstrates the word embedding model failed to learn which part of the sentence should be focused, although the useless word such as "Psyllium" and "a" are ignored, thus making it harder to identify the rela- tion.</p><p>Take <ref type="figure" target="#fig_2">Figure 3b</ref> for a comparison, from the fig- ure we can observe the pairs that associate with "not" and "good" which are import context to de- termine the semantic of the sentence get much higher scores. Moreover, the scores increase along with the sentence encoding procedure, especially when the last informative word "ruined" appears. Once again, some useless word are also ignored by this model. It demonstrates the bidirectional LSTM we used in our model could encode the contextual information to the intermediate repre- sentations, thus these information could help to determine which part of the two sentence should be focused when identifying their relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Discourse relations, which link clauses in text, are used to represent the overall text structure. Many downstream NLP tasks such as text summariza- tion, question answering, and textual entailment can benefit from the task. Along with the in- creasing requirement, many works have been con- structed to automatically identify these relations from different aspects ( <ref type="bibr" target="#b14">Pitler et al., 2008;</ref><ref type="bibr" target="#b15">Pitler et al., 2009;</ref><ref type="bibr" target="#b28">Zhou et al., 2010;</ref><ref type="bibr" target="#b11">McKeown and Biran, 2013;</ref><ref type="bibr" target="#b18">Rutherford and Xue, 2014;</ref><ref type="bibr" target="#b26">Xue et al., 2015)</ref>.</p><p>For training and comparing the performance of different methods, the Penn Discourse Treebank (PDTB) 2.0, which is large annotated discourse corpuses, were released in 2008 ( <ref type="bibr" target="#b16">Prasad et al., 2008)</ref>. The annotation methodology of it fol- lows the lexically grounded, predicate-argument approach. In PDTB, the discourse relations were predefined by <ref type="bibr" target="#b25">Webber (2004)</ref>. PDTB-styled dis- course relations hold in only a local contextual window, and these relations are organized hierar- chically. Also, every relation in PDTB has either an explicit or an implicit marker. Since explicit relations are easy to identify <ref type="bibr" target="#b14">(Pitler et al., 2008)</ref>, existing methods achieved good performance on the relations with explicit maker. In recent years, researchers mainly focused on implicit relations. For easily comparing with other methods, in this work, we also use PDTB as the training and test- ing corpus.</p><p>As we mentioned above, various approaches have been proposed to do the task. <ref type="bibr" target="#b15">Pitler et al. (2009)</ref> proposed to train four binary classifiers us- ing word pairs as well as other rich linguistic fea- tures to automatically identify the top-level PDTB relations. <ref type="bibr" target="#b13">Park and Cardie (2012)</ref> achieved a higher performance by optimizing the feature set. <ref type="bibr" target="#b11">McKeown and Biran (2013)</ref> aims at solving the data sparsity problem, and they extended the work of <ref type="bibr" target="#b15">Pitler et al. (2009)</ref> by aggregating word pairs. <ref type="bibr" target="#b18">Rutherford and Xue (2014)</ref> used Brown clusters and coreferential patterns as new features and im- proved the baseline a lot. <ref type="bibr" target="#b1">Braud and Denis (2015)</ref> compared different word representations for im- plicit relation classification. The word pairs fea- ture have been studied by all of the work above, showing its importance on discourse relation. We follow their work, and incorporate word embed- ding to deal with this problem.</p><p>There also exist some work performing this task from other perspectives. <ref type="bibr" target="#b28">Zhou et al. (2010)</ref> stud- ied the problem from predicting implicit marker. They used a language model to add implicit mark- ers as an additional feature to improve perfor- mance. Their approach can be seen as a semi- supervised method. <ref type="bibr" target="#b8">Ji and Eisenstein (2015)</ref> com- putes distributed meaning representations for each discourse argument by composition up the syn- tactic parse tree. <ref type="bibr" target="#b2">Chen et al. (2016)</ref> used vector offsets to represent this relation between sentence pairs, and aggregate this offsets through the Fisher vector.  used a a mutil-task deep learning framework to deal with this problem, they incorporate other similar corpus to deal with the data sparsity problem.</p><p>Most of the previous works mentioned above used rich linguistic features and supervised learn- ing methods to achieve the task. In this paper, we propose a deep architecture, which does not need these manually selected features and addi- tional linguistic knowledge base to do it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose to use word embeddings to fight against the data sparsity problem of word pairs. In order to preserve contextual information, we encode a sentence to its positional represen- tation via a recurrent neural network, specifically, a LSTM. To solve the semantic gap between the word pairs, we propose to use a gated relevance network which incorporates both the linear and nonlinear interactions between pairs. Experiment results on PDTB show the proposed model outper- forms the existing methods using traditional fea-tures on all of the relations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The processing framework of the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Treebank 2.0 (Prasad et al., 2008), which is one of the largest available annotated corpora of discourse relations. It contains 40,600 re- lations, which are manually annotated from the same 2,312 Wall Street Journal (WSJ) articles as the Penn Treebank. We follow the recommended section partition of PDTB 2.0, which is to use sections 2-20 for training, sections 21-22 for test- ing and the other sections for validation (Prasad et al., 2008). For comparison with the previous work (Pitler et al., 2009; Zhou et al., 2010; Park</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A visualization of the interaction score matrix between two relatively complex sentences. The darker patches denote the corresponding scores generated by the gated relevance network are higher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 explains</head><label>3</label><figDesc>Figure 3 explains this phenomenon, in Figure 3a, we can see that the word pairs which associate with "not" get high scores, scores on the other pairs are relatively arbitrary. It demonstrates the word embedding model failed to learn which part of the sentence should be focused, although the useless word such as "Psyllium" and "a" are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 : The unbalanced sample distribution of PDTB.</head><label>1</label><figDesc></figDesc><table>Relation 
Train Dev Test 
Comparison 1894 401 146 
Contingency 3281 628 276 
Expansion 
6792 1253 556 
Temporal 
665 
93 
68 

and Cardie, 2012; Rutherford and Xue, 2014; Ji 
and Eisenstein, 2015), we train four binary classi-
fiers to identify each of the top level relations, the 
EntRel relations are merged with Expansion rela-
tions. For each classifier, we use an equal number 
of positive and negative samples as training data, 
because each of the relations except Expansion is 
infrequent (Pitler et al., 2009) as what shows in 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors wish to thank the anonymous review-ers for their helpful comments. This work was par-tially funded by National Natural Science Foun-dation of <ref type="bibr">China (No. 61532011, 61473092, and 61472088)</ref>, the National High Technology Re-search and Development Program of China (No. 2015AA015408).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing word representations for implicit discourse relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discourse relations detection via a mixed generative-discriminative framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One vector is not enough: Entity-augmented distributed semantics for discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="329" to="344" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A pdtb-styled end-to-end discourse parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="151" to="184" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Implicit discourse relation classification via multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aggregated word pair features for implicit discourse relation disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving implicit discourse relation recognition through feature set optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="108" to="112" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Easily identifiable discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mridhula</forename><surname>Raghupathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hena</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind K</forename><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CIS</publisher>
			<biblScope unit="page">884</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Technical Reports</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic sense prediction for implicit discourse relations in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional neural tensor network architecture for community-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1305" to="1311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discovering implicit discourse relations through brown cluster pair representation and coreference patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Attapol</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">645</biblScope>
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sentence level discourse parsing using syntactic and lexical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08277</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">D-ltag: Extending lexicalized tag to discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="751" to="779" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The conll-2015 shared task on shallow discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi Prasado Christopher</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rutherford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Narrowing the semantic gap-improved text-based web document retrieval using visual features. Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William I Grosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="200" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting discourse connectives for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Min</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1507" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
