<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Greedy Constituent Parsing with Dynamic Oracles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
							<email>maximin.coavoux@inria.fr benoit.crabbe@linguist.univ-paris-diderot.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Paris Diderot</orgName>
								<orgName type="institution" key="instit2">Sorbonne Paris Cité</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Alpage, Inria</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıt</forename><forename type="middle">Benoˆıt</forename><surname>Crabbé</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Paris Diderot</orgName>
								<orgName type="institution" key="instit2">Sorbonne Paris Cité</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Alpage, Inria</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institut Universitaire</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Greedy Constituent Parsing with Dynamic Oracles</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="172" to="182"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Dynamic oracle training has shown substantial improvements for dependency parsing in various settings, but has not been explored for constituent parsing. The present article introduces a dynamic oracle for transition-based constituent parsing. Experiments on the 9 languages of the SPMRL dataset show that a neu-ral greedy parser with morphological features , trained with a dynamic oracle, leads to accuracies comparable with the best non-reranking and non-ensemble parsers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Constituent parsing often relies on search methods such as dynamic programming or beam search, be- cause the search space of all possible predictions is prohibitively large. In this article, we present a greedy parsing model. Our main contribution is the design of a dynamic oracle for transition- based constituent parsing. In NLP, dynamic or- acles were first proposed to improve greedy de- pendency parsing training without involving addi- tional computational costs at test time <ref type="bibr" target="#b12">(Goldberg and Nivre, 2012;</ref>.</p><p>The training of a transition-based parser in- volves an oracle, that is a function mapping a con- figuration to the best transition. Transition-based parsers usually rely on a static oracle, only well- defined for gold configurations, which transforms trees into sequences of gold actions. Training against a static oracle restricts the exploration of the search space to the gold sequence of actions. At test time, due to error propagation, the parser will be in a very different situation than at train- ing time. It will have to infer good actions from noisy configurations. To alleviate error propaga- tion, a solution is to train the parser to predict the best action given any configuration, by allowing it to explore a greater part of the search space at train time. Dynamic oracles are non-deterministic oracles well-defined for any configuration. They give the best possible transitions for any config- uration. Although dynamic oracles are widely used in dependency parsing and available for most standard transition systems <ref type="bibr" target="#b14">Goldberg et al., 2014;</ref><ref type="bibr" target="#b16">Gómez-Rodríguez et al., 2014;</ref><ref type="bibr" target="#b28">Straka et al., 2015)</ref>, no dynamic oracle parsing model has yet been proposed for phrase structure grammars.</p><p>The model we present aims at parsing mor- phologically rich languages (MRL). Recent re- search has shown that morphological features are very important for MRL parsing <ref type="bibr">(Björkelund et al., 2013;</ref><ref type="bibr" target="#b7">Crabbé, 2015)</ref>. However, traditional linear models (such as the structured perceptron) need to define rather complex feature templates to capture interactions between features. Addi- tional morphological features complicate this task <ref type="bibr" target="#b7">(Crabbé, 2015)</ref>. Instead, we propose to rely on a neural network weighting function which uses a non-linear hidden layer to automatically capture interactions between variables, and embeds mor- phological features in a vector space, as is usual for words and other symbols <ref type="bibr" target="#b6">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b4">Chen and Manning, 2014)</ref>.</p><p>The article is structured as follows. In Section 2, we present neural transition-based parsing. Sec- tion 3 motivates learning with a dynamic oracle and presents an algorithm to do so. Section 4 in- troduces the dynamic oracle. Finally, we present parsing experiments in Section 5 to evaluate our proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transition-Based Constituent Parsing</head><p>Transition-based parsers for phrase structure grammars generally derive from the work of Sagae and <ref type="bibr" target="#b25">Lavie (2005)</ref>. In the present paper, we extend Crabbé (2015)'s transition system.</p><p>Grammar form We extract the grammar from a head-annotated preprocessed constituent tree- bank (cf Section 5). The preprocessing involves two steps. First, unary chains are merged, ex- cept at the preterminal level, where at most one unary production is allowed. Second, an order-0 head-markovization is performed <ref type="figure" target="#fig_0">(Figure 1</ref>). This step introduces temporary symbols in the bina- rized grammar, which are suffixed by ":". The re- sulting productions have one the following form:</p><formula xml:id="formula_0">X[h] → A[a] B[b] X[h] → A[a] b X[h] → h X[h] → a B[b]</formula><p>where X, A, B are delexicalised non-terminals, a, b and h ∈ {a, b} are tokens, and X[h] is a lexical- ized non-terminal. The purpose of lexicalization is to allow the extraction of features involving the heads of phrases together with their tags and mor- phological attributes.</p><p>Transition System In the transition-based framework, parsing relies on two data structures: a buffer containing the sequence of tokens to parse and a stack containing partial instantiated trees. A configuration C = j, S, b, γ is a tuple where j is the index of the next token in the buffer, S is the current stack, b is a boolean, and γ is the set of constituents constructed so far. 1 Constituents are instantiated non-terminals, i.e. tuples (X, i, j) such that X is a non-terminal and (i, j) are two integers denoting its span. Although the content of γ could be retrieved from the stack, we make it explicit because it will be useful for the design of the oracle in Section 4.</p><p>From an initial configuration C 0 = 0, , ⊥, ∅∅, the parser incrementally derives new configura- tions by performing actions until a final configura- tion is reached. S(HIFT) pops an element from the <ref type="bibr">1</ref> The introduction of γ is the main difference with Crabbé (2015)'s transition system. <ref type="table">Table 1</ref>: Constraints to ensure that binary trees can be unbinarized. n is the sentence length.</p><formula xml:id="formula_1">Stack: S|(C, l, i)|(B, i, k)|(A, k, j) Action Constraints RL(X) or RR(X), X∈ N A / ∈ N tmp and B / ∈ N tmp RL(X:) or RR(X:), X:∈ N tmp C / ∈ N tmp or j &lt; n RR(X) B / ∈ N tmp RL(X) A / ∈ N tmp</formula><formula xml:id="formula_2">Input w 0 w 1 . . . w n−1 Axiom 0, , ⊥, ∅∅ S j, S, ⊥, γ j + 1, S|(t j , j, j + 1), , γ RL(X) j, S|(A, i, k)|(B, k, j), ⊥, γ j, S|(X, i, j), ⊥, γ ∪ {(X, i, j)}} RU(X) j, S|(t j−1 , j − 1, j), , γ j, S|(X, j − 1, j), ⊥, γ ∪ {(X, j − 1, j)}} GR j, S, , γ j, S, ⊥, γ</formula><p>Figure 2: Transition system, the transition RR(X) and the lexicalization of symbols are omitted.</p><p>buffer and pushes it on the stack. R(EDUCE)(X) pops two elements from the stack, and pushes a new non-terminal X on the stack with the two el- ements as its children. There are two kinds of bi- nary reductions, left (RL) or right (RR), depend- ing on the position of the head. Finally, unary re- ductions (RU(X)) pops only one element from the stack and pushes a new non-terminal X. A deriva- tion</p><formula xml:id="formula_3">C 0⇒τ = C 0 a 0 ⇒ . . . a τ −1</formula><p>⇒ C τ is a sequence of configurations linked by actions and leading to a final configuration. <ref type="figure">Figure 2</ref> presents the algo- rithm as a deductive system. G(HOST)R(EDUCE) actions and boolean b ( or ⊥) are used to ensure that unary reductions (RU) can only take place once after a SHIFT action. <ref type="bibr">2</ref> Constraints on the transitions make sure that predicted trees can be unbinarized. <ref type="figure">Figure 3</ref> shows two examples of trees that could not have been obtained by the binarization process. In the first tree, a temporary symbol rewrites as two tempo-</p><formula xml:id="formula_4">A[h] C:[c] A:[h] A[h] C[h] A:[a]</formula><p>Figure 3: Examples of ill-formed binary trees rary symbols. In the second one, the head of a temporary symbol is not the head of its direct par- ent. <ref type="table">Table 1</ref> shows a summary of the constraints used to ensure that any predicted tree is a well- formed binarized tree. <ref type="bibr">3</ref> In this table, N is the set of non-terminals and N tmp ⊂ N is the set of tem- porary non-terminals.</p><p>Weighted Parsing The deductive system is in- herently non-deterministic. Determinism is pro- vided by a scoring function</p><formula xml:id="formula_5">s(C 0⇒τ ) = τ i=1 f θ (C i−1 , a i )</formula><p>where θ is a set of parameters. The score of a derivation decomposes as a sum of scores of ac- tions. In practice, we used a feed-forward neu- ral network very similar to the scoring model of <ref type="bibr" target="#b4">Chen and Manning (2014)</ref>. The input of the net- work is a sequence of typed symbols. We consider three main types (non-terminals, tags and termi- nals) plus a language-dependent set of morpholog- ical attribute types, for example, gender, number, or case <ref type="bibr" target="#b7">(Crabbé, 2015)</ref>. The first layer h (0) is a lookup layer which concatenates the embeddings of each typed symbol extracted from a configura- tion. The second layer h (1) is a non-linear layer with a rectifier activation (ReLU). Finally, the last layer h (2) is a softmax layer giving a distribution over possible actions, given a configuration. The score of an action is its log probability.</p><p>Assuming v 1 , v 2 . . . , v α are the embeddings of the sequence of symbols extracted from a config- uration, the forward pass is summed up by the fol- lowing equations:</p><formula xml:id="formula_6">h (0) = [v 1 ; v 2 ; . . . ; v α ] h (1) = max{0, W (h) · h (0) + b (h) } h (2) = Softmax(W (o) · h (1) + b (o) ) f θ (C, a) = log(h (2) a )</formula><p>3 There are additional constraints which are not presented here. For example, SHIFT assumes that the buffer is not empty. A full description of constraints typically used in a slightly different transition system can be found in <ref type="bibr">Zhang and Clark (2009)</ref>'s appendix section. Thus, θ includes the weights and biases for each layer (</p><formula xml:id="formula_7">s2.ct[s2.wt] s1.ct[s1.wt] s1.cl[s1.wl] s1.cr[s1.wr] s0.ct[s0.wt] s0.cl[s0.wl] s0.cr[s0.wr] q1 . . . q4</formula><formula xml:id="formula_8">W (h) , W (o) , b (h) , b (o) )</formula><p>, and the embed- ding lookup table for each symbol type. We perform greedy search to infer the best- scoring derivation. Note that this is not an exact inference. Most propositions in phrase structure parsing rely on dynamic programming <ref type="bibr" target="#b10">(Durrett and Klein, 2015;</ref><ref type="bibr" target="#b19">Mi and Huang, 2015)</ref> or beam search <ref type="bibr" target="#b7">(Crabbé, 2015;</ref><ref type="bibr" target="#b29">Watanabe and Sumita, 2015;</ref><ref type="bibr">Zhu et al., 2013</ref>). However we found that with a scoring function expressive enough and a rich feature set, greedy decoding can be surpris- ingly accurate (see Section 5).</p><p>Features Each terminal is a tuple containing the word form, its part-of-speech tag and an arbi- trary number of language-specific morphological attributes, such as CASE, GENDER, NUMBER, ASPECT and others <ref type="bibr" target="#b27">(Seddah et al., 2013;</ref><ref type="bibr" target="#b7">Crabbé, 2015)</ref>. The representation of a configuration de- pends on symbols at the top of the two data struc- tures, including the first tokens in the buffer, the first lexicalised non-terminals in the stack and pos- sibly their immediate descendants ( <ref type="figure" target="#fig_1">Figure 4</ref>). The full set of templates is specified in <ref type="table">Table 6</ref> of An- nex A. The sequence of symbols that forms the in- put of the network is the instanciation of each posi- tion described in this table with a discrete symbol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training a Greedy Parser with an Oracle</head><p>An important component for the training of a parser is an oracle, that is a function mapping a gold tree and a configuration to an action. The oracle is used to generate local training examples from trees, and feed them to the local classifier. A static oracle <ref type="bibr" target="#b12">(Goldberg and Nivre, 2012</ref>) is an incomplete and deterministic oracle. It is only well-defined for gold configurations (the configu- rations derived by the gold action sequence) and returns the unique gold action. Usually, parsers use a static oracle to transform the set of bina- rized trees into a set D = {C (i) , a (i) } 1≤i≤T of training examples. Training consists in minimiz-ing the negative log likelihood of these examples. The limitation of this training method is that only gold configurations are seen during training. At test time, due to error propagation, the parser will have to predict good actions from noisy configu- rations, and will have much difficulty to recover after mistakes.</p><p>To alleviate this problem, a line of work <ref type="bibr">(Daumé III et al., 2006;</ref><ref type="bibr" target="#b24">Ross et al., 2011</ref>) has cast the prob- lem of structured prediction as a search problem and developed training algorithms aiming at ex- ploring a greater part of the search space. These methods require an oracle well-defined for every search state, that is, for every parsing configura- tion.</p><p>A dynamic oracle is a complete and non- deterministic oracle <ref type="bibr" target="#b12">(Goldberg and Nivre, 2012)</ref>. It returns the non-empty set of the best transitions given a configuration and a gold tree. In depen- dency parsing, starting from Goldberg and Nivre (2012), dynamic oracle algorithms and training methods have been proposed for a variety of tran- sition systems and led to substantial improvements in accuracy ( <ref type="bibr" target="#b14">Goldberg et al., 2014;</ref><ref type="bibr" target="#b16">Gómez-Rodríguez et al., 2014;</ref><ref type="bibr" target="#b28">Straka et al., 2015;</ref><ref type="bibr" target="#b15">Gómez-Rodríguez and FernándezGonzález, 2015</ref>).</p><p>Online training An online trainer iterates sev- eral times over each sentence in the treebank, and updates its parameters until convergence. When a static oracle is used, the training examples can be pregenerated from the sentences. When we use a dynamic oracle instead, we generate training ex- amples on the fly, by following the prediction of the parser (given the current parameters) instead of the gold action, with probability p, where p is a hyperparameter which controls the degree of ex- ploration. The online training algorithm for a sin- gle sentence s, with an oracle function o is shown in <ref type="figure" target="#fig_2">Figure 5</ref>. It is a slightly modified version of Goldberg and Nivre (2013)'s algorithm 3, an ap- proach they called learning with exploration.</p><p>In particular, as our neural network uses a cross- entropy loss, and not the perceptron loss used in , updates are performed even when the prediction is correct. When p = 0, the algorithm acts identically to a static oracle trainer, as the parser always follows the gold tran- sition. When the set of actions predicted by the oracle has more than one element, the best scor- ing element among them is chosen as the reference</p><formula xml:id="formula_9">function TRAINONESENTENCE(s, θ, p, o) C ← INITIAL(s) while C is not a final configuration do A ← o(C, s) set of best actionsâ actionsˆactionsâ ← argmax a f θ (C) a ifâifˆifâ ∈ A then t ← ˆ a t: target else t ← argmax a∈A f θ (C) a θ ← UPDATE(θ, C, t) backprop if RANDOM() &lt; p then C ← ˆ a(C) Follow prediction else C ← t(C)</formula><p>Follow best action return θ action to update the parameters of the neural net- work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Dynamic Oracle for Transition-Based Parsing</head><p>This section introduces a dynamic oracle algo- rithm for the parsing model presented in the pre- vious 2 sections, that is the function o used in the algorithm in <ref type="figure" target="#fig_2">Figure 5</ref>. The dynamic oracle must minimize a cost func- tion L(c; t, T ) computing the cost of applying transition t in configuration c, with respect to a gold parse T . As is shown by , the oracle's correctness depends on the cost function. A correct dynamic oracle o will have the following general formulation:</p><formula xml:id="formula_10">o(c, T ) = {t|L(c; t, T ) = min t L(c; t , T )} (1)</formula><p>The correctness of the oracle is not necessary to improve training. The oracle needs only to be good enough <ref type="bibr" target="#b8">(Daumé et al., 2009</ref>), which is confirmed by empirical results ( <ref type="bibr" target="#b28">Straka et al., 2015)</ref>.  identified arc-decomposability, a powerful property of cer- tain dependency parsing transition systems for which we can easily derive correct efficient or- acles. When this property holds, we can infer whether a tree is reachable from the reachability of individual arcs. This simplifies the calculation of each transition cost. We rely on an analogue property we call constituent decomposition. A set of constituents is tree-consistent if it is a sub- set of a set corresponding to a well-formed tree. A phrase structure transition system is constituent- decomposable iff for any configuration C and any tree-consistent set of constituents γ, if every con- stituent in γ is reachable from C, then the whole set is reachable from C (constituent reachability will be formally defined in Section 4.1).</p><p>The following subsections are structured as fol- lows. First of all, we present a cost function (Sec- tion 4.1). Then, we derive a correct dynamic ora- cle algorithm for an ideal case where we assume that there is no temporary symbols in the grammar <ref type="figure" target="#fig_1">(Section 4.2)</ref>. Finally, we present some heuris- tics to define a dynamic oracle for the general case (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cost Function</head><p>The cost function we use ignores the lexicalization of the symbols. For the sake of simplicity, we mo- mentarily leave apart the headedness of the binary reductions (until the last paragraph of Section 4) and assume a unique binary REDUCE action.</p><p>For the purpose of defining a cost func- tion for transitions, we adopt a represen- tation of trees as sets of constituents.</p><p>For example, (S (NP (D the) (N cat)) (VP (V sleeps))) corresponds to the set {(S, 0, 3), (NP, 0, 2), (VP, 2, 3)}. As is shown in <ref type="figure">Figure 2</ref>, every reduction action (unary or binary) adds a new constituent to the set γ of already predicted constituents, which was introduced in Section 2. We define the cost of a predicted set of constituentsˆγconstituentsˆ constituentsˆγ with respect to a gold set γ * as the number of constituents in γ * which are not inˆγinˆ inˆγ penalized by the number of predicted unary constituents which are not in the gold set:</p><formula xml:id="formula_11">L r (ˆ γ, γ * ) = |γ * − ˆ γ| + |{(X, i, i + 1) ∈ ˆ γ|(X, i, i + 1) / ∈ γ * }| (2)</formula><p>The first term penalizes false negatives and the second one penalizes unary false positives. The number of binary constituents in γ * andˆγandˆ andˆγ depends only on the sentence length n, thus binary false positives are implicitly taken into account by the fist term. The cost of a transition and that of a configura- tion are based on constituent reachability. The relation C C holds iff C can be deduced from C by performing a transition. Let * de- note the reflexive transitive closure of . A set of constituents γ (possibly a singleton) is reachable from a configuration C iff there is a configuration C = j, S, b, γ such that C * C and γ ⊆ γ , which we write C ; γ.</p><p>Then, the cost of an action t for a configura- tion C is the cost difference between the best tree reachable from t(C) and the best tree reachable from C:</p><formula xml:id="formula_12">L r (t; C, γ * ) = min γ:t(C);γ L(γ, γ * )− min γ:C;γ L(γ, γ * )</formula><p>This cost function is easily decomposable (as a sum of costs of transitions) whereas F1 measure is not.</p><p>By definition, for each configuration, there is at least one transition with cost 0 with respect to the gold parse. Otherwise, it would entail that there is a tree reachable from C but unreachable from t(C), for any t. Therefore, we reformulate equa- tion 1:</p><formula xml:id="formula_13">o(C, γ * ) = {t|L r (C; t, γ * ) = 0}<label>(3)</label></formula><p>In the transition system, the grammar is left im- plicit: any reduction is allowed (even if the corre- sponding grammar rule has never been seen in the training corpus). However, due to the introduction of temporary symbols during binarization, there are constraints to ensure that any derivation cor- responds to a well-formed unbinarized tree. These constraints make it difficult to test the reachability of constituents. For this reason, we instantiate two transition systems. We call SR-TMP the transition system in <ref type="figure">Figure 2</ref> which enforces the constraints in <ref type="table">Table 1</ref>, and SR-BIN, the same transition system without any of such constraints. SR-BIN assumes an idealized case where the grammar contains no temporary symbols, whereas SR-TMP is the actual system we use in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Correct Oracle for SR-BIN Transition System</head><p>SR-BIN transition system provides no guarantees that predicted trees are unbinarisable. The only condition for a binary reduction to be allowed is that the stack contains at least two symbols. If so, any non-terminal in the grammar could be used. In such a case, we can define a simple necessary and sufficient condition for constituent reachability.</p><p>Constituent reachability Let γ * be a tree- consistent constituent set, and C = j, S, b, γ a parsing configuration, such that:</p><formula xml:id="formula_14">S = (X 1 , i 0 , i 1 ) . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. (X p , i p−1 , i)|(A, i, k)|(B, k, j)</head><p>A binary constituent (X, m, n) is reachable iff it satisfies one of the three following properties :</p><p>1. (X, m, n) ∈ γ 2. j &lt; m &lt; n 3. m ∈ {i 0 , . . . i p−1 , i, k}, n ≥ j and (m, n) = (k, j)</p><p>The first two cases are trivial and correspond re- spectively to a constituent already constructed and to a constituent spanning words which are still in the buffer.</p><p>In the third case, (X, m, n) can be constructed by performing n − j times the transitions SHIFT and GHOST-REDUCE (or REDUCE-UNARY), and then a sequence of binary reductions ended by an X reduction. Note that as the index j in the config- uration is non-decreasing during a derivation, the constituents whose span end is inferior to j are not reachable if they are not already constructed. For a unary constituent, the condition for reachability is straightforward: a constituent (</p><formula xml:id="formula_15">X, i − 1, i) is reachable from configuration C = j, S, b, γ iff (X, i − 1, i) ∈ γ or i &gt; j or i = j ∧ b = .</formula><p>Constituent decomposability SR-BIN is con- stituent decomposable. In this paragraph, we give some intuition about why this holds. Reason- ing by contradiction, let's assume that every con- stituent of a tree-consistent set γ * is reachable from C = j, S|(A, i, k)|(B, k, j), b, γ and that γ * is not reachable (contraposition). This entails that at some point during a derivation, there is no possible transition which maintains reachabil- ity for all constituents of γ * . Let's assume C is in such a case. If some constituent of γ * is reach- able from C, but not from SHIFT(C), its span must have the form (m, j), where m ≤ i. If some con- stituent of γ * is reachable from C, but not from REDUCE(X)(C), for any label X, its span must have the form (k, n), where n &gt; j. If both condi- tions hold, γ * contains incompatible constituents (crossing brackets), which contradicts the assump- tion that γ * is tree-consistent.</p><p>Computing the cost of a transition The condi- tions on constituent reachability makes it easy to compute the cost of a transition t for a given con- figuration C = j, S|(A, i, k)|(B, k, j), b, γ and a gold set γ * :</p><formula xml:id="formula_16">1: function O(j, S|(A, i, k)|(B, k, j), b, γ, γ * ) 2:</formula><p>if b = then Last action was SHIFT <ref type="bibr">3:</ref> if (X, j − 1, j) ∈ γ * then <ref type="bibr">4:</ref> return {REDUCEUNARY(X)} 5:</p><formula xml:id="formula_17">else 6: return {GHOSTREDUCE} 7:</formula><p>if ∃n &gt; j, (X, k, n) ∈ γ * then 8:</p><p>return {SHIFT}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>if (X, i, j) ∈ γ * then <ref type="bibr">10:</ref> return {REDUCE(X)}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>if ∃m &lt; i, (X, m, j) ∈ γ * then <ref type="bibr">12:</ref> return {REDUCE(Y), ∀Y }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>return {a ∈ A|a is a possible action} • The cost of a SHIFT is the number of con- stituents not in γ, reachable from C and whose span ends in j.</p><p>• The cost of a binary reduction REDUCE(X) is a sum of two terms. The first one is the num- ber of constituents of γ * whose span has the form (k, n) with n &gt; j. These are no longer compatible with (X, i, j) in a tree. The sec- ond one is one if (Y, i, j) ∈ γ * and Y = X and zero otherwise. It is the cost of misla- belling a constituent with a gold span.</p><p>• The cost of a unary reduction or that of a ghost reduction can be computed straightfor- wardly by looking at the gold set of con- stituents.</p><p>We present in <ref type="figure" target="#fig_3">Figure 6</ref> an oracle algorithm derived from these observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A Heuristic-based Dynamic Oracle for SR-TMP transition system</head><p>The conditions for constituent reachability for SR- BIN do not hold any longer for SR-TMP. In par- ticular, constituent reachability depends crucially on the distinction between temporary and non- temporary symbols. The algorithm in <ref type="figure" target="#fig_3">Figure 6</ref> is not correct for this transition system. In <ref type="figure">Figure  7</ref>, we give an illustration of a prototypical case in which the algorithm in <ref type="figure" target="#fig_3">Figure 6</ref> will fail. The con- stituent (C:, i, j) is in the gold set of constituents and could be constructed with REDUCE(C:). The third symbol on the stack being temporary symbol D:, the reduction to a temporary symbol will jeop- ardize the reachability of (C, m, j) because reduc-tions are not possible when the two symbols at the top of the stack are temporary symbols. The best course of action is then a reduction to any non-temporary symbol, so as to keep (C, m, j) reachable. Note that in this case, the cost of RE- DUCE(C:) cannot be smaller than that of a single mislabelled constituent.</p><p>In fact, this example shows that the constraints inherent to SR-TMP makes it non constituent- decomposable. In the example in <ref type="figure">Figure 7</ref>, both constituents in the set {(C, m, j), (C:, i, j)}, a tree-consistent constituent set, is reachable. How- ever, the whole set is not reachable, as RE- DUCE(C:) would make (C, m, j) not reachable.</p><p>In dependency parsing, several exact dy- namic oracles have been proposed for non arc- decomposable transition systems ( <ref type="bibr" target="#b14">Goldberg et al., 2014</ref>), including systems for non-projective pars- ing ( <ref type="bibr" target="#b16">Gómez-Rodríguez et al., 2014</ref>). These ora- cles rely on tabular methods to compute the cost of transitions and have (high-degree) polynomial worst case running time. Instead, to avoid re- sorting to more computationally expensive exact methods, we adapt the algorithm in <ref type="figure" target="#fig_3">Figure 6</ref> to the constraints involving temporary symbols using the following heuristics:</p><p>• If the standard oracle predicts a reduction, make sure to choose its label so that every reachable constituent (X, m, j) ∈ γ * (m &lt; i) is still reachable after the transition. Practi- cally, if such constituent exists and if the third symbol on the stack is a temporary symbol, then do not predict a temporary symbol.</p><p>• When reductions to both temporary symbols and non-temporary symbols have cost zero, only predict temporary symbols. This should not harm training and improve precision for the unbinarized tree, as any non temporary</p><p>Configuration stack Gold tree</p><formula xml:id="formula_18">D: m,i A i,k B k,j C m,j D m,i C: i,j A i,k B k,j</formula><p>Figure 7: Problematic case. Due to the temporary symbol constraints enforced by SR-TMP, the algo- rithm in <ref type="figure" target="#fig_3">Figure 6</ref> will fail on this example.  symbol in the binarized tree corresponds to a constituent in the n-ary tree.</p><p>Head choice In some cases, namely when re- ducing two non-temporary symbols to a new con- stituent (X, i, j), the oracle must determine the head position in the reduction (REDUCE-RIGHT or REDUCE-LEFT). We used the following heuris- tic: if (X, i, j) is in the gold set, choose the same head position, otherwise, predict both RR(X) and RL(X) to keep the non-determinism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conducted parsing experiments to evaluate our proposal. We compare two experimental set- tings. In the 'static' setting, the parser is trained only on gold configurations; in the 'dynamic' set- ting, we use the dynamic oracle and the training method in <ref type="figure" target="#fig_2">Figure 5</ref> to explore non-gold configura- tions. We used both the SPMRL dataset ( <ref type="bibr" target="#b27">Seddah et al., 2013</ref>) in the 'predicted tag' scenario, and the Penn Treebank ( <ref type="bibr" target="#b18">Marcus et al., 1993)</ref>, to com- pare our proposal to existing systems. The tags and morphological attributes were predicted using Marmot ( <ref type="bibr" target="#b20">Mueller et al., 2013)</ref>, by 10-fold jack- knifing for the train and development sets. For the SPMRL dataset, the head annotation was car- ried out with the procedures described in Crabbé</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of possible values ≤ 8 ≤ 32 &gt; 32</head><p>Dimensions for embedding 4 8 16   <ref type="table" target="#tab_2">(Table 4</ref>). The hidden layer has 512 units. <ref type="bibr">4</ref> For the 'dynamic' setting, we trained every other k sentence with the dynamic oracle and the other sentences with the static oracle. This method, used by <ref type="bibr" target="#b28">Straka et al. (2015)</ref>, allows for high values of p, without slowing or preventing convergence. We used several hyperparameters combinations (see <ref type="table" target="#tab_5">Table 5</ref> of Annex A). For each language, we present the model with the combi- nation which maximizes the developement set F- score. We used Averaged Stochastic Gradient De- scent <ref type="bibr" target="#b23">(Polyak and Juditsky, 1992)</ref> to minimize the negative log likelihood of the training examples. We shuffled the sentences in the training set be- fore each iteration.</p><p>Results Results for English are shown in <ref type="table" target="#tab_1">Table  3</ref>. The use of the dynamic oracle improves F-score by 0.4 on the development set and 0.6 on the test set. The resulting parser, despite using greedy de- coding and no additional data, is quite accurate. For example, it compares well with <ref type="bibr" target="#b17">Hall et al. (2014)</ref>'s span based model and is much faster.</p><p>For the SPMRL dataset, we report results on the development sets and test sets in <ref type="table" target="#tab_3">Table 2</ref>. The metrics take punctuation and unparsed sentences into account <ref type="bibr" target="#b27">(Seddah et al., 2013)</ref>. We compare our results with the SPMRL shared task baselines ( <ref type="bibr" target="#b27">Seddah et al., 2013</ref>) and several other parsing models. The model of <ref type="bibr">Björkelund et al. (2014)</ref> obtained the best results on this dataset. It is based on a product grammar and a discriminative reranker, together with morphological features and word clusters learned on unannotated data. <ref type="bibr" target="#b10">Durrett and Klein (2015)</ref> use a neural CRF based on CKY decoding algorithm, with word embeddings pre- trained on unannotated data. <ref type="bibr" target="#b11">Fernández-González and Martins (2015)</ref> use a parsing-as-reduction ap- proach, based on a dependency parser with a la- bel set rich enough to reconstruct constituent trees from dependency trees. Finally, Crabbé (2015) uses a structured perceptron with rich features and beam-search decoding. Both Crabbé (2015) and <ref type="bibr">Björkelund et al. (2014)</ref> use MARMOT-predicted morphological tags <ref type="bibr" target="#b20">(Mueller et al., 2013)</ref>, as is done in our experiments.</p><p>Our results show that, despite using a very sim- ple greedy inference and being strictly supervised, our base model (static oracle training) is compet- itive with the best single parsers on this dataset. We hypothesize that these surprising results come both from the neural scoring model and the mor- phological attribute embeddings (especially for Basque, Hebrew, Polish and Swedish). We did not test these hypotheses systematically and leave this investigation for future work.</p><p>Furthermore, we observe that the dynamic ora- cle improves training by up to 0.6 F-score (aver- aged over all languages). The improvement de- pends on the language. For example, Swedish, Arabic, Basque and German are the languages with the most important improvement. In terms of absolute score, the parser also achieves very good results on <ref type="bibr">Korean and Basque, and even outperforms Björkelund et al. (2014)</ref>'s reranker on Ko- rean.</p><p>Combined effect of beam and dynamic ora- cle Although initially, dynamic oracle training was designed to improve parsing without rely- ing on more complex search methods <ref type="bibr" target="#b12">(Goldberg and Nivre, 2012)</ref>, we tested the combined effects of dynamic oracle training and beam search de- coding. In <ref type="table" target="#tab_3">Table 2</ref>, we provide results for beam decoding with the already trained local models in the 'dynamic' setting. The transition from greedy search to a beam of size two brings an im- provement comparable to that of the dynamic or- acle. Further increase in beam size does not seem to have any noticeable effect, except for Arabic. These results show that effects of the dynamic or- acle and beam decoding are complementary and suggest that a good tradeoff between speed and accuracy is already achieved in a greedy setting or with a very small beam size</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have described a dynamic oracle for con- stituent parsing. Experiments show that training a parser against this oracle leads to an improve- ment in accuracy over a static oracle. Together with morphological features, we obtain a greedy parser as accurate as state-of-the-art (non rerank- ing) parsers for morphologically-rich languages.</p><p>(Volume 1: Long Papers), pages 1169-1179, Bei- jing, China, July. Association for Computational Linguistics.</p><p>Yue <ref type="bibr">Zhang and Stephen Clark. 2009</ref>   <ref type="table">Table 6</ref>: These templates specify a list of ad- dresses in a configuration. The input of the neural network is the instanciation of each address by a discrete typed symbol. Each v i (Section 2) is the embedding of the i th instantiated symbol of this list. M is the set of all available morphological attributes for a given language. We use the follow- ing notations (cf <ref type="figure" target="#fig_1">Figure 4</ref>): s i is the i th item in the stack, c denotes non-terminals, top, left and right, indicate the position of an element in the subtree. Finally, w and q are respectively stack and buffer tokens.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Order-0 head markovization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Schematic representation of local elements in a configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Online training for a single annotated sentence s, using an oracle function o.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Oracle algorithm for SR-BIN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on the Penn Treebank (Mar-
cus et al., 1993).  † use clusters or word vectors 
learned on unannotated data. different architec-
ture (2.3Ghz Intel), single processor. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Size of morphological attributes embed-
dings. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on development and test corpora. Metrics are provided by evalb spmrl with 
spmrl.prm parameters (http://www.spmrl.org/spmrl2013-sharedtask.html).  † use 
clusters or word vectors learned on unannotated data.  *  Björkelund et al. (2013). 

(2015), using the alignment between dependency 
treebanks and constituent treebanks. For English, 
we used Collins' head annotation rules (Collins, 
2003). Our system is entirely supervised and 
uses no external data. Every embedding was 
initialised randomly (uniformly) in the interval 
[−0.01, 0.01]. Word embeddings have 32 dimen-
sions, tags and non-terminal embeddings have 16 
dimensions. The dimensions of the morphological 
attributes depend on the number of values they can 
have </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Transition-
based parsing of the chinese treebank using a global 
discriminative model. In Proceedings of the 11th 
International Conference on Parsing Technologies, 
IWPT '09, pages 162-171, Stroudsburg, PA, USA. 
Association for Computational Linguistics. 

Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, 
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In ACL (1), pages 434-
443. The Association for Computer Linguistics. 

A Supplementary Material 

'static' and 'dynamic' setting 
'dynamic' setting 

learning rate 
α 
iterations 
k 
p 

{0.01, 0.02} {0, 10 −6 } 
[1, 24] 
{8, 16} {0.5, 0.9} 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Hyperparameters. α is the decrease con-
stant used for the learning rate (Bottou, 2010). 

s 0 .c t s 0 .w t .tag s 0 .w t .form q 1 .tag 
s 0 .c l s 0 .w l .tag s 0 .w l .form 
q 2 .tag 
s 0 .c r s 0 .w r .tag s 0 .w r .form q 3 .tag 
s 1 .c t s 1 .w t .tag s 1 .w t .form q 4 .tag 
s 1 .c l s 1 .w l .tag s 1 .w l .form q 1 .form 
s 1 .c r s 1 .w r .tag s 1 .w r .form q 2 .form 
s 2 .c t s 2 .w t .tag s 2 .w t .form q 3 .form 
q 4 .form 
s 0 .w t .m∀m ∈ M 
q 0 .m∀m ∈ M 
s 1 .w t .m∀m ∈ M 
q 1 .m∀m ∈ M 

</table></figure>

			<note place="foot" n="2"> This transition system is similar to the extended system of Zhu et al. (2013). The main difference is the strategy used to deal with unary reductions. Our strategy ensures that derivations for a sentence all have the same number of steps, which can have an effect when using beam search. We use a GHOST-REDUCE action, whereas they use a padding strategy with an IDLE action.</note>

			<note place="foot" n="4"> We did not tune these hyperparameters for each language. Instead, we chose a set of hyperparameters which achieved a tradeoff between training time and model accuracy. The effect of the morphological features and their dimensionality are left to future work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers, along with Héctor Martínez Alonso and Olga Seminck for valuable suggestions to improve prior versions of this article.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2013. (re)ranking meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Ozlem C ¸ Etino˘ Glu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seeker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages<address><addrLine>Seattle, Washington, USA,</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="135" to="145" />
		</imprint>
	</monogr>
	<note>October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C ¸ Etino˘</forename><surname>Ozlem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Introducing the ims-wrocław-szeged-cis entry at the spmrl 2014 shared task: Reranking and morpho-syntax meet unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Fale´nskafale´nska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szántó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</title>
		<meeting>the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-Scale Machine Learning with Stochastic Gradient Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<editor>Yves Lechevallier and Gilbert Saporta</editor>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>PhysicaVerlag HD</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilingual discriminative lexicalized phrase structure parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benoit Crabbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1847" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Daniel Marcu. 2006. Searn in practice</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural crf parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parsing as reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-González</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1523" to="1533" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dynamic oracle for arc-eager dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mumbai, India, December. The COLING 2012 Organizing Committee</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="959" to="976" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A tabular method for dynamic oracles in transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Sartorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An efficient dynamic oracle for unrestricted non-projective parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Rodríguez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fernándezgonzález</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="256" to="261" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A polynomial-time dynamic oracle for non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Sartorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="917" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Less grammar, more features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Baltimore. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shift-reduce constituency parsing with dynamic programming and pos tag lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="1030" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient higher-order CRFs for morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013</title>
		<meeting>the 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">Seattle, Washington, USA, October. Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
	<note>Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR Proceedings</title>
		<editor>Geoffrey J. Gordon, David B. Dunson, and Miroslav Dudík</editor>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="627" to="635" />
			<date type="published" when="2011" />
			<publisher>JMLR.org</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A classifier-based parser with linear run-time complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology</title>
		<meeting>the Ninth International Workshop on Parsing Technology</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A best-first probabilistic shift-reduce parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Main conference poster sessions</title>
		<meeting>the COLING/ACL on Main conference poster sessions</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="691" to="698" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nskiWoli´nski, Alina Wróblewska, and Eric Villemonte de la Clergerie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iakes</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koldo</forename><forename type="middle">Gojenola</forename><surname>Galletebeitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Przepiórkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="146" to="182" />
		</imprint>
	</monogr>
	<note>October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parsing universal dependency treebanks using neural networks and search-based oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič Jr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fourteenth International Workshop on Treebanks and Linguistic Theories</title>
		<meeting>Fourteenth International Workshop on Treebanks and Linguistic Theories</meeting>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>TLT 14</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transitionbased neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
