<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sampling Informative Training Data for RNN Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Fernandez</surname></persName>
							<email>jared.fern@u.northwestern.edu, ddowney@eecs.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<postCode>60208</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Northwestern University Evanston</orgName>
								<address>
									<postCode>60208</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sampling Informative Training Data for RNN Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2018, Student Research Workshop</title>
						<meeting>ACL 2018, Student Research Workshop <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="9" to="13"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>9</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose an unsupervised importance sampling approach to selecting training data for recurrent neural network (RNN) language models. To increase the information content of the training set, our approach preferentially samples high per-plexity sentences, as determined by an easily queryable n-gram language model. We experimentally evaluate the heldout per-plexity of models trained with our various importance sampling distributions. We show that language models trained on data sampled using our proposed approach outperform models trained over randomly sampled subsets of both the Billion Word (Chelba et al., 2014) and Wikitext-103 benchmark corpora (Merity et al., 2016).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of statistical language modeling seeks to learn a joint probability distribution over se- quences of natural language words. In recent work, recurrent neural network (RNN) language models ( <ref type="bibr" target="#b13">Mikolov et al., 2010</ref>) have produced state- of-the-art perplexities in sentence-level language modeling, far below those of traditional n-gram models ( <ref type="bibr" target="#b11">Melis et al., 2017)</ref>. Models trained on large, diverse benchmark corpora such as the Bil- lion Word Corpus and Wikitext-103 have seen re- ported perplexities as low as 23.7 and 37.2, respec- tively ( <ref type="bibr" target="#b10">Kuchaiev and Ginsburg, 2017;</ref><ref type="bibr" target="#b3">Dauphin et al., 2017)</ref>.</p><p>However, building models on large corpora is limited by prohibitive computational costs, as the number of training steps scales linearly with the number of tokens in the training corpus. Sentence- level language models for these large corpora can be learned by training on a set of sentences sub- sampled from the original corpus. We seek to de- termine whether it is possible to select a set of training sentences that is significantly more infor- mative than a randomly drawn training set. We hypothesize that by training on higher informa- tion and more difficult training sentences, RNN language models can learn the language distribu- tion more accurately and produce lower perplexi- ties than models trained on similar-sized randomly sampled training sets.</p><p>We propose an unsupervised importance sam- pling technique for selecting training data for sentence-level RNN language models. We lever- age n-gram language models' rapid training and query time, which often requires just a single pass over the training data. We determine a prelimi- nary heuristic for each sentence's importance and information content by calculating its average per- word perplexity. Our technique uses an offline n- gram model to score sentences and then samples higher perplexity sentences with increased proba- bility. Selected sentences are then used for training with corrective weights to remove the sampling bias. As entropy and perplexity have a monotonic relationship, selecting sentences with higher aver- age n-gram perplexity also increases the average entropy and information content.</p><p>We experimentally evaluate the effectiveness of multiple importance sampling distributions at se- lecting training data for RNN language models. We compare the heldout perplexities of models trained with randomly sampled and importance sampled training data on both the One Billion Word and Wikitext-103 corpora. We show that our importance sampling techniques yield lower perplexities than models trained on similarly sized random samples. By using an n-gram model to de- termine the sampling distribution, we limit added computational costs of our importance sampling approach. We also find that applying perplexity- based importance sampling requires maintaining a relatively high weight on low perplexity sen- tences. We hypothesize that this is because low perplexity sentences frequently contain common subsequences that are useful in modeling other sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Standard stochastic gradient descent (SGD) iter- atively selects random examples from the train- ing set to perform gradient updates. In con- trast, weighted SGD has been proven to acceler- ate the convergence rates of SGD by leveraging importance sampling as a means of variance re- duction ( <ref type="bibr">Needell et al., 2014;</ref><ref type="bibr">Zhao and Zhang, 2015)</ref>. Weighted SGD selects examples from an importance sampling distribution and then trains on the selected examples with corrective weights. Weights of each training example i are set to be 1 P r(i) , where P r(i) is the probability of selecting example i. The weighting provides an unbiased estimator of overall loss by removing the bias of the importance sampling distribution. In expecta- tion, each example's contribution to the total loss function is the same as if the example had been drawn uniformly at random. <ref type="bibr" target="#b0">Alain et al. (2015)</ref> developed an importance sampling technique for training deep neural net- works by sampling examples directly accord- ing to their gradient norm. To avoid the high computational costs of gradient computations, <ref type="bibr" target="#b6">Katharopoulos and Fleuret (2018)</ref> sample accord- ing to losses as approximated by a lightweight RNN model trained along side their larger pri- mary RNN model. Both techniques observed in- creased convergence rates and reduced errors in image classification tasks. In comparison, we use a fixed offline n-gram model to compute our sampling distribution, which can be trained and queried much more efficiently than a neural net- work model.</p><p>In natural language processing, subsampling of large corpora has been used to speed up training for both language modeling and machine trans- lation. For domain specific language modeling, <ref type="bibr">Moore and Lewis (2010)</ref> used an n-gram model trained on in-domain data to score sentences and then selected the sentences with low perplexities for training. Both <ref type="bibr" target="#b2">Cho et al. (2014)</ref> and <ref type="bibr" target="#b9">Koehn and Haddow (2012)</ref> used similar perplexity-based sampling to select training data for domain spe- cific machine translation systems. Importance sampling has also been used to increase rate of convergence for a class of neural network lan- guage models which use a set of binary classifiers to determine sequence likelihood, rather than cal- culating the probabilities jointly ( <ref type="bibr">Xu et al., 2011</ref>).</p><p>Because these subsampling techniques are used to learn domain specific distributions different from the distribution of the original corpus, they target lower perplexity sentences and have no need for corrective weighting. In contrast, we study how training sets generated using weighted im- portance sampling can be selected to maximize knowledge of the entire corpus for the standard language modeling task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>First, we train an offline n-gram model over sen- tences randomly sampled from the training corpus. Using the n-gram model, we score perplexities for the remaining sentences in the training corpus.</p><p>We propose multiple importance sampling and likelihood weighting schemes for selecting train- ing sequences for an RNN language model. Our proposed sampling distributions (discussed in de- tail below) bias the training set to select higher perplexity sentences in order to increase the train- ing set's information content. We then train an RNN language model on the sampled sentences with weights set to the reciprocal of the sentence's selection probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Z-Score Sampling (Z f ull )</head><p>This sampling distribution naively selects sen- tences according to their z-score, as calculated in terms of their n-gram perplexities. The selection probability of sequence s is set as:</p><formula xml:id="formula_0">P Keep (s) = k pr ppl(s) − µ ppl σ ppl + 1 ,</formula><p>where ppl(s) is the n-gram perplexity of sentence s, µ ppl is the average n-gram perplexity, σ ppl is the standard deviation of n-gram perplexities, and k pr is a normalizing constant to ensure a proper probability distribution. For sentences with z-scores less than −1.00 or sequences where ppl(s) was in the 99 th percentile of n-gram perplexities, sequences are assigned P keep (s) = k pr . This ensured all sequences had positive selection probability and limited bias to- wards the selection of high perplexity sequences in the tail of the distribution. Upon examination, sequences with perplexities in the 99 th percentile were generally esoteric or nonsensical. Selection of these high perplexity sentences provided min- imal accuracy gain in exchange for their boosted selection probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Limited Z-Score Sampling (Z α )</head><p>Training on low perplexity sentences can be help- ful in learning to model higher perplexity sen- tences which share common sub-sequences. How- ever, naive z-score sampling results in the selec- tion of few low perplexity sentences. Additionally, the low perplexity sentences that are selected tend to dominate the training weight space due to their low selection probability.</p><p>To smooth the distribution in the weight space, selection probability is only determined using z- scores for sentences where their perplexities are greater than the mean. Thus, the selection proba- bility of sentence s is:</p><formula xml:id="formula_1">P Keep (s) = k pr α ppl(s)−µ ppl σ ppl + 1 , if ppl(s) &gt; µ ppl . k pr , else.</formula><p>where α is a constant parameter that determines the weight of the z-score in calculating the se- quence's selection probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Squared Z-Score Sampling (Z 2 )</head><p>To investigate the effects of sampling from more complex distributions, we also evaluate an impor- tance sampling scheme where sentences are sam- pled according to their squared z-score.</p><formula xml:id="formula_2">P Keep (s) =    k pr α ppl(s)−µ ppl σ ppl 2 + 1 , if ppl(s) &gt; µ ppl .</formula><p>k pr , else.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We experimentally evaluate the effectiveness of the Z f ull and Z 2 sampling methods, as well as the Z α method for various values of parameter α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Details</head><p>Sentence-level models were trained and evaluated on samples from Wikitext-103 and the One Bil- lion Word Benchmark corpus. To create a dataset of independent sentences, the Wikitext-103 corpus was parsed to remove headers and to create indi- vidual sentences. The training and heldout sets were combined, shuffled, and then split to cre- ate new 250k token test and validation sets. The remaining sequences were set as a new training set of approximately 99 million tokens. In Bil- lion Word experiments, training sequences were sampled from a 500 million subset of the released training split. Billion Word models were evaluated on 250k token test and validation sets randomly sampled from the released heldout split. Models were trained on 500 thousand, 1 mil- lion, and 2 million token training sets sampled from each training split. Rare words were replaced with &lt;unk&gt; tokens, resulting in vocabularies of 267K and 250K for the Wikitext and Billion Word corpora, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Details</head><p>To calculate the sampling distribution, an n-gram model was trained on a heldout set with the same number tokens used to train each RNN model <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>). For exam- ple, the sampling distribution used to build a 1 million token RNN training set was determined using perplexities calculated by an n-gram model also trained on 1 million tokens. N-gram mod- els were trained as 5-gram models with Kneser- Ney discounting <ref type="bibr" target="#b8">(Kneser and Ney, 1995</ref>) using SRILM <ref type="bibr">(Stolcke, 2002</ref>). For efficient calculation of sentence perplexities, we query our models us- ing <ref type="bibr">KenLM (Heafield, 2011)</ref>.</p><p>RNN models were built using a two-layer long short-term memory (LSTM) neural network, with 200-dimensional hidden and embedding layers. Each training set was trained on for 10 epochs us- ing the Adam gradient optimizer <ref type="bibr" target="#b7">(Kingma and Ba, 2014</ref>) with a mini-batch size of 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>In <ref type="table">Tables 1 and 2</ref>, we summarize the performances of models trained on samples from Wikitext-103 and the Billion Word Corpus, respectively. We re- port the Random and n-gram baseline perplexities for RNN and n-gram language models trained on randomly sampled data. We also report µ ngram and σ ngram for each training set, which are the mean and standard deviation in sentence perplex- ity as evaluated by the offline n-gram model.</p><p>In all experiments, RNN language models trained using our sampling approaches yield a de- crease in model perplexity as compared to RNN models trained on similar sized randomly sam- pled sets. As size of the training set increases, the RNNs trained on importance sampling datasets  <ref type="table">Table 1</ref>: Perplexities for Wikitext models. All pro- posed models outperform the random and n-gram baselines as number of training tokens increases.</p><p>also yield significantly lower perplexities than the n-gram models trained on randomly sampled training sets. As expected, µ ngram and σ ngram in- crease substantially for training sets generated us- ing our proposed sampling methods. Overall, the Z 4.0 sampling method produced the most consistent reductions in average perplex- ity of 102.9 and 54.2 compared to the Random and n-gram baselines, respectively. Z F ull and Z 2 exhibit higher variance in their heldout perplex- ity as compared to the Z α and baseline methods. We expect that this is because these methods se- lect higher perplexity sequences with significantly higher probability than Z α methods. As a result, low perplexity sentences, which may contain com- mon subsequences helpful in modeling other sen- tences, are ignored in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We introduce a weighted importance sampling scheme for selecting RNN language model train- ing data from large corpora. We demonstrate that models trained with data generated using this ap- proach yield perplexity reductions of up to 24% when compared to models trained over randomly sampled training sets of similar size. This tech- nique leverages higher perplexity training sen-  <ref type="table">Table 2</ref>: Perplexities for Billion Word models. Z α and Z 2 both outperform the random baseline and are comparable to the n-gram baseline.</p><p>tences to learn more accurate language models, while limiting added computational cost of impor- tance calculations.</p><p>In future work, we will examine the perfor- mance of our proposed selection techniques in ad- ditional parameter settings, with various values of α and thresholds in the limited z-score methods Z α . We will evaluate the performance of sam- pling distributions based on perplexities calculated using small, lightweight RNN language models rather than n-gram language models. Addition- ally, we will also be evaluating the performance of sampling distributions calculated based on a sen- tence's subsequences and unique n-gram content. Furthermore, we plan on adapting this importance sampling approach to use online n-gram models trained alongside the RNN language models for determining the importance sampling distribution.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by NSF Grant IIS-1351029. Support for travel provided in part by the ACL Student Travel Grant (NSF Grant IIS-1827830) and the Conference Travel Grant from Northwestern University's Office of the Provost. We thank Dave Demeter, Thanapon Noraset, Yiben Yang, and Zheng Yuan for helpful comments.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06481</idno>
		<title level="m">Variance reduction in sgd by distributed importance sampling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Not all samples are created equal: Deep learning with importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00942</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note>International Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards effective use of training data in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="317" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Factorization tricks for lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10722</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05589</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">JaňJaň Cernock`Cernock`y, and Sanjeev Khudanpur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Recurrent neural network based language model</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
