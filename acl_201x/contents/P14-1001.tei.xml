<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Ensembles of Structured Prediction Rules</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
							<email>corinna@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>111 8th Avenue</addrLine>
									<postCode>10011</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Kuznetsov</surname></persName>
							<email>vitaly@cims.nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Courant Institute</orgName>
								<address>
									<addrLine>251 Mercer Street</addrLine>
									<postCode>10012</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
							<email>mohri@cims.nyu.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Courant Institute and Google Research</orgName>
								<address>
									<addrLine>251 Mercer Street</addrLine>
									<postCode>10012</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Ensembles of Structured Prediction Rules</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1" to="12"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a series of algorithms with theoretical guarantees for learning accurate ensembles of several structured prediction rules for which no prior knowledge is assumed. This includes a number of ran-domized and deterministic algorithms devised by converting on-line learning algorithms to batch ones, and a boosting-style algorithm applicable in the context of structured prediction with a large number of labels. We also report the results of extensive experiments with these algorithms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We study the problem of learning accurate en- sembles of structured prediction experts. Ensem- ble methods are widely used in machine learn- ing and have been shown to be often very effec- tive <ref type="bibr" target="#b0">(Breiman, 1996;</ref><ref type="bibr" target="#b29">Freund and Schapire, 1997;</ref><ref type="bibr" target="#b30">Smyth and Wolpert, 1999;</ref><ref type="bibr" target="#b17">MacKay, 1991;</ref><ref type="bibr" target="#b10">Freund et al., 2004</ref>). However, ensemble methods and their theory have been developed primarily for bi- nary classification or regression tasks. Their tech- niques do not readily apply to structured predic- tion problems. While it is straightforward to com- bine scalar outputs for a classification or regres- sion problem, it is less clear how to combine struc- tured predictions such as phonemic pronuncia- tion hypotheses, speech recognition lattices, parse trees, or alternative machine translations.</p><p>Consider for example the problem of devising an ensemble method for pronunciation, a crit- ical component of modern speech recognition ( <ref type="bibr" target="#b11">Ghoshal et al., 2009</ref>). Often, several pronunci- ation models or experts are available for transcrib- ing words into sequences of phonemes. These models may have been derived using other ma- chine learning algorithms or they may be based on carefully hand-crafted rules. In general, none of these pronunciation experts is fully accurate and each expert may be making mistakes at different positions along the output sequence. One can hope that a model that patches together the pronuncia- tion of different experts could achieve a superior performance.</p><p>Similar ensemble structured prediction problems arise in other tasks, including machine translation, part-of-speech tagging, optical character recogni- tion and computer vision, with structures or sub- structures varying with each task. We seek to tackle all of these problems simultaneously and consider the general setting where the label or out- put associated to an input x ∈ X is a structure y ∈ Y that can be decomposed and represented by l substructures y 1 , . . . , y l . For the pronuncia- tion example just discussed, x is a specific word or word sequence and y its phonemic transcrip- tion. A natural choice for the substructures y k is then the individual phonemes forming y. Other possible choices include n-grams of consecutive phonemes or more general subsequences.</p><p>We will assume that the loss function considered admits an additive decomposition over the sub- structures, as is common in structured prediction. We also assume access to a set of structured pre- diction experts h 1 , . . . , h p that we treat as black boxes. Given an input x ∈ X , each expert pre- dicts a structure h j (x) = (h 1 j (x), . . . , h l j (x)). The hypotheses h j may be the output of a structured prediction algorithm such as Conditional Random Fields ( <ref type="bibr">Lafferty et al., 2001</ref>), Averaged Perceptron <ref type="bibr" target="#b4">(Collins, 2002</ref>), <ref type="bibr">StructSVM (Tsochantaridis et al., 2005</ref>), Max Margin Markov Networks ( <ref type="bibr" target="#b31">Taskar et al., 2004</ref>) or the Regression Technique for <ref type="bibr">Learning Transductions (Cortes et al., 2005</ref>), or some other algorithmic or human expert. Given a la- beled training sample (x 1 , y 1 ), . . . , (x m , y m ), our objective is to use the predictions of these experts 1 to form an accurate ensemble.</p><p>Most of the references just mentioned do not give a rigorous theoretical justification for the techniques proposed. We are not aware of any prior theoret- ical analysis for the ensemble structured predic- tion problem we consider. Here, we present two families of algorithms for learning ensembles of structured prediction rules that both perform well in practice and enjoy strong theoretical guaran- tees. In Section 3, we develop ensemble methods based on on-line algorithms. To do so, we extend existing on-line-to-batch conversions to our more general setting. In Section 4, we present a new boosting-style algorithm which is applicable even with a large set of classes as in the problem we consider, and for which we present margin-based learning guarantees. Section 5 reports the results of our extensive experiments. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning scenario</head><p>As in standard supervised learning problems, we assume that the learner receives a training sample S = ((x 1 , y 1 ), . . . , (x m , y m )) ∈ X × Y of m labeled points drawn i.i.d. according to the some distribution D used both for training and testing. We also assume that the learner has access to a set of p predictors h 1 , . . . , h p mapping X to Y to de- vise an accurate ensemble prediction. Thus, for any input x ∈ X , he can use the prediction of the p experts h 1 (x), . . . , h p (x). No other infor- mation is available to the learner about these p ex- perts, in particular the way they have been trained or derived is not known to the learner. But, we will assume that the training sample S is distinct from what may have been used for training the al- gorithms that generated h 1 (x), . . . , h p (x).</p><p>To simplify our analysis, we assume that the num- ber of substructures l ≥ 1 is fixed. This does not cause any loss of generality so long as the maxi- mum number of substructures is bounded, which is the case in all the applications we consider. The quality of the predictions is measured by a loss function L : Y × Y → R + that can be de- composed as a sum of loss functions k :</p><formula xml:id="formula_0">Y k → R + over the substructure sets Y k , that is, for all y = (y 1 , . . . , y l ) ∈ Y with y k ∈ Y k and y = (y 1 , . . . , y l ) ∈ Y with y k ∈ Y k , L(y, y ) = l k=1 k (y k , y k ).<label>(1)</label></formula><p>We will assume in all that follows that the loss function L is bounded: L(y, y ) ≤ M for all (y, y ) for some M &gt; 0. A prototypical example of such loss functions is the normalized Hamming loss L Ham , which is the fraction of substructures for which two labels y and y disagree, thus in that case k (y k , y k ) = 1 l I y k =y k and M = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">On-line learning approach</head><p>In this section, we present an on-line learning so- lution to the ensemble structured prediction prob- lem just discussed. We first give a new formula- tion of the problem as that of on-line learning with expert advice, where the experts correspond to the paths of an acyclic automaton. The on-line algo- rithm generates at each iteration a distribution over the path-experts. A critical component of our ap- proach consists of using these distributions to de- fine a prediction algorithm with favorable gener- alization guarantees. This requires an extension of the existing on-line-to-batch conversion tech- niques to the more general case of combining dis- tributions over path-experts, as opposed to com- bining single hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Path experts</head><p>Each expert h j induces a set of substructure hy- potheses h 1 j , . . . , h l j . As already discussed, one particular expert may be better at predicting the kth substructure while some other expert may be more accurate at predicting another substructure. Therefore, it is desirable to combine the substruc- ture predictions of all experts to derive a more ac- curate prediction. This leads us to considering an acyclic finite automaton G such as that of <ref type="figure" target="#fig_0">Figure 1</ref> which admits all possible sequences of substruc- ture hypotheses, or, more generally, a finite au- tomaton such as that of <ref type="figure" target="#fig_1">Figure 2</ref> which only allows a subset of these sequences.</p><p>An automaton such as G compactly represents a set of path experts: each path from the initial vertex 0 to the final vertex l is labeled with a sequence of substructure hypotheses h 1</p><formula xml:id="formula_1">j 1 , . . . , h l j l</formula><p>and defines a hypothesis which associates to input x the output h 1</p><formula xml:id="formula_2">j 1 (x) · · · h l j l (x).</formula><p>We will denote by H the set of all path experts. We also denote by h each path expert defined by h 1</p><formula xml:id="formula_3">j 1 , . . . , h l j l</formula><p>, with j k ∈ {1, . . . , p}, and denote by h k its kth sub- structure hypothesis h k j k . Our ensemble structure prediction problem can then be formulated as that of selecting the best path expert (or collection of path experts) in G. Note that, in general, the path expert selected does not coincide with any of the original experts h 1 , . . . , h p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">On-line algorithm</head><p>Using an automaton G, the size of the pool of ex- perts H we consider can be very large. For ex- ample, in the case of the automaton of <ref type="figure" target="#fig_0">Figure 1</ref>, the size of the pool of experts is p l , and thus is exponentially large with respect to p. But, since learning guarantees in on-line learning admit only a logarithmic dependence on that size, they re- main informative in this context. Nevertheless, the computational complexity of most on-line al- gorithms also directly depends on that size, which could make them impractical in this context. But, there exist several on-line solutions precisely de- signed to address this issue by exploiting the struc- ture of the experts as in the case of our path ex- perts. These include the algorithm of (Takimoto and Warmuth, 2003) denoted by WMWP, which is an extension of the (randomized) weighted- majority (WM) algorithm of ( <ref type="bibr" target="#b15">Littlestone and Warmuth, 1994</ref>) to more general bounded loss func- tions combined with the Weight Pushing (WP) al- gorithm of (Mohri, 1997); and the Follow the Per- turbed Leader (FPL) algorithm of <ref type="bibr" target="#b13">(Kalai and Vempala, 2005</ref>). The WMWP algorithm admits a more favorable regret guarantee than the FPL algorithm in our context and our discussion will focus on the use of WMWP for the design of our batch al- gorithm. However, we have also fully analyzed and implemented a batch algorithm based on FPL ( <ref type="bibr" target="#b6">Cortes et al., 2014a</ref>).</p><p>As in the standard WM algorithm ( <ref type="bibr" target="#b15">Littlestone and Warmuth, 1994)</ref>, WMWP maintains at each round t ∈ <ref type="bibr">[1, T ]</ref>, a distribution p t over the set of all ex- perts, which in this context are the path experts h ∈ H. At each round t ∈ <ref type="bibr">[1, T ]</ref>, the algo- rithm receives an input sequence x t , incurs the loss</p><formula xml:id="formula_4">E h∼pt [L(h(x t ), y t )] = h p t (h)L(h(x t )</formula><p>, y t ) and multiplicatively updates the distribution weight per expert: where β ∈ (0, 1) is some fixed parameter. The number of paths is exponentially large in p and the cost of updating all paths is therefore prohibitive. However, since the loss function is additive in the substructures and the updates are multiplicative, it suffices to maintain instead a weight w t (e) per transition e, following the update</p><formula xml:id="formula_5">∀h ∈ H, p t+1 (h) = p t (h)β L(h(xt),yt) h ∈H p t (h )β L(h (xt),yt) , (2)</formula><formula xml:id="formula_6">w t+1 (e) = w t (e)β e(xt,yt) orig(e )=orig(e) w t (e )β e (xt,yt) (3)</formula><p>where e (x t , y t ) denotes the loss incurred by the substructure predictor labeling e for the input x t and output y t , and orig(e ) denotes the origin state of a transition e ( <ref type="bibr">Takimoto and Warmuth, 2003)</ref>. Thus, the cost of the update is then linear in the size of the automaton. To use the result- ing weighted automaton for sampling, the weight pushing algorithm is used, whose complexity is also linear in the size of the automaton (Mohri, 1997).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">On-line-to-batch conversion</head><p>The WMWP algorithm does not produce a se- quence of path experts, rather, a sequence of dis- tributions p 1 , . . . , p T over path experts. Thus, the on-line-to-batch conversion techniques described in <ref type="bibr" target="#b16">(Littlestone, 1989;</ref><ref type="bibr" target="#b2">Cesa-Bianchi et al., 2004;</ref><ref type="bibr" target="#b8">Dekel and Singer, 2005</ref>) do not readily apply. In- stead, we propose a generalization of the tech- niques of <ref type="bibr" target="#b8">(Dekel and Singer, 2005</ref>). The conver- sion consists of two steps: extract a good collec- tion of distributions P ⊆ {p 1 , . . . , p T }; next use P to define an accurate hypothesis for prediction. For a subset P ⊆ {p 1 , . . . , p T }, we define</p><formula xml:id="formula_7">Γ(P) = 1 |P| pt∈P h∈H p t (h)L(h(x t ), y t )+M log 1 δ |P| = 1 |P| pt∈P e w t (e) e (x t ), y t )+M log 1 δ |P| ,</formula><p>where δ &gt; 0 is a fixed parameter. With this defini- tion, we choose P δ as a minimizer of Γ(P) over some collection P of subsets of {p 1 , . . . , p T }: P δ ∈ argmin P∈P Γ(P). The choice of P is re- stricted by computational considerations. One nat- ural option is to let P be the union of the suf- fix sets {p t , . . . , p T }, t = 1, . . . , T . We will assume in what follows that P includes the set {p 1 , . . . , p T }.</p><p>Next, we define a randomized algorithm based on P δ . Given an input x, the algorithm consists of randomly selecting a path h according to</p><formula xml:id="formula_8">p(h) = 1 |P δ | pt∈P δ p t (h),<label>(4)</label></formula><p>and returning the prediction h(x). Note that com- puting and storing p directly is not efficient. To sample from p, we first choose p t ∈ P δ uniformly at random and then sample a path h according to that p t . Sampling a path according to p t can be done efficiently using the weight pushing algo- rithm. Note that once an input x is received, the distribution p over the path experts h induces a probability distribution p x over the output space Y. It is not hard to see that sampling a predic- tion y according to p x is statistically equivalent to first sampling h according to p and then predicting h(x). We will denote by H Rand the randomized hypothesis thereby generated.</p><p>An inherent drawback of randomized solutions such as the one just described is that for the same input x the user can receive different predictions over time. Randomized solutions are also typi- cally more costly to store. A collection of distri- butions P can also be used to define a determin- istic prediction rule based on the scoring function approach. The majority vote scoring function is defined by</p><formula xml:id="formula_9">h MVote (x, y) = l k=1 1 |P δ | pt∈P δ p j=1 w t,kj 1 h k j (x)=y k .<label>(5)</label></formula><p>The majority vote algorithm denoted by H MVote is then defined for all x ∈ X , by H MVote (x) = argmax y∈Y h MVote (x, y). For an expert automa- ton accepting all path experts such as that of <ref type="figure" target="#fig_0">Fig- ure 1</ref>, the maximizer of h MVote can be found very efficiently by choosing y such that y k has the max- imum weight in position k.</p><p>In the next section, we present learning guarantees for H Rand and H MVote . For a more extensive dis-cussion of alternative prediction rules, see (Cortes et al., 2014a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Batch learning guarantees</head><p>We first present learning bounds for the random- ized prediction rule H Rand . Next, we upper bound the generalization error of H MVote in terms of that of H Rand . Theorem 1. For any δ &gt; 0, with probabil- ity at least 1 − δ over the choice of the sample</p><formula xml:id="formula_10">((x 1 , y 1 ), . . . , (x T , y T )) drawn i.i.d. according to D, the following inequalities hold: E[L(H Rand (x), y)] ≤ inf h∈H E[L(h(x), y)] + 2M l log p T + 2M log 2 δ T .</formula><p>For the normalized Hamming loss L Ham , the bound of Theorem 1 holds with M = 1.</p><p>We now upper bound the generalization error of the majority-vote algorithm H MVote in terms of that of the randomized algorithm H Rand , which, combined with Theorem 1, immediately yields generalization bounds for the majority-vote algo- rithm H MVote . Proposition 2. The following inequality relates the generalization error of the majority-vote algo- rithm to that of the randomized one:</p><formula xml:id="formula_11">E[L Ham (H MVote (x),y)] ≤ 2 E[L Ham (H Rand (x),y)],</formula><p>where the expectations are taken over (x, y) ∼ D and h ∼ p.</p><p>Proposition 2 suggests that the price to pay for derandomization is a factor of 2. More refined and more favorable guarantees can be proven for the majority-vote algorithm (Cortes et al., 2014a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Boosting-style algorithm</head><p>In this section, we devise a boosting-style al- gorithm for our ensemble structured prediction problem. The variants of AdaBoost for multi- class classification such as AdaBoost.MH or Ad- aBoost.MR <ref type="bibr" target="#b29">(Freund and Schapire, 1997;</ref><ref type="bibr" target="#b27">Schapire and Singer, 1999;</ref><ref type="bibr" target="#b28">Schapire and Singer, 2000</ref>) can- not be readily applied in this context. First, the number of classes to consider here is quite large, as in all structured prediction problems, since it is exponential in the number of substructures l. For example, in the case of the pronunciation prob- lem where the number of phonemes for English is in the order of 50, the number of classes is 50 l . But, the objective function for AdaBoost.MH or AdaBoost.MR as well as the main steps of the al- gorithms include a sum over all possible labels, whose computational cost in this context would be prohibitive. Second, the loss function we consider is the normalized Hamming loss over the substruc- tures predictions, which does not match the multi- class losses for the variants of AdaBoost. <ref type="bibr">2</ref> Finally, the natural base hypotheses for this problem admit a structure that can be exploited to devise a more efficient solution, which of course was not part of the original considerations for the design of these variants of AdaBoost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hypothesis sets</head><p>The predictor H Boost returned by our boosting al- gorithm is based on a scoring function h : X × Y → R, which, as for standard ensemble algo- rithms such as AdaBoost, is a convex combination of base scoring functions h t : h = T t=1 α t h t , with α t ≥ 0. The base scoring functions used in our al- gorithm have the form</p><formula xml:id="formula_12">∀(x, y) ∈ X × Y, h t (x, y) = l k=1 h k t (x, y).</formula><p>In particular, these can be derived from the path experts in H by letting h k t (x, y) = 1 h k t (x)=y k . Thus, the score assigned to y by the base scoring function h t is the number of positions at which y matches the prediction of path expert h t given in- put x. H Boost is defined as follows in terms of h or h t s:</p><formula xml:id="formula_13">∀x ∈ X , H Boost (x) = argmax y∈Y h(x, y)</formula><p>We remark that the analysis and algorithm pre- sented in this section are also applicable with a scoring function that is the product of the scores at each substructure k as opposed to a sum, that is,</p><formula xml:id="formula_14">h(x, y) = l k=1 T t=1 α t h k t (x, y)</formula><p>. This can be used for example in the case where the experts are derived from probabilistic mod- els.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ESPBoost algorithm</head><p>To simplify our exposition, the algorithm that we now present uses base learners of the form h k t (x, y) = 1 h k t (x)=y k . The general case can be handled in the same fashion with the only dif- ference being the definition of the direction and step of the optimization procedure described be- low. For any i ∈ <ref type="bibr">[1, m]</ref> and k ∈ <ref type="bibr">[1, l]</ref>, we define the margin of h k for point (x i , y i ) by</p><formula xml:id="formula_15">ρ( h k , x i , y i ) = h k (x i , y k i )−max y k =y k i h k (x i , y k ).</formula><p>We first derive an upper bound on the empirical normalized Hamming loss of a hypothesis H Boost , with h = T t=1 α t h t . Lemma 3. The following upper bound holds for the empirical normalized Hamming loss of the hy- pothesis H Boost :</p><formula xml:id="formula_16">E (x,y)∼S [L Ham (H Boost (x), y)] ≤ 1 ml m i=1 l k=1 exp − T t=1 α t ρ( h k t , x i , y i ) .</formula><p>The proof of this lemma as well as that of sev- eral other theorems related to this algorithm can be found in ( <ref type="bibr" target="#b6">Cortes et al., 2014a</ref>).</p><p>In view of this upper bound, we consider the ob- jective function F :</p><formula xml:id="formula_17">R N → R defined for all α = (α 1 , . . . , α N ) ∈ R N by F (α) = 1 ml m i=1 l k=1 exp − N j=1 α j ρ( h k j , x i , y i ) ,</formula><p>where h 1 , . . . , h N denote the set of all path ex- perts in H. F is a convex and differentiable func- tion of α. Our algorithm, ESPBoost (Ensemble Structured Prediction Boosting), is defined by the application of coordinate descent to the objective F . Algorithm 1 shows the pseudocode of the ESP- Boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 ESPBoost Algorithm</head><p>Inputs: S = ((x 1 , y 1 ), . . . , (x m , y m )); set of experts {h 1 , . . . , h p } for i = 1 to m and k = 1 to l do</p><formula xml:id="formula_18">D 1 (i, k) ← 1 ml end for for t = 1 to T do h t ← argmin h∈H E (i,k)∼Dt [1 h k (x i ) =y k i ] t ← E (i,k)∼Dt [1 h k t (x i ) =y k i ] α t ← 1 2 log 1−t t Z t ← 2 t (1 − t ) for i = 1 to m and k = 1 to l do D t+1 (i, k) ← exp(−αtρ( e h k t ,x i ,y i ))Dt(i,k) Zt end for end for Return h = T t=1 α t h t</formula><p>Let α t−1 ∈ R N denote the vector obtained after t − 1 iterations and e t the tth unit vector in R N .</p><p>We denote by D t the distribution over <ref type="bibr">[1, m]</ref></p><formula xml:id="formula_19">×[1, l] defined by D t (i, k) = 1 ml exp − t−1 u=1 α u ρ( h k u , x i , y i ) A t−1</formula><p>where A t−1 is a normalization factor,</p><formula xml:id="formula_20">A t−1 = 1 ml m i=1 l k=1 exp − t−1 u=1 α u ρ( h k u , x i , y i ) .</formula><p>The direction e t selected at the tth round is the one minimizing the directional derivative, that is</p><formula xml:id="formula_21">dF (α t−1 + ηe t ) dη η=0 = − m i=1 l k=1 ρ( h k t , x i , y i )D t (i, k)A t−1 = 2 i,k:h k t (x i ) =y k i D t (i, k) − 1 A t−1 = (2 t − 1)A t−1 ,</formula><p>where t is the average error of h t given by</p><formula xml:id="formula_22">t = m i=1 l k=1 D t (i, k)1 h k t (x i ) =y k i = E (i,k)∼Dt [1 h k t (x i ) =y k i ].</formula><p>The remaining steps of our algorithm can be de- termined as in the case of AdaBoost. In particu- lar, given the direction e t , the best step α t is ob- tained by solving the equation dF (α t−1 +αtet) dαt = 0, which admits the closed-form solution α t = 1 2 log 1−t t . The distribution D t+1 can be ex- pressed in terms of D t with the normalization fac- tor Z t = 2 t (1 − t ).</p><p>Our weak learning assumption in this context is that there exists γ &gt; 0 such that at each round, t verifies t &lt; 1 2 − γ. Note that, at each round, the path expert h t with the smallest error t can be determined easily and efficiently by first finding for each substructure k, the h k t that is the best with respect to the distribution weights D t (i, k).</p><p>Observe that, while the steps of our algorithm are syntactically close to those of AdaBoost and its multi-class variants, our algorithm is distinct and does not require sums over the exponential number of all possible labelings of the substructures and is quite efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning guarantees</head><p>We have derived both a margin-based generaliza- tion bound in support of the ESPBoost algorithm and a bound on the empirical margin loss.</p><p>For any ρ &gt; 0, define the empirical margin loss of H Boost by the following:</p><formula xml:id="formula_23">R ρ h α 1 = 1 ml m i=1 l k=1 1 ρ( e h k ,x i ,y i )≤ρα 1 ,</formula><p>where h is the corresponding scoring function. The following theorem can be proven using the multi-class classification bounds of ( <ref type="bibr">Koltchinskii and Panchenko, 2002;</ref><ref type="bibr" target="#b20">Mohri et al., 2012</ref>) as can be shown in <ref type="bibr" target="#b6">(Cortes et al., 2014a</ref>). Theorem 4. Let F denote the set of func- tions H Boost with h = T t=1 α t h t for some α 1 , . . . , α t ≥ 0 and h t ∈ H for all t ∈ <ref type="bibr">[1, T ]</ref>. Fix ρ &gt; 0. Then, for any δ &gt; 0, with probability at least 1−δ, the following holds for all H Boost ∈ F:</p><formula xml:id="formula_24">E (x,y)∼D [L Ham (H Boost (x), y)] ≤ R ρ h α 1 + 2 ρl l k=1 |Y k | 2 R m (H k ) + log l δ 2m ,</formula><p>where R m (H k ) denotes the Rademacher com- plexity of the class of functions <ref type="table">Table 1</ref>: Average Normalized Hamming Loss, ADS1 and ADS2. β ADS1 = 0.95, β ADS2 = 0.95, T SLE = 100, δ = 0.05. This theorem provides a margin-based guarantee for convex ensembles such as those returned by ESPBoost. The following theorem further pro- vides an upper bound on the empirical margin loss for ESPBoost. Theorem 5. Let h denote the scoring function re- turned by ESPBoost after T ≥ 1 rounds. Then, for any ρ &gt; 0, the following inequality holds:</p><formula xml:id="formula_25">H k = {x → h k t : j ∈ [1, p], y ∈ Y k }.</formula><formula xml:id="formula_26">R ρ h α 1 ≤ 2 T T t=1 1−ρ t (1 − t ) 1+ρ .</formula><p>As in the case of AdaBoost ( <ref type="bibr" target="#b29">Schapire et al., 1997)</ref>, it can be shown that for ρ &lt; γ, 1−ρ</p><formula xml:id="formula_27">t (1 − t ) 1+ρ ≤ (1 − 2γ) 1−ρ (1 + 2γ) 1+ρ &lt; 1</formula><p>and the right-hand side of this bound decreases exponentially with T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We used a number of artificial and real-world data sets for our experiments. For each data set, we performed 10-fold cross-validation with dis- joint training sets. <ref type="bibr">3</ref> We report the average error for each task. In addition to the H MVote , H Rand and H ESPBoost hypotheses, we experimented with two algorithms discussed in more detail in <ref type="bibr" target="#b6">(Cortes et al., 2014a</ref>): a cross-validation on-line-to- batch conversion of the WMWP algorithm, H CV , a majority-vote on-line-to-batch conversion with FPL, H FPL , and a cross-validation on-line-to- batch conversion with FPL, H FPL-CV . Finally, we compare with the H SLE algorithm of <ref type="bibr" target="#b22">(Nguyen and Guo, 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Artificial data sets</head><p>Our artificial data set, ADS1 and ADS2 simulate the scenarios described in Section 1. In ADS1 the kth expert has a high accuracy on the kth position, in ADS2 an expert has low accuracy in a fixed set of positions.</p><p>For the first artificial data set, ADS1, we used lo- cal experts h 1 , . . . , h p with p = 5. To generate the data we chose an arbitrary Markov chain over the English alphabet and sampled 40,000 random sequences each consisting of 10 symbols. Each of the five experts was designed to have a certain probability of making a mistake at each position in the sequence. Expert h j correctly predicted posi- tions 2j − 1 and 2j with probability 0.97 and other positions with probability 0.5. We forced experts to make similar mistakes by making them select an adjacent alphabet symbol in case of an error. For example, when a mistake was made on a symbol b, the expert prediction was forced to be either a or c. The second artificial data set, ADS2, modeled the case of rather poor experts. ADS2 was generated in the same way as ADS1, but the expert predic- tions were different. This time each expert made mistakes at four out of the ten distinct random po- sitions in each sequence. <ref type="table">Table 1</ref> reports the results of our experiments. For all experiments with the algorithms H Rand , H MVote , and H CV , we ran the WMWP algorithm for T = m rounds with the β values listed in the caption of <ref type="table">Table 1</ref>, generating distributions P ⊆ {p 1 , . . . , p T }. For P we used the collection of all suffix sets {p t , . . . , p T } and δ = 0.05. For the algorithms based on FPL, we used = 0.5/pl. The same parameter choices were used for the subsequent experiments.</p><p>As can be seen from <ref type="table">Table 1</ref>, in both cases, H MVote , our majority-vote algorithm based on our on-line-to-batch conversion using the WMWP al- gorithm (together with most of the other on-line based algorithms), yields a significant improve- ment over the best expert. It also outperforms H SLE , which in the case of ADS1 even fails to outperform the best h j . After 100 iterations on ADS1, the ensemble learned by H SLE consists of a single expert, which is why it leads to such a poor performance.</p><p>It is also worth pointing out that H FPL-CV and H Rand fail to outperform the best model on ADS2 set. This is in total agreement with our theoreti- cal analysis since, in this case, any path expert has exactly the same performance and the error of the   best path expert is an asymptotic upper bound on the errors of these algorithms. The superior perfor- mance of the majority-vote-based algorithms sug- gests that these algorithms may have an advantage over other prediction rules beyond what is sug- gested by our learning bounds.</p><p>We also synthesized a third data set, ADS3. Here, we simulated the case where each expert special- ized in predicting some subset of the labels. In particular, we generated 40,000 random sequences over the English alphabet in the same way as for ADS1 and ADS2. To generate expert predictions, we partitioned the alphabet into 5 disjoint subsets A j . Expert j always correctly predicted the label in A j and the probability of correctly predicting the label not in A j was set to 0.7. To train the en- semble algorithms, we used a training set of size m = 200.</p><p>The results are presented in <ref type="table" target="#tab_0">Table 2</ref>. H MVote , H CV and H ESPBoost achieve the best performance on this data set with a considerable improvement in accuracy over the best expert h j . We also ob- serve as for the ADS2 experiment that H Rand and H FPL-CV fail to outperform the best model and ap- proach the accuracy of the best path expert only asymptotically. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pronunciation data sets</head><p>We had access to two proprietary pronunciation data sets, PDS1 and PDS2. In both sets, each example is an English word, typically a proper name. For each word, 20 possible phonemic se- quences are available, ranked by some pronuncia- tion model. Since the true pronunciation was not available, we set the top sequence to be the tar- get label and used the remaining as the predictions made by the experts. The only difference between PDS1 and PDS2 is their size: 1,313 words for PDS1 and 6,354 for PDS2.</p><p>In both cases, on-line based algorithms, specif- ically H MVote , significantly outperform the best model as well as H SLE , see <ref type="table">Table 3</ref>. The poor performance of H ESPBoost is due to the fact that the weak learning assumption is violated after 5-8 iter- ations and hence the algorithm terminates.</p><p>It can be argued that for this task the edit-distance is a more suitable measure of performance than the average Hamming loss. Thus, we also re- port the results of our experiments in terms of the edit-distance in <ref type="table" target="#tab_2">Table 4</ref>. Remarkably, our on-line based algorithms achieve a comparable improve- ment over the performance of the best model in the case of edit-distance as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">OCR data set</head><p>Rob Kassel's OCR data set is available for down- load from http://ai.stanford.edu/˜btaskar/ ocr/. It contains 6,877 word instances with a to- tal of 52,152 characters. Each character is rep- resented by 16 × 8 = 128 binary pixels. The task is to predict a word given its sequence of pixel vectors. To generate experts, we used several software packages: CRFsuite <ref type="bibr" target="#b23">(Okazaki, 2007)</ref> and SVM struct , SVM multiclass (Joachims, 2008), and <ref type="table">Table 5</ref>: Average Normalized Hamming Loss, TR1 and TR2. β T R1 = 0.95, β T R2 = 0.98, T SLE = 100, δ = 0.05.</p><p>TR1, m = 800 TR2, m = 1000 HMVote 0.0850 ± 0.00096 0.0746 ± 0.00014 HFPL 0.0859 ± 0.00110 0.0769 ± 0.00218 HCV 0.0843 ± 0.00006 0.0741 ± 0.00011 HFPL-CV 0.1093 ± 0.00129 0.1550 ± 0.00182 HESPBoost 0.1041 ± 0.00056 0.1414 ± 0.00233 HSLE 0.0778 ± 0.00934 0.0814 ± 0.02558 HRand 0.1128 ± 0.00048 0.1652 ± 0.00077 Best hj 0.1032 ± 0.00007 0.1415 ± 0.00005 the Stanford Classifier ( <ref type="bibr" target="#b25">Rafferty et al., 2014</ref>). We trained these algorithms on each of the predefined folds of the data set and generated predictions on the test fold using the resulting models.</p><p>Our results (see <ref type="bibr" target="#b6">(Cortes et al., 2014a)</ref>) show that ensemble methods lead only to a small improve- ment in performance over the best h j . This is be- cause here the best model h j dominates all other experts and ensemble methods cannot benefit from patching together different outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Penn Treebank data set</head><p>The part-of-speech task, POS, consists of label- ing each word of a sentence with its correct part-of-speech tag. The Penn Treebank 2 data set is available through LDC license at http: //www.cis.upenn.edu/˜treebank/ and contains 251,854 sentences with a total of 6,080,493 tokens and 45 different parts-of-speech.</p><p>For the first experiment, TR1, we used 4 disjoint training sets to produce 4 SVM multiclass mod- els and 4 maximum entropy models using the Stanford Classifier. We also used the union of these training sets to devise one CRFsuite model. For the second experiment, TR2, we trained 5 SVM struct models. The same features were used for both experiments. For the SVM algorithms, we generated 267,214 bag-of-word binary features. The Stanford Classifier and CRFsuite packages use internal routines to generate features.</p><p>The results of the experiments are summarized in <ref type="table">Table 5</ref>. For TR1, our on-line ensemble meth- ods improve over the best model. Note that H SLE has the best average loss over 10 runs for this ex- periment. This comes at a price of much higher standard deviation which does not allow us to con- clude that the difference in performance between our methods and H SLE is statistically significant. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Finite automaton G of path experts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Alternative experts automaton.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :</head><label>3</label><figDesc>Average Normalized Hamming Loss, PDS1 and PDS2. β P DS1 = 0.85, β P DS2 = 0.97, T SLE = 100, δ = 0.05.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Average Normalized Hamming Loss for 
ADS3. β ADS1 = 0.95, β ADS2 = 0.95, T SLE = 
100, δ = 0.05. 

HMVote 
0.1788 ± 0.00004 
HFPL 
0.2189 ± 0.04097 
HCV 
0.1788 ± 0.00004 
HFPL-CV 
0.3148 ± 0.00387 
HESPBoost 0.1831 ± 0.00240 
HSLE 
0.1954 ± 0.00185 
HRand 
0.3196 ± 0.00018 
Best hj 
0.2957 ± 0.00005 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Average edit distance, PDS1 and PDS2. 
β P DS1 = 0.85, β P DS2 = 0.97, T SLE = 100, 
δ = 0.05. 

PDS1, m = 130 
PDS2, m = 400 
HMVote 
0.8395 ± 0.01076 0.9626 ± 0.00341 
HFPL 
1.0158 ± 0.34379 0.9744 ± 0.01277 
HCV 
0.8668 ± 0.00553 0.9840 ± 0.00364 
HFPL-CV 
1.8044 ± 0.09315 1.8625 ± 0.06016 
HESPBoost 1.3977 ± 0.06017 1.4092 ± 0.04352 
HSLE 
1.1762 ± 0.12530 1.2477 ± 0.12267 
HRand 
1.8962 ± 0.01064 2.0838 ± 0.00518 
Best hj 
1.2163 ± 0.00619 1.2883 ± 0.00219 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 : Average Normalized Hamming Loss,</head><label>6</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> This paper is a modified version of (Cortes et al., 2014a) to which we refer the reader for the proofs of the theorems stated and a more detailed discussion of our algorithms.</note>

			<note place="foot" n="2"> (Schapire and Singer, 1999) also present an algorithm using the Hamming loss for multi-class classification, but that is a Hamming loss over the set of classes and differs from the loss function relevant to our problem. Additionally, the main steps of that algorithm are also based on a sum over all classes.</note>

			<note place="foot" n="3"> For the OCR data set, these subsets are predefined.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We warmly thank our colleagues Francoise Bea-ufays and Fuchun Peng for kindly extracting and making available to us the pronunciation data sets, Cyril Allauzen for providing us with the speech recognition data, and Richard Sproat and Brian Roark for help with other data sets. This work was partly funded by the NSF award IIS-1117591 and the NSERC PGS D3 award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Speech recognition data set</head><p>For our last set of experiments, we used another proprietary speech recognition data set, SDS. Each example in this data set is represented by a se- quence of length l ∈ <ref type="bibr">[2,</ref><ref type="bibr">15]</ref>. Therefore, for train- ing we padded the true labels and the expert pre- dictions to normalize the sequence lengths. For each of the 22,298 examples, there are between 2 and 251 expert predictions available. Since the ensemble methods we presented assume that the predictions of all p experts are available for each example in the training and test sets, we needed to restrict ourselves to the subsets of the data where at least some fixed number of expert predictions were available. In particular, we considered p = 5, 10, 20 and 50. For each value of p we used only the top p experts in our ensembles.</p><p>Our initial experiments showed that, as in the case of OCR data set, ensemble methods offer only a modest increase in performance over the best h j . This is again largely due to the dominant perfor- mance of the best expert h j . However, it was ob- served that the accuracy of the best model is a de- creasing function of l, suggesting that ensemble algorithm may be used to improve performance for longer sequences. Subsequent experiments show that this is indeed the case: when training and testing with l ≥ 4, ensemble algorithms out- perform the best model. <ref type="table">Table 6 and Table 7</ref> sum- marize these results for p = 5, 10, 20, 50.</p><p>Our results suggest that the following simple scheme can be used: for short sequences use the best expert model and for longer sequences, use the ensemble model. A more elaborate variant of this algorithm can be derived based on the obser- <ref type="table">Table 7</ref>: Average Normalized Hamming Loss, SDS. l ≥ 4, β = 0.97,δ = 0.05, T SLE = 100. p = 20, m = 900 p = 50, m = 700 HMVote 0.2773 ± 0.00139 0.3217 ± 0.00375 HFPL 0.2797 ± 0.00154 0.3189 ± 0.00344 HCV 0.2986 ± 0.00075 0.3401 ± 0.00054 HFPL-CV 0.3816 ± 0.01457 0.4451 ± 0.01360 HESPBoost 0.3115 ± 0.00089 0.3426 ± 0.00071 HSLE 0.3114 ± 0.00087 0.3425 ± 0.00076 HRand 0.3977 ± 0.00302 0.4608 ± 0.00303 Best hj 0.3116 ± 0.00087 0.3427 ± 0.00077 vation that the improvement in accuracy of the en- semble model over the best expert increases with the number of experts available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a broad analysis of the problem of ensemble structured prediction, including a series of algorithms with learning guarantees and exten- sive experiments. Our results show that our al- gorithms, most notably H MVote , can result in sig- nificant benefits in several tasks, which can be of a critical practical importance. We also reported very favorable results for H MVote when used with the edit-distance, which is the standard loss used in many applications. A natural extension of this work consists of devising new algorithms and pro- viding learning guarantees specific to other loss functions such as the edit-distance. While we aimed for an exhaustive study, including multi- ple on-learning algorithms, different conversions to batch and derandomizations, we are aware that the problem we studied is very rich and admits many more facets and scenarios that we plan to in- vestigate in the future. Finally, the boosting-style algorithm we presented can be enhanced using re- cent theoretical and algorithmic results on deep boosting ( <ref type="bibr" target="#b7">Cortes et al., 2014b</ref>).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ensemble selection from libraries of models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the generalization ability of on-line learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cesa-Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2050" to="2057" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koo2005] Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A general regression technique for learning transductions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cortes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2005</title>
		<meeting>ICML 2005<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ensemble methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cortes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cortes</surname></persName>
		</author>
		<title level="m">Proceedings of the Fourteenth International Conference on Machine Learning</title>
		<meeting>the Fourteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Corinna Cortes, Mehryar Mohri, and Umar Syed. 2014b. Deep boosting</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data-driven online to batch conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Singer2005] O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS 18</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Postprocessing system to yield reduced word error rates: Recognizer output voting error reduction (rover)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fiscus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1997 IEEE ASRU Workshop</title>
		<meeting>the 1997 IEEE ASRU Workshop<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="119" to="139" />
		</imprint>
	</monogr>
	<note>A decision-theoretic generalization of on-line learning and application to boosting</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalization bounds for averaged classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Freund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1698" to="1722" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Web-derived pronunciations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghoshal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="4289" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Support vector machines for complex outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient algorithms for online decision problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">A</forename><surname>Vempala2005</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="307" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Empirical margin distributions and bounding the generalization error of combined classifiers. Annals of Statistics, 30</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kocev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<editor>Lafferty et al.2001] J. Lafferty, A. McCallum, and F. Pereira</editor>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
	<note>Conditional random fields: Probabilistic models for segmenting and labeling sequence data</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The weighted majority algorithm. Information and Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warmuth1994] N</forename><surname>Littlestone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Littlestone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warmuth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="212" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From on-line to batch learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Littlestone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLT 2</title>
		<meeting>COLT 2</meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bayesian methods for adaptive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ensemble learning for hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<pubPlace>Cavendish Laboratory, Cambridge UK</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speech recognition with weighted finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook on Speech Processing and Speech Communication, Part E: Speech recognition</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mohri</surname></persName>
		</author>
		<title level="m">Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2012. Foundations of Machine Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finite-state transducers in language and speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="269" to="311" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comparison of sequence labeling algorithms and extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo2007] N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">CRFsuite: a fast implementation of conditional random fields (crfs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Products of random latent variable grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Stanford classifer</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parser combination by reparsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">K</forename><surname>Lavie2006</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/NAACL</title>
		<meeting>HLT/NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="129" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved boosting algorithms using confidence-rated predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">E</forename><surname>Singer1999] Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="336" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boostexter: A boosting-based system for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">E</forename><surname>Singer2000] Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="135" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Boosting the margin: A new explanation for the effectiveness of voting methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Linearly combining density estimators via stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Takimoto and Warmuth2003] E. Takimoto and M. K. Warmuth. 2003. Path kernels and multiplicative updates. JMLR</title>
		<imprint>
			<date type="published" when="1999-07" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="773" to="818" />
		</imprint>
	</monogr>
	<note>Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 20</title>
		<editor>Wang et al.2007] Q. Wang, D. Lin, and D.</editor>
		<meeting>IJCAI 20<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1756" to="1762" />
		</imprint>
	</monogr>
	<note>Advances in NIPS 16</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving parsing accuracy by combining diverse dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">D</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zabokrtsk´yzabokrtsk´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWPT 9</title>
		<meeting>IWPT 9</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">K-best combination of syntactic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1552" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
