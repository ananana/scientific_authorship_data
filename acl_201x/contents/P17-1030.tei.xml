<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
						</author>
						<title level="a" type="main">Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="321" to="331"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1030</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfit-ting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language modeling is a fundamental task, used for example to predict the next word or charac- ter in a text sequence given the context. Recently, recurrent neural networks (RNNs) have shown promising performance on this task ( <ref type="bibr" target="#b39">Mikolov et al., 2010;</ref><ref type="bibr" target="#b52">Sutskever et al., 2011)</ref>. RNNs with Long Short-Term Memory (LSTM) units <ref type="bibr" target="#b22">(Hochreiter and Schmidhuber, 1997</ref>) have emerged as a popular architecture, due to their representational power and effectiveness at capturing long-term de- pendencies.</p><p>RNNs are usually trained via back-propagation through time <ref type="bibr" target="#b62">(Werbos, 1990)</ref>, using stochastic op- * Equal contribution. † Corresponding author. timization methods such as stochastic gradient de- scent (SGD) <ref type="bibr" target="#b49">(Robbins and Monro, 1951)</ref>; stochas- tic methods of this type are particularly important for training with large data sets. However, this approach often provides a maximum a posteriori (MAP) estimate of model parameters. The MAP solution is a single point estimate, ignoring weight uncertainty ( <ref type="bibr" target="#b3">Blundell et al., 2015;</ref><ref type="bibr">HernándezLobato and Adams, 2015)</ref>. Natural language of- ten exhibits significant variability, and hence such a point estimate may make over-confident predic- tions on test data.</p><p>To alleviate overfitting RNNs, good regular- ization is known as a key factor to successful applications. In the neural network literature, Bayesian learning has been proposed as a princi- pled method to impose regularization and incor- porate model uncertainty <ref type="bibr" target="#b37">(MacKay, 1992;</ref><ref type="bibr" target="#b41">Neal, 1995)</ref>, by imposing prior distributions on model parameters. Due to the intractability of poste- rior distributions in neural networks, Hamiltonian Monte Carlo (HMC) <ref type="bibr" target="#b41">(Neal, 1995)</ref> has been used to provide sample-based approximations to the true posterior. Despite the elegant theoretical prop- erty of asymptotic convergence to the true poste- rior, HMC and other conventional Markov Chain Monte Carlo methods are not scalable to large training sets.</p><p>This paper seeks to scale up Bayesian learning of RNNs to meet the challenge of the increasing amount of "big" sequential data in natural lan- guage processing, leveraging recent advances in stochastic gradient Markov Chain Monte Carlo (SG-MCMC) algorithms ( <ref type="bibr" target="#b61">Welling and Teh, 2011;</ref><ref type="bibr" target="#b11">Ding et al., 2014;</ref><ref type="bibr">Li et al., 2016a,b)</ref>. Specifically, instead of training a sin- gle network, SG-MCMC is employed to train an ensemble of networks, where each network has its parameters drawn from a shared posterior distri- bution. This is implemented by adding additional This simple procedure has the following salu- tary properties for training neural networks: (i) When training, the injected noise encourages model-parameter trajectories to better explore the parameter space. This procedure was also empiri- cally found effective in <ref type="bibr" target="#b42">Neelakantan et al. (2016)</ref>.</p><p>(ii) Model averaging when testing alleviates over- fitting and hence improves generalization, trans- ferring uncertainty in the learned model parame- ters to subsequent prediction. (iii) In theory, both asymptotic and non-asymptotic consistency prop- erties of SG-MCMC methods in posterior estima- tion have been recently established to guarantee convergence <ref type="bibr" target="#b6">(Chen et al., 2015a;</ref><ref type="bibr" target="#b53">Teh et al., 2016)</ref>. (iv) SG-MCMC is scalable; it shares the same level of computational cost as SGD in training, by only requiring the evaluation of gradients on a small mini-batch. To the authors' knowledge, RNN training using SG-MCMC has not been in- vestigated previously, and is a contribution of this paper. We also perform extensive experiments on several natural language processing tasks, demon- strating the effectiveness of SG-MCMC for RNNs, including character/word-level language model- ing, image captioning and sentence classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several scalable Bayesian learning methods have been proposed recently for neural networks. These come in two broad categories: stochastic vari- ational inference <ref type="bibr" target="#b17">(Graves, 2011;</ref><ref type="bibr" target="#b3">Blundell et al., 2015;</ref><ref type="bibr" target="#b20">Hernández-Lobato and Adams, 2015)</ref> and SG-MCMC methods ( <ref type="bibr" target="#b30">Korattikara et al., 2015;</ref><ref type="bibr" target="#b32">Li et al., 2016a</ref>). While prior work focuses on feed-forward neural networks, there has been lit- tle if any research reported for RNNs using SG- MCMC.</p><p>Dropout ( <ref type="bibr" target="#b51">Srivastava et al., 2014</ref>) is a commonly used regularization method for training neural networks. Recently, several works have studied how to apply dropout to RNNs ( <ref type="bibr" target="#b43">Pachitariu and Sahani, 2013;</ref><ref type="bibr" target="#b1">Bayer et al., 2013;</ref><ref type="bibr" target="#b48">Pham et al., 2014;</ref><ref type="bibr" target="#b65">Zaremba et al., 2014;</ref><ref type="bibr" target="#b2">Bluche et al., 2015;</ref><ref type="bibr" target="#b40">Moon et al., 2015;</ref><ref type="bibr" target="#b50">Semeniuta et al., 2016;</ref><ref type="bibr" target="#b14">Gal and Ghahramani, 2016b</ref>). Among them, naive dropout ( <ref type="bibr" target="#b65">Zaremba et al., 2014</ref>) can im- pose weight uncertainty only on encoding weights (those that connect input to hidden units) and de- coding weights (those that connect hidden units to output), but not the recurrent weights (those that connect consecutive hidden states). It has been concluded that noise added in the recurrent con- nections leads to model instabilities, hence dis- rupting the RNN's ability to model sequences.</p><p>Dropout has been recently shown to be a varia- tional approximation technique in Bayesian learn- ing ( <ref type="bibr" target="#b13">Gal and Ghahramani, 2016a;</ref>. Based on this, ( <ref type="bibr" target="#b14">Gal and Ghahramani, 2016b)</ref> proposed a new variant of dropout that can be successfully applied to recurrent layers, where the same dropout masks are shared along time for encoding, decoding and recurrent weights, respec- tively. Alternatively, we focus on SG-MCMC, which can be viewed as the Bayesian interpreta- tion of dropout from the perspective of posterior sampling ( <ref type="bibr" target="#b34">Li et al., 2016c)</ref>; this also allows im- position of model uncertainty on recurrent layers, enhancing performance. A comparison of naive dropout and SG-MCMC is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recurrent Neural Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RNN as Bayesian Predictive Models</head><formula xml:id="formula_0">Consider data D = {D 1 , · · · , D N }, where D n (X n , Y n )</formula><p>, with input X n and output Y n . Our goal is to learn model parameters θ to best characterize the relationship from X n to Y n , with corresponding data likelihood p(D|θ) = N n=1 p(D n |θ). In Bayesian statistics, one sets a prior on θ via distribution p(θ). The posterior p(θ|D) ∝ p(θ)p(D|θ) reflects the belief concern- ing the model parameter distribution after observ- ing the data. Given a test input˜Xinput˜ input˜X (with miss- ing output˜Youtput˜ output˜Y), the uncertainty learned in training is transferred to prediction, yielding the posterior predictive distribution:</p><formula xml:id="formula_1">p( ˜ Y|˜XY|˜ Y|˜X, D) = θ p( ˜ Y|˜XY|˜ Y|˜X, θ)p(θ|D)dθ . (1)</formula><p>When the input is a sequence, RNNs may be used to parameterize the input-output relation- ship. Specifically, consider input sequence X = {x 1 , . . . , x T }, where x t is the input data vector at time t. There is a corresponding hidden state vec- tor h t at each time t, obtained by recursively ap- plying the transition function h t = H(h t−1 , x t ) (specified in Section 3.2; see <ref type="figure" target="#fig_0">Fig. 1</ref>). The output Y differs depending on the application: a sequence {y 1 , . . . , y T } in language modeling or a discrete label in sentence classification. In RNNs the cor- responding decoding function is p(y|h), described in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RNN Architectures</head><p>The transition function H(·) can be implemented with a gated activation function, such as Long Short-Term Memory (LSTM) <ref type="bibr" target="#b22">(Hochreiter and Schmidhuber, 1997</ref>) or a Gated Recurrent Unit (GRU) ( ). Both the LSTM and GRU have been proposed to address the issue of learning long-term sequential dependencies.</p><p>Long Short-Term Memory The LSTM archi- tecture addresses the problem of learning long- term dependencies by introducing a memory cell, that is able to preserve the state over long periods of time. Specifically, each LSTM unit has a cell containing a state c t at time t. This cell can be viewed as a memory unit. Reading or writing the cell is controlled through sigmoid gates: input gate i t , forget gate f t , and output gate o t . The hidden units h t are updated as</p><formula xml:id="formula_2">i t = σ(W i x t + U i h t−1 + b i ) , f t = σ(W f x t + U f h t−1 + b f ) , o t = σ(W o x t + U o h t−1 + b o ) , ˜ c t = tanh(W c x t + U c h t−1 + b c ) , c t = f t c t−1 + i t ˜ c t , h t = o t tanh(c t ) ,</formula><p>where σ(·) denotes the logistic sigmoid func- tion, and represents the element-wise matrix multiplication operator. W {i,f,o,c} are encoding weights, and U {i,f,o,c} are recurrent weights, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. b {i,f,o,c} are bias terms.</p><p>Variants Similar to the LSTM unit, the GRU also has gating units that modulate the flow of information inside the hidden unit. It has been shown that a GRU can achieve similar perfor- mance to an LSTM in sequence modeling ( <ref type="bibr" target="#b10">Chung et al., 2014</ref>). We specify the GRU in the Supple- mentary Material. The LSTM can be extended to the bidirec- tional LSTM and multilayer LSTM. A bidirec- tional LSTM consists of two LSTMs that are run in parallel: one on the input sequence and the other on the reverse of the input sequence. At each time step, the hidden state of the bidirectional LSTM is the concatenation of the forward and backward hidden states. In multilayer LSTMs, the hidden state of an LSTM unit in layer is used as input to the LSTM unit in layer + 1 at the same time step (Graves, 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Applications</head><p>The proposed Bayesian framework can be applied to any RNN model; we focus on the following tasks to demonstrate the ideas.</p><p>Language Modeling In word-level language modeling, the input to the network is a sequence of words, and the network is trained to predict the next word in the sequence with a softmax classi- fier. Specifically, for a length-T sequence, denote y t = x t+1 for t = 1, . . . , T − 1. x 1 and y T are always set to a special START and END token, respectively. At each time t, there is a decoding function p(y t |h t ) = softmax(Vh t ) to compute the distribution over words, where V are the de- coding weights (the number of rows of V corre- sponds to the number of words/characters). We also extend this basic language model to consider other applications: (i) a character-level language model can be specified in a similar manner by replacing words with characters ( <ref type="bibr" target="#b25">Karpathy et al., 2016)</ref>. (ii) Image captioning can be considered as a conditional language modeling problem, in which we learn a generative language model of the caption conditioned on an image ( <ref type="bibr" target="#b58">Vinyals et al., 2015;</ref><ref type="bibr" target="#b16">Gan et al., 2017</ref>).</p><p>Sentence Classification Sentence classification aims to assign a semantic category label y to a whole sentence X. This is usually implemented through applying the decoding function once at the end of sequence: p(y|h T ) = softmax(Vh T ), where the final hidden state of a RNN h T is often considered as the summary of the sentence (here the number of rows of V corresponds to the num- ber of classes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Scalable Learning with SG-MCMC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Pitfall of Stochastic Optimization</head><p>Typically there is no closed-form solution for the posterior p(θ|D), and traditional Markov Chain Monte Carlo (MCMC) methods <ref type="bibr" target="#b41">(Neal, 1995)</ref> scale poorly for large N . To ease the computational bur- den, stochastic optimization is often employed to find the MAP solution. This is equivalent to min- imizing an objective of regularized loss function U (θ) that corresponds to a (non-convex) model of interest: θ MAP = arg min U (θ), U (θ) = − log p(θ|D). The expectation in (1) is approxi- mated as:</p><formula xml:id="formula_3">p( ˜ Y|˜XY|˜ Y|˜X, D) = p( ˜ Y|˜XY|˜ Y|˜X, θ MAP ) .<label>(2)</label></formula><p>Though simple and effective, this procedure largely loses the benefit of the Bayesian approach, because the uncertainty on weights is ignored.</p><p>To more accurately approximate (1), we employ stochastic gradient (SG) MCMC (Welling and Teh, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Large-scale Bayesian Learning</head><p>The negative log-posterior is</p><formula xml:id="formula_4">U (θ) − log p(θ) − N n=1 log p(D n |θ).<label>(3)</label></formula><p>In optimization, E = − N n=1 log p(D n |θ) is typ- ically referred to as the loss function, and R ∝ − log p(θ) as a regularizer.</p><p>For large N , stochastic approximations are of- ten employed:</p><formula xml:id="formula_5">˜ U t (θ) − log p(θ) − N M M m=1 log p(D im |θ),<label>(4)</label></formula><p>where S m = {i 1 , · · · , i M } is a random subset of the set {1, 2, · · · , N }, with M N . The gradi- ent on this mini-batch is denoted as˜fas˜ as˜f t = ˜ U t (θ), which is an unbiased estimate of the true gradi- ent. The evaluation of <ref type="formula" target="#formula_5">(4)</ref> is cheap even when N is large, allowing one to efficiently collect a suf- ficient number of samples in large-scale Bayesian learning, {θ s } S s=1 , where S is the number of sam- ples (this will be specified later). These samples are used to construct a sample-based estimation to the expectation in (1): <ref type="table">Table 1</ref>: SG-MCMC algorithms and their optimiza- tion counterparts. Algorithms in the same row share similar characteristics.</p><formula xml:id="formula_6">Algorithms SG-MCMC Optimization Basic SGLD SGD Precondition pSGLD RMSprop/Adagrad Momentum SGHMC momentum SGD Thermostat SGNHT Santa p( ˜ Y|˜XY|˜ Y|˜X, D) ≈ 1 S S s=1 p( ˜ Y|˜XY|˜ Y|˜X, θ s ) .<label>(5)</label></formula><p>The finite-time estimation errors of SG-MCMC methods are bounded ( <ref type="bibr" target="#b6">Chen et al., 2015a</ref>), which guarantees <ref type="formula" target="#formula_6">(5)</ref> is an unbiased estimate of <ref type="formula">(1)</ref> asymptotically under appropriate decreasing step- sizes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">SG-MCMC Algorithms</head><formula xml:id="formula_7">θ t = θ t−1 − η t ˜ f t−1 + 2η t ξ t ,<label>(6)</label></formula><p>where η t is the learning rate, and ξ t ∼ N (0, I p ) is a standard Gaussian random vector. SGLD is the SG-MCMC analog to stochastic gradient descent (SGD), whose parameter updates are given by:</p><formula xml:id="formula_8">θ t = θ t−1 − η t ˜ f t−1 .<label>(7)</label></formula><p>Algorithm 1: pSGLD Input: Default hyperparameter settings:</p><formula xml:id="formula_9">η t = 1×10 −3 , λ = 10 −8 , β 1 = 0.99. Initialize: v 0 ← 0, θ 1 ∼ N (0, I) ; for t = 1, 2, . . . , T do % Estimate gradient from minibatch St˜f St˜ St˜f t = ˜ U t (θ); % Preconditioning v t ← β 1 v t−1 + (1 − β 1 ) ˜ f t ˜ f t ; G −1 t ← diag 1 λ1 + v 1 2 t ; % Parameter update ξ t ∼ N (0, η t G −1 t ); θ t+1 ← θ t + ηt 2 G −1 t ˜ f t + ξ t ; end</formula><p>SGD is guaranteed to converge to a local mini- mum under mild conditions <ref type="bibr" target="#b4">(Bottou, 2010)</ref>. The additional Gaussian term in SGLD helps the learn- ing trajectory to explore the parameter space to ap- proximate posterior samples, instead of obtaining a local minimum.</p><p>pSGLD Preconditioned SGLD (pSGLD) ( <ref type="bibr" target="#b32">Li et al., 2016a</ref>) was proposed recently to improve the mixing of SGLD. It utilizes magnitudes of re- cent gradients to construct a diagonal precondi- tioner to approximate the Fisher information ma- trix, and thus adjusts to the local geometry of parameter space by equalizing the gradients so that a constant stepsize is adequate for all dimen- sions. This is important for RNNs, whose parame- ter space often exhibits pathological curvature and saddle points ( <ref type="bibr" target="#b47">Pascanu et al., 2013)</ref>, resulting in slow mixing. There are multiple choices of pre- conditioners; similar ideas in optimization include Adagrad (Duchi et al., 2011), Adam (Kingma and Ba, 2015) and RMSprop <ref type="bibr" target="#b55">(Tieleman and Hinton, 2012</ref>). An efficient version of pSGLD, adopt- ing RMSprop as the preconditioner G, is summa- rized in Algorithm 1, where denotes element- wise matrix division. When the preconditioner is fixed as the identity matrix, the method reduces to SGLD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Understanding SG-MCMC</head><p>To further understand SG-MCMC, we show its close connection to dropout/dropConnect <ref type="bibr" target="#b51">(Srivastava et al., 2014;</ref><ref type="bibr" target="#b59">Wan et al., 2013</ref>). These methods improve the generalization ability of deep models, by randomly adding binary/Gaussian noise to the local units or global weights. For neural networks with the nonlinear function q(·) and consecutive layers h 1 and h 2 , dropout and dropConnect are denoted as:</p><formula xml:id="formula_10">Dropout: h 2 = ξ 0 q(θh 1 ), DropConnect: h 2 = q((ξ 0 θ)h 1 ),</formula><p>where the injected noise ξ 0 can be binary-valued with dropping rate p or its equivalent Gaussian form ( <ref type="bibr" target="#b60">Wang and Manning, 2013)</ref>:</p><formula xml:id="formula_11">Binary noise: ξ 0 ∼ Ber(p),</formula><p>Gaussian noise:</p><formula xml:id="formula_12">ξ 0 ∼ N (1, p 1 − p ).</formula><p>Note that ξ 0 is defined as a vector for dropout, and a matrix for dropConnect. By combining drop- Connect and Gaussian noise from the above, we have the update rule ( <ref type="bibr" target="#b34">Li et al., 2016c</ref>):</p><formula xml:id="formula_13">θ t+1 = ξ 0 θ t − η 2 ˜ f t = θ t − η 2 ˜ f t + ξ 0 ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_14">ξ 0 ∼ N 0, p (1−p) diag(θ 2 t )</formula><p>; <ref type="formula" target="#formula_13">(8)</ref> shows that dropout/ dropConnect and SGLD in (6) share the same form of update rule, with the distinc- tion being that the level of injected noise is dif- ferent. In practice, the noise injected by SGLD may not be enough. A better way that we find to improve the performance is to jointly apply SGLD and dropout. This method can be interpreted as using SGLD to sample the posterior distribution of a mixture of RNNs, with mixture probability controlled by the dropout rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We present results on several tasks, including character/word-level language modeling, image captioning, and sentence classification. We do not perform any dataset-specific tuning other than early stopping on validation sets. When dropout is utilized, the dropout rate is set to 0.5. All experi- ments are implemented in Theano (Theano Devel- opment Team, 2016), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory.</p><p>The hyper-parameters for the proposed algo- rithm include step size, minibatch size, thinning interval, number of burn-in epochs and variance of the Gaussian priors. We list the specific val- ues used in our experiments in <ref type="table">Table 2</ref>. The ex- planation of these hyperparameters, the initializa- tion of model parameters and model specifications on each dataset are provided in the Supplementary Material. <ref type="table">Table 2</ref>: Hyper-parameter settings of pSGLD for different datasets. For PTB, SGLD is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>WP PTB <ref type="table" target="#tab_1">Flickr8k Flickr30k  MR  CR  SUBJ MPQA TREC  Minibatch Size  100  32  64  64  50  50  50  50  50</ref> Step Size 2×10 −3 1 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 10 −3 # <ref type="table" target="#tab_1">Total Epoch  20  40  20  20  20  20  20  20  20  Burn-in (#Epoch)  4  4  3  3  1  1  1  1  1  Thinning Interval (#Epoch)  1/2  1/2  1  1/2  1  1  1  1  1  # Samples Collected  32  72  17  34  19  19  19</ref> 19 19</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language Modeling</head><p>We first test character-level and word-level lan- guage modeling. The setup is as follows.</p><p>•  <ref type="table" target="#tab_2">Table 3</ref>. We observe that pSGLD con- sistently outperforms RMSprop.   that our sampling-based method consistently out- performs the optimization counterpart, where the performance gain mainly comes from adding gra- dient noise and model averaging. When com- pared with dropout, SGLD performs better on the small LSTM model, but worse on the medium and large LSTM model. This may imply that dropout is suitable to regularizing large networks, while SGLD exhibits better regularization ability on small networks, partially due to the fact that dropout may inject a higher level of noise during training than SGLD. In order to inject a higher level of noise into SGLD, we empirically apply SGLD and dropout jointly, and found that this provided the best performace on the medium and large LSTM model. We study three strategies to do model averaging, i.e., forward collection, backward collection and thinned collection. Given samples (θ 1 , · · · , θ K ) and the number of samples S used for averaging, forward collection refers to using (θ 1 , · · · , θ S ) for the evaluation of a test function, backward col- lection refers to using (θ K−S+1 , · · · , θ K ), while thinned collection chooses samples from θ 1 to θ K with interval K/S. <ref type="figure" target="#fig_2">Fig. 2</ref> plots the effects of these strategies, where <ref type="figure" target="#fig_2">Fig. 2(a)</ref> plots the perplex- ity of every single sample, <ref type="figure" target="#fig_2">Fig. 2(b)</ref> plots the per- plexities using the three schemes. Only after 20 vide a fair comparison to all methods. samples is a converged perplexity achieved in the thinned collection, while it requires 30 samples for forward collection or 60 samples for backward collection. This is unsurprising, because thinned collection provides a better way to select samples. Nevertheless, averaging of samples provides sig- nificantly lower perplexity than using single sam- ples. Note that the overfitting problem in <ref type="figure" target="#fig_2">Fig. 2(a)</ref> is also alleviated by model averaging.</p><p>To better illustrate the benefit of model averag- ing, we visualize in <ref type="figure" target="#fig_3">Fig. 3</ref> the probabilities of each word in a randomly chosen test sentence. The first 3 rows are the results predicted by 3 distinctive model samples, respectively; the bottom row is the result after averaging. Their corresponding per- plexities for the test sentence are also shown on the right of each row. The 3 individual samples provide reasonable probabilities. For example, the consecutive words "New York", "stock exchange" and "did not" are assigned with a higher proba- bility. After averaging, we can see a much lower perplexity, as the samples can complement each other. For example, though the second sample can yield the lowest single-model perplexity, its pre- diction on word "York" is still benefited from the other two via averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image Caption Generation</head><p>We next consider the problem of image caption generation, which is a conditional RNN model, where image features are extracted by residual net- work ( <ref type="bibr" target="#b19">He et al., 2016)</ref>, and then fed into the RNN to generate the caption. We present results on two benchmark datasets, <ref type="bibr">Flickr8k (Hodosh et al., 2013</ref>) and Flickr30k ( <ref type="bibr" target="#b64">Young et al., 2014</ref>  a"tan"dog"is"playing"in"the"grass a"tan"dog"is"playing"with"a"red"ball"in"the"grass a"tan"dog"with"a"red"collar"is"running"in"the"grass a"yellow"dog"runs"through"the"grass a"yellow"dog"is"running"through"the"grass a"brown"dog"is"running"through"the"grass a"group"of"people"stand"in"front"of"a"building a"group"of"people"stand"in"front"of"a"white"building a"group"of"people"stand"in"front"of"a"large"building a"man"and"a"woman"walking"on"a"sidewalk a"man"and"a"woman"stand"on"a"balcony a"man"and"a"woman"standing"on"the"ground datasets contain 8,000 and 31,000 images, respec- tively. Each image is annotated with 5 sentences. A single-layer LSTM is employed with the num- ber of hidden units set to 512.</p><p>The widely used BLEU ( <ref type="bibr" target="#b46">Papineni et al., 2002</ref>), METEOR (Banerjee and <ref type="bibr" target="#b0">Lavie, 2005</ref>), ROUGE- L <ref type="bibr" target="#b36">(Lin, 2004)</ref>, and CIDEr-D ( <ref type="bibr" target="#b57">Vedantam et al., 2015)</ref> metrics are used to evaluate the perfor- mance. All the metrics are computed by us- ing the code released by the COCO evaluation server <ref type="bibr" target="#b8">(Chen et al., 2015b)</ref>. <ref type="table">Table 5</ref> presents results for pSGLD/RMSprop <ref type="table">Table 5</ref>: Performance on Flickr8k &amp; Flickr30k: BLEU's, METEOR, CIDEr, ROUGE-L and perplexity. with or without dropout. In addition to (naive) dropout, we further compare pSGLD with the Gal's dropout, recently proposed in <ref type="bibr" target="#b14">Gal and Ghahramani (2016b)</ref>, which is shown to be ap- plicable to recurrent layers. Consistent with the results in the basic language modeling, pS- GLD yields improved performance compared to RMSprop. For example, pSGLD provides 2.7 BLEU-4 score improvement over RMSprop on the Flickr8k dataset. By comparing pSGLD with RM- Sprop with dropout, we conclude that pSGLD ex- hibits better regularization ability than dropout on these two datasets. Apart from modeling weight uncertainty, differ- ent samples from our algorithm may capture dif- ferent aspects of the input image. An example with two images is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, where 2 ran- domly chosen model samples are considered for each image. For each model sample, the top 3 gen- erated captions are presented. We use the beam search approach ( <ref type="bibr" target="#b58">Vinyals et al., 2015</ref>) to gener- ate captions, with a beam of size 5. <ref type="figure" target="#fig_4">In Fig. 4</ref>, the two samples for the first image mainly differ in the color and activity of the dog, e.g., "tan" or "yellow", "playing" or "running", whereas for the second image, the two samples reflect different un- derstanding of the image content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B-1 B-2 B-3 B-4 METEOR CIDEr ROUGE-L Perp. Results on Flickr8k</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sentence Classification</head><p>We study the task of sentence classification on 5 datasets as in <ref type="bibr" target="#b29">Kiros et al. (2015)</ref>: MR (Pang and <ref type="bibr" target="#b45">Lee, 2005</ref>), CR (Hu and Liu, 2004), SUBJ (Pang and <ref type="bibr" target="#b44">Lee, 2004</ref>), MPQA ( <ref type="bibr" target="#b63">Wiebe et al., 2005</ref>) and TREC ( <ref type="bibr" target="#b35">Li and Roth, 2002)</ref>. A single-layer bidi- rectional LSTM is employed with the number of hidden units set to 400. <ref type="table" target="#tab_6">Table 6</ref> shows the test- ing classification errors. 10-fold cross-validation is used for evaluation on the first 4 datasets, while TREC has a pre-defined training/test split, and we run each algorithm 10 times on TREC. The com- bination of pSGLD and dropout consistently pro- vides the lowest errors.</p><p>In the following, we focus on the analysis of TREC. Each sentence of TREC is a question, and the goal is to decide which topic type the ques- tion is most related to: location, human, numeric, abbreviation, entity or description. <ref type="figure" target="#fig_5">Fig. 5</ref> plots the learning curves of different algorithms on the training, validation and testing sets of the TREC dataset. pSGLD and dropout have similar behav- ior: they explore the parameter space during learn- ing, and thus coverge slower than RMSprop on the training dataset. However, the learned uncertainty alleviates overfitting and results in lower errors on the validation and testing datasets.</p><p>To further study the Bayesian nature of the pro- posed approach, in <ref type="figure" target="#fig_6">Fig. 6</ref> we choose two test- ing sentences with high uncertainty (i.e., standard derivation in prediction) from the TREC dataset. Interestingly, after embedding to 2d-space with tSNE (Van der Maaten and Hinton, 2008), the two  sentences correspond to points lying on the bound- ary of different classes. We use 20 model sam- ples to estimate the prediction mean and standard derivation on the true type and predicted type. The classifier yields higher probability on the wrong types, associated with higher standard derivations. One can leverage the uncertainty information to make decisions: either manually make a human judgement when uncertainty is high, or automat- ically choose the one with lower standard deriva- tions when both types exhibits similar prediction means. A more rigorous usage of the uncertainty information is left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>Ablation Study We investigate the effectivenss of each module in the proposed algorithm in Ta- ble 7 on two datasets: TREC and PTB. The small network size is used on PTB. Let M 1 denote only gradient noise, and M 2 denote only model averag- ing. As can be seen, The last sample in pSGLD (M 1 ) does not necessarily bring better results than RMSprop, but the model averaging over the sam- ples of pSGLD indeed provide better results than model averaging of RMSprop (M 2 ). This indi- cates that both gradient noise and model averaging are crucial for good performance in pSGLD.   <ref type="table" target="#tab_8">Table 8</ref>. For pSGLD, the extra cost in training comes from adding gradient noise, and the extra cost in testing comes from model averaging. However, the cost in model averaging can be alle- viated via the distillation methods: learning a sin- gle neural network that approximates the results of either a large model or an ensemble of mod- els ( <ref type="bibr" target="#b30">Korattikara et al., 2015;</ref><ref type="bibr" target="#b26">Kim and Rush, 2016;</ref><ref type="bibr" target="#b31">Kuncoro et al., 2016</ref>). The idea can be incorpo- rated with our SG-MCMC technique to achieve the same goal, which we leave for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a scalable Bayesian learning frame- work using SG-MCMC, to model weight uncer- tainty in recurrent neural networks. The learn- ing framework is tested on several tasks, includ- ing language models, image caption generation and sentence classification. Our algorithm outper- forms stochastic optimization algorithms, indicat- ing the importance of learning weight uncertainty in recurrent neural networks. Our algorithm re- quires little additional computational overhead in training, and multiple times of forward-passing for model averaging in testing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of different weight learning strategies in a single-hidden-layer RNN. Stochastic optimization used for MAP estimation puts fixed values on all weights. Naive dropout is allowed to put weight uncertainty only on encoding and decoding weights, and fixed values on recurrent weights. The proposed SG-MCMC scheme imposes distributions on all weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>SG-MCMC and stochastic optimization are par- allel lines of work, designed for different pur- poses; their relationship has recently been re- vealed in the context of deep learning. The most basic SG-MCMC algorithm has been applied to Langevin dynamics, and is termed SGLD (Welling and Teh, 2011). To help convergence, a momen- tum term has been introduced in SGHMC (Chen et al., 2014), a "thermostat" has been devised in SGNHT (Ding et al., 2014; Gan et al., 2015) and preconditioners have been employed in pS- GLD (Li et al., 2016a). These SG-MCMC algo- rithms often share similar characteristics with their counterpart approaches from the optimization lit- erature such as the momentum SGD, Santa (Chen et al., 2016) and RMSprop/Adagrad (Tieleman and Hinton, 2012; Duchi et al., 2011). The interre- lationships between SG-MCMC and optimization- based approaches are summarized in Table 1. SGLD Stochastic Gradient Langevin Dynamics (SGLD) (Welling and Teh, 2011) draws posterior samples, with updates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effects of collected samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Predictive probabilities obtained by 3 samples and their average. Colors indicate normalized probability of each word. Best viewed in color.</figDesc><graphic url="image-26.png" coords="7,307.87,415.28,77.93,51.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Image captioning with different samples. Left are the given images, right are the corresponding captions. The captions in each box are from the same model sample.</figDesc><graphic url="image-27.png" coords="7,307.87,470.94,77.93,51.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Learning curves on TREC dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Abbreviation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 summa</head><label>4</label><figDesc>- rizes the test set performance on PTB 1 . It is clear 1 The results reported here do not match Zaremba et al. (2014) due to the implementation details. However, we pro-</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Test cross-entropy loss on WP dataset. 
Methods LSTM GRU 
RNN 
RMSprop 1.3607 1.2759 1.4239 
pSGLD 
1.3375 1.2561 1.4093 

10 
20 
30 
40 
50 
60 
Individual Sample 

110 
120 
130 
140 
150 
160 
170 
180 

Perplexity 

0 
10 
20 
30 
40 
50 
60 

Number of Samples for Model Averaging 

110 
120 
130 
140 
150 
160 
170 
180 

Perplexity 

forward collection 
backward collection 
thinned collection 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Test perplexity on Penn Treebank. Methods Small Medium Large</head><label>4</label><figDesc></figDesc><table>Random minibatches 

SGD 
123.85 126.31 130.25 
SGD+Dropout 
136.39 100.12 
97.65 
SGLD 
117.36 109.14 105.86 
SGLD+Dropout 
139.54 
99.58 
94.03 

Successive minibatches 

SGD 
113.45 123.14 127.68 
SGD+Dropout 
117.85 
84.60 
80.85 
SGLD 
108.61 121.16 131.40 
SGLD+Dropout 
125.44 
82.71 
78.91 

Literature 

Moon et al. (2015) 
− 
97.0 
118.7 
Moon et al. (2015)+ emb. dropout 
− 
86.5 
86.0 
Zaremba et al. (2014) 
− 
82.7 
78.4 
Gal and Ghahramani (2016b) 
− 
78.6 
73.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 6 : Sentence classification errors on five benchmark datasets.</head><label>6</label><figDesc></figDesc><table>Methods 
MR 
CR 
SUBJ 
MPQA 
TREC 
RMSprop 
21.86±1.19 20.20±1.35 8.13±1.19 10.60±1.28 8.14±0.63 
RMSprop + Dropout 
20.52±0.99 19.57±1.79 7.24±0.86 10.66±0.74 7.48±0.47 
RMSprop + Gal's Dropout 20.22±1.12 19.29±1.93 7.52±1.17 10.59±1.12 7.34±0.66 
pSGLD 
20.36±0.85 18.72±1.28 7.00±0.89 10.54±0.99 7.48±0.82 
pSGLD + Dropout 
19.33±1.10 18.18±1.32 6.61±1.06 10.22±0.89 6.88±0.65 

What does cc in engines mean? 

What does a defibrillator do? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Ablation study on TREC and PTB. 

Datasets RMSprop 
M 1 
M 2 
pSGLD 
TREC 
8.14 
8.34 
7.54 
7.48 
PTB 
120.45 
122.14 114.86 109.44 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Running time on Flickr30k in seconds. 
Stages 
pSGLD RMSprop+Dropout 
Training 20324 
12578 
Testing 
7047 
1311 

Running Time We report the training and test-
ing time for image captioning on the Flickr30k 
dataset in </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Osendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.0701</idno>
		<title level="m">On fast dropout and its applicability to recurrent networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Where to apply dropout in recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COMPSTAT</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bridging the gap between stochastic gradient MCMC and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the convergence of stochastic gradient MCMC algorithms with high-order integrators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic gradient Hamiltonian Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian sampling using stochastic gradient thermostats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Babbush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Skeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable deep poisson factor analysis for topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic compositional networks for visual captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic backpropagation for scalable learning of Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<title level="m">Skip-thought vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distilling an ensemble of greedy dependency parsers into one mst parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Preconditioned stochastic gradient Langevin dynamics for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Highorder stochastic gradient thermostats for Bayesian learning of deep models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning weight uncertainty with stochastic gradient mcmc for shape classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A practical Bayesian framework for backpropagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernock`cernock`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Song</surname></persName>
		</author>
		<title level="m">Rnndrop: A novel dropout for rnns in asr. ASRU</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adding gradient noise improves learning for very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Regularization and nonlinearities for neural language models: when are they needed?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pachitariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICFHR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of mathematical statistics</title>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05118</idno>
		<title level="m">Recurrent dropout without memory loss</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Consistency and fluctuations for stochastic gradient Langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Thiéry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Vollmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno type="arXiv">arXiv:1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coursera: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using DropConnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fast Dropout training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient Langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
