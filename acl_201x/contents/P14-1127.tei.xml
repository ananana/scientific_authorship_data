<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Morphology-Based Vocabulary Expansion</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Sadegh</forename><surname>Rasooli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Computational Learning Systems</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lippincott</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Computational Learning Systems</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Computational Learning Systems</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Computational Learning Systems</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Morphology-Based Vocabulary Expansion</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1349" to="1359"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel way of generating unseen words, which is useful for certain applications such as automatic speech recognition or optical character recognition in low-resource languages. We test our vocabulary generator on seven low-resource languages by measuring the decrease in out-of-vocabulary word rate on a held-out test set. The languages we study have very different morphological properties; we show how our results differ depending on the morphological complexity of the language. In our best result (on As-samese), our approach can predict 29% of the token-based out-of-vocabulary with a small amount of unlabeled training data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many applications in human language technolo- gies (HLT), the goal is to generate text in a target language, using its standard orthography. Typical examples include automatic speech recognition (ASR, also known as STT or speech-to-text), opti- cal character recognition (OCR), or machine trans- lation (MT) into a target language. We will call such HLT applications "target-language genera- tion technologies" (TLGT). The best-performing systems for these applications today rely on train- ing on large amounts of data: in the case of ASR, the data is aligned audio and transcription, plus large unannotated data for the language model- ing; in the case of OCR, it is transcribed optical data; in the case of MT, it is aligned bitexts. More data provides for better results. For languages with rich resources, such as English, more data is often the best solution, since the required data is readily available (including bitexts), and the cost of anno- tating (e.g., transcribing) data is outweighed by the potential significance of the systems that the data will enable. Thus, in HLT, improvements in qual- ity are often brought about by using larger data sets <ref type="bibr" target="#b0">(Banko and Brill, 2001</ref>).</p><p>When we move to low-resource languages, the solution of simply using more data becomes less appealing. Unannotated data is less readily avail- able: for example, at the time of publishing this paper, 55% of all websites are in English, the top 10 languages collectively account for 90% of web presence, and the top 36 languages have a web presence that covers at least 0.1% of web sites. <ref type="bibr">1</ref> All other languages (and all languages considered in this paper except Persian) have a web presence of less than 0.1%. Considering Wikipedia, another resource often used in HLT, English has 4.4 mil- lion articles, while only 48 other languages have more than 100,000. <ref type="bibr">2</ref> As attention turns to de- veloping HLT for more languages, including low- resource languages, alternatives to "more-data" approaches become important.</p><p>At the same time, it is often not possible to use knowledge-rich approaches. For low-resource lan- guages, resources such as morphological analyz- ers are not usually available, and even good schol- arly descriptions of the morphology (from which a tool could be built) are often not available. The challenge is therefore to use data, but to make do with a small amount of data, and thus to use data better. This paper is a contribution to this goal. Specifically, we address TLGTs, i.e., the types of HLT mentioned above that generate target lan- guage text. We propose a new approach to gener- ating unseen words of the target language which have not been seen in the training data. Our ap- proach is entirely unsupervised. It assumes that word-units are specified, typically by whitespace and punctuation.</p><p>Expanding the vocabulary of the target lan- guage can be useful for TLGTs in different ways. For ASR and OCR, which can compose words from smaller units (phones or graphically recog- nized letters), an expanded target language vocab- ulary can be directly exploited without the need for changing the technology at all: the new words need to be inserted into the relevant resources (lex- icon, language model) etc, with appropriately es- timated probabilities. In the case of MT into mor- phologically rich low-resource languages, mor- phological segmentation is typically used in devel- oping the translation models to reduce sparsity, but this does not guarantee against generating wrong word combinations. The expanded word combi- nations can be used to extend the language models used for MT to bias against incoherent hypothe- sized new sequences of segmented words.</p><p>Our approach relies on unsupervised morpho- logical segmentation. We do not in this paper con- tribute to research in unsupervised morphological segmentation; we only use it. The contribution of this paper lies in proposing how to use the re- sults of unsupervised morphological segmentation in order to generate unseen words of the language. We investigate several ways of doing so, and we test them on seven low-resource languages. These languages have very different morphological prop- erties, and we show how our results differ depend- ing on the morphological complexity of the lan- guage. In our best result (on Assamese), we show that our approach can predict 29% of the token- based out-of-vocabulary with a small amount of unlabeled training data.</p><p>The paper is structured as follows. We first dis- cuss related work in Section 2. We then present our method in Section 3, and present experimental results in Section 4. We conclude with a discus- sion of future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Approaches to Morphological Modeling Computational morphology is a very active area of research with a multitude of approaches that vary in the degree of manual annotation needed, and the amount of machine learning used. At one extreme, we find systems that are painstakingly and carefully designed by hand <ref type="bibr" target="#b17">(Koskenniemi, 1983;</ref><ref type="bibr" target="#b2">Buckwalter, 2004;</ref><ref type="bibr" target="#b14">Habash and Rambow, 2006;</ref><ref type="bibr" target="#b7">Détrez and Ranta, 2012)</ref>. Next on the continuum, we find work that focuses on defining morphological models with limited lexica that are then extended using raw text <ref type="bibr" target="#b4">(Clément et al., 2004;</ref><ref type="bibr" target="#b11">Forsberg et al., 2006</ref>). In the middle of this continuum, we find efforts to learn complete paradigms using fully supervised methods relying on completely annotated data points with rich morphological information <ref type="bibr" target="#b9">(Durrett and DeNero, 2013;</ref><ref type="bibr" target="#b10">Eskander et al., 2013)</ref>. Next, there is work on minimally supervised methods that use available resources such as dictionaries, bitexts, and other additional morphological annotations <ref type="bibr">(Yarowsky and Wicentowski, 2000;</ref><ref type="bibr" target="#b6">Cucerzan and Yarowsky, 2002;</ref><ref type="bibr" target="#b20">Neuvel and Fulop, 2002;</ref><ref type="bibr">Snyder and Barzilay, 2008)</ref>. At the other extreme, we find unsupervised methods that learn morphology models from unannotated data <ref type="bibr" target="#b5">(Creutz and Lagus, 2007;</ref><ref type="bibr" target="#b19">Monson et al., 2008;</ref><ref type="bibr" target="#b8">Dreyer and Eisner, 2011;</ref><ref type="bibr">Sirts and Goldwater, 2013)</ref>.</p><p>The work we present in this paper makes no use of any morphological annotations whatsoever, yet we are quite distinct from the approaches cited above. We compare our work to two efforts specif- ically. First, consider work in automatic mor- phological segmentation learning from unanno- tated data <ref type="bibr" target="#b5">(Creutz and Lagus, 2007;</ref><ref type="bibr" target="#b19">Monson et al., 2008)</ref>. Unlike these approaches which provide segmentations for training data and produce mod- els that can be used to segment unseen words, our approach can generate words that have not been seen in the training data. The focus of efforts is rather complementary: we actually use an off-the- shelf unsupervised segmentation system <ref type="bibr" target="#b5">(Creutz and Lagus, 2007)</ref> as part of our approach. Second, consider paradigm completion methods such as the work of <ref type="bibr" target="#b8">Dreyer and Eisner (2011)</ref>. This effort is closely related to our work although unlike it, we make no assumptions about the data and do not introduce any restrictions along the lines of deriva- tion/inflectional morphology: Dreyer and Eisner (2011) limited their work to verbal paradigms and used annotated training data in addition to basic assumptions about the problem such as the size of the paradigms. In our approach, we have zero annotated information and we do not distinguish between inflectional and derivational morphology, nor do we limit ourselves to a specific part-of- speech (POS).</p><p>Vocabulary Expansion in HLT There have been diverse approaches towards dealing with out- of-vocabulary (OOV) words in ASR. In some models, the approach is to expand the lexicon by adding new words or pronunciations. <ref type="bibr">Ohtsuki et al. (2005)</ref> propose a two-run model where in the first run, the input speech is recognized by the reference vocabulary and relevant words are ex- tracted from the vocabulary database and added thereafter to the reference vocabulary to build an expanded lexicon. Word recognition is done in the second run based on the lexicon. <ref type="bibr" target="#b18">Lei et al. (2009)</ref> expanded the pronunciation lexicon via generat- ing all possible pronunciations for a word be- fore lattice generation and indexation. There are also other methods for generating abbreviations in voice search systems such as <ref type="bibr">Yang et al. (2012)</ref>. While all of these approaches involve lexicon ex- pansion, they do not employ any morphological information.</p><p>In the context of MT, several researchers have addressed the problem of OOV words by relating them to known in-vocabulary (INV) words. <ref type="bibr">Yang and Kirchhoff (2006)</ref> anticipated OOV words that are potentially morphologically related using phrase-based backoff models. Habash (2008) con- sidered different techniques for vocabulary expan- sion online. One of their techniques learned mod- els of morphological mapping between morpho- logically rich source words in Arabic that pro- duce the same English translation. This was used to relate an OOV word to a morphologically re- lated INV word. Another technique expanded the MT phrase tables with possible transliterations and spelling alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Morphology-based Vocabulary Expansion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach</head><p>Our approach to morphology-based vocabulary expansion consists of three steps ( <ref type="figure" target="#fig_0">Figure 1</ref>). We start with a "training" corpus of (unannotated) words and generate a list of new (unseen) words that expands the vocabulary of the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Unsupervised Morphology Segmentation</head><p>The first step is to segment each word in the training corpus into sequences of prefixes, stem and suffixes, where the prefixes and suf- fixes are optional. <ref type="bibr">3</ref> 2. FST-based Morphology Expansion We then construct new word models using the segmented stems and affixes. We explore two different techniques for morphology-based vocabulary expansion that we discuss below. The output of these models is represented as a weighted finite state machine (WFST).</p><p>3. Reranking Models Given that the size of the expanded vocabulary can be quite large and it may include a lot of over-generation, we rerank the expanded set of words before tak- ing the top n words to use in downstream processes. We consider four reranking con- ditions which we describe below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Morphology Expansion Techniques</head><p>As stated above, the input to the morphology ex- pansion step is a list of words segmented into mor- phemes: zero or more prefixes, one stem, and zero or more suffixes. <ref type="figure" target="#fig_2">Figure 2a</ref> presents an example of such input using English words (for clarity).</p><p>We use two different models of morphology ex- pansion in this paper: Fixed Affix model and Bi- gram Affix model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Fixed Affix Expansion Model</head><p>In the Fixed Affix model, we construct a set of fused prefixes from all the unique prefix sequences in the training data; and we similarly construct a re+ pro+ duc +e func +tion +al re+ duc +e re+ duc +tion +s in pro+ duct concept +u +al + ly (a) Training data with morpheme boundaries. Prefixes end with and suffixes start with "+" signs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Bigram Affix Expansion Model</head><p>In the Bigram Affix model, we do the same for the stem as in the Fixed Affix model, but for prefixes and suffixes, we create a bigram language model in the finite state machine. The advantage of this technique is that unseen compound affixes can be generated by our model. For example, the Fixed Affix model in <ref type="figure" target="#fig_2">Figure 2b</ref> cannot generate the word func+tion+al+ly since the suffix +tionally is not seen in the training data. However, this word can be generated in the Bigram Affix model as shown in <ref type="figure" target="#fig_2">Figure 2c</ref>: there is a path passing 0 → 4 → 1 → 2 → 5 → 6 → 3 in the FST that can produce this word. We expect this model to have better recall for generating new words in the language because of its affixation flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reranking Techniques</head><p>The expanded models allow for a large number of words to be generated. We limit the number of vo- cabulary expansion using different thresholds af- ter reranking or reweighing the WFSTs generated above. We consider four reranking conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">No Reranking (NRR)</head><p>The baseline reranking option is no reranking (NRR). In this approach we use the weights in the WFST, which are based on the independent prefix/stem/suffix probabilities, to determine the ranking of the expanded vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Trigraph-based Reweighting (W•Tr)</head><p>We reweight the weights in the WFST model (Fixed or Bigram) by composing it with a letter trigraph language model (W •Tr). A letter tri- graph LM is itself a WFST where each trigraph (a sequence of three consequent letters) has an asso- ciated weight equal to its negative log-likelihood in the training data. This reweighting allows us to model preferences of sequences of word letters seen more in the training data. For example, in a word like producttions, the trigraphs ctt and tti are very rare and thus decrease its probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Trigraph-based Reranking (TRR)</head><p>When we compose our initial WFST with the tri- graph FST, the probability of each generated word from the new FST is equal to the product of the probability of its morphemes and the probabilities of each trigraph in that word. This basically makes the model prefer shorter words and may degrade the effect of morphology information. Instead of reweighting the WFST, we get the n-best list of generated words and rerank them using their tri- graph probabilities. We will refer to this technique as TRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Reranking Morpheme Boundaries (BRR)</head><p>The last reranking technique reranks the n-best generated word list with trigraphs that are incident on the morpheme boundaries (in case of Bigram Affix model, the last prefix and first suffix). The intuition is that we already know that any mor- pheme that is generated from the morphology FST is already seen in the training data but the bound- ary for different morphemes are not guaranteed to be seen in the training data. For example, for the word producttions, we only take into account the trigraphs rod, odu, ctt and tti instead of all possible trigraphs. We will refer to this technique as BRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Data and Tools</head><p>Evaluation Data The IARPA Babel program is a research program for developing rapid spoken detection systems for under-resourced languages <ref type="bibr" target="#b16">(Harper, 2013)</ref>. We use the IARPA Babel pro- gram limited language pack data which consists of 20 hours of telephone speech with transcrip- tion. We use six languages which are known to have rich morphology: Assamese (IARPA- babel102b-v0.5a), Bengali (IARPA-babel103b- v0.4b), Pashto (IARPA-babel104b-v0.4bY), Taga- log (IARPA-babel106-v0.2g), Turkish (IARPA- babel105b-v0.4) and Zulu (IARPA-babel206b- v0.1e). Speech annotation such as silences and hesitations are removed from transcription and all words are turned into lower-case (for languages using the Roman script -Tagalog, Turkish and Zulu). Moreover, in order to be able to perform a manual error analysis, we include a language that has rich morphology and of which the first author is a native speaker: Persian. We sampled data from the training and development set of the Persian de- pendency treebank ( <ref type="bibr">Rasooli et al., 2013</ref>) to create a comparable seventh dataset in Persian. Statis- tics about the datasets are shown in <ref type="table">Table 1</ref>. We also conduct further experiments on just verbs and nouns in the data set for Persian (Persian-N and Persian V). As shown in <ref type="table">Table 1</ref>, the training data is very small and the OOV rate is high especially in terms of types. For some languages that have richer morphology such as Turkish and Zulu, the OOV rate is much higher than other languages.</p><p>Word Generation Tools and Settings For un- supervised learning of morphology, we use Mor- fessor CAT-MAP (v. 0.9.2) which was shown to be a very accurate morphological analyzer for mor- phologically rich languages <ref type="bibr" target="#b5">(Creutz and Lagus, 2007</ref>). In order to be able to analyze Unicode- based data, we convert each character in our dataset to some conventional ASCII character and then train Morfessor on the mapped dataset; after finishing the training, we map the data back to the original character set. We use the default setting in Morfessor for unsupervised learning.</p><p>For preparing the WFST, we use OpenFST ( <ref type="bibr">Riley et al., 2009</ref>). We get the top one million short- est paths (i.e., least costly paths of words) and ap- ply our reranking models on them. It is worth pointing out that our WFSTs are character-based and thus we also have a morphological analyzer that can give all possible segmentations for a given word. By running the morphological analyzer on the OOVs, we can have the potential upper bound of OOV reduction by the system (labeled "∞" in <ref type="table">Tables 2 and 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lexicon Expansion Results</head><p>The results for lexicon expansion are shown in <ref type="table">Ta- ble 2 for types and Table 3</ref> for tokens.</p><p>We use the trigraph WFST as our baseline model. This model does not use any morphologi- cal information. In this case, words are generated according to the likelihood of their trigraphs, with- out using any information from the morphologi- cal segmentation. We call this model the trigraph WFST (Tr. WFST). We consistently have better numbers than this baseline in all of our models except for Pashto when measured by tokens. ∞ is the upper-bound OOV reduction for our expan- sion model: for each word in the development set, we ask if our model, without any vocabulary size restriction at all, could generate it.</p><p>The best results (again, except for Pashto) are achieved using one of the three reranking methods (reranking by trigraph probabilities or morpheme boundaries) as opposed to doing no reranking. To our surprise, the Fixed Affix model does a slightly better job in reducing out of vocabulary than the Bigram Affix model. We can also see from the results that reranking in general is very effective.</p><p>We also compare our models with the case that there is much more training data and we do not do vocabulary expansion at all. In <ref type="table">Table 2</ref> and Ta- ble 3, "FP" indicates the full language pack for the Babel project data which is approximately six to eight times larger than the limited pack training data, and the full training data for Persian which is approximately five times larger. We see that the larger training data outperforms our methods in all languages. However, from the results of ∞, which is the upper-bound OOV reduction by our expansion model, for some languages such as As- samese, our numbers are close to the FP results and for Zulu it is even better than FP.</p><p>We also study how OOV reduction is affected by the size of the generated vocabulary. The trends for different sizes of the lexicon expansion by Fixed Affix model that is reranked by trigraph probabilities is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. As seen in the results, for languages that have richer morphol- ogy, it is harder to achieve results near to the up- per bound. As an outlier, morphology does not help for Pashto. One possible reason might be that based on the results in <ref type="table" target="#tab_4">Table 4</ref>, Morfessor does not explore morphology in Pashto as well as other lan- guages.</p><p>Morphological Complexity As for further anal- ysis, we can study the correlation between mor- phological complexity and hardness of reducing OOVs. Much work has been done in linguis- tics to classify languages <ref type="bibr">(Sapir, 1921;</ref><ref type="bibr" target="#b13">Greenberg, 1960)</ref>. The common wisdom is that languages are not either agglutinative or fusional, but are on a spectrum; however, no work to our knowl- edge places all languages (or at least the ones we worked on) on such a spectrum. We propose sev- eral metrics. First, we can consider the number of unique affixival morphemes in each language, as determined by Morfessor. As shown in <ref type="table" target="#tab_4">Table 4</ref> (|pr| + |sf |), Zulu has the most morphemes and Pashto the fewest. A second possible metric of the  <ref type="table">Table 2</ref>: Type-based expansion results for the 50k-best list for different models. Tr. WFST stands for trigraph WFST, NRR for no reranking, W•Tr for trigraph reweighting, TRR for trigraph-based rereank- ing, BRR for reranking morpheme boundary, and ∞ for the upper bound of OOV reduction via lexicon expansion if we produce all words. FP (full-pack data) shows the effect of using bigger data with the size of about seven times larger than our data set, instead of using our unsupervised approach.  <ref type="table">Table 3</ref>: Token-based expansion results for the 50k-best list for different models. Abbreviations are the same as <ref type="table">Table 2.</ref> complexity of the morphology is by calculating the average number of unique prefix-suffix pairs in the training data after morpheme segmentation which is shown as |If | in <ref type="table" target="#tab_4">Table 4</ref>. Finally, a third possible metric is the number of all possible words that can be generated (|L|). These three metrics correlate fairly well across the languages.</p><note type="other">Language Tr. Fixed Affix Model Bigram Affix Model FP WFST</note><p>The metrics we propose also correlate with commonly accepted classifications: e.g., Zulu and Turkish (highly agglutinative) have higher scores in terms of our |pr| + |sf |, |If | and |L| metrics in <ref type="table" target="#tab_4">Table 4</ref> than other languages. The results from full language packs in <ref type="table">Table 3</ref> also show that there is a reverse interaction of morphological complexity and the effect of blindly adding more data. Thus for morphologically rich languages, adding more data is less effective than for languages with poor morphology.</p><p>The size of the languages (|L|) suggests that we are suffering from vast overgeneration; we over- generate because in our model any affix can at- tach to any stem, which is not in general true. Thus there is a lack of linguistic knowledge such as paradigm information <ref type="bibr">(Stump, 2001</ref>) for each word category in our model. In other words, all morphemes are treated the same in our model which is not true in natural languages. One way to tackle this problem is through an unsupervised POS tagger. The challenge here is that fully unsu- pervised POS taggers (without any tag dictionary) are not very accurate <ref type="bibr" target="#b3">(Christodoulopoulos et al., 2010)</ref>. Another way is through using joint mor-   phology and tagging models such as <ref type="bibr" target="#b12">Frank et al. (2013)</ref>.</p><p>Error Analysis on Turkish Unfortunately for most languages we could not find an available rule-based or supervised morphological analyzer to verify the words generated by our model. The only available tool for us is a Turkish finite-state morphological analyzer <ref type="bibr">(Oflazer, 1996)</ref> imple- mented with the Xerox FST toolkit <ref type="bibr" target="#b1">(Beesley and Karttunen, 2003)</ref>. As we can see in  <ref type="table" target="#tab_5">Table 5</ref>: Results from running a hand-crafted Turkish morphological analyzer <ref type="bibr">(Oflazer, 1996)</ref> on different expansions and on the development set. Precision refers to the percentage of the words are recognized by the analyzer. The results on de- velopment are also separated into INV and OOV.</p><p>Error Analysis on Persian From the best 50k word result for Persian (Fixed Affix Model:BRR), we randomly picked 200 words and manually an- alyzed them. 89 words are correct (45.5%) where 55.0% of these words are from noun affixation, 23.6% from verb clitics, 9.0% from verb inflec- tions, 5.6% from incorrect affixations that acci- dentally resulted in possible words, 4.5% from un- inflected stems, and a few from adjective affixa- tion. Among incorrectly generated words, 65.8% are from combining a stem of one POS with af- fixes from another POS (e.g., attaching a noun af- fix to a verb stem), 14.4% from combining a stem with affixes which are compatible with POS but not allowed for that particular stem (e.g., there is a noun suffix that can only attach to a subset of noun stems), 9.0% are from wrong affixes pro- duced by Morfessor and others are from incorrect vowel harmony or double affixation. In order to study the effect of vocabulary ex- pansion more deeply, we trained a subset of all nouns and verbs in the same dataset (also shown in <ref type="table">Table 1</ref>). Verbs in Persian have rich but more or less regular morphology, while nouns, which have many irregular cases, have rich morphol- ogy but not as rich as verbs. The results in <ref type="table" target="#tab_4">Ta- ble 4</ref> show that Morfessor captures these phenom- ena. Furthermore, our results in <ref type="table">Table 2</ref> and <ref type="table">Ta- ble 3</ref> show that our performance on OOV reduc- tion with verbs is far superior to our performance with nouns. We also randomly picked 200 words from each of the experiments (noun and verbs) to study the degree of correctness of generated forms. For nouns, 94 words are correct and for verbs only 71 words are correct. Most verb errors are due to incorrect morpheme extraction by Mor- fessor. In contrast, most noun errors result from affixes that are only compatible with a subset of all possible noun stems. This suggests that if we conduct experiments using more accurate unsu- pervised morphology and also have a more fine- grained paradigm completion model, we might improve our performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We have presented an approach to generating new words. This approach is useful for low-resource, morphologically rich languages. It provides words that can be used in HLT applications that require target-language generation in this language, such as ASR, OCR, and MT. An implementation of our approach, named BabelGUM (Babel General Un- supervised Morphology), will be publicly avail- able. Please contact the authors for more infor- mation.</p><p>In future work we will explore the possibil- ity of jointly performing unsupervised morpho- logical segmentation with clustering of words into classes with similar morphological behavior. These classes will extend POS classes. We will tune the system for our purposes, namely OOV re- duction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The flowchart of the lexicon expansion system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two models of word generation from morphologically annotated data. In our experiments, we used weighted finite state machine. We use character-based WFST in the implementation to facilitate analyzing inputs as well as word generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Trends for token-based OOV reduction with different sizes for the Fixed Affix model with trigraph reranking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>NRR W•Tr TRR BRR ∞ NRR W•Tr TRR BRR ∞ Assamese 15.94 24.03 28.46 28.15 27.15 48.07 23.50 28.15 27.84 26.59 51.02 50.24.67 22.74 22.83 24.15 37.32 23.78 21.68 22.51 23.32 38.38 - Persian-V 54.84 68.19 72.39 73.49 71.12 80.44 67.28 71.48 72.58 70.02 80.62 -</figDesc><table>96 
Bengali 
15.68 20.09 24.75 24.49 22.54 40.98 21.78 24.65 24.67 23.51 42.55 48.83 
Pashto 
18.70 19.03 19.28 19.24 18.63 25.13 19.43 18.81 18.92 18.77 25.24 64.96 
Persian 
12.83 18.95 18.39 19.30 19.99 50.11 18.58 18.09 18.65 18.84 53.13 58.45 
Tagalog 
11.39 14.61 16.51 16.21 16.81 35.64 14.45 16.01 15.81 16.74 38.72 53.64 
Turkish 
07.75 09.11 14.79 14.79 14.71 55.48 09.04 13.63 14.34 13.52 66.54 53.54 
Zulu 
07.63 11.87 12.96 13.87 13.68 66.73 12.04 12.35 13.69 13.75 82.38 35.62 
Average 
12.85 16.81 19.31 19.31 19.07 46.02 17.02 18.81 19.13 18.81 51.37 52.29 
Persian-N 14.86 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Information about the number of unique 
morphemes in the Fixed Affix model for each 
dataset including empty affixes. |L| shows the 
upper bound of the number of possible unique 
words that can be generated from the word gener-
ation model. |If | is the average number of unique 
prefix-suffix pairs (including empty pairs) for each 
stem. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 ,</head><label>5</label><figDesc></figDesc><table>the 
system with the largest proportion of correct gen-
erated words reranks the expansion with trigraph 
probabilities using a Fixed Affix model. Results 
also show that we are overgenerating many non-
sense words that we ought to be pruning from our 
results. Another observation is that the recognition 
percentage of the morphological analyzer on INV 
words is much higher than on OOVs, which shows 
that OOVs in Turkish dataset are much harder to 
analyze. </table></figure>

			<note place="foot" n="1"> http://en.wikipedia.org/wiki/ Languages_used_on_the_Internet 2 http://meta.wikimedia.org/wiki/List_ of_Wikipedias</note>

			<note place="foot" n="3"> In this paper, we use an off-the-shelf system for this step but plan to explore new methods in the future, such as joint segmentation and expansion.</note>

			<note place="foot" n="4"> We convert the probability into a cost by taking the negative of the log of the probability.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Anahita Bhiwandiwalla, Brian Kings-bury, Lidia Mangu, Michael Picheny, BenoˆıtBenoˆıt Sagot, Murat Saraclar, and Géraldine Walther for helpful discussions. The project is supported by the Intelligence Advanced Research Projects Ac-tivity (IARPA) via Department of Defense U.S. Army Research Laboratory (DoD/ARL) contract number W911NF-12-C-0012. The U.S. Govern-ment is authorized to reproduce and distribute reprints for Governmental purposes notwithstand-ing any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scaling to very very large corpora for natural language disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL &apos;01</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics, ACL &apos;01<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Finitestate morphology: Xerox tools and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauri</forename><surname>Beesley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karttunen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>CSLI</publisher>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Buckwalter Arabic Morphological Analyzer Version 2.0. LDC catalog number LDC2004L02</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Buckwalter</surname></persName>
		</author>
		<idno>1-58563-324-0</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two decades of unsupervised pos induction: How far have we come?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Morphology based automatic acquisition of large-coverage lexica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Clément</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04). European Language Resources Association (ELRA)</title>
		<meeting>the Fourth International Conference on Language Resources and Evaluation (LREC&apos;04). European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised models for morpheme segmentation and morphology learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><surname>Lagus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing (TSLP)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bootstrapping a multilingual part-of-speech tagger in one person-day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 6th Conference on Natural Language Learning (CoNLL-2002)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Smart paradigms and the predictability and complexity of inflectional morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Détrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarne</forename><surname>Ranta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="645" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering morphological paradigms from plain text using a dirichlet process mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="616" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised learning of complete morphological paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1185" to="1195" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic extraction of morphological lexicons from morphologically annotated corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Eskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1032" to="1043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Morphological lexicon extraction from raw text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Forsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Hammarström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarne</forename><surname>Ranta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="488" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring the utility of joint morphological and syntactic learning from child-directed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="30" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A quantitative approach to the morphological typology of language. International journal of American linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="page" from="178" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MAGEAD: A morphological analyzer and generator for the Arabic dialects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<publisher>Australia</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The babel program and low resource speech technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Invited talk</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two-Level Model for Morphological Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimmo</forename><surname>Koskenniemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 8th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page" from="683" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Data-driven lexicon expansion for Mandarin broadcast news and conversation speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="4329" to="4332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Paramor: Finding paradigms across morphology. Advances in Multilingual and Multimodal Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Monson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="900" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of morphology without morphemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Neuvel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fulop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 workshop on Morphological and phonological learning</title>
		<meeting>the ACL-02 workshop on Morphological and phonological learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
