<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vancouver Welcomes You! Minimalist Location Metonymy Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Gritta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Nut Limsopatham</roleName><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
						</author>
						<title level="a" type="main">Vancouver Welcomes You! Minimalist Location Metonymy Resolution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1248" to="1259"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1115</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications , such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries , external word lists and other hand-crafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the Se-mEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In everyday language, we come across many types of figurative speech. These irregular expressions are understood with little difficulty by humans but require special attention in NLP. One of these is metonymy, a type of common figurative language, which stands for the substitution of the concept, phrase or word being meant with a semantically related one. For example, in "Moscow traded gas and aluminium with Beijing.", both location names were substituted in place of governments.</p><p>Named Entity Recognition (NER) taggers have no provision for handling metonymy, meaning that this frequent linguistic phenomenon goes largely undetected within current NLP. Classi- fication decisions presently focus on the entity using features such as orthography to infer its word sense, largely ignoring the context, which provides the strongest clue about whether a word is used metonymically. A common classifica- tion approach is choosing the N words to the immediate left and right of the entity or the whole paragraph as input to the model. However, this "greedy" approach also processes input that should in practice be ignored.</p><p>Metonymy is problematic for applications such as Geographical Parsing ( <ref type="bibr" target="#b24">Monteiro et al., 2016;</ref><ref type="bibr" target="#b10">Gritta et al., 2017</ref>, GP) and other information extraction tasks in NLP. In order to accurately identify and ground location entities, for example, we must recognise that metonymic entities consti- tute false positives and should not be treated the same way as regular locations. For example, in "London voted for the change.", London refers to the concept of "people" and should not be classified as a location. There are many types of metonymy ( <ref type="bibr" target="#b37">Shutova et al., 2013)</ref>, however, in this paper, we primarily address metonymic location mentions with reference to GP and NER.</p><p>Contributions: (1) We investigate how to improve classification tasks by introducing a novel minimalist method called Predicate Window (PreWin), which outperforms common feature se- lection baselines. Our final minimalist classifier is comparable to systems which use many external features and tools. (2) We improve the annota- tion guidelines in MR and contribute with a new Wikipedia-based MR dataset called ReLocaR to address the training data shortage. (3) We make an annotated subset of the CoNLL 2003 (NER) Shared Task available for extra MR training data, alongside models, tools and other data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Some of the earliest work on MR that used an approach similar to our method (machine learning and dependency parsing) was by <ref type="bibr" target="#b29">Nissim and Markert (2003a)</ref>. The decision list classifier with backoff was evaluated using syntactic head-modifier relations, grammatical roles and a thesaurus to overcome data sparseness and generalisation problems. However, the method was still limited for classifying unseen data. Our method uses the same paradigm but adds more features, a different machine learning architecture and a better usage of the parse tree structure.</p><p>Much of the later work on MR comes from the <ref type="bibr">SemEval 2007 Shared Task 8 (Markert and</ref><ref type="bibr" target="#b19">Nissim, 2007)</ref> and later by <ref type="bibr" target="#b20">Markert and Nissim (2009)</ref>. The feature set of <ref type="bibr" target="#b29">Nissim and Markert (2003a)</ref> was updated to include: grammatical role of the potentially metonymic word (PMW) (such as subj, obj), lemmatised head/modifier of PMW, determiner of PMW, grammatical number of PMW (singular, plural), number of words in PMW and number of grammatical roles of PMW in current context. The winning system by <ref type="bibr" target="#b7">Farkas et al. (2007)</ref> used these features and a maximum entropy classifier to achieve 85.2% accuracy. This was also the "leanest" system but still made use of feature engineering and some external tools. <ref type="bibr" target="#b1">Brun et al. (2007)</ref> achieved 85.1% accuracy using local syntactical and global distributional features generated with an adapted, proprietary Xerox deep parser. This was the only unsupervised approach, based on using syntactic context simi- larities calculated on large corpora such as the the British National Corpus (BNC) with 100M tokens. <ref type="bibr" target="#b26">Nastase and Strube (2009)</ref> used a Support Vec- tor Machine (SVM) with handcrafted features (in addition to the features provided by <ref type="bibr" target="#b19">Markert and Nissim (2007)</ref>) including grammatical colloca- tions extracted from the BNC to learn selectional preferences, WordNet 3.0, Wikipedia's category network, whether the entity "has-a-product" such as Suzuki and whether the entity "has-an-event" such as Vietnam (both obtained from Wikipedia). The bigger set of around 60 features and leverag- ing global (paragraph) context enabled them to achieve 86.1% accuracy. Once again, we draw attention to the extra training, external tools and additional feature generation. Similar recent work by <ref type="bibr" target="#b27">Nastase and Strube (2013)</ref> which extends that of <ref type="bibr" target="#b25">Nastase et al. (2012)</ref> involved transforming Wikipedia into a large-scale multilingual concept network called WikiNet. By building on Wikipedia's existing network of categories and articles, their method automatically discovers new relations and their instances. As one of their extrinsic evaluations, metonymy resolution was tested. Global context (whole paragraph) was used to interpret the target word. Using an SVM and a powerful knowledge base built from Wikipedia, the highest perfor- mance to date (a 0.1% improvement from <ref type="bibr" target="#b26">Nastase and Strube (2009)</ref>) was achieved at 86.2%, which has remained the SOTA until now.</p><p>The related work on MR so far has made limited use of dependency trees. Typical features came in the form of a head dependency of the target en- tity, its dependency label and its role (subj-of-win, dobj-of-visit, etc). However, other classification tasks made good use of dependency trees. <ref type="bibr" target="#b16">Liu et al. (2015)</ref> used the shortest dependency path and dependency sub-trees successfully to improve re- lation classification (new SOTA on SemEval 2010 Shared Task). <ref type="bibr" target="#b2">Bunescu and Mooney (2005)</ref> show that using dependency trees to generate the input sequence to a model performs well in relation ex- traction tasks. <ref type="bibr" target="#b6">Dong et al. (2014)</ref> used dependency parsing for Twitter sentiment classification to find the words syntactically connected to the target of interest. <ref type="bibr" target="#b13">Joshi and Penstein-Rosé (2009)</ref> used de- pendency parsing to explore how features based on syntactic dependency relations can be used to improve performance on opinion mining. In unsu- pervised lymphoma (type of cancer) classification, <ref type="bibr" target="#b17">Luo et al. (2014)</ref> constructed a sentence graph from the results of a two-phase dependency parse to mine pathology reports for the relationships be- tween medical concepts. Our methods also exploit the versatility of dependency parsing to leverage information about the sentence structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SemEval 2007 Dataset</head><p>Our main standard for performance evaluation is the <ref type="bibr">SemEval 2007 Shared Task 8 (Markert and</ref><ref type="bibr" target="#b19">Nissim, 2007</ref>) dataset first introduced in <ref type="bibr" target="#b31">Nissim and Markert (2003b)</ref>. Two types of entities were evaluated, organisations and locations, randomly retrieved from the British National Corpus (BNC).</p><p>We only use the locations dataset, which com- prises a train (925 samples) and a test (908 sam- ples) partition. For medium evaluation, the classes are literal (geographical territories and political entities), metonymic (place-for-people, place-for- product, place-for-event, capital-for-government or place-for-organisation) and mixed (metonymic and literal frames invoked simultaneously or un- able to distinguish). The metonymic class further breaks down into two levels of subclasses allowing for fine evaluation. The class distribution within SemEval is approx 80% literal, 18% metonymic and 2% mixed. This seems to be the approxi- mate natural distribution of the classes for location metonymy, which we have also observed while sampling Wikipedia for our new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Our contribution broadly divides into two main parts, data and methodology. Section 3 introduces our new dataset, Section 4 introduces our new fea- ture extraction method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design and Motivation</head><p>As part of our contribution, we created a new MR dataset called ReLocaR (Real Location Retrieval), partly due to the lack of quality annotated train/test data and partly because of the shortcomings with the SemEval 2007 dataset (see Section 3.2). Our corpus is designed to evaluate the capability of a classifier to distinguish literal, metonymic and mixed location mentions. In terms of dataset size, ReLocaR contains 1,026 training and 1,000 test in- stances. The data was sampled using Wikipedia's Random Article API 1 . We kept the sentences, which contained at least one of the places from a manually compiled list 2 of countries and capitals of the world. The natural distribution of literal ver- sus metonymic examples is approximately 80/20 so we had to discard the excess literal examples during sampling to balance the classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ReLocaR -Improvements over SemEval</head><p>1. We do not break down the metonymic class further as the distinction between the subclasses is subtle and hard to agree on.</p><p>2. The distribution of the three classes in ReLo- caR (literal, metonymic, mixed) is approximately (49%, 49%, 2%) eliminating the high bias (80%, 18%, 2%) of SemEval. We will show how such a high bias transpires in the test results (Section 5).</p><p>3. We have reviewed the annotation of the test partition and found that we disagreed with up to 11% of the annotations. <ref type="bibr" target="#b40">Zhang and Gelernter (2015)</ref> disagreed with the annotation 8% of the time. <ref type="bibr" target="#b35">Poibeau (2007)</ref> also challenged some annotation decisions. ReLocaR was annotated by 4 trained linguists (undergraduate and graduate) and 2 computational linguists (authors). Linguists were independently instructed (see section 3.3) to assign one of the two classes to each example with little guidance. We leveraged their linguistic training and expertise to make decisions rather than imposing some specific scheme. Unresolved sentences would receive the mixed class label.</p><p>4. The most prominent difference is a small change in the annotation scheme (after indepen- dent linguistic advice). The SemEval 2007 Task 8 annotation scheme <ref type="bibr" target="#b19">(Markert and Nissim, 2007)</ref> considers the political entity interpretation a lit- eral reading. It suggests that in "Britain's cur- rent account deficit...", Britain refers to a literal location, rather than a government (which is an organisation). This is despite acknowledging that "The locative and the political sense is often dis- tinguished in dictionaries as well as in the ACE annotation scheme...". In ReLocaR datasets, we consider a political entity a metonymic reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Why government is not a location</head><p>A government/nation/political entity is semanti- cally much closer to Organisation/Person than a Location. "Moscow talks to Beijing." does not tell us where this is happening. It most likely means a politician is talking to another politician. These are not places but people and/or groups. It is paramount to separate references to "inanimate" places from references to "animate" entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotation Guidelines (Summary)</head><p>ReLocaR has three classes, literal, metonymic and mixed. Literal reading comprises territorial interpretations (the geographical territory, the land, soil and physical location) i.e. inanimate places that serve to point to a set of coordi- nates (where something might be located and/or happening) such as "The treaty was signed in Italy.", "Peter comes from Russia.", "Britain's Andy Murray won the Grand Slam today.", "US companies increased exports by 50%.", "China's artists are among the best in the world." or "The reach of the transmission is as far as Brazil.".</p><p>A metonymic reading is any location oc- currence that expresses animacy <ref type="bibr" target="#b5">(Coulson and Oakley, 2003)</ref> such as "Jamaica's indifference will not improve the negotiations.", "Sweden's budget deficit may rise next year.". The following are other metonymic scenarios: a location name, which stands for any persons or organisations associated with it such as "We will give aid to Afghanistan.", a location as a product such as "I really enjoyed that delicious Bordeaux.", a location posing as a sports team "India beat Pakistan in the playoffs.", a governmental or other legal entity posing as a location "Zambia passed a new justice law today.", events acting as locations "Vietnam was a bad experience for me".</p><p>The mixed reading is assigned in two cases: ei- ther both readings are invoked at the same time such as in "The Central European country of Slo- vakia recently joined the EU." or there is not enough context to ascertain the reading i.e. both are plausible such as in "We marvelled at the art of ancient Mexico.". In difficult cases such as these, the mixed class is assigned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inter-Annotator Agreement</head><p>We give the IAA for the test partition only. The whole dataset was annotated by the first author as the main annotator. Two pairs of annotators (4 lin- guists) then labelled 25% of the dataset each for a 3-way agreement. The agreement before adjudi- cation was 91% and 93%, 97.2% and 99.2% after adjudication (for pair one and two respectively). The other 50% of sentences were then once again labelled by the main annotator with a 97% agree- ment with self. The remainder of the sentences (unable to agree on among annotators even after adjudication) were labelled as a mixed class (1.8% of all sentences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CoNLL 2003 and MR</head><p>We have also annotated a small subset of the CoNLL 2003 NER Shared Task data for metonymy resolution (locations only). Respect- ing the Reuters RCV1 Corpus ( <ref type="bibr" target="#b15">Lewis et al., 2004</ref>) distribution permissions <ref type="bibr">3</ref> , we make only a heav- ily processed subset available on GitHub <ref type="bibr">4</ref> . There are 4,089 positive (literal) and 2,126 negative (metonymic) sentences to assist with algorithm ex- perimentation and model prototyping. Due to the lack of annotated training data for MR, this is a valuable resource. The data was annotated by the first author, there are no IAA figures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Predicate Window (PreWin)</head><p>Through extensive experimentation and observa- tion, we arrived at the intuition behind PreWin, our novel feature extraction method. The classi- fication decision of the class of the target entity is mostly informed not by the whole sentence (or paragraph), rather it is a small and focused "predicate window" pointed to by the entity's head dependency. In other words, most of the sentence is not only superfluous for the task, it actually lowers the accuracy of the model due to irrelevant input. This is particularly important in metonymy resolution as the entity's surface form is not taken into consideration, only its context.</p><p>In <ref type="figure">Figure 1</ref>, we show the process of extracting the Predicate Window from a sample sentence (more examples are available in the Appendix). We start by using the SpaCy dependency parser by <ref type="bibr" target="#b12">Honnibal and Johnson (2015)</ref>, which is the fastest in the world, open source and highly customisable. Each dependency tree provides the following features: dependency labels and entity head dependency. Rather than using most of the tree, we only use a single local head dependency relationship to point to the predicate. Leveraging a dependency parser helps PreWin with selecting the minimum relevant input to the model while discarding irrelevant input, which may cause the neural model to behave unpredictably. Finally, the entity itself is never used as input in any of our methods, we only rely on context.</p><p>PreWin then extracts up to 5 words and their dependency labels starting at the head of the entity (see the next paragraph for exceptions), going in the away (from the entity) direction. The method always skips the conjunct ("and", "or") <ref type="figure">Figure 1</ref>: The predicate window starts at the head of the target entity and ends up to 4 words further, going away from the entity. The "conj" relations are always skipped. In the above example, the head of "UK" is "decided" so PreWin takes 5 words plus dependency labels as the input to the model. The left-hand side input to the model is empty and is set to zeroes (see <ref type="figure">Figure 2</ref> for a full model diagram).</p><p>relationships in order to find the predicate (see <ref type="figure" target="#fig_0">Figure 3</ref> in the Appendix for a visual example of why this is important). The reason for the choice of 5 words is the balance between too much input, feeding the model with less relevant context and just enough context to capture the necessary semantics. We have experimented with lengths of 3-10 words, however 5 words typically achieved the best results.</p><p>The following are the three types of exceptions when the output will not start with the head of the entity. In these cases, PreWin will include the neighbouring word as well. In a sentence "The pub is located in southern Zambia.", the head of the entity is "in", however in this case PreWin will include "southern" (adjectival modifier) as this carries important semantics for the classification. Similarly, PreWin will also include the neighbour- ing compound noun as in: "Lead coffins were very rare in colonial America.", the output will include "colonial" as a feature plus the next four words. In another sentence: "Vancouver's security is the best in the world.', PreWin will include the "'s" (case) plus the next four words continuing from the head of the entity (the word "security").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Neural Network Architecture</head><p>The output of PreWin is used to train the following machine learning model. We decided to use the Long Short Term Memory (LSTM) architecture by Keras 5 <ref type="bibr" target="#b3">(Chollet, 2015)</ref>. Two LSTMs are used, one for the left and right side (up to 5 words each). Two fully connected (dense) layers are used for the left and right dependency relation labels (up to 5 labels each, encoded as one-hot). The full ar- chitecture is available in the Appendix, please see <ref type="figure">Figure 2</ref>. You can download the models and data from GitHub 6 . LSTMs are excellent at process- ing language sequences <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b36">Sak et al., 2014;</ref><ref type="bibr" target="#b9">Graves et al., 2013)</ref>, which is why we use this architecture. It allows the model to encode the word sequences, preserve im- portant word order and provide superior classifica- tion performance. Both the Multilayer Perceptron and the Convolutional Neural Network were con- sistently inferior (typically 5% -10% lower accu- racy) in our earlier performance comparisons. For all experiments, we used a vocabulary of the first (most frequent) 100,000 word vectors in GloVe <ref type="bibr">7</ref> ( <ref type="bibr" target="#b33">Pennington et al., 2014</ref>). Finally, unless explicitly stated otherwise, the standard dimension of word embeddings was 50, which we found to work best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">"Immediate" Baseline</head><p>A common approach in lexical classification tasks is choosing the 5 to 10 words to the immediate right and left of the entity as input to a model ( <ref type="bibr" target="#b23">Mikolov et al., 2013;</ref><ref type="bibr" target="#b22">Mesnil et al., 2013;</ref><ref type="bibr" target="#b0">Baroni et al., 2014;</ref><ref type="bibr" target="#b4">Collobert et al., 2011</ref>). We evaluate this method (its 5 and 10-word variant) alongside PreWin and Paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Paragraph Baseline</head><p>The paragraph baseline method extends the "im- mediate" one by taking 50 words from each side of the entity as the input to the classifier. In practice, this extends the feature window to include extra- sentential evidence in the paragraph. This ap-proach is also popular in machine learning <ref type="bibr" target="#b21">(Melamud et al., 2016;</ref><ref type="bibr" target="#b39">Zhang et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ensemble of Models</head><p>In addition to a single best performing model, we have combined several models trained on different data and/or using different model configurations. For the SemEval test, we combined three separate models trained on the newly annotated CoNLL dataset and the training data for SemEval. For the ReLocaR test, we once again let three models vote, trained on CooNLL and ReLocaR data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We evaluate all methods using three datasets for training (ReLocaR, SemEval, CoNLL) and two for testing (ReLocaR, SemEval). Due to inherent randomness in the deep learning libraries, we per- formed 10 runs for each setup and averaged the figures (we also report standard deviation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Metrics and Significance</head><p>Following the SemEval 2007 convention, we use two metrics to evaluate performance, accuracy and f-scores (for each class). We only evaluate at the coarse level, which means literal versus non- literal (metonymic and mixed are merged into one class). In terms of statistical significance, our best score on the SemEval dataset (908 samples) is not significant at the 95% confidence level. However, the accuracy improvements of PreWin over the common baselines are highly statistically signifi- cant with 99.9%+ confidence. <ref type="table" target="#tab_2">Tables 1 and 2</ref> show PreWin performing con- sistently better than other baselines, in many instances, significantly better and with fewer words (smaller input). The standard deviation is also lower for PreWin meaning more stable test runs. Compared with the 5 and 10 window "im- mediate" baseline, which is the common approach in classification, PreWin is more discriminating with its input. Due to the linguistic variety and the myriad of ways the target word sense can be triggered in a sentence, it is not always the case that the 5 or 10 nearest words inform us of the target entity's meaning/type. We ought to ask what else is being expressed in the same 5 to 10-word window?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Predicate Window</head><p>Conventional classification methods (Immedi- ate, Paragraph) can also be seen as prioritising either feature precision or feature recall. Para- graph maximises the input sequence size, which maximises recall at the expense of including features that are either irrelevant or mislead the model, lowering precision. Immediate baseline maximises precision by using features close to the target entity at the expense of missing important features positioned outside of its small window, lowering recall. PreWin can be understood as an integration of both approaches. It retains high precision by limiting the size of the feature window to 5 while maximising recall by searching anywhere in the sentence, frequently outside of a limited "immediate" window.</p><p>Perhaps we can also caution against a simple adherence to <ref type="bibr" target="#b8">Firth (1957)</ref> "You shall know a word by the company it keeps". This does not appear to be the case in our experiments as PreWin reg- ularly performs better than the "immediate" base- line. Further prototypical examples of the method can be viewed in the Appendix. Our intuition that most words in the sentence, indeed in the para- graph do not carry the semantic information re- quired to classify the target entity is ultimately based on evidence. The model uses only a small window, linked to the entity via a head dependency relationship for the final classification decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Common Errors</head><p>Most of the time (typically 85% for the two datasets), PreWin is sufficient for an accurate clas- sification. However, it does not work well in some cases. The typical 15% error rate breaks down as follows (percentages were estimated based on extensive experimentation and observation):</p><p>Discarding important context (3%): Some- times the 5 or 10 word "immediate" baseline method would actually have been preferred such as in the sentence "...REF in 2014 ranked Essex in the top 20 universities...". PreWin discards the right-hand side input, which is required in this case for a correct classification. Since "ranked" is the head of "Essex", the rest of the sentence gets ignored and the valuable context gets lost.</p><p>More complex semantic patterns (11%): Many common mistakes were due to the lack of the model's understanding of more complex predicates such as in the following sentences: " ...of military presence of Germany.", "Houston also served as a member and treasurer of the..." or "...invitations were extended to Yugoslavia ...". We think this is due to a lack of training data (around 1,000 sentences per dataset). Additional examples such as "...days after the tour had exited Belgium." expose some of the limitations of the neural model to recognise uncommon ways of expressing a reference to a literal place. Recall that no external resources or tools were used to supplement the training/features, the model had to learn to generalise from what it has seen during training, which was limited in our experiments.</p><p>Parsing mistakes (1%): were less common though still present. It is important to choose the right dependency parser for the task since different parsers will often generate slightly different parse trees. We have used SpaCy 8 for all our experi- ments, which is a Python-based industrial strength NLP library. Sometimes, tokenisation errors for acronyms like "U.S.A." and wrongly hyphenated words may also cause parsing errors, however, this was infrequent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Flexibility of Neural Model</head><p>The top accuracy figures for ReLocaR are almost identical to SemEval. The highest single model accuracy for ReLocaR was 83.6% (84.8% with Ensemble), which was within 0.5% of the equiv- alent methods for SemEval (83.1%, 84.6% for Ensemble). Both were achieved using the same methods (PreWin or Ensemble), neural architec- ture and size of corpora. When the models were trained on the CoNLL data, the accuracies were 82.8% and 79.5%. However, when the models trained on ReLocaR and tested on SemEval (and vice versa), accuracy dropped to between 62.4% and 69% showing that what was learnt does not seem to transfer well to another dataset. We think the reason for this is the difference in annotation guidelines; the government is a metonymic read- ing, not a literal one. This causes the model to make more mistakes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Ensemble Method</head><p>The highest accuracy and f-scores were achieved with the ensemble method for both datasets. We combined three models (previously described in section 4.5) for SemEval to achieve 84.6% accu- racy and three models for ReLocaR to achieve 84.8% for the new dataset. Training separate mod- els with different parameters and/or on different datasets does increase classification capability as various models learn distinct aspects of the task, enabling the 1.2 -1.5% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Dimensionality of Word Embeddings</head><p>We found that increasing dimension size (up to 300) did not materially improve performance.</p><p>The neural network tended to overfit, even with fewer epochs, the results were comparable to our default 50-dimensional embeddings. We posit that fewer dimensions of the distributed word representations force the abstraction level higher as the meaning of words must be expressed more succinctly. We think this helps the model generalise better, particularly for smaller datasets. Lastly, learning word embeddings from scratch on datasets this small (around 1,000 samples) is pos- sible but impractical, the performance typically decreases by around 5% if word embeddings are not initialised first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset / Method Literal Non-Literal</head><p>SemEval / PreWin 90.6 57.3 SemEval / SOTA 91.6 59.1</p><p>ReLocaR / PreWin 84.4 84.8 <ref type="table">Table 3</ref>: Per class f-scores -all figures obtained us- ing the Ensemble method, averaged over 10 runs. Note the model class bias for SemEval. <ref type="table">Table 3</ref> shows the SOTA f-scores, our best results for SemEval 2007 and the best f-scores for ReLo- caR. The class imbalance inside SemEval (80% literal, 18% metonymic, 2% mixed) is reflected as a high bias in the final model. This is not the case with ReLocaR and its 49% literal, 49% metonymic and 2% mixed ratio of 3 classes. The model was equally capable of distinguishing be- tween literal and non-literal cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">F-Scores and Class Imbalance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Another baseline</head><p>There was another baseline we tested, however, it was not covered anywhere so far because of its low performance. It was a type of extreme parse tree pruning, during which most of the sentence gets discarded and we only retain 3 to 4 content words. The method uses non-local (long range) dependencies to construct a short input sequence. However, the method was a case of ignoring too many relevant words and accuracy was fluctuating in the mid-60% range, which is why we did not re- port the results. However, it serves to further jus- tify the choice of 5 words as the predicate window as fewer words caused the model to underperform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">NER, GP and Metonymy</head><p>We think the next frontier is a NER tagger, which actively handles metonymy. The task of labelling entities should be mainly driven by context rather than the word's surface form. If the target entity looks like "London", this should not mean the entity is automatically a location. Metonymy is a frequent linguistic phenomenon (around 20% of location mentions are metonymic, see section 3.1) and could be handled by NER taggers to enable many innovative downstream NLP applications.</p><p>Geographical Parsing is a pertinent use case. In order to monitor/mine text documents for geo- graphical information only, the current NER tech- nology does not have a solution. We think it is in- correct for any NER tagger to label "Vancouver" as a location in "Vancouver welcomes you!". A better output might be something like the follow- ing: Vancouver = location AND metonymy = True. This means Vancouver is usually a location but is used metonymically in this case. How this infor- mation is used will be up to the developer. Organ- isations behaving as persons, share prices or prod- ucts are but a few other examples of metonymy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Simplicity and Minimalism</head><p>Previous work in MR such as most of the SemEval 2007 participants <ref type="bibr" target="#b7">(Farkas et al., 2007;</ref><ref type="bibr" target="#b28">Nicolae et al., 2007;</ref><ref type="bibr" target="#b14">Leveling, 2007;</ref><ref type="bibr" target="#b1">Brun et al., 2007;</ref><ref type="bibr" target="#b35">Poibeau, 2007)</ref>   <ref type="bibr" target="#b18">Markert and Nissim (2002)</ref> and other extra resources including the SemEval Task 8 features. We managed to achieve comparable performance with a small neural net- work typically trained in no more than 5 epochs, minimal training data, a basic dependency parser and the new PreWin method by being highly dis- criminating in choosing signal over noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We showed how a minimalist neural approach can replace substantial external resources, handcrafted features and how the PreWin method can even ignore most of the paragraph where the entity is positioned and still achieve competitive perfor- mance in metonymy resolution. The pressing new question is: "How much better the performance could have been if our method availed itself of the extra training data and resources used by previous works?" Indeed this may be the next research chapter for PreWin.</p><p>We discussed how tasks such as Geographical Parsing can benefit from "metonymy-enhanced" NER tagging. We have also presented a case for better annotation guidelines for MR (after consulting with a number of linguists), which now means that a government is not a literal class, rather it is a metonymic one. We fully agreed with the rest of the previous annotation guidelines. We also introduced ReLocaR, a new corpus for (location) metonymy resolution and encourage researchers to make effective use of it (including the additional CoNLL 2003 subset we annotated for metonymy).</p><p>Future work may involve testing PreWin on an NER task to see if and how it can generalise to a different classification task and how the results compare to the SOTA and similar methods such as that of Collobert et al. (2011) using the CoNLL 2003 NER datasets. Word Sense Disambigua- tion <ref type="bibr" target="#b38">(Yarowsky, 2010;</ref><ref type="bibr" target="#b34">Pilehvar and Navigli, 2014</ref>) with neural networks ( <ref type="bibr" target="#b21">Melamud et al., 2016</ref>) is an- other related classification task suitable for test- ing PreWin. If it does perform better, this will be of considerable interest to classification research (and beyond) in NLP. <ref type="figure">Figure 2</ref>: The neural architecture of the final model. The sentence is Vancouver is the host city of the ACL 2017. Small, separate sequential models are merged and trained as one. The 50-dimensional embeddings were initiated using GloVe. The right hand input is processed from right to left, the left hand input is processed from left to right. This is to emphasise the importance of the words closer to the entity.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Why it is important for PreWin to always skip the conjunct dependency relation.</figDesc><graphic url="image-3.png" coords="12,72.00,306.11,453.53,113.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A lot of irrelevant input is skipped such as "is" and "Peter Pan in an interview.".</figDesc><graphic url="image-4.png" coords="12,72.00,462.52,453.53,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: By looking for the predicate window, the model skips many irrelevant words.</figDesc><graphic url="image-5.png" coords="12,72.00,619.29,453.53,113.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results for SemEval data. Figures are 
averaged over 10 runs. STD -standard deviation. 

</table></figure>

			<note place="foot" n="1"> https://www.mediawiki.org/wiki/API:Random 2 https://github.com/milangritta/Minimalist-LocationMetonymy-Resolution/data/locations.txt</note>

			<note place="foot" n="3"> http://trec.nist.gov/data/reuters/reuters.html 4 https://github.com/milangritta/Minimalist-LocationMetonymy-Resolution</note>

			<note place="foot" n="5"> https://keras.io/</note>

			<note place="foot" n="6"> https://github.com/milangritta/Minimalist-LocationMetonymy-Resolution 7 http://nlp.stanford.edu/projects/glove/</note>

			<note place="foot" n="8"> https://spacy.io/</note>

			<note place="foot" n="9"> http://homepages.inf.ed.ac.uk/mnissim/mascara/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the funding support of the Natural Environment Research Council (NERC) PhD Studentship NE/M009009/1 (Milan Gritta, DREAM CDT), EPSRC (Nigel Collier and Nut Limsopatham-Grant No. EP/M005089/1) and MRC (Mohammad Taher Pilehvar) Grant No. MR/M025160/1 for PheneBank.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">XRCE-M: A hybrid system for named entity metonymy resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maud</forename><surname>Ehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Jacquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="488" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on human language technology and empirical methods in natural language processing</title>
		<meeting>the conference on human language technology and empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seana</forename><surname>Coulson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Oakley</surname></persName>
		</author>
		<title level="m">Metonymy and conceptual blending. Pragmatics and beyond-new series pages</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="51" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gyder: maxent metonymy resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eszter</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="161" to="164" />
		</imprint>
	</monogr>
	<note>György Szarvas, and Dániel Varga</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<biblScope unit="page" from="1952" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What&apos;s missing in geographical parsing?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Gritta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An improved non-monotonic transition system for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D/D15/D15-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1373" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalizing dependency features for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Penstein-Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-IJCNLP 2009 conference short papers</title>
		<meeting>the ACL-IJCNLP 2009 conference short papers</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="313" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fuh (fernuniversität in hagen): Metonymy recognition using different kinds of context for a memory-based learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>David D Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="361" to="397" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04646</idno>
		<title level="m">A dependency-based neural network for relation classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic lymphoma classification with sentence subgraph mining from pathology reports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aliyah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ephraim</forename><forename type="middle">P</forename><surname>Sohani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="832" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Metonymy resolution as a classification task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semeval2007 task 08: Metonymy resolution at semeval2007</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="36" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Data and models for metonymy resolution. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="123" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">context2vec: Learning generic context embedding with bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CONLL</title>
		<meeting>CONLL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3771" to="3775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on the geographic scope of textual documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bruno R Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clodoveu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Geosciences</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="23" to="34" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Local and global context for supervised and unsupervised metonymy resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Judea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="183" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining collocations, lexical and encyclopedic knowledge for metonymy resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="910" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transforming wikipedia into a large scale multilingual concept network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="62" to="85" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Utd-hlt-cg: Semantic architecture for metonymy resolution and classification of nominal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="454" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Syntactic features and word similarity for supervised metonymy resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st</title>
		<meeting>the 41st</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annual Meeting on Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Syntactic features and word similarity for supervised metonymy resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malvina</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st</title>
		<meeting>the 41st</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annual Meeting on Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A large-scale pseudoword-based evaluation framework for state-of-the-art word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Up13: Knowledge-poor methods (sometimes) perform poorly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="418" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Has¸imhas¸im</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Françoise</forename><surname>Beaufays</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.1128</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A computational model of logical metonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>TSLP)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Natural Language Processing</title>
		<meeting><address><addrLine>Sec</addrLine></address></meeting>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="315" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Is local window essential for neural network based chinese word segmentation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daqi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China National Conference on Chinese Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="450" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Exploring metaphorical senses and word representations for identifying metonyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Gelernter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04515</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
