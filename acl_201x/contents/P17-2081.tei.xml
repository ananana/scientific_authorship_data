<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Question Answering through Transfer Learning from Large Fine-grained Supervision Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
						</author>
						<title level="a" type="main">Question Answering through Transfer Learning from Large Fine-grained Supervision Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="510" to="517"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2081</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We show that the task of question answering (QA) can significantly benefit from the transfer learning of models trained on a different large, fine-grained QA dataset. We achieve the state of the art in two well-studied QA datasets, WikiQA and SemEval-2016 (Task 3A), through a basic transfer learning technique from SQuAD. For WikiQA, our model outperforms the previous best model by more than 8%. We demonstrate that finer supervision provides better guidance for learning lexical and syntactic information than coarser supervision , through quantitative results and visual analysis. We also show that a similar transfer learning procedure achieves the state of the art on an entailment task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA) is a long-standing chal- lenge in NLP, and the community has introduced several paradigms and datasets for the task over the past few years. These paradigms differ from each other in the type of questions and answers and the size of the training data, from a few hun- dreds to millions of examples.</p><p>We are particularly interested in the context- aware QA paradigm, where the answer to each question can be obtained by referring to its accom- panying context (paragraph or a list of sentences). Under this setting, the two most notable types of supervisions are coarse sentence-level and fine- grained span-level. In sentence-level QA, the task is to pick sentences that are most relevant to the question among a list of candidates ( <ref type="bibr" target="#b14">Yang et al., 2015)</ref>. In span-level QA, the task is to locate the * All work was done while the author was an exchange student at University of <ref type="bibr">Washington.</ref> smallest span in the given paragraph that answers the question ( <ref type="bibr" target="#b5">Rajpurkar et al., 2016)</ref>.</p><p>In this paper, we address coarser, sentence- level QA through a standard transfer learning 1 technique of a model trained on a large, span- supervised QA dataset. We demonstrate that the target task not only benefits from the scale of the source dataset but also the capability of the fine- grained span supervision to better learn syntactic and lexical information.</p><p>For the source dataset, we pretrain on SQuAD ( <ref type="bibr" target="#b5">Rajpurkar et al., 2016</ref>), a recently- released, span-supervised QA dataset. For the source and target models, we adopt BiDAF ( <ref type="bibr" target="#b6">Seo et al., 2017)</ref>, one of the top-performing mod- els in the dataset's leaderboard. For the tar- get datasets, we evaluate on two recent QA datasets, WikiQA ( <ref type="bibr" target="#b14">Yang et al., 2015)</ref> and Se- mEval 2016 (Task 3A) ( <ref type="bibr" target="#b2">Nakov et al., 2016)</ref>, which possess sufficiently different characteristics from that of SQuAD. Our results show 8% improve- ment in WikiQA and 1% improevement in Se- mEval. In addition, we report state-of-the-art re- sults on recognizing textual entailment (RTE) in SICK ( <ref type="bibr">Marelli et al., 2014</ref>) with a similar transfer learning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Data</head><p>Modern machine learning models, especially deep neural networks, often significantly benefit from transfer learning. In computer vision, deep con- volutional neural networks trained on a large im- age classification dataset such as ImageNet ( <ref type="bibr">Deng et al., 2009</ref>) have proved to be useful for initial- izing models on other vision tasks, such as ob- ject detection <ref type="bibr" target="#b17">(Zeiler and Fergus, 2014</ref>). In nat-Span-level QA <ref type="table" target="#tab_2">Sentence-level QA  RTE  SQuAD  WikiQA  SemEval-2016 Task 3A  SICK   Q   Which company made  Q   I saw an ad, data entry jobs online. It req- Four kids are  Spectre?</ref> Who made airbus uired we give a fee and they promise fixed P doing backbends amount every month. Is this a scam? in the park. ural language processing, domain adaptation has traditionally been an important topic for syntactic parsing <ref type="bibr">(McClosky et al., 2010)</ref> and named entity recognition ( <ref type="bibr">Chiticariu et al., 2010</ref>), among oth- ers. With the popularity of distributed represen- tation, pre-trained word embedding models such as word2vec <ref type="bibr">(Mikolov et al., 2013b</ref>,a) and glove ( <ref type="bibr" target="#b4">Pennington et al., 2014</ref>) are also widely used for natural language tasks <ref type="bibr">(Karpathy and Fei-Fei, 2015;</ref><ref type="bibr">Kumar et al., 2016)</ref>. Instead of these, we initialize our models from a QA dataset and show how standard transfer learning can achieve state- of-the-art in target QA datasets. There have been several QA paradigms in NLP, which can be categorized by the context and su- pervision used to answer questions. This context can range from structured and confined knowl- edge bases <ref type="bibr">(Berant et al., 2013</ref>) to unstructured and unbounded natural language form (e.g., doc- uments on the web <ref type="bibr" target="#b9">(Voorhees and Tice, 2000)</ref>) and unstructured, but restricted in size (e.g., a paragraph or multiple sentences ( <ref type="bibr">Hermann et al., 2015)</ref>). The recent advances in neural question an- swering lead to numerous datasets and successful models in these paradigms <ref type="bibr" target="#b5">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b14">Yang et al., 2015;</ref><ref type="bibr" target="#b3">Nguyen et al., 2016;</ref><ref type="bibr" target="#b7">Trischler et al., 2016</ref>). The answer types in these datasets are largely divided into three categories: sentence- level, in-context span, and generation. In this paper, we specifically focus on the former two and show that span-supervised models can better learn syntactic and lexical features. Among these datasets, we briefly describe three QA datasets to be used for the experiments in this paper. We also give the description of an RTE dataset for an ex- ample of a non-QA task. Refer to <ref type="table">Table 1</ref> to see the examples of the datasets.</p><p>SQuAD ( <ref type="bibr" target="#b5">Rajpurkar et al., 2016</ref>) is a recent span- based QA dataset, containing 100k/10k train/dev examples. Each example is a pair of context para- graph from Wikipedia and a question created by a human, and the answer is a span in the context. SQUAD-T is our modification of SQuAD dataset to allow for sentence selection QA. ('T' for senTence). We split the context paragraph into sentences and formulate the task as classi- fying whether each sentence contains the answer. This enables us to make a fair comparison between pretraining with span-supervised and sentence- supervised QA datasets.</p><p>WikiQA ( <ref type="bibr" target="#b14">Yang et al., 2015</ref>) is a sentence-level QA dataset, containing 1.9k/0.3k train/dev an- swerable examples. Each example consists of a real user's Bing query and a snippet of a Wikipedia article retrieved by Bing, containing 18.6 sen- tences on average. The task is to classify whether each sentence provides the answer to the query. SICK ( <ref type="bibr">Marelli et al., 2014</ref>) is a dataset for recognizing textual entailment (RTE), containing 4.5K/0.5K/5.0K train/dev/test examples. Each ex- ample consists of a hypothesis and a premise, and the goal is to determine if the premise is entailed by, contradicts, or is neutral to the hypothesis (hence classification problem). We also report re- sults on SICK to show that span-supervised QA dataset can be also useful for non-QA datasets.</p><p>BiDAF. The inputs to the model are a ques- tion q, and a context paragraph x. Then the model selects the best answer span, which is arg max <ref type="bibr">(i,j)</ref>  Here, we briefly describe the answer mod- ule which is important for transfer learning to sentence-level QA. The input to the answer mod- ule is a sequence of vectors {h i } each of which encodes enough information about the i-th con- text word and its relationship with its surrounding words and the question words. Then the role of the answer module is to map each vector h i to its start and end position probabilities, y start i and y end i .</p><p>BiDAF-T refers to the modified version of BiDAF to make it compatible with sentence-level QA. ('T' for senTence). In this task, the inputs are a question q and a list of sentences,</p><formula xml:id="formula_0">x 1 , . . . , x T ,</formula><p>where T is the number of the sentences. Note that, unlike BiDAF, which outputs single answer per example, Here we need to output a C-way clas- sification for each k-th sentence.</p><p>Since BiDAF is a span-selection model, it can- not be directly used for sentence-level classifica- tion. Hence we replace the original answer mod- ule of BiDAF with a different answer module, and keep the other modules identical to those of BiDAF. Given the input to the new answer mod- ule, {h k 1 , . . . , h k N }, where the superscript is the sentence index (1 ≤ k ≤ T ), we obtain the C-way classification scores for the k-th sentence, ˜ y k ∈ [0, 1] C via max-pooling method:</p><formula xml:id="formula_1">˜ y k = softmax(W max(h k 1 , . . . , h k N ) + b) (1)</formula><p>where W ∈ R C×d , b ∈ R C are trainable weight matrix and bias, respectively, and max() function is applied elementwise. For WikiQA and SemEval 2016, the number of classes (C) is 2, i.e. each sentence (or comment) is either relevant or not relevant. Since some of the metrics used for these datasets require full rank- ing, we use the predicted probability for "relevant" label to rank the sentences.</p><p>Note that BiDAF-T can be also used for the RTE dataset, where we can consider the hypothesis as a question and the premise as a context sentence (T = 1), and classify each example into 'entail- ment', 'neutral', or 'contradiction' (C = 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer Learning. Transfer learning between the same model architectures 3 is straightforward:</head><p>we first initialize the weights of the target model with the weights of the source model pretrained on the source dataset, and then we further train (fine- tune) on the target model with the target dataset. To transfer from BiDAF (on SQuAD) to BiDAF- T, we transfer all the weights of the identical modules, and initialize the new answer module in BiDAF-T with random values. For more training details, refer to Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Pretrained  Question Answering Results. <ref type="table" target="#tab_2">Table 2</ref> reports the state-of-the-art results of our transfer learn- ing on WikiQA and SemEval-2016 and the per- formance of previous models as well as several ablations that use no pretraining or no finetuning. There are multiple interesting observations from <ref type="table" target="#tab_2">Table 2</ref> as follows: (a) If we only train the BiDAF-T model on the target datasets with no pretraining (first row of Ta- ble 2), the results are poor. This shows the impor- tance of both pretraining and finetuning.</p><p>(b) Pretraining on SQuAD and SQuAD-T with no finetuning (second and third row) achieves re- sults close to the state-of-the-art in the WikiQA dataset, but not in SemEval-2016. Interestingly, our result on SemEval-2016 is not better than only training without transfer learning. We con- jecture that this is due to the significant differ- ence between the domain of SemEval-2016 and that of SQuAD, which are from community and Wikipedia, respectively.</p><p>(c) Pretraining on SQuAD and SQuAD-T with finetuning (fourth and fifth row) significantly out- performs (by more than 5%) the highest-rank sys- tems on WikiQA. It also outperforms the second ranking system in SemEval-2016 and is only 1% behind the first ranking system.</p><p>(d) Transfer learning models achieve better re- sults with pretraining on span-level supervision (SQuAD) than coarser sentence-level supervision (SQuAD-T). <ref type="bibr">4</ref> Finally, we also use the ensemble of 12 differ- ent training runs on the same BiDAF architecture, which obtains the state of the art in both datasets. This system outperforms the highest-ranking sys- tem in WikiQA by more than 8% and the best sys- tem in SemEval-2016 by 1% in every metric. It is important to note that, while we definitely benefit from the scale of SQuAD for transfer learning to smaller WikiQA, given the gap between SQuAD- T and SQuAD (&gt; 3%), we see a clear sign that span-supervision plays a significant role well.</p><p>Varying the size of pretraining dataset. We vary the size of SQuAD dataset used during pre- training, and test on WikiQA with finetuning. Re- sults are shown in <ref type="table" target="#tab_4">Table 3</ref>. As expected, MAP on WikiQA drops as the size of SQuAD decreases. It is worth noting that pretraining on SQuAD-T (Ta- ble 2) yields 0.5 point lower MAP than pretraining on 50% of SQuAD. In other words, roughly speak- ing, span-level supervision data is worth more than twice the size of sentence-level supervision data for the purpose of pretraining. Also, even a small size of fine-grained supervision data helps; pre- training with 12.5% of SQuAD gives an advantage of more than 7 points than no pretraining.</p><p>Analysis. <ref type="figure" target="#fig_2">Figure 1</ref> shows the latently-learned at- tention maps between the question and one of the context sentences from a WikiQA example in Ta- ble 1. The top map is pretrained on SQuAD- T (corresponding to SQuAD-T&amp;Yes in <ref type="table" target="#tab_2">Table 2</ref>) and the bottom map is pretrained on SQuAD (SQuAD&amp;Yes). The more red the color, the higher <ref type="bibr">4</ref> We additionally perform Mann-Whitney U Test and Mc- Nemars Test to show the statistical significance of the advan- tage of span-level pretraining over sentence-level pretraining. For WikiQA, the advantage is statistically significant with the confidence levels of 97.1% and 99.6%, respectively. For Se- mEval, we obtain the confidence levels of 97.8% and 99.9%, respectively.   the relevance between the words. There are two interesting observations here. First, in SQuAD-pretrained model (bottom), we see a high correspondence between ques- tion's airbus and context's aircraft and aerospace, but the SQuAD-T-pretrained model fails to learn such correspondence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Percentage of used SQuAD dataset MAP</head><p>Second, we see that the attention map of the SQuAD-pretrained model is more sparse, indicat- ing that it is able to more precisely localize cor- respondence between question and context words. In fact, we compare the sparsity of WikiQA test examples in SQuAD&amp;Y and SQuAD-T&amp;Y. Fol- lowing <ref type="bibr">Hurley and Rickard (2009)</ref>, the sparsity of an attention map is defined by</p><formula xml:id="formula_2">sparsity = | {x ∈ V|x ≤ } | |V| (2)</formula><p>where V is a set of values between 0 and 1 in at- tention map, and is a small value which we de- fine 0.01 for here. A histogram of the sparsity is shown in <ref type="figure">Figure 2</ref>. There is a large gap in the av- erage sparsity of WikiQA test examples between SQuAD&amp;Yes and SQuAD-T&amp;Yes, which are 0.84 and 0.56, respectively. More analyses including error analysis and more visualizations are shown in Appendix B.</p><p>Entailment Results. In addition to QA experi- ments, we also show that the models trained on span-supervised QA can be useful for textual en- tailment task (RTE). <ref type="table" target="#tab_6">Table 4</ref> shows the trans-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we show state-of-the-art results on <ref type="bibr">WikiQA and SemEval-2016 (Task 3A)</ref> as well as an entailment task, SICK, outperforming previous results by 8%, 1%, and 2%, respectively. We show that question answering with sentence-level super- vision can greatly benefit from standard transfer learning of a question answering model trained on a large, span-level supervision. We additionally show that such transfer learning can be applicable in other NLP tasks such as textual entailment.</p><p>Shafiq Joty, Alessandro Moschitti, Fahad A Al Obaidli, Salvatore Romeo, Kateryna Tymoshenko, and Anto- nio Uva. 2016. Convkn at semeval-2016 task 3: An- swer and question selection for question answering on arabic and english fora. SemEval pages 896-903.</p><p>Andrej <ref type="bibr">Karpathy and Li Fei-Fei. 2015</ref>. Deep visual- semantic alignments for generating image descrip- tions. In CVPR.</p><p>Ankit Kumar, Ozan Irsoy, Jonathan Su, James Brad- bury, Robert English, Brian Pierce, Peter Ondruska, Ishaan Gulrajani, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natu- ral language processing. In ICML. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor- rado, and Jeff Dean. 2013b. Distributed representa- tions of words and phrases and their compositional- ity. In NIPS.</p><p>Parameters. For pretraining BiDAF on SQuAD, we follow the exact same procedure in <ref type="bibr" target="#b6">Seo et al. (2017)</ref>. For pretraining BiDAF-T on SQuAD-T, we use the same hyperparameters for all modules except the answer module, for which we use the hidden state size of 200. The learning rate is controlled by AdaDelta (Zeiler, 2012) with the initial learning rate of 0.5 and minibatch size of 50. We maintain the moving averages of all weights of the model with the exponential decay rate of 0.999 during training and use them at test. The loss function is the cross entropy betweeñ y k and the one-hot vector of the correct classification.</p><p>Convergence. For all settings, we train models until performance on development set continue to decrease for 5k steps. <ref type="table">Table 5</ref> shows the median selected step on each setting.</p><formula xml:id="formula_3">Dataset Pretrained selected step SQuAD - 18k SQuAD-T - 50k WikiQA - 6k WikiQA SQuAD-T 6k WikiQA SQuAD 3k SemEval-2016 - 9k SemEval-2016 SQuAD-T 4k SemEval-2016 SQuAD 3k SICK - 13k SNLI - 55k SICK SQuAD-T 9k SNLI SQuAD-T 31k SICK SQuAD 18k SNLI SQuAD 49k SICK SQuAD-T + SNLI 7k SICK SQuAD + SNLI 7k</formula><p>Table 5: Median global step, which has the best perfor- mance on development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Analysis</head><p>Attention maps. We show some more examples of attention maps in <ref type="figure" target="#fig_5">Figure 3</ref>. (Top) We see high correspondence between same word from ques- tion and context such as senator and john, in SQuAD-pretrained model, but the SQuAD-T- pretrained model fails to learn such correspon- dence. (Bottom) We see high correspondence between stems from question and stem from context (left) as well as plant from question and plants from context (right), in SQuAD- pretrained model, but the SQuAD-T-pretrained model fails to learn such correspondence.</p><p>Error Analysis. <ref type="table" target="#tab_9">Table 7</ref>  of WikiQA and SemEval-2016 from <ref type="table">Table 1</ref>. On WikiQA, SQuAD-T-pretrained model selects C2 instead of the groundtruth answer C1. On SemEval-2016, SQuAD-pretrained model ranks C3 (bad comment) higher than C2 (good com- ment).</p><p>In addition, we sampled 100 example randomly   <ref type="table" target="#tab_4">Task 3A)</ref>.   from WikiQA and SemEval-2016, and classified them into 6 categories <ref type="table" target="#tab_8">(Table 6</ref>). In <ref type="table" target="#tab_10">Table 8</ref>, we compare the performance on these WikiQA exam- ples by SQuAD-T-pretrained model and SQuAD- pretrained model. It shows that span supervision clearly helps answering questions on Category 1 and 2, which are easier to answer, with answering correctly on most of the questions in Category 1. Similarly, we show the comparison of the perfor- mance on classified examples of the model with- out pretraining and SQuAD-pretrained model on SemEval-2016. It also shows that span supervi- sion helps answering questions asking information or opinion/recommendation.</p><formula xml:id="formula_4">WikiQA SemEval-2016 SQuAD-T&amp;Yes C2 &gt; C1 &gt; C3 C1 &gt; C3 &gt; C2 SQuAD&amp;Yes C1 &gt; C2 &gt; C3 C2 &gt; C1 &gt; C3 Groundtruth C1(Y), C2(N), C3(N) C1(Good), C2(Good), C3(Bad)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Results</head><p>SQuAD-T. To better understand SQuAD-T dataset, we show the performance BiDAF-T with different training routines. We get MAP 89.46 and accuracy 85.34% with SQuAD-trained BiDAF model, and MAP 90.18 and accuracy 84.69% with SQuAD-T-trained BiDAF-T model. There is no large gap between the two models, as each para- graph of SQuAD-T has 5 sentences on average, which makes the classification problem easier than WikiQA.</p><p>SNLI. Other larger RTE datasets such as SNLI also benefit from transfer learning, although the improvement is smaller. We confirm the improve- ment by showing that the result on SNLI when pretraining on SQuAD with BiDAF is 82.6%, which is slightly higher than that of the model pretrained on SQuAD-T (81.6%). This, however, did not outperform the state of the art (88.8%) by <ref type="bibr" target="#b12">Wang et al. (2017)</ref>. This is mostly because BiDAF (or BiDAF-T) is a QA model, which is not de- signed for RTE tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>SemEval 2016 (</head><label>2016</label><figDesc>Task 3A) (Nakov et al., 2016) is a sentence-level QA dataset, containing 1.8k/0.2k/0.3k train/dev/test examples. Each ex- ample consists of a community question by a user and 10 comments. The task is to classify whether each comment is relevant to the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y start i y end j , where i &lt;= j. Here, y start i and y end i are start and end position proba- bilities of i-th element, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Attention maps showing correspondence between the words of a question (vertical) and the subset of its context (horizontal) in WikiQA for (top) SQuAD-T-pretrained model and (bottom) SQuAD-pretrained model. The more red, the higher the correspondence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A denotational and distributional approach to seman- tics. SemEval . Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zam- parelli. 2014. A sick cure for the evaluation of com- positional distributional semantic models. In LREC. David McClosky, Eugene Charniak, and Mark John- son. 2010. Automatic domain adaptation for pars- ing. In NAACL-HLT. Todor Mihaylov and Preslav Nakov. 2016. Semanticz at semeval-2016 task 3: Ranking relevant answers in community question answering using semantic sim- ilarity based on fine-tuned word embeddings. Se- mEval pages 879-886. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word represen- tations in vector space. In ICLR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: More attention maps showing correspondence between the words of a question (vertical) and one of candidate sentences (horizontal) in WikiQA for (top in each subfigure) SQuAD-MC-pretrained model and (bottom in each subfigure) SQuAD-pretrained model. The more red, the higher the correspondence.</figDesc><graphic url="image-4.png" coords="7,333.79,466.50,90.53,81.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on WikiQA and SemEval-2016 (Task 3A). 

The first row is a result from non-pretrained model, and * in-
dicates ensemble method. Metrics used are Mean Average 
Precision (MAP), Mean Reciprocal Rank (MRR), Precision 
at rank 1 (P@1), and Average Recall (AvgR). Rank 1,2,3 indi-
cate the results by previous works, ordered by MAP. For Wik-
iQA, they are from Wang and Jiang (2017a); Tymoshenko 
et al. (2016); Miller et al. (2016), respectively. For SemEval-
2016, they are from Filice et al. (2016); Joty et al. (2016); 
Mihaylov and Nakov (2016). SQuAD*&amp;Yes sets the new 
state of the art on both datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Results with varying sizes of SQuAD dataset used during pretraining. All of them are finetuned and tested on WikiQA.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on SICK after finetuning. The first row is 

only trained on SICK. * indicates ensemble method. 

fer learning results of BiDAF-T on SICK dataset 
(Marelli et al., 2014), with various pretraining rou-
tines. Note that SNLI (Bowman et al., 2015) is 
a similar task to SICK and is significantly larger 
(150K/10K/10K train/dev/test examples). Here 
we highlight three observations: 
(a) BiDAF-T pretrained on SQuAD outperforms 
that without any pretraining by 6% and that pre-
trained on SQuAD-T by 2%, which demonstrates 
that the transfer learning from large span-based 
QA gives a clear improvement. 
(b) Pretraining on SQuAD+SNLI outperforms 
pretraining on SNLI only. Given that SNLI is 
larger than SQuAD, the difference in their perfor-
mance is a strong indicator that we are benefiting 
from not only the scale of SQuAD, but also the 
fine-grained supervision that it provides. 
(c) We outperform the previous state of the art 
by 2% with the ensemble of SQuAD+SNLI pre-
training routine. 
It is worth noting that Mou et al. (2016) also 
shows improvement on SICK by pretraining on 
SNLI. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Examples from each category on (top) WikiQA and (bottom) SemEval-2016 (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Comparison of ranked answers by SQuAD-T-

pretrained model (SQuAD-T&amp;Yes) and SQuAD-pretrained 
model (SQuAD&amp;Yes) of examples from WikiQA and 
SemEval-2016 (Task 3A) in Table 1. 

Pretrained dataset 
total 
Category Id 
SQuAD-T-Y SQuAD-Y 
1 
2 3 4 5 6 
total 
100 37 38 6 15 2 2 
Correct 
Correct 
49 28 14 3 4 0 0 
Wrong 
Correct 
26 
8 14 1 3 0 0 
Correct 
Wrong 
9 
1 
4 1 3 0 0 
Wrong 
Wrong 
16 
0 
6 1 5 2 2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Comparison of performance of SQuAD-T-

pretrained model (SQuAD-T-Y) and SQuAD-pretrained 
model (SQuAD-Y) on WikiQA. 

Pretrained dataset 
total 
Category Id 
No Pretrain SQuAD-Y 
1 
2 3 4 5 6 
total 
100 29 38 7 12 9 5 
Correct 
Correct 
30 12 11 2 5 0 0 
Wrong 
Correct 
22 
6 10 0 2 2 2 
Correct 
Wrong 
5 
0 
1 2 1 0 1 
Wrong 
Wrong 
43 11 16 3 4 7 2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Comparison of performance of model with-

out pretraining (No Pretrain) and SQuAD-pretrained model 
(SQuAD-Y) on SemEval-2016 (Task 3A). 

</table></figure>

			<note place="foot" n="1"> The borderline between transfer learning and domain adaptation is often ambiguous (Mou et al., 2016). We choose the term &quot;transfer learning&quot; because we also adapt the pretrained QA model to an entirely different task, RTE.</note>

			<note place="foot" n="2"> https://allenai.github.io/bi-att-flow</note>

			<note place="foot" n="3"> Strictly speaking, this is a domain adaptation scenario.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>Among numerous models proposed for span-level QA tasks ( <ref type="bibr" target="#b13">Xiong et al., 2017;</ref><ref type="bibr" target="#b11">Wang and Jiang, 2017b)</ref>, we adopt an open-sourced model, BiDAF 2 ( <ref type="bibr" target="#b6">Seo et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the NSF (IIS 1616112), Allen Institute for AI (66-9175), Allen Distinguished Investigator Award, and Google Re-search Faculty Award. We thank the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>AmirHossein Karimi, Antoine Bordes, and Jason Weston</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<title level="m">How transferable are neural networks in nlp applications? In EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Walid Magdy Mubarak Hamdy Hamdy, abed Alhakim Freihat, Jim Glass, and Bilal Randeree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llus</forename><surname>Mrquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semeval-2016 task 3: Community question answering. SemEval pages</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="525" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<title level="m">Newsqa: A machine comprehension dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional neural networks vs. convolution kernels: Feature engineering for answer sentence reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kateryna</forename><surname>Tymoshenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Bonadiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building a question answering test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Tian Tian Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="271" to="277" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
