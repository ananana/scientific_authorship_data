<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2418" to="2428"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2418</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider the problem of learning tex-tual entailment models with limited supervision (5K-10K training examples), and present two complementary approaches for it. First, we propose knowledge-guided adversarial example generators for incorporating large lexical resources in entail-ment models via only a handful of rule templates. Second, to make the entailment model-a discriminator-more robust, we propose the first GAN-style approach for training it using a natural language example generator that iteratively adjusts based on the discriminator&apos;s performance. We demonstrate effectiveness using two entailment datasets, where the proposed methods increase accuracy by 4.7% on SciTail and by 2.8% on a 1% training sub-sample of SNLI. Notably, even a single handwritten rule, negate, improves the accuracy on the negation examples in SNLI by 6.1%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The impressive success of machine learning mod- els on large natural language datasets often does not carry over to moderate training data regimes, where models often struggle with infrequently ob- served patterns and simple adversarial variations. A prominent example of this phenomenon is tex- tual entailment, the fundamental task of decid- ing whether a premise text entails () a hypoth- esis text. On certain datasets, recent deep learn- ing entailment systems ( <ref type="bibr" target="#b26">Parikh et al., 2016;</ref><ref type="bibr" target="#b8">Gong et al., 2018</ref>) have achieved close to human level performance. Nevertheless, the problem is far from solved, as evidenced by how easy it is to generate minor adversarial ex- <ref type="table">Table 1</ref>: Failure examples from the SNLI dataset: negation (Top) and re-ordering (Bottom). P is premise, H is hypothesis, and S is prediction made by an entailment system ( <ref type="bibr" target="#b26">Parikh et al., 2016)</ref>. amples that break even the best systems. As Ta- ble 1 illustrates, a state-of-the-art neural system for this task, namely the Decomposable Attention Model ( <ref type="bibr" target="#b26">Parikh et al., 2016)</ref>, fails when faced with simple linguistic phenomena such as negation, or a re-ordering of words. This is not unique to a particular model or task. Minor adversarial exam- ples have also been found to easily break neural systems on other linguistic tasks such as reading comprehension <ref type="bibr" target="#b15">(Jia and Liang, 2017)</ref>.</p><p>A key contributor to this brittleness is the use of specific datasets such as SNLI ( <ref type="bibr" target="#b2">Bowman et al., 2015</ref>) and SQuAD ( <ref type="bibr" target="#b31">Rajpurkar et al., 2016</ref>) to drive model development. While large and challenging, these datasets also tend to be homogeneous. E.g., SNLI was created by asking crowd-source work- ers to generate entailing sentences, which then tend to have limited linguistic variations and an- notation artifacts <ref type="bibr" target="#b10">(Gururangan et al., 2018)</ref>. Con- sequently, models overfit to sufficiently repetitive patterns-and sometimes idiosyncrasies-in the datasets they are trained on. They fail to cover long-tail and rare patterns in the training distribu- tion, or linguistic phenomena such as negation that would be obvious to a layperson.</p><p>To address this challenge, we propose to train textual entailment models more robustly using ad-versarial examples generated in two ways: (a) by incorporating knowledge from large linguistic resources, and (b) using a sequence-to-sequence neural model in a GAN-style framework.</p><p>The motivation stems from the following ob- servation. While deep-learning based textual en- tailment models lead the pack, they generally do not incorporate intuitive rules such as negation, and ignore large-scale linguistic resources such as PPDB ( <ref type="bibr" target="#b6">Ganitkevitch et al., 2013)</ref> and <ref type="bibr">WordNet (Miller, 1995)</ref>. These resources could help them generalize beyond specific words observed during training. For instance, while the SNLI dataset contains the pattern two men people, it does not contain the analogous pattern two dogs animals found easily in WordNet.</p><p>Effectively integrating simple rules or linguis- tic resources in a deep learning model, however, is challenging. Doing so directly by substantially adapting the model architecture ( <ref type="bibr" target="#b32">Sha et al., 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2018)</ref> can be cumbersome and limit- ing. Incorporating such knowledge indirectly via modified word embeddings <ref type="bibr" target="#b5">(Faruqui et al., 2015;</ref><ref type="bibr" target="#b25">Mrkši´Mrkši´c et al., 2016)</ref>, as we show, can have little positive impact and can even be detrimental.</p><p>Our proposed method, which is task-specific but model-independent, is inspired by data- augmentation techniques.</p><p>We generate new training examples by applying knowledge-guided rules, via only a handful of rule templates, to the original training examples. Simultaneously, we also use a sequence-to-sequence or seq2seq model for each entailment class to generate new hypothe- ses from a given premise, adaptively creating new adversarial examples. These can be used with any entailment model without constraining model ar- chitecture.</p><p>We also introduce the first approach to train a robust entailment model using a Generative Ad- versarial Network or GAN ( <ref type="bibr" target="#b9">Goodfellow et al., 2014</ref>) style framework. We iteratively improve both the entailment system (the discriminator) and the differentiable part of the data-augmenter (specifically the neural generator), by training the generator based on the discriminator's perfor- mance on the generated examples. Importantly, unlike the typical use of GANs to create a strong generator, we use it as a mechanism to create a strong and robust discriminator.</p><p>Our new entailment system, called AdvEntuRe, demonstrates that in the moderate data regime, adversarial iterative data-augmentation via only a handful of linguistic rule templates can be sur- prisingly powerful. Specifically, we observe 4.7% accuracy improvement on the challenging SciTail dataset ( <ref type="bibr" target="#b17">Khot et al., 2018</ref>) and a 2.8% improve- ment on 10K-50K training subsets of SNLI. An evaluation of our algorithm on the negation ex- amples in the test set of SNLI reveals a 6.1% im- provement from just a single rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Adversarial example generation has recently re- ceived much attention in NLP. For example, Jia and Liang (2017) generate adversarial examples using manually defined templates for the SQuAD reading comprehension task. <ref type="bibr" target="#b7">Glockner et al. (2018)</ref> create an adversarial dataset from SNLI by using WordNet knowledge. Automatic meth- ods ( ) have also been proposed to generate adversarial examples through paraphras- ing. These works reveal how neural network sys- tems trained on a large corpus can easily break when faced with carefully designed unseen ad- versarial patterns at test time. Our motivation is different. We use adversarial examples at train- ing time, in a data augmentation setting, to train a more robust entailment discriminator. The gener- ator uses explicit knowledge or hand written rules, and is trained in a end-to-end fashion along with the discriminator.</p><p>Incorporating external rules or linguistic re- sources in a deep learning model generally re- quires substantially adapting the model architec- ture ( <ref type="bibr" target="#b32">Sha et al., 2016;</ref><ref type="bibr" target="#b16">Kang et al., 2017)</ref>. This is a model-dependent approach, which can be cumbersome and constraining. Sim- ilarly non-neural textual entailment models have been developed that incorporate knowledge bases. However, these also require model-specific engi- neering ( <ref type="bibr" target="#b30">Raina et al., 2005;</ref><ref type="bibr" target="#b33">Silva et al., 2018</ref>).</p><p>An alternative is the model-and task- independent route of incorporating linguis- tic resources via word embeddings that are retro-fitted <ref type="bibr" target="#b5">(Faruqui et al., 2015)</ref> or counter- fitted <ref type="bibr" target="#b25">(Mrkši´Mrkši´c et al., 2016</ref>) to such resources. We demonstrate, however, that this has little positive impact in our setting and can even be detrimen- tal. Further, it is unclear how to incorporate knowledge sources into advanced representations such as contextual embeddings ( <ref type="bibr" target="#b23">McCann et al., 2017;</ref><ref type="bibr" target="#b29">Peters et al., 2018</ref>). We thus focus on a task-specific but model-independent approach.</p><p>Logical rules have also been defined to label ex- isting examples based on external resources ( <ref type="bibr" target="#b12">Hu et al., 2016)</ref>. Our focus here is on generating new training examples.</p><p>Our use of the GAN framework to create a bet- ter discriminator is related to CatGANs ( <ref type="bibr" target="#b36">Wang and Zhang, 2017)</ref> and <ref type="bibr">TripleGANs (Chongxuan et al., 2017)</ref> where the discriminator is trained to classify the original training image classes as well as a new 'fake' image class. We, on the other hand, gener- ate examples belonging to the same classes as the training examples. Further, unlike the earlier fo- cus on the vision domain, this is the first approach to train a discriminator using GANs for a natural language task with discrete outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adversarial Example Generation</head><p>We present three different techniques to create ad- versarial examples for textual entailment. Specifi- cally, we show how external knowledge resources, hand-authored rules, and neural language genera- tion models can be used to generate such exam- ples. Before describing these generators in detail, we introduce the notation used henceforth.</p><p>We use lower-case letters for single instances (e.g., x; p; h), upper-case letters for sets of in- stances (e.g., X; P; H ), blackboard bold for mod- els (e.g., D), and calligraphic symbols for discrete spaces of possible values (e.g., class labels C). For the textual entailment task, we assume each exam- ple is represented as a triple (p, h, c), where p is a premise (a natural language sentence), h is a hy- pothesis, and c is an entailment label: (a) entails (v) if h is true whenever p is true; (b) contradicts () if h is false whenever p is true; or (c) neu- tral (#) if the truth value of h cannot be concluded from p being true. <ref type="bibr">1</ref> We will introduce various example generators in the rest of this section. Each such generator, G , is defined by a partial function f and a label g . If a sentence s has a certain property required by f (e.g., contains a particular string), f transforms it into another sentence s 0 and g provides an en- tailment label from s to s 0 . Applied to a sentence s, G thus either "fails" (if the pre-requisite isn't met) or generates a new entailment example triple, s; f (s); g . For instance, consider the generator <ref type="bibr">1</ref> The symbols are based on Natural Logic ( <ref type="bibr" target="#b18">Lakoff, 1970</ref>) and use the notation of <ref type="bibr" target="#b21">MacCartney and Manning (2012)</ref>. <ref type="table">Table 2</ref>: Various generators G characterized by their source, (partial) transformation function f as applied to a sentence s, and entailment label g for :=hypernym(car, vehicle) with the (partial) transformation function f :="Replace car with vehicle" and the label g :=entails. f would fail when applied to a sentence not containing the word "car". Applying f to the sentence s="A man is driving the car" would generate s'="A man is driving the vehicle", creating the example (s; s 0 ; entails). The seven generators we use for experimenta- tion are summarized in <ref type="table">Table 2</ref> and discussed in more detail subsequently. While these particu- lar generators are simplistic and one can easily imagine more advanced ones, we show that train- ing using adversarial examples created using even these simple generators leads to substantial accu- racy improvement on two datasets.</p><formula xml:id="formula_0">Source ρ f (s) g Knowledge Base, G KB WordNet hyper(x; y) v anto(x, y) syno(x, y) Replace x with y in s v PPDB x Á y v SICK c(x; y) c Hand-authored, G H Domain knowledge neg negate(s) Neural Model, G s2s Training data (s2s, c) G s2s c (s) c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Knowledge-Guided Generators</head><p>Large knowledge-bases such as WordNet and PPDB contain lexical equivalences and other re- lationships highly relevant for entailment models. However, even large datasets such as SNLI gen- erally do not contain most of these relationships in the training data. E.g., that two dogs entails animals isn't captured in the SNLI data. We de- fine simple generators based on lexical resources to create adversarial examples that capture the un- derlying knowledge. This allows models trained on these examples to learn these relationships.</p><p>As discussed earlier, there are different ways of incorporating such symbolic knowledge into neural models. Unlike task-agnostic ways of ap- proaching this goal from a word embedding per- spective ( <ref type="bibr" target="#b5">Faruqui et al., 2015;</ref><ref type="bibr" target="#b25">Mrkši´Mrkši´c et al., 2016)</ref> or the model-specific approach <ref type="bibr" target="#b32">(Sha et al., 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2018)</ref>, we use this knowledge to gener- ate task-specific examples. This allows any entail- ment model to learn how to use these relationships in the context of the entailment task, helping them outperform the above task-agnostic alternative.</p><p>Our knowledge-guided example genera- tors, G KB , use lexical relations available in a knowledge-base: := r(x; y) where the relation r (such as synonym, hypernym, etc.) may differ across knowledge bases. We use a simple (partial) transformation function, f (s):="Replace x in s with y", as described in an earlier example. In some cases, when part-of-speech (POS) tags are available, the partial function requires the tags for x in s and in r(x; y) to match. The entailment label g for the resulting examples is also defined based on the relation r, as summarized in <ref type="table">Table 2</ref>.</p><p>This idea is similar to Natural Logic Inference or NLI <ref type="bibr" target="#b18">(Lakoff, 1970;</ref><ref type="bibr" target="#b34">Sommers, 1982;</ref><ref type="bibr" target="#b0">Angeli and Manning, 2014</ref>) where words in a sentence can be replaced by their hypernym/hyponym to produce entailing/neutral sentences, depending on their context. We propose a context-agnostic use of lexical resources that, despite its simplicity, al- ready results in significant gains. We use three sources for generators:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WordNet</head><p>(Miller, 1995) is a large, hand- curated, semantic lexicon with synonymous words grouped into synsets. Synsets are connected by many semantic relations, from which we use hy- ponym and synonym relations to generate entailing sentences, and antonym relations to generate con- tradicting sentences 2 . Given a relation r(x; y), the (partial) transformation function f is the POS-tag matched replacement of x in s with y, and requires the POS tag to be noun or verb. NLI provides a more robust way of using these relations based on context, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PPDB</head><p>( <ref type="bibr" target="#b6">Ganitkevitch et al., 2013</ref>) is a large resource of lexical, phrasal, and syntactic para- phrases. We use 24,273 lexical paraphrases in their smallest set, PPDB-S ( <ref type="bibr" target="#b27">Pavlick et al., 2015)</ref>, as equivalence relations, x Á y. The (partial) transformation function f for this generator is POS-tagged matched replacement of x in s with y, and the label g is entails.</p><p>SICK ( <ref type="bibr" target="#b22">Marelli et al., 2014</ref>) is dataset with en- tailment examples of the form (p; h; c), created to evaluate an entailment model's ability to capture compositional knowledge via hand-authored rules. We use the 12,508 patterns of the form c(x; y) ex- tracted by <ref type="bibr" target="#b1">Beltagy et al. (2016)</ref> by comparing sen- tences in this dataset, with the property that for each SICK example (p; h; c), replacing (when ap- plicable) x with y in p produces h. For simplic- ity, we ignore positional information in these pat- terns. The (partial) transformation function f is replacement of x in s with y, and the label g is c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hand-Defined Generators</head><p>Even very large entailment datasets have no or very few examples of certain otherwise common linguistic constructs such as negation, 3 causing models trained on them to struggle with these con- structs. A simple model-agnostic way to allevi- ate this issue is via a negation example generator whose transformation function f (s) is negate(s), described below, and the label g is contradicts.</p><p>negate(s): If s contains a 'be' verb (e.g., is, was), add a "not" after the verb. If not, also add a "did" or "do" in front based on its tense. E.g., change "A person is crossing" to "A person is not crossing" and "A person crossed" to "A person did not cross." While many other rules could be added, we found that this single rule covered a majority of the cases. Verb tenses are also consid- ered <ref type="bibr">4</ref> and changed accordingly. Other functions such as dropping adverbial clauses or changing tenses could be defined in a similar manner.</p><p>Both the knowledge-guided and hand-defined generators make local changes to the sentences based on simple rules. It should be possible to ex- tend the hand-defined rules to cover the long tail (as long as they are procedurally definable). How- ever, a more scalable approach would be to extend our generators to trainable models that can cover a wider range of phenomena than hand-defined rules. Moreover, the applicability of these rules generally depends on the context which can also be incorporated in such trainable generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neural Generators</head><p>For each entailment class c, we use a trainable sequence-to-sequence neural model <ref type="bibr" target="#b35">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b20">Luong et al., 2015)</ref> to generate an en- tailment example (s; s 0 ; c) from an input sentence s. The seq2seq model, trained on examples la- beled c, itself acts as the transformation function f of the corresponding generator G s2s c . The la- bel g is set to c. The joint probability of seq2seq model is:</p><formula xml:id="formula_1">G s2s c (X c ; c ) = G s2s c (H c ; P c ; c ) (1) = Π i P (h i;c jp i;c ; c )P (h i ) (2)</formula><p>The loss function for training the seq2seq is:</p><formula xml:id="formula_2">O c = argmin c L(H c ; G s2s c (X c ; c ))<label>(3)</label></formula><p>where L is the cross-entropy loss between the original hypothesis H c and the predicted hypothe- sis. Cross-entropy is computed for each predicted word w i against the same in H c given the se- quence of previous words in H c . O c are the op- timal parameters in G s2s c that minimize the loss for class c. We use the single most likely output to generate sentences in order to reduce decoding time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Example Generation</head><p>The generators described above are used to cre- ate new entailment examples from the training data. For each example (p; h; c) in the data, we can create two new examples: p; f (p); g and h; f (h); g . The examples generated this way using G KB and G H can, however, be relatively easy, as the premise and hypothesis would differ by only a word or so. We therefore compose such simple ("first-order") generated examples with the orig- inal input example to create more challenging "second-order" examples. We can create second- order examples by composing the original exam- ple (p; h; c) with a generated sentence from hy- pothesis, f (h) and premise, f (p). <ref type="figure">Figure 1</ref> de- picts how these two kinds of examples are gener- ated from an input example (p; h; c).</p><p>First, we consider the second-order example be- tween the original premise and the transformed hypothesis:</p><formula xml:id="formula_3">(p; f (h); L (c; g )),</formula><p>where L , de- fined in the left half of <ref type="table">Table 3</ref>, composes the input example label c (connecting p and h) and the gen- erated example label g to produce a new label. For instance, if p entails h and h entails f (h), p would entail f . In other words,  <ref type="figure">Figure 1</ref>: Generating first-order (blue) and second-order (red) examples. <ref type="table">Table 3</ref>: Entailment label composition functions L (left) and N (right) for creating second-order examples. c and g are the original and generated labels, resp. v: entails, : contradicts, #: neutral, ?: undefined soccer", "A man is playing a game", v) with a generated hypothesis f (h): "A person is playing a game." will give a new second-order entailment example: ("A man is playing soccer", "A person is playing a game", v).</p><formula xml:id="formula_4">L (v; v) is v.</formula><formula xml:id="formula_5">p ) h h ) h p ) h p ) h p ) p p ) h c g L c g N v v v v v ? v v ? v # # v # # v ? v ? ? ? # # # # # v # # v # # # # # # # # # # #</formula><p>Second, we create an example from the generated premise to the original hypothesis: (f (p); h; N (g ; c)). The composition function here, denoted N and defined in the right half of <ref type="table">Table 3</ref>, is often undetermined. For example, if p entails f (p) and p entails h, the relation be- tween f (p) and h is undetermined i.e. N (v; v ) =?. While this particular composition N often leads to undetermined or neutral relations, we use it here for completeness. For example, compos- ing the previous example with a generated neu- tral premise, f (p): "A person is wearing a cap" would generate an example ("A person is wearing a cap", "A man is playing a game", #)</p><p>The composition function L is the same as the "join" operation in natural logic reason- ing <ref type="bibr" target="#b13">(Icard III and Moss, 2014)</ref>, except for two dif- ferences: (a) relations that do not belong to our three entailment classes are mapped to '?', and (b) the exclusivity/alternation relation is mapped to contradicts. The composition function N , on the other hand, does not map to the join operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details</head><p>Given the original training examples X, we gener- ate the examples from each premise and hypothe- sis in a batch using G KB and G H . We also generate new hypothesis per class for each premise using G s2s</p><p>c . Using all the generated examples to train the model would, however, overwhelm the original training set. For examples, our knowledge-guided generators G KB can be applied in 17,258,314 dif- ferent ways.</p><p>To avoid this, we sub-sample our synthetic ex- amples to ensure that they are proportional to the input examples X , specifically they are bounded to ˛jX j where ˛ is tuned for each dataset. Also, as seen in <ref type="table">Table 3</ref>, our knowledge-guided generators are more likely to generate neutral examples than any other class. To make sure that the labels are not skewed, we also sub-sample the examples to ensure that our generated examples have the same class distribution as the input batch. The SciTail dataset only contains two classes: entails mapped to v and neutral mapped to . As a result, gen- erated examples that do not belong to these two classes are ignored.</p><p>The sub-sampling, however, has a negative side- effect where our generated examples end up us- ing a small number of lexical relations from the large knowledge bases. On moderate datasets, this would cause the entailment model to potentially just memorize these few lexical relations. Hence, we generate new entailment examples for each mini-batch and update the model parameters based on the training+generated examples in this batch.</p><p>The overall example generation procedure goes as follows: For each mini-batch X (1) randomly choose 3 applicable rules per source and sentence (e.g., replacing men with people based on PPDB in premise is one rule), (2) produce examples Z al l using G KB , G H and G s2s , (3) randomly sub-select examples Z from Z al l to ensure the balance be- tween classes and jZj= ˛jX j. <ref type="figure">Figure 2</ref> shows the complete architecture of our model, AdvEntuRe (ADVersarial training for tex- tual ENTailment Using Rule-based Examples.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">AdvEntuRe</head><p>The entailment model D is shown with the white box and two proposed generators are shown using black boxes. We combine the two symbolic un- trained generators, G KB and G H into a single G rule model. We combine the generated adversarial ex- amples Z with the original training examples X to train the discriminator. Next, we describe how the individual models are trained and finally present our new approach to train the generator based on the discriminator's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Discriminator Training</head><p>We use one of the state-of-the-art entailment mod- els (at the time of its publication) on SNLI, de- composable attention model ( <ref type="bibr" target="#b26">Parikh et al., 2016)</ref> with intra-sentence attention as our discriminator D. The model attends each word in hypothesis with each word in the premise, compares each pair of the attentions, and then aggregates them as a fi- nal representation. This discriminator model can be easily replaced with any other entailment model without any other change to the AdvEntuRe archi- tecture. We pre-train our discriminator D on the original dataset, X=(P, H, C) using:</p><formula xml:id="formula_6">D(X; Â) = argmax O C D( O C jP; H ; Â)<label>(4)</label></formula><formula xml:id="formula_7">O Â = argmin Â L(C; D(X ; Â ))<label>(5)</label></formula><p>where L is cross-entropy loss function between the true labels, Y and the predicted classes, and O Â are the learned parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generator Training</head><p>Our knowledge-guided and hand-defined genera- tors are symbolic parameter-less methods which are not currently trained. For simplicity, we will refer to the set of symbolic rule-based generators as</p><formula xml:id="formula_8">G rule := G KB [ G H .</formula><p>The neural generator G s2s , on the other hand, can be trained as described ear- lier. We leave the training of the symbolic models for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adversarial Training</head><p>We now present our approach to iteratively train the discriminator and generator in a GAN-style framework. Unlike traditional GAN ( <ref type="bibr" target="#b9">Goodfellow et al., 2014</ref>) on image/text generation that aims to obtain better generators, our goal is to build a robust discriminator regularized by the genera- tors (G s2s and G rule ). The discriminator and gen- erator are iteratively trained against each other to </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>[P → H, C] The chromosomes are pulled to the two pairs of chromosomes, that are identical [P → H, C] The chromosomes are a part of our body cells  <ref type="figure">Figure 2</ref>: Overview of AdvEntuRe, our model for knowledge-guided textual entailment.</p><p>Algorithm 1 Training procedure for AdvEntuRe.</p><formula xml:id="formula_9">1: pretrain discriminator D( O Â) on X; 2: pretrain generators G s2s c ( O ) on X; 3:</formula><p>for number of training iterations do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for mini-batch B X do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>generate examples from G 6:</p><formula xml:id="formula_10">Z G (G(B; ), 7:</formula><p>balance X and</p><formula xml:id="formula_11">Z G s.t. jZ G j Ä ˛jX j 8:</formula><p>optimize discriminator:</p><formula xml:id="formula_12">9: O Â = argmin Â L D (X + Z G ; Â) 10:</formula><p>optimize generator:</p><p>11:</p><formula xml:id="formula_13">O = argmin L G s2s (Z G ; L D ; ) 12:</formula><p>Update Â O Â; O achieve better discrimination on the augmented data from the generator and better example gen- eration against the learned discriminator. Algo- rithm 1 shows our training procedure. First, we pre-train the discriminator D and the seq2seq generators G s2s on the original data X. We alternate the training of the discriminator and generators over K iterations (set to 30 in our ex- periments).</p><p>For each iteration, we take a mini-batch B from our original data X. For each mini-batch, we generate new entailment examples, Z G using our adversarial examples generator. Once we collect all the generated examples, we balance the ex- amples based on their source and label (as de- scribed in Section 3.5). In each training itera- tion, we optimize the discriminator against the augmented training data, X + Z G and use the discriminator loss to guide the generator to pick challenging examples. For every mini-batch of examples X + Z G , we compute the discrimina- tor loss L(C ; D(X + Z G ; Â)) and apply the neg- ative of this loss to each word of the generated sentence in G s2s . In other words, the discrimina- tor loss value replaces the cross-entropy loss used to train the seq2seq model (similar to a REIN- FORCE (Williams, 1992) reward). This basic ap- proach uses the loss over the entire batch to update the generator, ignoring whether specific examples were hard or easy for the discriminator. Instead, one could update the generator per example based on the discriminator's loss on that example. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our empirical assessment focuses on two key questions: (a) Can a handful of rule templates im- prove a state-of-the-art entailment system, espe- cially with moderate amounts of training data? (b) Can iterative GAN-style training lead to an im- proved discriminator?</p><p>To this end, we assess various models on the two entailment datasets mentioned earlier: SNLI (570K examples) and SciTail (27K examples). <ref type="bibr">5</ref> To test our hypothesis that adversarial example based training prevents overfitting in small to moderate training data regimes, we compare model accura- cies on the test sets when using 1%, 10%, 50%, and 100% subsamples of the train and dev sets.</p><p>We consider two baseline models: D, the De- composable Attention model ( <ref type="bibr" target="#b26">Parikh et al., 2016</ref>) with intra-sentence attention using pre-trained word embeddings ( <ref type="bibr" target="#b28">Pennington et al., 2014)</ref>; and D retro which extends D with word embeddings initialized by retrofitted vectors <ref type="bibr" target="#b5">(Faruqui et al., 2015)</ref>. The vectors are retrofitted on PPDB, Word- 5 SNLI has a 96.4%/1.7%/1.7% split and SciTail has a 87.3%/4.8%/7.8% split on train, valid, and test sets, resp. Net, FrameNet, and all of these, with the best re- sults for each dataset reported here.</p><p>Our proposed model, AdvEntuRe, is evaluated in three flavors: D augmented with examples gen- erated by G rule , G s2s , or both, where G rule = G KB [G H . In the first two cases, we create new ex- amples for each batch in every epoch using a fixed generator (cf. Section 3.5). In the third case (D + G rule + G s2s ), we use the GAN-style training.</p><p>We uses grid search to find the best hyper- parameters for D based on the validation set: hid- den size 200 for LSTM layer, embedding size 300, dropout ratio 0.2, and fine-tuned embeddings.</p><p>The ratio between the number of generated vs. original examples, ˛ is empirically chosen to be 1.0 for SNLI and 0.5 for SciTail, based on vali- dation set performance. Generally, very few gen- erated examples (small ˛) has little impact, while too many of them overwhelm the original dataset resulting in worse scores (cf. Appendix for more details). <ref type="table" target="#tab_2">Table 4</ref> summarizes the test set accuracies of the different models using various subsampling ratios for SNLI and SciTail training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>We make a few observations. First, D retro is in- effective or even detrimental in most cases, except on SciTail when 1% (235 examples) or 10% (2.3K examples) of the training data is used. The gain in these two cases is likely because retrofitted lexical rules are helpful with extremely less data training while not as data size increases.</p><p>On the other hand, our method always achieves  <ref type="bibr">et al., 2018)</ref>, which achieves 77.3%) for that dataset by 1.7%. Among the three different generators combined with D, both G rule and G s2s are useful in Sci- Tail, while G rule is much more useful than G s2s on SNLI. We hypothesize that seq2seq model trained on large training sets such as SNLI will be able to reproduce the input sentences. Adversarial ex- amples from such a model are not useful since the entailment model uses the same training exam- ples. However, on smaller sets, the seq2seq model would introduce noise that can improve the robust- ness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>To evaluate the impact of each generator, we per- form ablation tests against each symbolic genera- tor in D + G rule and the generator G s2s c for each entailment class c. We use a 5% sample of SNLI and a 10% sample of SciTail. The results are sum- marized in <ref type="table" target="#tab_3">Table 5</ref>.</p><p>Interestingly, while PPDB (phrasal para- phrases) helps the most (+3.6%) on SNLI, simple negation rules help significantly (+8.2%) on Sc- iTail dataset. Since most entailment examples in SNLI are minor rewrites by Turkers, PPDB often contains these simple paraphrases. For SciTail, the sentences are authored independently with lim- ited gains from simple paraphrasing. However, a model trained on only 10% of the dataset (2.3K <ref type="table">Table 6</ref>: Given a premise P (underlined), examples of hypothesis sentences H' generated by seq2seq generators G s2s , and premise sentences P' generated by rule based generators G rule , on the full SNLI data. Replaced words or phrases are shown in bold. This illustrates that even simple, easy-to-define rules can generate useful adversarial examples. On the other hand, using all classes for G s2s re- sults in the best performance, supporting the ef- fectiveness of the GAN framework for penaliz- ing or rewarding generated sentences based on D's loss. Preferential selection of rules within the GAN framework remains a promising direction. <ref type="table">Table 6</ref> shows examples generated by various methods in AdvEntuRe. As shown, both seq2seq and rule based generators produce reasonable sen- tences according to classes and rules. As ex- pected, seq2seq models trained on very few exam- ples generate noisy sentences. The quality of our knowledge-guided generators, on the other hand, does not depend on the training set size and they still produce reliable sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Study: Negation</head><p>For further analysis of the negation-based gener- ator in <ref type="table">Table 1</ref>, we collect only the negation ex- amples in test set of SNLI, henceforth referred to as nega-SNLI. Specifically, we extract examples where either the premise or the hypothesis con- tains "not", "no", "never", or a word that ends with "n't'. These do not cover more subtle ways of ex- pressing negation such as "seldom" and the use of antonyms. nega-SNLI contains 201 examples with the following label distribution: 51 (25.4%) neu- tral, 42 (20.9%) entails, 108 (53.7%) contradicts. <ref type="table" target="#tab_4">Table 7</ref> shows examples in each category. While D achieves an accuracy of only 76.64% 6 on nega-SNLI, D + G H with negate is substan- tially more successful (+6.1%) at handling nega- tion, achieving an accuracy of 82.74%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced an adversarial training architec- ture for textual entailment. Our seq2seq and knowledge-guided example generators, trained in an end-to-end fashion, can be used to make any base entailment model more robust. The effec- tiveness of this approach is demonstrated by the significant improvement it achieves on both SNLI and SciTail, especially in the low to medium data regimes. Our rule-based generators can be ex- panded to cover more patterns and phenomena, and the seq2seq generator extended to incorporate per-example loss for adversarial training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>P</head><label></label><figDesc>: The dog did not eat all of the chickens. H: The dog ate all of the chickens. S: entails (score 56:5%) P: The red box is in the blue box. H: The blue box is in the red box. S: entails (score 92:1%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>[H → H',</head><label></label><figDesc>WordNet("part of" → "piece of"), C] The chromosomes are a piece of our body cells [P → P', NEG, C] Humans don't have 23 chromosome pairs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>P</head><label></label><figDesc>a person on a horse jumps over a broken down airplane H': G s2s c=v a person is on a horse jumps over a rail, a person jumping over a plane H': G s2s c= a person is riding a horse in a field with a dog in a red coat H': G s2s c=# a person is in a blue dog is in a park P (or H) a dirt bike rider catches some air going off a large hill P': G KB(PPDB) =Á;g =v a dirt motorcycle rider catches some air going off a large hill P': G KB(SICK) =c;g =# a dirt bike man on yellow bike catches some air going off a large hill P': G KB(WordNet) =syno;g =v a dirt bike rider catches some atmosphere going off a large hill P': G Hand =neg;g = a dirt bike rider do not catch some air going off a large hill examples) would end up learning a model relying on purely word overlap. We believe that the sim- ple negation examples introduce neutral examples with high lexical overlap, forcing the model to find a more informative signal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 : Test accuracies with different subsam- pling ratios on SNLI (top) and SciTail (bottom).</head><label>4</label><figDesc></figDesc><table>SNLI 
1% 
10% 
50% 100% 

D 
57.68 75.03 82.77 84.52 
D retro 
57.04 73.45 81.18 84.14 
AdvEntuRe 
D + G s2s 
58.35 75.66 82.91 84.68 
D + G rule 
60.45 77.11 83.51 84.40 
D + G rule + G s2s 59.33 76.03 83.02 83.25 

SciTail 
1% 
10% 
50% 100% 

D 
56.60 60.84 73.24 74.29 
D retro 
59.75 67.99 69.05 72.63 
AdvEntuRe 
D + G s2s 
65.78 70.77 74.68 76.92 
D + G rule 
61.74 66.53 73.99 79.03 
D + G rule + G s2s 63.28 66.78 74.77 78.60 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Test accuracies across various rules R 
and classes C. Since SciTail has two classes, we 
only report results on two classes of G s2s 

R/C 
SNLI (5%) 
SciTail (10%) 

D +G rule 

D 
69.18 
60.84 
+ PPDB 
72.81 (+3.6%) 65.52 (+4.6%) 
+ SICK 
71.32 (+2.1%) 67.49 (+6.5%) 
+ WordNet 71.54 (+2.3%) 64.67 (+3.8%) 
+ HAND 
71.15 (+1.9%) 69.05 (+8.2%) 
+ all 
71.31 (+2.1%) 64.16 (+3.3%) 

D +G s2s 

D 
69.18 
60.84 
+ positive 
71.21 (+2.0%) 67.49 (+6.6%) 
+ negative 71.76 (+2.6%) 68.95 (+8.1%) 
+ neutral 
71.72 (+2.5%) -
+ all 
72.28 (+3.1%) 70.77 (+9.9%) 

the best result compared to the baselines (D and 
D retro ). Especially, significant improvements are 
made in less data setting: +2.77% in SNLI (1%) 
and 9.18% in SciTail (1%). Moreover, D + G rule 's 
accuracy on SciTail (100%) also outperforms 
the previous state-of-the-art model (DGEM (Khot 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Negation examples in nega-SNLI 

v 
P: several women are playing volleyball. 
H: this doesn't look like soccer. 

# 

P: a man with no shirt on is performing 
with a baton. 
H: a man is trying his best at the national 
championship of baton. 

P: island native fishermen reeling in their 
nets after a long day's work. 
H: the men did not go to work today but 
instead played bridge. 

</table></figure>

			<note place="foot" n="2"> A similar approach was used in a parallel work to generate an adversarial dataset from SNLI (Glockner et al., 2018).</note>

			<note place="foot" n="3"> Only 211 examples (2.11%) in the SNLI training set contain negation triggers such as not, &apos;nt, etc. 4 https://www.nodebox.net/code/index.php/Linguistics</note>

			<note place="foot" n="6"> This is much less than the full test accuracy of 84.52%.</note>

			<note place="foot">Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. In Reinforcement Learning, pages 5-32. Springer.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NaturalLI: Natural logic inference for common sense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="534" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representing meaning with a combination of logical and distributional models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="763" to="808" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language inference with external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Triple generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taufik</forename><surname>Li Chongxuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4091" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Breaking nli systems with sentences that require simple lexical inferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Glockner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Natural language inference over interaction space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust textual inference via graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<title level="m">Harnessing deep neural networks with logic rules. ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Recent progress in monotonicity. LiLT (Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Moss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial example generation with syntactically controlled paraphrase networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting and explaining causes from text for a time series event</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">SciTail: A textual entailment dataset from science question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linguistics and Natural Logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Lakoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthese</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="271" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Natural logic and natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing Meaning. Text, Speech and Language Technology</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Counter-fitting word vectors to linguistic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust textual inference using diverse knowledge sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Michels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st PASCAL Recognition Textual Entailment Challenge Workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognizing textual entailment via multi-task knowledge assisted lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="285" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing and justifying text entailment through distributional navigation on definition graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vivian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Handschuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The logic of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Sommers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">CatGAN: Coupled adversarial transfer for domain generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1711.08904</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
