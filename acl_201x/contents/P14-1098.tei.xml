<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured Learning for Taxonomy Induction with Belief Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TTI Chicago</orgName>
								<orgName type="institution" key="instit2">Twitter Inc</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<orgName type="institution" key="instit4">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burkett</surname></persName>
							<email>dburkett@twitter.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TTI Chicago</orgName>
								<orgName type="institution" key="instit2">Twitter Inc</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<orgName type="institution" key="instit4">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TTI Chicago</orgName>
								<orgName type="institution" key="instit2">Twitter Inc</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<orgName type="institution" key="instit4">UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
							<email>klein@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TTI Chicago</orgName>
								<orgName type="institution" key="instit2">Twitter Inc</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<orgName type="institution" key="instit4">UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structured Learning for Taxonomy Induction with Belief Propagation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1041" to="1051"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation. Our model incorporates heterogeneous re-lational evidence about both hypernymy and siblinghood, captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts. For efficient inference over tax-onomy structures, we use loopy belief propagation along with a directed spanning tree algorithm for the core hyper-nymy factor. To train the system, we extract sub-structures of WordNet and dis-criminatively learn to reproduce them, using adaptive subgradient stochastic optimization. On the task of reproducing sub-hierarchies of WordNet, our approach achieves a 51% error reduction over a chance baseline, including a 15% error reduction due to the non-hypernym-factored sibling features. On a comparison setup, we find up to 29% relative error reduction over previous work on ancestor F1.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many tasks in natural language understanding, such as question answering, information extrac- tion, and textual entailment, benefit from lexical semantic information in the form of types and hy- pernyms. A recent example is IBM's Jeopardy! system <ref type="bibr">Watson (Ferrucci et al., 2010)</ref>, which used type information to restrict the set of answer can- didates. Information of this sort is present in term taxonomies (e.g., <ref type="figure" target="#fig_0">Figure 1</ref>), ontologies, and the- sauri. However, currently available taxonomies such as WordNet are incomplete in coverage <ref type="bibr" target="#b17">Hovy et al., 2009)</ref>, unavailable in many domains and languages, and time-intensive to create or extend manually. There has thus been considerable interest in building lex- ical taxonomies automatically.</p><p>In this work, we focus on the task of taking col- lections of terms as input and predicting a com- plete taxonomy structure over them as output. Our model takes a loglinear form and is represented using a factor graph that includes both 1st-order scoring factors on directed hypernymy edges (a parent and child in the taxonomy) and 2nd-order scoring factors on sibling edge pairs (pairs of hy- pernym edges with a shared parent), as well as in- corporating a global (directed spanning tree) struc- tural constraint. Inference for both learning and decoding uses structured loopy belief propagation (BP), incorporating standard spanning tree algo- rithms ( <ref type="bibr" target="#b3">Chu and Liu, 1965;</ref><ref type="bibr" target="#b8">Edmonds, 1967;</ref><ref type="bibr" target="#b41">Tutte, 1984)</ref>. The belief propagation approach allows us to efficiently and effectively incorporate hetero- geneous relational evidence via hypernymy and siblinghood (e.g., coordination) cues, which we capture by semantic features based on simple sur- face patterns and statistics from Web n-grams and Wikipedia abstracts. We train our model to max- imize the likelihood of existing example ontolo- gies using stochastic optimization, automatically learning the most useful relational patterns for full taxonomy induction.</p><p>As an example of the relational patterns that our system learns, suppose we are interested in build- ing a taxonomy for types of mammals (see <ref type="bibr">Figure 1)</ref>. Frequent attestation of hypernymy patterns like rat is a rodent in large corpora is a strong sig- nal of the link rodent → rat. Moreover, sibling or coordination cues like either rats or squirrels suggest that rat is a sibling of squirrel and adds evidence for the links rodent → rat and rodent → squirrel. Our supervised model captures ex- actly these types of intuitions by automatically dis- covering such heterogeneous relational patterns as features (and learning their weights) on edges and on sibling edge pairs, respectively. There have been several previous studies on taxonomy induction. e.g., the incremental tax- onomy induction system of <ref type="bibr" target="#b37">Snow et al. (2006)</ref>, the longest path approach of <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref>, and the maximum spanning tree (MST) approach of <ref type="bibr" target="#b27">Navigli et al. (2011)</ref>  <ref type="table">(see Section 4</ref> for a more detailed overview). The main contribution of this work is that we present the first discrimina- tively trained, structured probabilistic model over the full space of taxonomy trees, using a struc- tured inference procedure through both the learn- ing and decoding phases. Our model is also the first to directly learn relational patterns as part of the process of training an end-to-end taxonomic induction system, rather than using patterns that were hand-selected or learned via pairwise clas- sifiers on manually annotated co-occurrence pat- terns. Finally, it is the first end-to-end (i.e., non- incremental) system to include sibling (e.g., coor- dination) patterns at all.</p><p>We test our approach in two ways. First, on the task of recreating fragments of WordNet, we achieve a 51% error reduction on ancestor-based F1 over a chance baseline, including a 15% error reduction due to the non-hypernym-factored sib- ling features. Second, we also compare to the re- sults of <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref> by predicting the large animal subtree of WordNet. Here, we get up to 29% relative error reduction on ancestor- based F1. We note that our approach falls at a different point in the space of performance trade- offs from past work -by producing complete, highly articulated trees, we naturally see a more even balance between precision and recall, while past work generally focused on precision. 1 To <ref type="bibr">1</ref> While different applications will value precision and recall differently, and past work was often intentionally precision-focused, it is certainly the case that an ideal solu- tion would maximize both. avoid presumption of a single optimal tradeoff, we also present results for precision-based decoding, where we trade off recall for precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structured Taxonomy Induction</head><p>Given an input term set x = {x 1 , x 2 , . . . , x n }, we wish to compute the conditional distribution over taxonomy trees y. This distribution P (y|x) is represented using the graphical model formu- lation shown in <ref type="figure" target="#fig_1">Figure 2</ref>. A taxonomy tree y is composed of a set of indicator random variables y ij (circles in <ref type="figure" target="#fig_1">Figure 2</ref>), where y ij = ON means that x i is the parent of x j in the taxonomy tree (i.e. there exists a directed edge from x i to x j ). One such variable exists for each pair (i, j) with 0 ≤ i ≤ n, 1 ≤ j ≤ n, and i = j. <ref type="bibr">2</ref> In a factor graph formulation, a set of factors (squares and rectangles in <ref type="figure" target="#fig_1">Figure 2</ref>) determines the probability of each possible variable assignment. Each factor F has an associated scoring function φ F , with the probability of a total assignment de- termined by the product of all these scores:</p><formula xml:id="formula_0">P (y|x) ∝ F φ F (y)<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Factor Types</head><p>In the models we present here, there are three types of factors: EDGE factors that score individ- ual edges in the taxonomy tree, SIBLING factors that score pairs of edges with a shared parent, and a global TREE factor that imposes the structural constraint that y form a legal taxonomy tree.</p><p>EDGE Factors. For each edge variable y ij in the model, there is a corresponding factor E ij (small blue squares in <ref type="figure" target="#fig_1">Figure 2</ref>) that depends only on y ij . We score each edge by extracting a set of features f (x i , x j ) and weighting them by the (learned) weight vector w. So, the factor scoring function is:</p><formula xml:id="formula_1">φ E ij (y ij ) = exp(w · f (x i , x j )) y ij = ON exp(0) = 1 y ij = OFF</formula><p>SIBLING Factors. Our second model also in- cludes factors that permit 2nd-order features look- ing at terms that are siblings in the taxonomy tree. For each triple (i, j, k) with i = j, i = k, and j &lt; k, <ref type="bibr">3</ref> we have a factor S ijk (green rectangles in   ) that depends on y ij and y ik , and thus can be used to encode features that should be ac- tive whenever x j and x k share the same parent,</p><formula xml:id="formula_2">E 02 E 01 E 0n E 1n E 12 E 21 E 2n E n1 E n2 S 12n S 21n S n12 T (b) Full Model</formula><formula xml:id="formula_3">x i .</formula><p>The scoring function is similar to the one above:</p><formula xml:id="formula_4">φS ijk (yij, y ik ) = exp(w · f (xi, xj, x k )) yij = y ik = ON 1 otherwise TREE Factor.</formula><p>Of course, not all variable as- signments y form legal taxonomy trees (i.e., di- rected spanning trees). For example, the assign- ment ∀i, j, y ij = ON might get a high score, but would not be a valid output of the model. Thus, we need to impose a structural constraint to ensure that such illegal variable assignments are assigned 0 probability by the model. We encode this in our factor graph setting using a single global factor T (shown as a large red square in <ref type="figure" target="#fig_1">Figure 2</ref>) with the following scoring function:</p><p>φ T (y) = 1 y forms a legal taxonomy tree 0 otherwise</p><p>Model. For a given global assignment y, let</p><formula xml:id="formula_5">f (y) = i,j y ij =ON f (x i , x j ) + i,j,k y ij =y ik =ON f (x i , x j , x k )</formula><p>Note that by substituting our model's factor scor- ing functions into Equation 1, we get:</p><formula xml:id="formula_6">P (y|x) ∝ exp(w · f (y)) y is a tree 0 otherwise</formula><p>Thus, our model has the form of a standard loglin- ear model with feature function f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inference via Belief Propagation</head><p>With the model defined, there are two main in- ference tasks we wish to accomplish: computing expected feature counts and selecting a particular taxonomy tree for a given set of input terms (de- coding). As an initial step to each of these pro- cedures, we wish to compute the marginal prob- abilities of particular edges (and pairs of edges) being on. In a factor graph, the natural infer- ence procedure for computing marginals is belief propagation. Note that finding taxonomy trees is a structurally identical problem to directed span- ning trees (and thereby non-projective dependency parsing), for which belief propagation has previ- ously been worked out in depth <ref type="bibr" target="#b36">(Smith and Eisner, 2008)</ref>. Therefore, we will only briefly sketch the procedure here.</p><p>Belief propagation is a general-purpose infer- ence method that computes marginals via directed messages passed from variables to adjacent fac- tors (and vice versa) in the factor graph. These messages take the form of (possibly unnormal- ized) distributions over values of the variable. The two types of messages (variable to factor or fac- tor to variable) have mutually recursive defini- tions. The message from a factor F to an adjacent variable V involves a sum over all possible val- ues of every other variable that F touches. While the EDGE and SIBLING factors are simple enough to compute this sum by brute force, performing the sum na¨ıvelyna¨ıvely for computing messages from the TREE factor would take exponential time. How-ever, due to the structure of that particular factor, all of its outgoing messages can be computed si- multaneously in O(n 3 ) time via an efficient adap- tation of Kirchhoff's Matrix Tree Theorem (MTT) <ref type="bibr" target="#b41">(Tutte, 1984)</ref> which computes partition functions and marginals for directed spanning trees.</p><p>Once message passing is completed, marginal beliefs are computed by merely multiplying to- gether all the messages received by a particular variable or factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Loopy Belief Propagation</head><p>Looking closely at <ref type="figure" target="#fig_1">Figure 2a</ref>, one can observe that the factor graph for the first version of our model, containing only EDGE and TREE factors, is acyclic. In this special case, belief propagation is exact: after one round of message passing, the beliefs computed (as discussed in Section 2.2) will be the true marginal probabilities under the cur- rent model. However, in the full model, shown in <ref type="figure" target="#fig_1">Figure 2b</ref>, the SIBLING factors introduce cy- cles into the factor graph, and now the messages being passed around often depend on each other and so they will change as they are recomputed. The process of iteratively recomputing messages based on earlier messages is known as loopy belief propagation. This procedure only finds approx- imate marginal beliefs, and is not actually guar- anteed to converge, but in practice can be quite effective for finding workable marginals in mod- els for which exact inference is intractable, as is the case here. All else equal, the more rounds of message passing that are performed, the closer the computed marginal beliefs will be to the true marginals, though in practice, there are usually di- minishing returns after the first few iterations. In our experiments, we used a fairly conservative up- per bound of 20 iterations, but in most cases, the messages converged much earlier than that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Training</head><p>We used gradient-based maximum likelihood training to learn the model parameters w. Since our model has a loglinear form, the derivative of w with respect to the likelihood objective is computed by just taking the gold feature vec- tor and subtracting the vector of expected feature counts. For computing expected counts, we run belief propagation until completion and then, for each factor in the model, we simply read off the marginal probability of that factor being active (as computed in Section 2.2), and accumulate a par- tial count for each feature that is fired by that fac- tor. This method of computing the gradient can be incorporated into any gradient-based optimizer in order to learn the weights w. In our experiments we used AdaGrad ( <ref type="bibr" target="#b7">Duchi et al., 2011</ref>), an adaptive subgradient variant of standard stochastic gradient ascent for online learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Decoding</head><p>Finally, once the model parameters have been learned, we want to use the model to find taxon- omy trees for particular sets of input terms. Note that if we limit our scores to be edge-factored, then finding the highest scoring taxonomy tree becomes an instance of the MST problem (also known as the maximum arborescence problem for the directed case), which can be solved effi- ciently in O(n 2 ) quadratic time <ref type="bibr" target="#b40">(Tarjan, 1977)</ref> us- ing the greedy, recursive Chu-Liu-Edmonds algo- rithm ( <ref type="bibr" target="#b3">Chu and Liu, 1965;</ref><ref type="bibr" target="#b8">Edmonds, 1967)</ref>. <ref type="bibr">4</ref> Since the MST problem can be solved effi- ciently, the main challenge becomes finding a way to ensure that our scores are edge-factored. In the first version of our model, we could simply set the score of each edge to be w·f (x i , x j ), and the MST recovered in this way would indeed be the high- est scoring tree: arg max y P (y|x). However, this straightforward approach doesn't apply to the full model which also uses sibling features. Hence, at decoding time, we instead start out by once more using belief propagation to find marginal beliefs, and then set the score of each edge to be its belief odds ratio:</p><formula xml:id="formula_7">b Y ij (ON) b Y ij (OFF) . 5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Features</head><p>While spanning trees are familiar from non- projective dependency parsing, features based on the linear order of the words or on lexical identi- 4 See Georgiadis (2003) for a detailed algorithmic proof, and <ref type="bibr" target="#b25">McDonald et al. (2005)</ref> for an illustrative example. Also, we constrain the Chu-Liu-Edmonds MST algorithm to out- put only single-root MSTs, where the (dummy) root has ex- actly one child ( <ref type="bibr" target="#b19">Koo et al., 2007)</ref>, because multi-root span- ning 'forests' are not applicable to our task. Also, note that we currently assume one node per term. We are following the task description from previous work where the goal is to create a taxonomy for a specific domain (e.g., animals). Within a specific domain, terms typically just have a single sense. However, our algorithms could certainly be adapted to the case of multiple term senses (by treating the different senses as unique nodes in the tree) in future work. <ref type="bibr">5</ref> The MST that is found using these edge scores is actually the minimum Bayes risk tree <ref type="bibr" target="#b14">(Goodman, 1996)</ref> for an edge accuracy loss function <ref type="bibr" target="#b36">(Smith and Eisner, 2008).</ref> ties or syntactic word classes, which are primary drivers for dependency parsing, are mostly unin- formative for taxonomy induction. Instead, induc- ing taxonomies requires world knowledge to cap- ture the semantic relations between various unseen terms. For this, we use semantic cues to hyper- nymy and siblinghood via features on simple sur- face patterns and statistics in large text corpora. We fire features on both the edge and the sibling factors. We first describe all the edge features in detail (Section 3.1 and Section 3.2), and then briefly describe the sibling features (Section 3.3), which are quite similar to the edge ones.</p><p>For each edge factor E ij , which represents the potential parent-child term pair (x i , x j ), we add the surface and semantic features discussed below. Note that since edges are directed, we have sepa- rate features for the factors E ij versus E ji .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Surface Features</head><p>Capitalization: Checks which of x i and x j are capitalized, with one feature for each value of the tuple (isCap(x i ), isCap(x j )). The intuition is that leaves of a taxonomy are often proper names and hence capitalized, e.g., (bison, American bison). Therefore, the feature for (true, false) (i.e., parent capitalized but not the child) gets a substantially negative weight.   <ref type="bibr">(1992)</ref>. For each potential parent-child edge (P=x i , C=x j ), we mine the top k strings (based on count) in which both x i and x j occur (we use k=200). We collect patterns in both direc- tions, which allows us to judge the correct direc- tion of an edge (e.g., C is a P is a positive signal for hypernymy whereas P is a C is a negative sig- nal). <ref type="bibr">6</ref> Next, for each pattern in this top-k list, we compute its normalized pattern count c, and fire an indicator feature on the tuple (pattern, t), for all thresholds t (in a fixed set) s.t. c ≥ t. Our supervised model then automatically learns which patterns are good indicators of hypernymy.</p><note type="other">in large text corpora, an observation going back to Hearst</note><p>Pattern order: We add features on the order (di- rection) in which the pair (x i , x j ) found a pattern (in its top-k list) -indicator features for boolean values of the four cases: P . . . C, C . . . P , neither direction, and both directions. <ref type="bibr" target="#b35">Ritter et al. (2009)</ref> used the 'both' case of this feature.</p><p>Individual counts: We also compute the indi- vidual Web-scale term counts c x i and c x j , and add a comparison feature (c x i &gt;c x j ), plus features on values of the signed count difference (|c</p><formula xml:id="formula_8">x i | − |c x j |)/((|c x i | + |c x j |)/2)</formula><p>, after rounding off, and binning at multiple granularities. The intuition is that this feature could learn whether the relative popularity of the terms signals their hypernymy di- rection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Wikipedia Abstract Features</head><p>The Web n-grams corpus has broad coverage but is limited to up to 5-grams, so it may not contain pattern-based evidence for various longer multi- word terms and pairs. Therefore, we supplement it with a full-sentence resource, namely Wikipedia abstracts, which are concise descriptions (hence useful to signal hypernymy) of a large variety of world entities.</p><p>Presence and distance: For each potential edge (x i , x j ), we mine patterns from all abstracts in which the two terms co-occur in either order, al- lowing a maximum term distance of 20 (because beyond that, co-occurrence may not imply a rela- tion). We add a presence feature based on whether the process above found at least one pattern for that term pair, or not. We also fire features on the value of the minimum distance d min at which the two terms were found in some abstract (plus thresholded versions).</p><p>Patterns: For each term pair, we take the top-k patterns (based on count) of length up to l from its full list of patterns, and add an indicator feature on each pattern string (without the counts). We use k =5, l=10. Similar to the Web n-grams case, we also fire Wikipedia-based pattern order features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sibling Features</head><p>We also incorporate similar features on sibling factors. For each sibling factor S ijk which rep- resents the potential parent-children term triple (x i , x j , x k ), we consider the potential sibling term pair (x j , x k ). Siblinghood for this pair would be indicated by the presence of surface patterns such as either C 1 or C 2 , C 1 is similar to C 2 in large cor- pora. Hence, we fire Web n-gram pattern features and Wikipedia presence, distance, and pattern fea- tures, similar to those described above, on each potential sibling term pair. <ref type="bibr">7</ref> The main difference here from the edge factors is that the sibling fac- tors are symmetric (in the sense that S ijk is redun- dant to S ikj ) and hence the patterns are undirected. Therefore, for each term pair, we first symmetrize the collected Web n-grams and Wikipedia patterns by accumulating the counts of symmetric patterns like rats or squirrels and squirrels or rats. <ref type="bibr">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>In our work, we assume a known term set and do not address the problem of extracting related terms from text. However, a great deal of past work has considered automating this process, typ- ically taking one of two major approaches. The clustering-based approach <ref type="bibr" target="#b24">(Lin, 1998;</ref><ref type="bibr" target="#b23">Lin and Pantel, 2002;</ref><ref type="bibr" target="#b6">Davidov and Rappoport, 2006;</ref><ref type="bibr" target="#b43">Yamada et al., 2009</ref>) discovers relations based on the assumption that similar concepts appear in sim- <ref type="bibr">7</ref> One can also add features on the full triple (xi, xj, x k ) but most such features will be sparse. <ref type="bibr">8</ref> All the patterns and counts for our Web and Wikipedia edge and sibling features described above are extracted after stemming the words in the terms, the n-grams, and the ab- stracts (using the Porter stemmer). Also, we threshold the features (to prune away the sparse ones) by considering only those that fire for at least t trees in the training data (t = 4 in our experiments). Note that one could also add various complementary types of useful features presented by previous work, e.g., bootstrap- ping using syntactic heuristics ( <ref type="bibr" target="#b31">Phillips and Riloff, 2002</ref>), dependency patterns ( <ref type="bibr" target="#b37">Snow et al., 2006</ref>), doubly anchored patterns ( <ref type="bibr" target="#b21">Kozareva et al., 2008;</ref><ref type="bibr" target="#b17">Hovy et al., 2009)</ref>, and Web definition classifiers <ref type="bibr" target="#b27">(Navigli et al., 2011</ref>). ilar contexts <ref type="bibr" target="#b15">(Harris, 1954)</ref>. The pattern-based approach uses special lexico-syntactic patterns to extract pairwise relation lists ( <ref type="bibr" target="#b31">Phillips and Riloff, 2002</ref>; <ref type="bibr" target="#b13">Girju et al., 2003;</ref><ref type="bibr" target="#b38">Suchanek et al., 2007;</ref><ref type="bibr" target="#b35">Ritter et al., 2009;</ref><ref type="bibr" target="#b17">Hovy et al., 2009;</ref><ref type="bibr" target="#b1">Baroni et al., 2010;</ref><ref type="bibr" target="#b32">Ponzetto and Strube, 2011</ref>) and semantic classes or class- instance pairs <ref type="bibr" target="#b34">(Riloff and Shepherd, 1997;</ref><ref type="bibr" target="#b18">Katz and Lin, 2003;</ref><ref type="bibr" target="#b29">Pas¸caPas¸ca, 2004;</ref><ref type="bibr" target="#b9">Etzioni et al., 2005;</ref><ref type="bibr" target="#b39">Talukdar et al., 2008)</ref>.</p><p>We focus on the second step of taxonomy induc- tion, namely the structured organization of terms into a complete and coherent tree-like hierarchy. 9 Early work on this task assumes a starting par- tial taxonomy and inserts missing terms into it. <ref type="bibr" target="#b42">Widdows (2003)</ref> place unknown words into a re- gion with the most semantically-similar neigh- bors. <ref type="bibr" target="#b37">Snow et al. (2006)</ref> add novel terms by greed- ily maximizing the conditional probability of a set of relational evidence given a taxonomy. <ref type="bibr" target="#b44">Yang and Callan (2009)</ref> incrementally cluster terms based on a pairwise semantic distance. <ref type="bibr" target="#b22">Lao et al. (2012)</ref> extend a knowledge base using a random walk model to learn binary relational inference rules.</p><p>However, the task of inducing full taxonomies without assuming a substantial initial partial tax- onomy is relatively less well studied. There is some prior work on the related task of hierarchical clustering, or grouping together of semantically related words ( <ref type="bibr" target="#b33">Poon and Domingos, 2010;</ref><ref type="bibr" target="#b11">Fountain and Lapata, 2012</ref>). The task we focus on, though, is the discovery of direct taxonomic relationships (e.g., hypernymy) between words.</p><p>We know of two closely-related previous sys- tems, <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref> and <ref type="bibr" target="#b27">Navigli et al. (2011)</ref>, that build full taxonomies from scratch. Both of these systems use a process that starts by finding basic level terms (leaves of the fi- nal taxonomy tree, typically) and then using re- lational patterns (hand-selected ones in the case of <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref>, and ones learned sep- arately by a pairwise classifier on manually anno- tated co-occurrence patterns for Navigli and Ve- lardi (2010), <ref type="bibr" target="#b27">Navigli et al. (2011)</ref>) to find interme- diate terms and all the attested hypernymy links between them. 10 To prune down the resulting tax-onomy graph, <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref> use a procedure that iteratively retains the longest paths between root and leaf terms, removing conflicting graph edges as they go. The end result is acyclic, though not necessarily a tree; <ref type="bibr" target="#b27">Navigli et al. (2011)</ref> instead use the longest path intuition to weight edges in the graph and then find the highest weight taxonomic tree using a standard MST algorithm.</p><p>Our work differs from the two systems above in that ours is the first discriminatively trained, structured probabilistic model over the full space of taxonomy trees that uses structured inference via spanning tree algorithms (MST and MTT) through both the learning and decoding phases. Our model also automatically learns relational pat- terns as a part of the taxonomic training phase, in- stead of relying on hand-picked rules or pairwise classifiers on manually annotated co-occurrence patterns, and it is the first end-to-end (i.e., non- incremental) system to include heterogeneous re- lational information via sibling (e.g., coordina- tion) patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data and Experimental Regime</head><p>We considered two distinct experimental setups, one that illustrates the general performance of our model by reproducing various medium-sized WordNet domains, and another that facilitates comparison to previous work by reproducing the much larger animal subtree provided by <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref>.</p><p>General setup: In order to test the accuracy of structured prediction on medium-sized full- domain taxonomies, we extracted from WordNet 3.0 all bottomed-out full subtrees which had a tree-height of 3 (i.e., 4 nodes from root to leaf), and contained <ref type="bibr">(10,</ref><ref type="bibr">50]</ref> terms. 11 This gives us 761 non-overlapping trees, which we partition into both these systems include term discovery in the taxonomy building process.</p><p>11 Subtrees that had a smaller or larger tree height were dis- carded in order to avoid overlap between the training and test divisions. This makes it a much stricter setting than other tasks such as parsing, which usually has repeated sentences, clauses and phrases between training and test sets. To project WordNet synsets to terms, we used the first (most frequent) term in each synset. A few WordNet synsets have multiple parents so we only keep the first of each such pair of overlapping trees. We also discard a few trees with duplicate terms because this is mostly due to the projection of different synsets to the same term, and theoretically makes the tree a graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>70/15/15% (533/114/114 trees) train/dev/test sets.</head><p>Comparison setup: We also compare our method (as closely as possible) with related previous work by testing on the much larger animal subtree made available by <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref>, who cre- ated this dataset by selecting a set of 'harvested' terms and retrieving all the WordNet hypernyms between each input term and the root (i.e., an- imal), resulting in ∼700 terms and ∼4,300 is-a ancestor-child links. <ref type="bibr">12</ref> Our training set for this an- imal test case was generated from WordNet us- ing the following process: First, we strictly re- move the full animal subtree from WordNet in or- der to avoid any possible overlap with the test data. Next, we create random 25-sized trees by picking random nodes as singleton trees, and repeatedly adding child edges from WordNet to the tree. This process gives us a total of ∼1600 training trees. <ref type="bibr">13</ref> Feature sources: The n-gram semantic features are extracted from the Google n-grams corpus ( <ref type="bibr" target="#b2">Brants and Franz, 2006</ref>), a large collection of English n-grams (for n = 1 to 5) and their fre- quencies computed from almost 1 trillion tokens (95 billion sentences) of Web text. The Wikipedia abstracts are obtained via the publicly available dump, which contains almost ∼4.1 million ar- ticles. <ref type="bibr">14</ref> Preprocessing includes standard XML parsing and tokenization. Efficient collection of feature statistics is important because these must be extracted for millions of query pairs (for each potential edge and sibling pair in each term set). For this, we use a hash-trie on term pairs (sim- ilar to that of <ref type="bibr" target="#b0">Bansal and Klein (2011)</ref>), and scan once through the n-gram (or abstract) set, skipping many n-grams (or abstracts) based on fast checks of missing unigrams, exceeding length, suffix mis- matches, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metric</head><p>Ancestor F1: Measures the precision, recall, and F 1 = 2P R/(P + R) of correctly predicted ances- 12 This is somewhat different from our general setup where we work with any given set of terms; they start with a large set of leaves which have substantial Web-based relational information based on their selected, hand-picked patterns. Their data is available at http://www.isi.edu/ ˜ kozareva/ downloads.html. <ref type="bibr">13</ref> We tried this training regimen as different from that of the general setup (which contains only bottomed-out sub- trees), so as to match the animal test tree, which is of depth 12 and has intermediate nodes from higher up in WordNet. <ref type="bibr">14</ref> We used the 20130102 dump. tors, i.e., pairwise is-a relations: <ref type="table" target="#tab_1">Table 1</ref> shows our main results for ancestor-based evaluation on the general setup. We present a de- velopment set ablation study where we start with the edges-only model <ref type="figure" target="#fig_1">(Figure 2a</ref>) and its random tree baseline (which chooses any arbitrary span- ning tree for the term set). Next, we show results on the edges-only model with surface features (Section 3.1), semantic features (Section 3.2), and both. We see that both surface and semantic fea- tures make substantial contributions, and they also stack. Finally, we add the sibling factors and fea- tures <ref type="figure" target="#fig_1">(Figure 2b</ref>, Section 3.3), which further im- proves the results significantly (8% absolute and 15% relative error reduction over the edges-only results on the ancestor F1 metric). The last row shows the final test set results for the full model with all features. <ref type="table" target="#tab_3">Table 2</ref> shows our results for comparison to the larger animal dataset of <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref>. <ref type="bibr">15</ref> In the table, 'Kozareva2010' refers to <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref> and 'Navigli2011' refers to <ref type="bibr" target="#b27">Navigli et al. (2011)</ref>. <ref type="bibr">16</ref> For appropri- <ref type="bibr">15</ref> These results are for the 1st order model due to the scale of the animal taxonomy (∼700 terms). For scaling the 2nd order sibling model, one can use approximations, e.g., prun- ing the set of sibling factors based on 1st order link marginals, or a hierarchical coarse-to-fine approach based on taxonomy induction on subtrees, or a greedy approach of adding a few sibling factors at a time. This is future work. <ref type="bibr">16</ref> The <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref> ancestor results are ob- tained by using the output files provided on their webpage.  ate comparison to each previous work, we show results for two different setups. The first setup 'Fixed Prediction' assumes that the model knows the true root and leaves of the taxonomy to provide for a somewhat fairer comparison to <ref type="bibr" target="#b20">Kozareva and Hovy (2010)</ref>. We get substantial improvements on ancestor-based recall and F1 (a 29% relative error reduction). The second setup 'Free Predic- tion' assumes no prior knowledge and predicts the full tree (similar to the general setup case). On this setup, we do compare as closely as possible to <ref type="bibr" target="#b27">Navigli et al. (2011)</ref> and see a small gain in F1, but regardless, we should note that their results are incomparable (denoted by in <ref type="table" target="#tab_3">Table 2)</ref> because they have a different ground-truth data condition: their definition and hypernym extraction phase in- volves using the Google define keyword, which often returns WordNet glosses itself.</p><formula xml:id="formula_9">P = |isa gold ∩ isa predicted | |isa predicted | , R = |isa gold ∩ isa predicted | |isa gold |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We note that previous work achieves higher an- cestor precision, while our approach achieves a more even balance between precision and recall. Of course, precision and recall should both ide- ally be high, even if some applications weigh one over the other. This is why our tuning optimized for F1, which represents a neutral combination for comparison, but other F α metrics could also be optimized. In this direction, we also tried an experiment on precision-based decoding (for the 'Free Prediction' scenario), where we discard any edges with score (i.e., the belief odds ratio de- scribed in Section 2.4) less than a certain thresh- old. This allowed us to achieve high values of pre- cision (e.g., 90.8%) at still high enough F1 values (e.g., 61.7%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypernymy features C and other P</head><p>&gt; P &gt; C C , P of C is a P C , a P P , including C C or other P P ( C C : a P C , american P C -like P C , the P Siblinghood features C 1 and C 2 C 1 , C 2 ( C 1 or C 2 of C 1 and / or C 2 , C 1 , C 2 and either C 1 or C 2 the C 1 / C 2 &lt;s&gt; C 1 and C 2 &lt;/s&gt;   <ref type="table" target="#tab_4">Table 3</ref> shows some of the hypernymy and sibling- hood features given highest weight by our model (in general-setup development experiments). The training process not only rediscovers most of the standard Hearst-style hypernymy patterns (e.g., C and other P, C is a P), but also finds various novel, intuitive patterns. For example, the pattern C, american P is prominent because it captures pairs like Lemmon, american actor and Bryon, american politician, etc. Another pattern &gt; P &gt; C captures webpage navigation breadcrumb trails (representing category hierarchies). Similarly, the algorithm also discovers useful siblinghood fea- tures, e.g., either C 1 or C 2 , C 1 and / or C 2 , etc. Finally, we look at some specific output errors to give as concrete a sense as possible of some sys- tem confusions, though of course any hand-chosen examples must be taken as illustrative. In <ref type="figure" target="#fig_5">Figure  3</ref>, we attach white admiral to admiral, whereas the gold standard makes these two terms siblings. In reality, however, white admirals are indeed a species of admirals, so WordNet's ground truth turns out to be incomplete. Another such example is that we place logistic assessment in the evalu- ation subtree of judgment, but WordNet makes it a direct child of judgment. However, other dictio- naries do consider logistic assessments to be eval- uations. Hence, this illustrates that there may be more than one right answer, and that the low re- sults on this task should only be interpreted as such. In <ref type="figure" target="#fig_6">Figure 4</ref>, our algorithm did not recog- nize that thermos is a hyponym of vacuum flask, and that jeroboam is a kind of wine bottle. Here, our Web n-grams dataset (which only contains fre- quent n-grams) and Wikipedia abstracts do not suffice and we would need to add richer Web data for such world knowledge to be reflected in the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our approach to taxonomy induction allows het- erogeneous information sources to be combined and balanced in an error-driven way. Direct indi- cators of hypernymy, such as Hearst-style context patterns, are the core feature for the model and are discovered automatically via discriminative train- ing. However, other indicators, such as coordina- tion cues, can indicate that two words might be siblings, independently of what their shared par- ent might be. Adding second-order factors to our model allows these two kinds of evidence to be weighed and balanced in a discriminative, struc- tured probabilistic framework. Empirically, we see substantial gains (in ancestor F1) from sibling features, and also over comparable previous work. We also present results on the precision and recall trade-offs inherent in this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An excerpt of WordNet's vertebrates taxonomy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Factor graph representation of our model, both without (a) and with (b) SIBLING factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2b</head><label></label><figDesc>Figure 2b) that depends on y ij and y ik , and thus can be used to encode features that should be active whenever x j and x k share the same parent, x i. The scoring function is similar to the one above:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Ends with:</head><label></label><figDesc>Checks if x j ends with x i , or not. This captures pairs such as (fish, bony fish) in our data. Contains: Checks if x j contains x i , or not. This captures pairs such as (bird, bird of prey). Suffix match: Checks whether the k-length suf- fixes of x i and x j match, or not, for k = 1, 2, . . . , 7. LCS: We compute the longest common substring of x i and x j , and create indicator features for rounded-off and binned values of |LCS|/((|x i | + |x j |)/2). Length difference: We compute the signed length difference between x j and x i , and create indica- tor features for rounded-off and binned values of (|x j | − |x i |)/((|x i | + |x j |)/2). Yang and Callan (2009) use a similar feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Web n-gram Features Patterns and counts: Hypernymy for a term pair (P=x i , C=x j ) is often signaled by the presence of surface patterns like C is a P, P such as C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Excerpt from the predicted butterfly tree. The terms attached erroneously according to WordNet are marked in red and italicized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Excerpt from the predicted bottle tree. The terms attached erroneously according to WordNet are marked in red and italicized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Main results on our general setup. On the devel- opment set, we present incremental results on the edges-only model where we start with the chance baseline, then use sur- face features only, semantic features only, and both. Finally, we add sibling factors and features to get results for the full, edges+siblings model with all features, and also report the final test result for this setting.</figDesc><table>System 

P 
R 
F1 
Edges-Only Model 
Baseline 
5.9 
8.3 
6.9 
Surface Features 
17.5 41.3 24.6 
Semantic Features 
37.0 49.1 42.2 
Surface+Semantic 
41.1 54.4 46.8 
Edges + Siblings Model 
Surface+Semantic 
53.1 56.6 54.8 
Surface+Semantic (Test) 48.0 55.2 51.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison results on the animal dataset of 
Kozareva and Hovy (2010). Here, 'Kozareva2010' refers to 
Kozareva and Hovy (2010) and 'Navigli2011' refers to Nav-
igli et al. (2011). For appropriate comparison to each previ-
ous work, we show our results both for the 'Fixed Prediction' 
setup, which assumes the true root and leaves, and for the 
'Free Prediction' setup, which doesn't assume any prior in-
formation. The results of Navigli et al. (2011) represent a 
different ground-truth data condition, making them incompa-
rable to our results; see Section 5.3 for details. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Examples of high-weighted hypernymy and sibling-
hood features learned during development. 

</table></figure>

			<note place="foot" n="2"> We assume a special dummy root symbol x0. 3 The ordering of the siblings xj and x k doesn&apos;t matter here, so having separate factors for (i, j, k) and (i, k, j) would be redundant.</note>

			<note place="foot" n="6"> We also allow patterns with surrounding words, e.g., the C is a P and C , P of.</note>

			<note place="foot" n="9"> Determining the set of input terms is orthogonal to our work, and our method can be used in conjunction with various term extraction approaches described above. 10 Unlike our system, which assumes a complete set of terms and only attempts to induce the taxonomic structure,</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their insightful comments. This work was supported by BBN under DARPA contract HR0011-12-C-0014, 973 Program China Grants 2011CBA00300, 2011CBA00301, and NSFC Grants 61033001, 61361136003.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Web-scale features for full-scale parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Strudel: A corpus-based semantic model based on properties and types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="254" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Google Web 1T 5-gram corpus version 1.1. LDC2006T13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Franz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Sinica</title>
		<imprint>
			<biblScope unit="page">270</biblScope>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning concept hierarchies from text with a guided agglomerative clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML 2005 Workshop on Learning and Extending Lexical Ontologies with Machine Learning Methods</title>
		<meeting>the ICML 2005 Workshop on Learning and Extending Lexical Ontologies with Machine Learning Methods</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning concept hierarchies from text corpora using formal concept analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hotho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="305" to="339" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimum branchings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research of the National Bureau of Standards B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="233" to="240" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised named-entity extraction from the Web: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anamaria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="134" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building watson: An overview of the DeepQA project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Welty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Taxonomy induction using hierarchical random graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Fountain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Arborescence optimization problems solvable by edmonds algorithm. Theoretical Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Georgiadis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">301</biblScope>
			<biblScope unit="page" from="427" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning semantic constraints for the automatic discovery of part-whole relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Badulescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parsing algorithms and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic acquisition of hyponyms from large text corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward completeness in concept extraction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selectively using relations to improve precision in question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on NLP for Question Answering</title>
		<meeting>the Workshop on NLP for Question Answering</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structured prediction models via the matrix-tree theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A semi-supervised method to learn and construct taxonomies using the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic class learning from the web with hyponym pattern linkage graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Concept discovery from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-EMNLP</title>
		<meeting>HLT-EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning word-class lattices for definition and hypernym extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A graph-based algorithm for inducing lexical taxonomies from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Espresso: Leveraging generic patterns for automatically harvesting semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Acquisition of categorized named entities for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pas¸capas¸ca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ontologizing semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting strong syntactic heuristics and co-training to learn semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Taxonomy induction based on a collaboratively built knowledge repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1737" to="1756" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised ontology induction from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A corpusbased approach for building semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Shepherd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What is this, anyway: Automatic hypernym discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Spring Symposium on Learning by Reading and Learning to Read</title>
		<meeting>AAAI Spring Symposium on Learning by Reading and Learning to Read</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dependency parsing by belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic taxonomy induction from heterogenous evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weakly-supervised acquisition of labeled class instances using graph random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pas¸capas¸ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
		<title level="m">Finding optimum branchings. Networks</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Tutte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>AddisonWesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised methods for developing taxonomies by combining syntactic and statistical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Widdows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL</title>
		<meeting>HLTNAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Hypernym discovery based on distributional similarity and hierarchical structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kow</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Kuroda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stijn De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Saeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asuka</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A metric-based framework for automatic taxonomy induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
