<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Embeddings with Limited Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshi</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Word Embeddings with Limited Memory</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="387" to="392"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
					<note>2 Department of Computer Science and Engineering, HKUST 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper studies the effect of limited precision data representation and computation on word embeddings. We present a systematic evaluation of word embeddings with limited memory and discuss methods that directly train the limited precision representation with limited memory. Our results show that it is possible to use and train an 8-bit fixed-point value for word embedding without loss of performance in word/phrase similarity and dependency parsing tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is an accumulation of evidence that the use of dense distributional lexical representations, known as word embeddings, often supports bet- ter performance on a range of NLP tasks <ref type="bibr" target="#b0">(Bengio et al., 2003;</ref><ref type="bibr" target="#b15">Turian et al., 2010;</ref><ref type="bibr" target="#b4">Collobert et al., 2011;</ref><ref type="bibr" target="#b11">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b10">Levy et al., 2015)</ref>. Consequently, word embeddings have been commonly used in the last few years for lexical similarity tasks and as fea- tures in multiple, syntactic and semantic, NLP ap- plications.</p><p>However, keeping embedding vectors for hun- dreds of thousands of words for repeated use could take its toll both on storing the word vectors on disk and, even more so, on loading them into memory. For example, for 1 million words, load- ing 200 dimensional vectors takes up to 1.6 GB memory on a 64-bit system. Considering applica- tions that make use of billions of tokens and mul- tiple languages, size issues impose significant lim- itations on the practical use of word embeddings.</p><p>This paper presents the question of whether it is possible to significantly reduce the memory need- s for the use and training of word embeddings.</p><p>Specifically, we ask "what is the impact of repre- senting each dimension of a dense representation with significantly fewer bits than the standard 64 bits?" Moreover, we investigate the possibility of directly training dense embedding vectors using significantly fewer bits than typically used.</p><p>The results we present are quite surprising. We show that it is possible to reduce the memory con- sumption by an order of magnitude both when word embeddings are being used and in training. In the first case, as we show, simply truncating the resulting representations after training and us- ing a smaller number of bits (as low as 4 bits per dimension) results in comparable performance to the use of 64 bits. Moreover, we provide t- wo ways to train existing algorithms ( <ref type="bibr" target="#b11">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013b</ref>) when the memory is limited during training and show that, here, too, an order of magnitude saving in mem- ory is possible without degrading performance. We conduct comprehensive experiments on ex- isting word and phrase similarity and relatedness datasets as well as on dependency parsing, to e- valuate these results. Our experiments show that, in all cases and without loss in performance, 8 bits can be used when the current standard is 64 and, in some cases, only 4 bits per dimension are sufficient, reducing the amount of space re- quired by a factor of 16. The truncated word embeddings are available from the papers web page at https://cogcomp.cs.illinois. edu/page/publication_view/790.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>If we consider traditional cluster encoded word representation, e.g., Brown clusters ( <ref type="bibr">Brown et al., 1992)</ref>, it only uses a small number of bits to track the path on a hierarchical tree of word clusters to represent each word. In fact, word embedding generalized the idea of discrete clustering repre- sentation to continuous vector representation in language models, with the goal of improving the continuous word analogy prediction and general- ization ability ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b11">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013b</ref>). However, it has been proven that Brown clusters as discrete fea- tures are even better than continuous word em- bedding as features for named entity recognition tasks <ref type="bibr" target="#b14">(Ratinov and Roth, 2009)</ref>. <ref type="bibr" target="#b7">Guo et al. (Guo et al., 2014</ref>) further tried to binarize embeddings using a threshold tuned for each dimension, and essentially used less than two bits to represen- t each dimension. They have shown that bina- rization can be comparable to or even better than the original word embeddings when used as fea- tures for named entity recognition tasks. More- over, <ref type="bibr" target="#b6">Faruqui et al. (Faruqui et al., 2015)</ref> showed that imposing sparsity constraints over the em- bedding vectors can further improve the represen- tation interpretability and performance on sever- al word similarity and text classification bench- mark datasets. These works indicate that, for some tasks, we do not need all the information encoded in "standard" word embeddings. Nonetheless, it is clear that binarization loses a lot of information, and this calls for a systematic comparison of how many bits are needed to maintain the expressivity needed from word embeddings for different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Value Truncation</head><p>In this section, we introduce approaches for word embedding when the memory is limited. We trun- cate any value x in the word embedding into an n bit representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Post-processing Rounding</head><p>When the word embedding vectors are given, the most intuitive and simple way is to round the num- bers to their n-bit precision. Then we can use the truncated values as features for any tasks that word embedding can be used for. For example, if we want to round x to be in the range of [−r, r], a simple function can be applied as follows.</p><formula xml:id="formula_0">R d (x, n) = x if x ≤ x ≤ x + 2 x + if x + 2 &lt; x ≤ x +</formula><p>(1) where = 2 1−n r. For example, if we want to use 8 bits to represent any value in the vectors, then we only have 256 numbers ranging from -128 to 127 for each value. In practice, we first scale all the values and then round them to the 256 numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training with Limited Memory</head><p>When the memory for training word embedding is also limited, we need to modify the training algorithms by introducing new data structures to reduce the bits used to encode the values. In practice, we found that in the stochastic gradien- t descent (SGD) iteration in word2vec algorithm- s ( <ref type="bibr" target="#b11">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013b)</ref>, the updating vector's values are often very small numbers (e.g., &lt; 10 −5 ). In this case, if we direct- ly apply the rounding method to certain precisions (e.g., 8 bits), the update of word vectors will al- ways be zero. For example, the 8-bit precision is 2 −7 = 0.0078, so 10 −5 is not significant enough to update the vector with 8-bit values. Therefore, we consider the following two ways to improve this.</p><p>Stochastic Rounding. We first consider us- ing stochastic rounding ( <ref type="bibr" target="#b9">Gupta et al., 2015</ref>) to train word embedding. Stochastic rounding intro- duces some randomness into the rounding mech- anism, which has been proven to be helpful when there are many parameters in the learning system, such as deep learning systems ( <ref type="bibr" target="#b9">Gupta et al., 2015</ref>). Here we also introduce this approach to update word embedding vectors in SGD. The probability of rounding x to x is proportional to the prox- imity of x to x:</p><formula xml:id="formula_1">R s (x, n) = x w.p. 1 − x−−x x + w.p. x−−x . (2)</formula><p>In this case, even though the update values are not significant enough to update the word embedding vectors, we randomly choose some of the values being updated proportional to the value of how close the update value is to the rounding precision.</p><p>Auxiliary Update Vectors. In addition to the method of directly applying rounding to the val- ues, we also provide a method using auxiliary update vectors to trade precision for more space. Suppose we know the range of update value in S- GD as [−r , r ], and we use additional m bits to store all the values less than the limited numeri- cal precision . Here r can be easily estimated by running SGD for several examples. Then the real precision is = 2 1−m r . For example, if r = 10 −4 and m = 8, then the numerical pre- cision is 7.8 · 10 −7 which can capture much higher precision than the SGD update values have. When  the cumulated values in the auxiliary update vec- tors are greater than the original numerical preci- sion , e.g., = 2 −7 for 8 bits, we update the o- riginal vector and clear the value in the auxiliary vector. In this case, we can have final n-bit values in word embedding vectors as good as the method presented in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on Word/Phrase Similarity</head><p>In this section, we describe a comprehensive study on tasks that have been used for evaluating word embeddings. We train the word embedding algo- rithms, word2vec <ref type="bibr" target="#b11">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013b</ref>), based on the Oct. 2013 Wikipedi- a dump. <ref type="bibr">1</ref> We first compare levels of truncation of word2vec embeddings, and then evaluate the s- tochastic rounding and the auxiliary vectors based methods for training word2vec vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use multiple test datasets as follows. Word Similarity. Word similarity datasets have been widely used to evaluate word embed- ding results. We use the datasets summarized by <ref type="bibr" target="#b5">Faruqui and Dyer (Faruqui and Dyer, 2014</ref>): wordsim-353, wordsim-sim, wordsim-rel, MC-30, RG-65, MTurk-287, MTurk-771, MEN 3000, YP- 130, Rare-Word, Verb-143, and SimLex-999. <ref type="bibr">2</ref> We compute the similarities between pairs of words <ref type="table">Table 1</ref>: The detailed average results for word similarity and paraphrases of <ref type="figure" target="#fig_1">Fig. 1</ref> and check the Spearman's rank correlation coeffi- cient ( <ref type="bibr" target="#b13">Myers and Well., 1995)</ref> between the com- puter and the human labeled ranks. Paraphrases (bigrams). We use the paraphrase (bigram) datasets used in ( <ref type="bibr" target="#b16">Wieting et al., 2015)</ref>, ppdb all, bigrams vn, bigrams nn, and bigram- s jnn, to test whether the truncation affects phrase level embedding. Our phrase level embedding is based on the average of the words inside each phrase. Note that it is also easy to incorporate our truncation methods into existing phrase em- bedding algorithms. We follow ( <ref type="bibr" target="#b16">Wieting et al., 2015</ref>) in using cosine similarity to evaluate the correlation between the computed similarity and annotated similarity between paraphrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of Bits Needed</head><p>We ran both CBOW and skipgram with negative sampling ( <ref type="bibr" target="#b11">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b12">Mikolov et al., 2013b</ref>) on the Wikipedia dump data, and set the window size of context to be five. Then we per- formed value truncation with 4 bits, 6 bits, and 8 bits. The results are shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, and the num- bers of the averaged results are shown in <ref type="table">Table 1</ref>. We also used the binarization algorithm ( <ref type="bibr" target="#b7">Guo et al., 2014</ref>) to truncate each dimension to three val- ues; these experiments are is denoted using the suffix "binary" in the figure. For both CBOW and skipgram models, we train the vectors with 25 and 200 dimensions respectively.</p><p>The representations used in our experiments were trained using the whole Wikipedia dump. A first observation is that, in general, CBOW per- forms better than the skipgram model. When us- ing the truncation method, the memory required to store the embedding is significantly reduced, while the performance on the test datasets remains almost the same until we truncate down to 4 bit- s. When comparing CBOW and skipgram models, we again see that the drop in performance with 4- bit values for the skipgram model is greater than the one for the CBOW model. For the CBOW model, the drop in performance with 4-bit values is greater when using 200 dimensions than it is when using 25 dimensions. However, when using skipgram, this drop is slightly greater when using 25 dimensions than 200.</p><p>We also evaluated the binarization ap- proach ( <ref type="bibr" target="#b7">Guo et al., 2014</ref>). This model uses three values, represented using two bits. We observe that, when the dimension is 25, the bina- rization is worse than truncation. One possible explanation has to do merely with the size of the space; while 3 25 is much larger than the size of the word space, it does not provide enough redundancy to exploit similarity as needed in the tasks. Consequently, the binarization approach results in worse performance. However, when the dimension is 200, this approach works much better, and outperforms the 4-bit truncation. In particular, binarization works better for skipgram than for CBOW with 200 dimensions. One possible explanation is that the binarization algorithm computes, for each dimension of the word vectors, the positive and negative means of the values and uses it to split the original values in that dimension, thus behaving like a model that clusters the values in each dimension. The success of the binarization then indicates that skipgram embeddings might be more discriminative than CBOW embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparing Training Methods</head><p>We compare the training methods for the CBOW model in <ref type="table">Table 2</ref>. For stochastic rounding, we s- cale the probability of rounding up to make sure that small gradient values will still update the val- ues. Both stochastic rounding and truncation with auxiliary update vectors (shown in Sec. 3.2) re- quire 16 bits for each value in the training phase. Truncation with auxiliary update vectors finally produces 8-bit-value based vectors while stochas- tic rounding produces 16-bit-value based vectors. Even though our auxiliary update algorithm uses smaller memory/disk to store vectors, its perfor- mance is still better than that of stochastic round- ing. This is simply because the update values in SGD are too small to allow the stochastic round- <ref type="table">Table 2</ref>: Comparing the training CBOW models: We set the average value of the original word2vec embeddings to be 1, and the values in the table are relative to the original embeddings baselines. "avg. (w.)" represents the average values of all word similarity datasets. "avg. (b.)" represents the average values of all bigram phrase similarity datasets. "Stoch. (16 b.)" represents the method using stochastic rounding applied to 16-bit precision. "Trunc. (8 b.)" represents the method using truncation with 8-bit auxiliary update vectors applied to 8-bit precision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on Dependency Parsing</head><p>We also incorporate word embedding results into a downstream task, dependency parsing, to eval- uate whether the truncated embedding results are still good features compared to the original fea- tures. We follow the setup of ( <ref type="bibr" target="#b8">Guo et al., 2015</ref>) in a monolingual setting 3 . We train the parser with 5,000 iterations using different truncation set- tings for word2vec embedding. The data used to train and evaluate the parser is the English data in the CoNLL-X shared task <ref type="bibr" target="#b3">(Buchholz and Marsi, 2006</ref>). We follow ( <ref type="bibr" target="#b8">Guo et al., 2015</ref>) in using the labeled attachment score (LAS) to evaluate the different parsing results. Here we only show the word embedding results for 200 dimensions, since empirically we found 25-dimension results were not as stable as 200 dimensions. The results shown in <ref type="table" target="#tab_1">Table 3</ref> for dependency parsing are consistent with word similarity and paraphrasing. First, we see that binarization for CBOW and skipgram is again better than the trun- cation approach. Second, for truncation results, more bits leads to better results. With 8-bits, we can again obtain results similar to those obtained from the original word2vec embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We systematically evaluated how small can the representation size of dense word embedding be before it starts to impact the performance of NLP tasks that use them. We considered both the final size of the size we provide it while learning it. Our study considers both the CBOW and the skipgram models at 25 and 200 dimensions and showed that 8 bits per dimension (and sometimes even less) are sufficient to represent each value and maintain per- formance on a range of lexical tasks. We also pro- vided two ways to train the embeddings with re- duced memory use. The natural future step is to extend these experiments and study the impact of the representation size on more advanced tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>a) CBOW model with 25 dimensions. (b) Skipgram model with 25 dimensions. (c) CBOW model with 200 dimensions. (d) Skipgram model with 200 dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparing performance on multiple similarity tasks, with different values of truncation. The y-axis represents the Spearman's rank correlation coefficient for word similarity datasets, and the cosine value for paraphrase (bigram) datasets (see Sec. 4.2).</figDesc><graphic url="image-4.png" coords="3,83.34,361.42,430.87,80.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Auxiliary update vectors achieve very similar results to the original vectors, and, in fact, result in almost the same vectors as produced by the original truncation method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Average 
CBOW 
Skipgram 
Original Binary 
4-bits 
6-bits 
8-bits 
Original Binary 
4-bits 
6-bits 
8-bits 
wordsim (25) 
0.5331 
0.4534 0.5223 0.5235 0.5242 
0.4894 
0.4128 0.4333 0.4877 0.4906 
wordsim (200) 
0.5818 
0.5598 0.4542 0.5805 0.5825 
0.5642 
0.5588 0.4681 0.5621 0.5637 
bigram (25) 
0.3023 
0.2553 0.3164 0.3160 0.3153 
0.3110 
0.2146 0.2498 0.3050 0.3082 
bigram (200) 
0.3864 
0.3614 0.2954 0.3802 0.3858 
0.3565 
0.3562 0.2868 0.3529 0.3548 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Evaluation results for dependency 
parsing (in LAS). 
Bits 
CBOW Skipgram 
Original 88.58% 88.15% 
Binary 89.25% 88.41% 
4-bits 
87.56% 86.46% 
6-bits 
88.62% 87.98% 
8-bits 
88.63% 88.16% 

</table></figure>

			<note place="foot" n="1"> https://dumps.wikimedia.org/ 2 http://www.wordvectors.org/</note>

			<note place="foot" n="3"> https://github.com/jiangfeng1124/ acl15-clnndep</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors thank Shyam Upadhyay for his help with the dependency parser embeddings results, and Eric Horn for his help with this write-up. This work was supported by DARPA under agreemen-t numbers HR0011-15-2-0025 and FA8750-13-2-0008. The U.S. Government is authorized to re-produce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or im-plied, of any of the organizations that supported the work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Researc</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">C</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Conll-x shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="149" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">Sparse overcomplete word vector representations. In ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1491" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Revisiting embedding features for simple semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="110" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing based on distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailash</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritish</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1737" to="1746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><forename type="middle">L</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><forename type="middle">D</forename><surname>Well</surname></persName>
		</author>
		<title level="m">Research Design &amp; Statistical Analysisn. Routledge</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-09</title>
		<meeting>CoNLL-09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="345" to="358" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
