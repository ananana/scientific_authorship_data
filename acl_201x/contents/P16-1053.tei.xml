<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Sentence Interaction Network for Modeling Dependence between Sentences</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Lab. of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science and Technology</orgName>
								<orgName type="laboratory">National Lab. for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Lab. of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science and Technology</orgName>
								<orgName type="laboratory">National Lab. for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Samsung R&amp;D Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Samsung R&amp;D Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">State Key Lab. of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science and Technology</orgName>
								<orgName type="laboratory">National Lab. for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Sentence Interaction Network for Modeling Dependence between Sentences</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="558" to="567"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modeling interactions between two sentences is crucial for a number of natural language processing tasks including Answer Selection, Dialogue Act Analysis , etc. While deep learning methods like Recurrent Neural Network or Convo-lutional Neural Network have been proved to be powerful for sentence modeling, prior studies paid less attention on interactions between sentences. In this work, we propose a Sentence Interaction Network (SIN) for modeling the complex interactions between two sentences. By introducing &quot;interaction states&quot; for word and phrase pairs, SIN is powerful and flexible in capturing sentence interactions for different tasks. We obtain significant improvements on Answer Selection and Dialogue Act Analysis without any feature engineering .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There exist complex interactions between sen- tences in many natural language processing (NLP) tasks such as Answer Selection ( <ref type="bibr" target="#b24">Yu et al., 2014;</ref><ref type="bibr" target="#b23">Yin et al., 2015)</ref>, Dialogue Act Analysis <ref type="bibr" target="#b7">(Kalchbrenner and Blunsom, 2013)</ref>, etc. For instance, given a question and two candidate answers below, though they are all talking about cats, only the first Q What do cats look like? A1 Cats have large eyes and furry bodies. A2 Cats like to play with boxes and bags.</p><p>answer correctly answers the question about cats' appearance. It is important to appropriately model the relation between two sentences in such cases. * Correspondence author For sentence pair modeling, some methods first project the two sentences to fix-sized vectors sep- arately without considering the interactions be- tween them, and then fed the sentence vectors to other classifiers as features for a specific task <ref type="bibr" target="#b7">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b18">Tai et al., 2015</ref>). Such methods suffer from being unable to encode context information during sentence em- bedding.</p><p>A more reasonable way to capture sentence in- teractions is to introduce some mechanisms to uti- lize information from both sentences at the same time. Some methods attempt to introduce an at- tention matrix which contains similarity scores be- tween words and phrases to approach sentence in- teractions <ref type="bibr" target="#b15">(Socher et al., 2011;</ref><ref type="bibr" target="#b23">Yin et al., 2015)</ref>. While the meaning of words and phrases may drift from contexts to contexts, simple similarity scores may be too weak to capture the complex interac- tions, and a more powerful interaction mechanism is needed.</p><p>In this work, we propose a Sentence Interaction Network (SIN) focusing on modeling sentence in- teractions. The main idea behind this model is that each word in one sentence may potentially in- fluence every word in another sentence in some degree (the word "influence" here may refer to "answer" or "match" in different tasks). So, we introduce a mechanism that allows information to flow from every word (or phrase) in one sen- tence to every word (or phrase) in another sen- tence. These "information flows" are real-valued vectors describing how words and phrases interact with each other, for example, a word (or phrase) in one sentence can modify the meaning of a word (or phrase) in another sentence through such "in- formation flows".</p><p>Specifically, given two sentences s 1 and s 2 , for every word x t in s 1 , we introduce a "candidate interaction state" for every word x τ in s 2 . This state is regarded as the "influence" of x τ to x t , and is actually the "information flow" from x τ to x t mentioned above. By summing over all the "can- didate interaction states", we generate an "interac- tion state" for x t , which represents the influence of the whole sentence s 2 to word x t . When feeding the "interaction state" and the word embedding to- gether into Recurrent Neural Network (with Long Short-Time Memory unit in our model), we ob- tain a sentence vector with context information encoded. We also add a convolution layer on the word embeddings so that interactions between phrases can also be modeled.</p><p>SIN is powerful and flexible for modeling sen- tence interactions in different tasks. First, the "in- teraction state" is a vector, compared with a single similarity score, it is able to encode more informa- tion for word or phrase interactions. Second, the interaction mechanism in SIN can be adapted to different functions for different tasks during train- ing, such as "word meaning adjustment" for Di- alogue Act Analysis or "Answering" for Answer Selection.</p><p>Our main contributions are as follows:</p><p>• We propose a Sentence Interaction Network (SIN) which utilizes a new mechanism to model sentence interactions.</p><p>• We add convolution layers to SIN, which im- proves the ability to model interactions be- tween phrases.</p><p>• We obtain significant improvements on An- swer Selection and Dialogue Act Analysis without any handcrafted features.</p><p>The rest of the paper is structured as follows: We survey related work in Section 2, introduce our method in Section 3, present the experiments in Section 4, and summarize our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is mainly related to deep learning for sentence modeling and sentence pair modeling.</p><p>For sentence modeling, we have to first repre- sent each word as a real-valued vector <ref type="bibr" target="#b10">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b13">Pennington et al., 2014)</ref> , and then com- pose word vectors into a sentence vector. Several methods have been proposed for sentence model- ing. Recurrent Neural Network (RNN) <ref type="bibr" target="#b1">(Elman, 1990;</ref><ref type="bibr" target="#b10">Mikolov et al., 2010</ref>) introduces a hidden state to represent contexts, and repeatedly feed the hidden state and word embeddings to the network to update the context representation. RNN suf- fers from gradient vanishing and exploding prob- lems which limit the length of reachable context. RNN with Long Short-Time Memory Network unit (LSTM) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b2">Gers, 2001</ref>) solves such problems by introducing a "memory cell" and "gates" into the network. Re- cursive Neural Network ( <ref type="bibr" target="#b16">Socher et al., 2013;</ref><ref type="bibr" target="#b14">Qian et al., 2015)</ref> and LSTM over tree structures ( <ref type="bibr" target="#b18">Tai et al., 2015</ref>) are able to utilize some syntactic information for sentence modeling. <ref type="bibr" target="#b8">Kim (2014)</ref> proposed a Convolutional Neural Network (CNN) for sentence classification which models a sentence in multiple granularities.</p><p>For sentence pair modeling, a simple idea is to first project the sentences to two sentence vectors separately with sentence modeling methods, and then feed these two vectors into other classifiers for classification <ref type="bibr" target="#b18">(Tai et al., 2015;</ref><ref type="bibr" target="#b24">Yu et al., 2014;</ref>. The drawback of such meth- ods is that separately modeling the two sentences is unable to capture the complex sentence inter- actions. <ref type="bibr" target="#b15">Socher et al. (2011)</ref> model the two sen- tences with Recursive Neural Networks (Unfold- ing Recursive Autoencoders), and then feed sim- ilarity scores between words and phrases (syntax tree nodes) to a CNN with dynamic pooling to cap- ture sentence interactions. <ref type="bibr" target="#b6">Hu et al. (2014)</ref> first create an "interaction space" (matching score ma- trix) by feeding word and phrase pairs into a multi- layer perceptron (MLP), and then apply CNN to such a space for interaction modeling. <ref type="bibr" target="#b23">Yin et al. (2015)</ref> proposed an Attention based Convolutional Neural Network (ABCNN) for sentence pair mod- eling. ABCNN introduces an attention matrix be- tween the convolution layers of the two sentences, and feed the matrix back to CNN to model sen- tence interactions. There are also some methods that make use of rich lexical semantic features for sentence pair modeling ( <ref type="bibr" target="#b22">Yih et al., 2013;</ref>), but these methods can not be easily adapted to different tasks.</p><p>Our work is also related to context modeling. <ref type="bibr" target="#b4">Hermann et al. (2015)</ref> proposed a LSTM-based method for reading comprehension. Their model is able to effectively utilize the context (given by a document) to answer questions. <ref type="bibr" target="#b3">Ghosh et al. (2016)</ref> proposed a Contextual LSTM (CLSTM) which introduces a topic vector into LSTM for context modeling. The topic vector in CLSTM is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background: RNN and LSTM</head><p>Recurrent Neural Network (RNN) <ref type="bibr" target="#b1">(Elman, 1990;</ref><ref type="bibr" target="#b10">Mikolov et al., 2010)</ref>, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>(a), is proposed for modeling long-distance dependence in a sequence. Its hidden layer is connected to it- self so that previous information is considered in later times. RNN can be formalized as</p><formula xml:id="formula_0">h t = f (W x x t + W h h t−1 + b h )</formula><p>where x t is the input at time step t and h t is the hidden state. Though theoretically, RNN is able to capture dependence of arbitrary length, it tends to suffer from the gradient vanishing and explod- ing problems which limit the length of reachable context. In addition, an additive function of the previous hidden layer and the current input is too simple to describe the complex interactions within a sequence.</p><p>RNN with Long Short-Time Memory Network unit (LSTM, <ref type="figure" target="#fig_0">Figure 1</ref>(b)) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b2">Gers, 2001</ref>) solves such problems by introducing a "memory cell" and "gates" into the network. Each time step is associated with a sub- net known as a memory block in which a "memory cell" stores the context information and "gates" control which information should be added or dis- carded or reserved. LSTM can be formalized as</p><formula xml:id="formula_1">f t = σ(W f · [x t , h t−1 ] + b f ) i t = σ(W i · [x t , h t−1 ] + b i ) ˜ C t = tanh(W C · [x t , h t−1 ] + b C ) C t = f t * C t−1 + i t * ˜ C t o t = σ(W o · [x t , h t−1 ] + b o ) h t = o t * tanh(C t )</formula><p>where * means element-wise multiplication, f t , i t , o t is the forget, input and output gate that control which information should be forgot, input and output, respectively. ˜ C t is the candidate infor- mation to be added to the memory cell state C t . h t is the hidden state which is regarded as a represen- tation of the current time step with contexts.</p><p>In this work, we use LSTM with peephole con- nections, namely adding C t−1 to compute the for- get gate f t and the input gate i t , and adding C t to compute the output gate o t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Interaction Network (SIN)</head><p>Sentence Interaction Network (SIN, <ref type="figure" target="#fig_2">Figure 2</ref>) models the interactions between two sentences in two steps.</p><p>First, we use a LSTM (referred to as LSTM 1 ) to model the two sentences s 1 and s 2 separately, and the hidden states related to the t-th word in s 1 and the τ -th word in s 2 are denoted as z (1) t and z <ref type="bibr">(2)</ref> τ respectively. For simplicity, we will use the position (t, τ ) to denote the corresponding words hereafter.</p><p>Second, we propose a new mechanism to model the interactions between s 1 and s 2 by allowing information to flow between them. Specifically, word t in s 1 may be potentially influenced by all words in s 2 in some degree. Thus, for word t in s 1 , a candidate interaction state˜cstate˜ state˜c </p><formula xml:id="formula_2">˜ c (i) tτ = tanh(W (i) c · [z (1) t , z (2) τ ] + b (i) c ) i (i) tτ = σ(W (i) i · [z (1) t , z (2) τ ] + b (i) i )</formula><p>here, the superscript "i" indicates "interaction".</p><formula xml:id="formula_3">W (i) c , W (i) i , b (i) c , b (i) i are model parameters. The interaction state c (i)</formula><p>t for word t in s 1 can then be formalized as</p><formula xml:id="formula_4">c (i) t = |s 2 | τ =1˜c =1˜ =1˜c (i) tτ * i (i) tτ</formula><p>where |s 2 | is the length of sentence s 2 , and c (i) t can be viewed as the total interaction information received by word t in s 1 from sentence s 2 . The interaction states of words in s 2 can be similarly tτ while sharing the model param- eters.</p><p>We now introduce the interaction states into another LSTM (referred to as LSTM 2 ) to com- pute the sentence vectors. Therefore, information can flow between the two sentences through these states. For sentence s 1 , at timestep t, we have</p><formula xml:id="formula_5">f t = σ(W f · [x t , h t−1 , c (i) t , C t−1 ] + b f ) i t = σ(W i · [x t , h t−1 , c (i) t , C t−1 ] + b i ) ˜ C t = tanh(W C · [x t , h t−1 , c (i) t ] + b C ) C t = f t * C t−1 + i t * ˜ C t o t = σ(W o · [x t , h t−1 , c (i) t , C t ] + b o ) h t = o t * tanh(C t )</formula><p>By averaging all hidden states of LSTM 2 , we ob- tain the sentence vector v s 1 of s 1 , and the sentence vector v s 2 of s 2 can be computed similarly. v s 1 and v s 2 can then be used as features for different tasks.</p><p>In SIN, the candidate interaction state˜cstate˜ state˜c</p><formula xml:id="formula_6">(i)</formula><p>tτ rep- resents the potential influence of word τ in s 2 to word t in s 1 , and the related input gate i t gives the influence of the whole sentence s 2 to word t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SIN with Convolution (SIN-CONV)</head><p>SIN is good at capturing the complex interactions of words in two sentences, but not strong enough for phrase interactions. Since convolutional neural network is widely and successfully used for mod- eling phrases, we add a convolution layer before SIN to model phrase interactions between two sen- tences.</p><p>Let v 1 , v 2 , ..., v |s| be the word embeddings of a sentence s, and let c i ∈ R wd , 1 ≤ i ≤ |s| − w + 1, be the concatenation of v i:i+w−1 , where w is the window size. The representation p i for phrase v i:i+w−1 is computed as:</p><formula xml:id="formula_7">p i = tanh(F · c i + b)</formula><p>where F ∈ R d×wd is the convolution filter, and d is the dimension of the word embeddings.</p><p>In SIN-CONV, we first use a convolution layer to obtain phrase representations for the two sen- tences s 1 and s 2 , and the SIN interaction proce- dure is then applied to these phrase representations as before to model phrase interactions. The aver- age of all hidden states are treated as sentence vec- tors v cnn s 1 and v cnn s 2 . Thus, SIN-CONV is SIN with word vectors substituted by phrase vectors. The two phrase-based sentence vectors are then fed to a classifier along with the two word-based sentence vectors together for classification.</p><p>The LSTM and interaction parameters are not shared between SIN and SIN-CONV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we test our model on two tasks: Answer Selection and Dialogue Act Analysis. Both tasks require to model interactions between sentences. We also conduct auxiliary experiments for analyzing the interaction mechanism in our SIN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Answer Selection</head><p>Selecting correct answers from a set of candidates for a given question is quite crucial for a number of NLP tasks including question-answering, natu- ral language generation, information retrieval, etc.</p><p>The key challenge for answer selection is to appro- priately model the complex interactions between the question and the answer, and hence our SIN model is suitable for this task.</p><p>We treat Answer Selection as a classification task, namely to classify each question-answer pair as "correct" or "incorrect". Given a question- answer pair (q, a), after generating the question and answer vectors v q and v a using SIN, we feed them to a logistic regression layer to output a prob- ability. And we maximize the following objective function:</p><formula xml:id="formula_8">p θ (q, a) = σ(W · [v q , v a ]) + b) L = (q,a) ˆ y q,a log p θ (q, a)+ (1 − ˆ y q,a ) log(1 − p θ (q, a))</formula><p>wherê y q,a is the true label for the question-answer pair (q, a) (1 for correct, 0 for incorrect). For SIN- CONV, the sentence vector v cnn q and v cnn a are also fed to the logistic regression layer.</p><p>During evaluation, we rank the answers of a question q according to the probability p θ (q, a). The evaluation metrics are mean average precision (MAP) and mean reciprocal rank (MRR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Dataset</head><p>The WikiQA <ref type="bibr">2</ref>  correct answers from the development and test set. Some statistics are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Setup</head><p>We use the 100-dimensional GloVe vectors 3 (Pen- nington et al., 2014) to initialize our word embed- dings, and those words that do not appear in Glove vectors are treated as unknown. The dimension of all hidden states is set to 100 as well. The window size of the convolution layer is 2. To avoid overfit- ting, dropout is introduced to the sentence vectors, namely setting some dimensions of the sentence vectors to 0 with a probability p (0.5 in our experi- ment) randomly. No handcrafted features are used in our methods and the baselines. Mini-batch Gradient Descent (30 question- answer pairs for each mini batch), with AdaDelta tuning learning rate, is used for model training. We update model parameters after every mini batch, check validation MAP and save model af- ter every 10 batches. We run 10 epochs in to- tal, and the model with highest validation MAP is treated as the optimal model, and we report the corresponding test MAP and MRR metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Baselines</head><p>We compare our SIN and SIN-CONV model with 5 baselines listed below:</p><p>• LCLR: The model utilizes rich semantic and lexical features <ref type="bibr" target="#b22">(Yih et al., 2013</ref>).</p><p>• PV: The cosine similarity score of paragraph vectors of the two sentences is used to rank answers ( <ref type="bibr" target="#b9">Le and Mikolov, 2014</ref>).</p><p>• CNN: Bigram CNN ( <ref type="bibr" target="#b24">Yu et al., 2014</ref>).</p><p>• ABCNN: Attention based CNN, no hand- crafted features are used here ( <ref type="bibr" target="#b23">Yin et al., 2015</ref>).</p><p>• LSTM:</p><p>The question and answer are modeled by a simple LSTM. Different from SIN, there is no interaction between sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Results</head><p>Results are shown in <ref type="table">Table 2</ref>. SIN performs much better than LSTM, PV and CNN, this justifies that the proposed interaction mechanism well captures the complex interactions between the question and the answer. But SIN performs slightly worse than ABCNN because it is not strong enough at model- ing phrases. By introducing a simple convolution layer to improve its phrase-modeling ability, SIN- CONV outperforms all the other models.</p><p>For SIN-CONV, we do not observe much im- provements by using larger convolution filters (window size ≥ 3) or stacking more convolution layers. The reason may be the fact that interactions between long phrases is relatively rare, and in ad- dition, the QA pairs in the WikiQA dataset may be insufficient for training such a complex model with long convolution windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dialogue Act Analysis</head><p>Dialogue acts (DA), such as Statement, Yes-No- Question, Agreement, indicate the sentence prag- matic role as well as the intention of the speakers (Williams, 2012). They are widely used in natu- ral language generation ), speech and meeting summarization ( <ref type="bibr" target="#b11">Murray et al., 2006;</ref><ref type="bibr" target="#b12">Murray et al., 2010)</ref>, etc. In a dialogue, the DA of a sentence is highly relevant to the content of itself and the previous sentences. As a result, to model the interactions and long-range dependence between sentences in a dialogue is crucial for dia- logue act analysis.</p><p>Given a dialogue (n sentences) d = [s 1 , s 2 , ..., s n ], we first use a LSTM (LSTM 1 ) to model all the sentences independently. The hidden states of sentence s i obtained at this step are used to compute the interaction states of sentence s i+1 , and SIN will generate a sentence vector v s i using another LSTM (LSTM 2 ) for each sentence s i in the dialogue (see Section 3.2) . These sentence vectors can be used as features for dialogue act analysis. We refer to this method as SIN (or SIN-CONV for adding a convolution layer).</p><p>For dialogue act analysis, we add a softmax layer on the sentence vector v s i to predict the prob- ability distribution:</p><formula xml:id="formula_9">p θ (y j |v s i ) = exp(v T s i · w j + b j ) k exp(v T s i · w k + b k ) 4</formula><p>With extra handcrafted features, ABCNN's performance is: MAP(0.692), MRR(0.711).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>MAP MRR LCLR 0.599 0.609 PV 0.511 0.516 CNN 0.619 0.628 ABCNN 0.660 0.677 LSTM 0.634 0.648 SIN 0.657 0.672 SIN-CONV 0.674 0.693 <ref type="table">Table 2</ref>: Results on answer selection 4 .  where y j is the j-th DA tag, w j and b j is the weight vector and bias corresponding to y j . We maximize the following objective function:</p><formula xml:id="formula_10">L = d∈D |d| i=1 log p θ (ˆ y s i |v s i )</formula><p>where D is the training set, namely a set of dia- logues, |d| is the length of the dialogue, s i is the i-th sentence in d, ˆ y s i is the true dialogue act label of s i .</p><p>In order to capture long-range dependence in the dialogue, we can further join up the sentence vector v s i with another LSTM <ref type="bibr">(LSTM 3</ref> ). The hidden state h s i of LSTM 3 are treated as the fi- nal sentence vector, and the probability distri- bution is given by substituting v s i with h s i in p θ (y j |v s i ). We refer to this method as SIN-LD (or SIN-CONV-LD for adding a convolution layer), where LD means long-range dependence. <ref type="figure" target="#fig_4">Figure  3</ref> shows the whole structure (LSTM 1 is not shown here for simplicity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dialogue Act</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Train(%) Test(%) Statement-non-Opinion</p><p>Me, I'm in the legal department. 37.0 31.5 Backchannel/Acknowledge Uh-huh.</p><p>18.8 18.3 Statement-Opinion I think it's great 12.8 17.2 Abandoned/Uninterpretable So,-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">8.6 Agreement/Accept</head><p>That's exactly it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">5.0 Appreciation</head><p>I can imagine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">1.8 Yes-No-Question</head><p>Do you have to have any special training?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">2.0 Non-Verbal</head><p>[Laughter], [Throat-clearing]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.8">2.3 Yes-Answers</head><p>Yes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">1.7 Conventional-closing</head><p>Well, it's been nice talking to you. <ref type="formula">(32)</ref> 9.1 9.8 Total number of sentences 196258 4186 Total number of dialogues 1115 19 <ref type="table">Table 3</ref>: Dialogue act labels</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">1.9 Other Labels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Dataset</head><p>We use the Switch-board Dialogue Act (SwDA) corpus <ref type="bibr" target="#b0">(Calhoun et al., 2010</ref>) in our experiments 5 .</p><p>SwDA contains the transcripts of several people discussing a given topic on the telephone. There are 42 dialogue act tags in SwDA, <ref type="bibr">6</ref> and we list the 10 most frequent tags in <ref type="table">Table 3</ref>. The same data split as in <ref type="bibr" target="#b17">Stolcke et al. (2000)</ref> is used in our experiments. There are 1,115 dia- logues in the training set and 19 dialogues in the test set <ref type="bibr">7</ref> . We also randomly split the original train- ing set as a new training set (1,085 dialogues) and a validation set (30 dialogues).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Setup</head><p>The setup is the same as that in Answer Selection except: (1) Only the most common 10,000 words are used, other words are all treated as unknown.</p><p>(2) Each mini batch contains all sentences from 3 dialogues for Mini-batch Gradient Descent. <ref type="formula">(3)</ref> The evaluation metric is accuracy. (4) We run 30 epochs in total. (5) We use the last hidden state of LSTM 2 as sentence representation since the sen- tences here are much shorter compared with those in Answer Selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Baselines</head><p>We compare with the following baselines:</p><p>• unigram, bigram, trigram LM-HMM: HMM variants ( <ref type="bibr" target="#b17">Stolcke et al., 2000</ref>  <ref type="table" target="#tab_2">Table 4</ref>: Accuracy on dialogue act analysis. Inter- annotator agreement is 84%.</p><p>• RCNN: Recurrent Convolutional Neural Net- works <ref type="bibr" target="#b7">(Kalchbrenner and Blunsom, 2013)</ref>. Sentences are first separately embedded with CNN, and then joined up with RNN.</p><p>• LSTM: All sentences are modeled separately by one LSTM. Different from SIN, there is no sentence interactions in this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Results</head><p>Results are shown in  Q: what creates a cloud A: in meteorology , a cloud is a visible mass of liquid droplets or frozen crystals made of water or various chemicals suspended in the atmosphere above the surface of a planetary body. long-range dependence in the dialogue with an- other LSTM, and obtain further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interaction Mechanism Analysis</head><p>We investigate into the interaction states of SIN for Answer Selection to see how our proposed in- teraction mechanism works. Given a question-answer pair in <ref type="table" target="#tab_3">Table 5</ref>, for SIN, there is a candidate interaction state˜cstate˜ state˜c (i) τ t and an input gate i (i) τ t from each word t in the ques- tion to each word τ in the answer. We investigate into the L 2 -norm ||˜c||˜c</p><formula xml:id="formula_11">(i) τ t * i (i)</formula><p>τ t || 2 to see how words in the two sentences interact with each other. Note that we have linearly mapped the original L 2 -norm value to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> as follows:</p><formula xml:id="formula_12">f (x) = x − x min x max − x min</formula><p>As depicted in <ref type="figure" target="#fig_6">Figure 4</ref>, we can see that the word "what" in the question has little impact to the answer through interactions. This is reason- able since "what" appears frequently in questions, and does not carry much information for answer selection <ref type="bibr">8</ref> . On the contrary, the phrase "creates a cloud", especially the word "cloud", transmits much information through interactions to the an- swer, this conforms with human knowledge since we rely on these words to answer the question as well.</p><p>In the answer, interactions concentrate on the phrase "a cloud is a visible mass of liquid droplets" which seems to be a good and com- plete answer to the question. Although there are also other highly related words in the answer, they are almost ignored. The reason may be failing to model such a complex phrase (three relatively sim- ple sentences joined by "or") or the existence of the previous phrase which is already a good an- swer.</p><p>This experiment clearly shows how the interac- tion mechanism works in SIN. Through interac- tion states, SIN is able to figure out what the ques- tion is asking about, namely to detect those highly informative words in the question, and which part in the answer can answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this work, we propose Sentence Interaction Net- work (SIN) which utilizes a new mechanism for modeling interactions between two sentences. We also introduce a convolution layer into SIN (SIN- CONV) to improve its phrase modeling ability so that phrase interactions can be handled. SIN is powerful and flexible to model sentence interac- tions for different tasks. Experiments show that the proposed interaction mechanism is effective, and we obtain significant improvements on An- swer Selection and Dialogue Act Analysis without any handcrafted features.</p><p>Previous works have showed that it is important to utilize the syntactic structures for modeling sen- tences. We also find out that LSTM is sometimes unable to model complex phrases. So, we are go- ing to extend SIN to tree-based SIN for sentence modeling as future work. Moreover, applying the models to other tasks, such as semantic relatedness measurement and paraphrase identification, would also be interesting attempts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: RNN (a) and LSTM (b) 1</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,219.18,82.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>tτ and an input gate i (i) tτ are introduced for each word τ in s 2 as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SIN for modeling sentence s 1 at timestep t. First, we model s 1 and s 2 separately with LSTM 1 and obtain the hidden states z (1) t for s 1 and z (2) τ for s 2. Second, we compute interaction states based on these hidden states, and incorporate c (i) t into LSTM 2. Information flows (interaction states) from s 1 to s 2 are not depicted here for simplicity.</figDesc><graphic url="image-2.png" coords="4,119.45,62.80,358.67,220.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>tτ con- trols the degree of the influence. The element-wise multiplicatioñ c (i) tτ * i (i) tτ is then the actual influence. By summing over all words in s 2 , the interaction state c (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SIN-LD for dialogue act analysis. LSTM 1 is not shown here for simplicity. x</figDesc><graphic url="image-3.png" coords="6,307.28,208.70,219.18,148.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc>s j ) t means word t in s j , c (i,s j ) t means the interaction state for word t in s j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: L 2-norm of the interaction states from question to answer (linearly mapped to [0, 1]).</figDesc><graphic url="image-4.png" coords="8,74.61,62.81,448.31,105.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>(Yang et al., 2015) dataset is used for this task. Following Yin et al. (2015), we filtered out those questions that do not have any</figDesc><table>2 http://aka.ms/WikiQA 

Q 
QA pair A/Q correct A/Q 
Train 2,118 20,360 9.61 
0.49 
Dev 
126 
1,130 
8.97 
1.11 
Test 
243 
2,351 
9.67 
1.21 

Table 1: Statistics of WikiQA (Q=Question, 
A=Answer) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>HMM variants, 
RCNN and LSTM model the sentences separately 
during sentence embedding, and are unable to cap-
ture the sentence interactions. With our inter-
action mechanism, SIN outperforms LSTM, and 
proves that well modeling the interactions be-
tween sentences in a dialogue is important for di-
alogue act analysis. After introducing a convo-
lution layer, SIN-CONV performs slightly better 
than SIN. SIN-LD and SIN-CONV-LD model the </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 : A question-answer pair example.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> This figure referred to http://colah.github.io/posts/201508-Understanding-LSTMs/</note>

			<note place="foot" n="3"> http://nlp.stanford.edu/projects/glove/</note>

			<note place="foot" n="8"> Our statements focus on the interaction, in a sense of &quot;answering&quot; or &quot;matching&quot;. Definitely, such words like &quot;what&quot; and &quot;why&quot; are very important for answering questions from the general QA perspective since they determine the type of answers.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The nxt-format switchboard corpus: a rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">M</forename><surname>Brenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Beaver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="387" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Long short-term memory in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Lausanne, Switzerland</pubPlace>
		</imprint>
		<respStmt>
			<orgName>´ Ecole Polytechnique Fédérale de Lausanne</orgName>
		</respStmt>
	</monogr>
	<note>Unpublished PhD dissertation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06291</idno>
		<title level="m">Contextual lstm (clstm) models for large scale nlp tasks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.3584</idno>
		<title level="m">Recurrent convolutional neural networks for discourse compositionality</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incorporating speaker and discourse features into speech summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating and validating abstracts of meeting conversations: a user study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Natural Language Generation Conference</title>
		<meeting>the 6th International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning tag embeddings and tag-specific composition functions in recursive neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dialogue act modeling for automatic tagging and recognition of conversational speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Van Ess-Dykema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Meteer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="373" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01745</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A belief tracking challenge task for spoken dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT Workshop on Future Directions and Needs in the Spoken Dialog Community: Tools and Data</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="23" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pastusiak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05193</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1632</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Long short-term memory over tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04881</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
