<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Omnia Mutantur, Nihil Interit: Connecting Past with Present by Find-ing Corresponding Terms across Time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yating</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Kyoto University</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Kyoto University</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><forename type="middle">S</forename><surname>Bhowmick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Kyoto University</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsumi</forename><surname>Tanaka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">School of Computer Engineering</orgName>
								<orgName type="institution" key="instit1">Kyoto University</orgName>
								<orgName type="institution" key="instit2">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Omnia Mutantur, Nihil Interit: Connecting Past with Present by Find-ing Corresponding Terms across Time</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="645" to="655"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In the current fast-paced world, people tend to possess limited knowledge about things from the past. For example, some young users may not know that Walkman played similar function as iPod does nowadays. In this paper, we approach the temporal correspondence problem in which, given an input term (e.g., iPod) and the target time (e.g. 1980s), the task is to find the counterpart of the query that existed in the target time. We propose an approach that transforms word contexts across time based on their neural network representations. We then experimentally demonstrate the effectiveness of our method on the New York Times Annotated Corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>What music device 30 years ago played similar role as iPod does nowadays? Who are today's Beatles? Who was a counterpart of President Chi- rac in 1988? These and many other similar ques- tions may be difficult to answer by average users (especially, by young ones). This is because peo- ple tend to possess less knowledge about the past than about the contemporary time.</p><p>In this work we propose an effective method to solve the problem of finding counterpart terms across time. In particular, for an input pair of a term (e.g., iPod) and the target time (e.g. 1980s), we find the corresponding term that existed in the target time (walkman). We consider temporal counterparts to be terms which are semantically similar, yet, which existed in different time.</p><p>Knowledge of temporal counterparts can help to alleviate the problem of terminology gap for us- ers searching within temporal document collec- tions such as archives. For example, given a user's query and the target time frame, a new modified query that represents the same meaning could be suggested to improve search results. Essentially, it would mean letting searchers use the knowledge they possess on the current world to perform search within unknown collections such as ones containing documents from the distant past. Fur- thermore, solving temporal correspondence prob- lem can help timeline construction, temporal sum- marization, reference forecasting and can have ap- plications in education.</p><p>The problem of temporal counterpart detection is however not trivial. The key difficulty comes from the change of the entire context that results in low overlap of context across time. In other words, it is difficult to find temporal counterpart terms by directly comparing context vectors across time. This fact is nicely portrayed by the Latin proverb: "omnia mutantur, nihil interit" (in English: "everything changes, nothing perishes") which indicates that there are no completely static things, yet, many things and concepts are still sim- ilar across time. Another challenge is the lack of training data. If we have had enough training pairs of input terms and their temporal counterparts, then it would have become possible to represent the task as a typical machine learning problem. However, it is difficult to collect multiple training pairs over various domains and for arbitrary time.</p><p>In view of the challenges mentioned above, we propose an approach that transforms term repre- sentations from one vector space (e.g., one de- rived from the present documents) to another vec- tor space (e.g., one obtained from the past docu- ments). Terms in both the vector spaces are repre- sented by the distributed vector representation ( <ref type="bibr" target="#b19">Mikolov et al. 2013a;</ref><ref type="bibr" target="#b21">Mikolov et al. 2013c</ref>). Our method then matches the terms by comparing their relative positions in the vector spaces of dif- ferent time periods alleviating the problem of low overlap between word contexts over time. It also does not require to manually prepare seed pairs of temporal counterparts. We further improve this method by automatically generating reference points that more precisely represent target terms in the form of local graphs. In result, our approach consists of finding global and local correspond- ence between terms over time.</p><p>To sum up, we make the following contribu- tions in this paper: (1) we propose an efficient method to find temporal counterparts by trans- forming the representation of terms within differ- ent temporal spaces, (2) we then enhance the global correspondence method by considering also the local context of terms (local correspond- ence) and (3) we perform extensive experiments on the New York Times Annotated Corpus <ref type="bibr" target="#b26">(Sandhaus, 2008)</ref>, including the search from the present to the past and vice versa, which prove the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Global Correspondence Across Time</head><p>Let the base time denoted as T B mean the time pe- riod associated with the input term and let the tar- get time, T T , mean the time period in which we want to find this term's counterparts. Typically, for users, the base time is the present time and the target time is some selected time period in the past. Note however, that we do not impose any re- striction on the order and the distance of the both times. Hence, it is possible to search for present counterparts of terms that existed in the past.</p><p>In our approach we first represent all the terms in the base time and in the target time within their respective semantic vector spaces, χ B and χ T . Then, we construct a transformation matrix to bridge the two vector spaces. Algorithm 1 sum- marizes the procedures needed to compute the global transformation. We will explain it in Sec- tion 2.1 and 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Overview of Global Transformation</head><p>Input: query q, base time T B and target time </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Vector space word representations</head><p>Distributed representation of words by neural network was first proposed by <ref type="bibr">Rumelhart et al. (1986)</ref>. More recently, <ref type="bibr" target="#b19">Mikolov et al. (2013a</ref><ref type="bibr" target="#b21">Mikolov et al. ( , 2013c</ref>) introduced the Skip-gram model which utilizes a simplified neural network architecture for learning vector representations of words from unstructured text data. We apply this model due to its advantages: <ref type="table" target="#tab_5">(1) it can capture precise semantic  word relationships; (2) due to the simplified neu- ral network architecture, the model can easily  scale to millions of words. After applying the  Skip-gram model, the documents in the base time,  D(T B )</ref>, are converted to a m×p matrix where n is the vocabulary size and p are the dimensions of feature vectors. Similarly, the documents in the target time, D(T T ), are represented as a n×q matrix (as shown in <ref type="figure">Fig. 1</ref>).</p><p>Figure 1: Word vector representations for the base and the target time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformation across vector spaces</head><p>Our goal is to compare words in the base time and the target time in order to find temporal counter- parts. However, it is impossible to directly com- pare words in two different semantic vector spaces, as the features in both spaces have no di- rect correspondence between each other (as can be seen in <ref type="figure">Fig. 1</ref>). To solve this problem, we propose to train a transformation matrix in order to build the connection between different vector spaces.</p><p>The key idea is that the relative positions of words in each vector space should remain more or less stable. In other words, a temporal counterpart term should have similar relative position in its own vector space as the position of the queried term in the base time space. <ref type="figure" target="#fig_0">Fig. 2</ref> conceptually portrays this idea as the correspondence between the context of Walkman and the context of iPod (only two dimensions are shown for simplicity). Our task is then to train the transformation ma- trix to automatically "rotate" the base vector space into the target vector space. Suppose we have K pairs of temporal counterparts {(1, w1),…,(k, wk,)} where i is a base time term and wi is its counterpart in the target time. Then the transfor- mation matrix Μ can be computed by minimizing the differences between Μ•i and wi as given in Eq. 1. The latter part of Eq. 1 is added as regular- ization to overcome the problem of overfitting. In- tuitively, matrix M is obtained by making sure that the sum of Euclidean 2-norms between trans- formed query vectors and their counterparts is minimal on K seed query-counterpart pairs. Eq.1 is used for solving regularized least squares prob- lem (γ equals to 0.02).</p><formula xml:id="formula_0">2 2 1 2 2 min arg M w M M K i i i M         (1)</formula><p>However, as mentioned before, the other chal- lenge is that the training pairs are difficult to be obtained. It is non-trivial to prepare large enough training data that would also cover various do- mains and any possible combinations of the base and target time periods. We apply here a simple trick that performs reasonably well. We select terms that (a) have the same syntactic forms in the base and the target time periods and (b) are fre- quent in the both time periods. Such Common Frequent Terms (CFTs) are then used as the train- ing data. Essentially, we assume here that very frequent terms (e.g., man, women, water, dog, see, three) change their meanings only to small extent. The reasoning is that the more frequently the word is used, the harder is to change its dominant mean- ing (or the longer time it takes to make the mean- ing shift) as the word is commonly used by many people. The phenomenon that words used more often in everyday language had evolved more slowly has been observed in several languages in- cluding English, Spanish, Russian and Greek ( <ref type="bibr" target="#b24">Pargel et al., 2007;</ref><ref type="bibr" target="#b16">Lieberman et al. 2007</ref>). Then, using the common frequent terms as the training pairs, we solve Eq. 1 as the least squares problem. Note that the number of CFTs is heuristically de- cided. In Sec. 5 we discuss transformation perfor- mance with regards to different numbers of CFTs.</p><p>After obtaining matrix Μ, we can then trans- form the base time term, q, first by multiplying its vector representation with the transformation ma- trix Μ, and then by calculating the cosine similar- ity between such transformed vector and the vec- tors of all the terms in the target time. We call the result of this similarity comparison the corre- spondence score between the input term q in the base time and a given term w in the target time (see Eq. 2). A term which has the highest corre- spondence score could be then considered as tem- poral counterpart of q.</p><formula xml:id="formula_1">    w q M w q ence Correspond , cos ,   (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Local Correspondence across Time</head><p>The method described above computes "global similarity" between terms across time. In result, the discovered counterparts can be similar to the query term for variety of reasons, some of which may not always lead to the best results. For in- stance, the global transformation finds VCR as the temporal counterpart of iPod in 1980s simply be- cause both of them can have recording and play- back functions. Macintosh is another term judged to be strongly corresponding to iPod since both are produced by Apple. Clearly, although VCR and Macintosh are somewhat similar to iPod, they are far from being its counterparts. The global transformation, as presented in the previous sec- tion, may thus fail to find correct counterparts due to neglecting fundamental relations between a query term and its context.</p><p>Inspired by these observations, we propose an- other method for leveraging the informative con- text terms of an input query term called reference points. They are used to help mapping the query to its correct temporal counterpart by considering the relation between the query and the reference points. We call this kind of similarity matching as local correspondence in contrast to global corre- spondence described in Sec. 2. In the following sub-sections, we first introduce the desired char- acteristics of the reference points and we then pro- pose three computation methods for selecting them. Finally, we describe how to find temporal counterparts using the selected reference points. Algorithm 2 shows the process of computing the local transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Overview of Local Transformation</head><p>Input: query q, base time T B and target time T T 1. Construct the local graph of q by detecting the reference points in the context of q.</p><p>(Section 3.1) 2. Compute similarity of the local graph of q with all the local graphs of candidate tem- poral counterparts in the target time. (Sec- tion 3.2) 3. Rank the candidate temporal counterparts in the target time by graph similarity score (Eq. 4). Output: ranked list of temporal counterparts</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reference points detection</head><p>Reference points are terms in the query's context which help to build connection between the query and its temporal counterparts. Reference points should have at least some of the following charac- teristics: (a) have high relation with the query (b) be sufficiently general and (c) be independent from each other.</p><p>Note that it does not mean that the selected ref- erence point should have exactly same surface form across time. Let us consider the previous ex- ample query iPod and 1980s as the target time. The term music could be a candidate reference point for this query. Its temporal counterpart has exactly the same syntax form in the target time (music). However, mp3 could be another refer- ence point. Even though mp3 did not exist in 1980s, it can still be referred to storage devices at the target time such as cassette or disk helping thus to find the correct counterparts of iPod, that is, walkman and CD player.</p><p>Since different reference points will lead to dif- ferent answers, we propose three methods for se- lecting the reference points. Each one considers the previously mentioned characteristics of refer- ence points to different extent. Note that, if neces- sary, the choice of the references points can be left to users.</p><p>Term co-occurrence. The first approach satis- fies the reference points' characteristics of being related to the query. To select reference points us- ing this approach we rank context terms by multi- plying two factors: tf(c) and relatedness(q,c), where tf(c) is the frequency of a context term c, while relatedness(q,c) is the relation strength of q and c measured by the χ 2 test. The test is con- ducted based on the hypothesis that P(c|q)=P(c|q̄ ), according to which the term c has the same probability of occurring in documents containing query q and in the documents not con- taining q. We then use the inverse of the p-value obtained from the test as relatedness(q,c).</p><p>Lexico-syntactic patterns. As the second ap- proach we propose using hypernyms of terms. This corresponds to the characteristic of reference points to be general words. General terms are pre- ferred rather than specific or detailed ones since the former are more probable to be associated with correct temporal counterparts <ref type="bibr">1</ref> . This is because detailed or specific terms are less likely to have corresponding terms in the target time. To detect hypernyms on the fly, we adopt the method pro- posed by <ref type="bibr" target="#b22">Ohshima et al. (2010)</ref> that uses bi-direc- tional lexico-syntactic patterns due to its high speed and the lack of requirements for using ex- ternal ontologies. The latter is important since, to the best of our knowledge, there are no ready on- tology resources for arbitrary periods in the past (e.g., there seems to be no Wordnet for the past).</p><p>Semantic clustering. The last method chooses reference points from clusters of context terms. The purpose of applying clustering is to avoid choosing semantically similar reference points. Clustering helps to select typical terms from dif- ferent sematic clusters to provide diverse informa- tive context.</p><p>For grouping the context terms we utilize the bisecting k-means algorithm. It is superior over k- means and the agglomerative approach ( <ref type="bibr" target="#b27">Steinbach et al., 2000</ref>) in terms of accuracy. The procedure of bisecting k-means is to, first, select a cluster to split and then to utilize the basic k-means to form two sub-clusters. These two steps are repeated un- til the desired number of clusters is obtained. The distance between any two terms w1, <ref type="bibr">w2</ref> is the in- verse of cosine similarity between their vector representations.</p><formula xml:id="formula_2">) , cos( 1 ) , ( 2 1 2 1 w w w w Dist   (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local graph matching</head><p>Formulation. The local graph of query q is a star shaped graph, denoted as Sq F B , in which q is the internal node, and the set of reference points, í µí°¹B = {f1, f2,…, fu}, are leaf nodes where u is the number of reference points. Our objective is to find a local graph Sw F T in the target vector space that is most similar to Sq F B in the base vector space. w denotes here the temporal counterpart of q and FT is the set of terms in the target vector space that corresponds to FB.</p><p>Algorithm.</p><p>Step <ref type="formula">(1)</ref>: to compare the similarity between two graphs in different vector spaces, every node (i.e. term) in Sq F B is required to be transformed first to allow for comparison under the same vector space. So the transformed vector representation of q becomes Μ•q and FB is trans- formed to {Μ•f1, Μ•f2 …, Μ•fu} (recall that Μ is the transformation matrix).</p><p>Step <ref type="formula">(2)</ref>: for each node in Sq F B , we then choose the top k candidate terms with the highest correspondence score in the tar- get space. Note that we would need to perform k•k u combinations of nodes (or candidate local graphs) in total, to find the best graph with the highest graph similarity. The computation time becomes then an issue as the number of comparisons grows in polynomial way with the increase in the number of candidate terms. However, we manage to re- duce the number of combinations to k•k•u by as- suming the reference points be independent of each other. Then, for every selected candidate temporal counterpart, we only choose the set of corresponding terms FT which maximizes the cur- rent graph similarity. By default we set k equal to 1000. The process is shown in Algorithm 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Local Graph Matching</head><p>Input Graph similarity computation. To compute the similarity of two star shaped graphs, we take both the semantic and relational similarities into consideration. <ref type="figure" target="#fig_2">Fig. 3</ref> conceptually portrays this idea. Since all the computation is done under the same vector space (after transformation), the se- mantic meaning is represented by the absolute po- sition of the term, that is, by its vector representa- tion in the vector space. On the other hand, the re- lation is described by the difference of two term vectors. Finally, the graph similarity function g(Sq F B ,Sw F T ) is defined as the combination of the relational similarity function, h(Sq F B ,Sw F T ), and se- mantic similarity function, z(Sq F B ,Sw F T ), as follows:</p><formula xml:id="formula_3">) ) , cos( ) , cos( max( ) ( max ) 1 ( ) , ( ) , ( ) 1 ( ) , ( , ,                   T T B B T T B B T B T B T B T B F f F f T B F f F f f w f q F w F q F w F q F w F q w q f f R R S S z S S h S S g     (4)</formula><p>where Rq f B is the difference of vectors between q and fB in FB represented as <ref type="bibr">[q-fB]</ref>. Rw f T is the differ- ence of vectors between w and fT in FT, <ref type="bibr">[w-fT]</ref>, where fT is selected from k candidates correspond- ing terms of fB. fT maximizes the cosine similarity between <ref type="bibr">[q-fB]</ref> and <ref type="bibr">[w-fT]</ref>. λ is set to 0.5 by de- fault. Intuitively, Sq F B is a graph composed of query and its reference points, while Sw F T is a graph containing candidate word w and its refer- ence points. The first maximum in Eq. 4 finds for each reference point in the base time, fB, the top-k candidate terms corresponding to fB in the target time. Next, it finds within k such fT that similarity between <ref type="bibr">[q-fB]</ref> and <ref type="bibr">[w-fT]</ref> is maximum (relational similarity). The second maximum in Eq. 4 is same as the first one with the exception that it computes the semantic similarity instead of the relational similarity. The two summations in Eq. 4 aggregate both the similarity scores over all the reference points. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training sets</head><p>For the experiments we use the New York Times Annotated Corpus <ref type="bibr" target="#b26">(Sandhaus, 2008)</ref>. This dataset contains over 1.8 million newspaper articles pub- lished between 1987 and 2007. We first divide it into four parts according to article publication time: <ref type="bibr">[1987]</ref><ref type="bibr">[1988]</ref><ref type="bibr">[1989]</ref><ref type="bibr">[1990]</ref><ref type="bibr">[1991]</ref>, <ref type="bibr">[1992]</ref><ref type="bibr">[1993]</ref><ref type="bibr">[1994]</ref><ref type="bibr">[1995]</ref><ref type="bibr">[1996]</ref>, <ref type="bibr">[1997]</ref><ref type="bibr">[1998]</ref><ref type="bibr">[1999]</ref><ref type="bibr">[2000]</ref><ref type="bibr">[2001]</ref> and <ref type="bibr">[2002]</ref><ref type="bibr">[2003]</ref><ref type="bibr">[2004]</ref><ref type="bibr">[2005]</ref><ref type="bibr">[2006]</ref><ref type="bibr">[2007]</ref>. Each time period contains then around half a million articles. We next train the model of distributed vector representation sepa- rately for each time period. The vocabulary size of the entire corpus is 360k, while the vocabulary size of each time period is around 300k.</p><p>In the experiments, we first focus on the pair of time periods separated by the longest time gap, that is, <ref type="bibr">[2002,</ref><ref type="bibr">2007]</ref> as the base time and <ref type="bibr">[1987,</ref><ref type="bibr">1991]</ref> as the target time. We also repeat the exper- iment using more recent target time: <ref type="bibr">[1992,</ref><ref type="bibr">1996]</ref>  <ref type="table">Table 1</ref>: Example results where q is the input term and tc is the matching temporal counterpart. The numbers are the ranks of the correct temporal counterpart in the results ranked by each method. Since we output only the top 1000 results, ranks lower than 1000 are represented as 1000+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Test sets</head><p>As far as we know there is no standard test bench for temporal correspondence finding. We then had to manually create test sets containing queries in the base time and their correct temporal counter- parts in the target time. In this process we used external resources including the Wikipedia, a Web search engine and several historical text- books. The test terms cover three types of entities: persons, locations and objects.</p><p>The examples of the test queries and their tem- poral counterparts for <ref type="bibr">[1987,</ref><ref type="bibr">1991]</ref> are shown in <ref type="table">Table 1</ref> where q denotes the input term and tc is the correct counterpart. Note that the expected an- swer is not required to be single neither exhaus- tive. For example, there can be many answers for the same query term, such as letter, mail, fax, all being commonly used counterparts in 1980s for email. Furthermore, as we do not care for recall in this research, we do not require all the correct counterpart terms to be found. In total, there are 95 pairs of terms (query and its counterpart) re- sulting from 54 input query terms for the task of mapping <ref type="bibr">[2002,</ref><ref type="bibr">2007]</ref> with <ref type="bibr">[1987,</ref><ref type="bibr">1991]</ref>, and 50 term pairs created from 25 input query terms for matching <ref type="bibr">[2002,</ref><ref type="bibr">2007]</ref> and <ref type="bibr">[1992,</ref><ref type="bibr">1996]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation measures and baselines</head><p>We use the Mean Reciprocal Rank (MRR) as a main metric to evaluate the ranked search results for each method. MRR is expressed as the mean of the inverse ranks for each test where a correct result appears. It is calculated as follows:</p><formula xml:id="formula_4">   N i i rank N MRR 1 1 1 (5)</formula><p>where ranki is the rank of a correct counterpart at the i-th test. N is the number of query-answer pairs. MRR's values range between <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. The higher the value, the more correct the method is. Besides MRR, we also report precision @1, @5, @10 and @20. They are equal to the rates of tests in which the correct counterpart term tc was found in the top 1, 5, 10 and 20 results, respectively.</p><p>Baselines. We prepare three baselines:</p><p>(1) Bag of words approach (BOW) without transformation: this method directly compares the context of the query in the base time with the con- text of the candidate term in the target time. We use it to examine whether the distributed vector representation and transformation are necessary.</p><p>(2) Latent Semantic Indexing (LSI) without transformation (LSI-Com): we first merge the documents in the base time and the documents in the target time. Then, we train LSI <ref type="bibr" target="#b6">(Deerwester, 1988)</ref> on such combined collection to represent each term by the same distribution of detected top- ics. We next search for the terms that exist in the target period and that are also semantically similar to the queried terms by comparing their vector q <ref type="bibr">[2002,</ref><ref type="bibr">2007]</ref> tc <ref type="bibr">[1987,</ref><ref type="bibr">1991]</ref> </p><formula xml:id="formula_5">BOW (baseline) LSI-Com (baseline) LSI-Tran (baseline) GT (proposed) LT-Cooc (proposed) LT-Lex (proposed)</formula><p>LT-Clust <ref type="table" target="#tab_5">(proposed)  Putin  Yeltsin  1000+  252  353  24  1  1  1  Chirac  Mitterrand  1000+  8  1  7  19  1  3  iPod  Walkman  1000+  20  131  3  13  1  16  Merkel  Kohl  1000+  1000+  537  142  76  7  102  Facebook  Usenet  1000+  1000+  1000+  1  1  1</ref>  representations. The purpose of using LSI-Com is to check the need for the transformation over time.</p><p>(3) Latent Semantic Indexing (LSI) with transformation (LSI-Tran): we train two LSI models separately on the documents in the base time and the documents in the target time. Then we train the transformation matrix in the same way as we did for our proposed methods. Lastly, for a given input query, we compare its trans- formed vector representation with terms in the tar- get time. LSI-Tran is used to investigate if LSI can be an alternative for the vector representation un- der our transformation scenario.</p><p>Proposed Methods. All our methods use the neural network based term representation. The first one is the method without considering the lo- cal context graph called GT (see Sec. 2). By test- ing it we want to investigate the necessity of trans- forming the context of the query in the target time.</p><p>We also test the three variants of the proposed approach that applies the local graph (explained in Sec. 3). The first one, LT-Lex, constructs the lo- cal graph by using the hypernyms of terms. LT- Cooc applies term co-occurrence to select the ref- erence points. Finally, LT-Clust clusters the con- text terms by their semantic meanings and selects the most common term from each cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter settings</head><p>We set the parameters as follows:</p><p>(1) num_of_dim: we experimentally set the num- ber of dimensions of the Skip-gram model and the number of topics of LSI to be 200. (2) num_of_CFTs: we utilize the top 5% (18k words) of Common Frequent Terms to train the transformation matrix. We have tried other num- bers but we found 5% to perform best (see <ref type="figure" target="#fig_3">Fig. 4</ref>). (3) u: the number of reference points (same as the number of semantic clusters) is set to be 5. Ac- cording to the results, we found that increasing the number of reference points does not always im- prove the results. The performance depends rather on whether the reference points are general enough, as too detailed ones hurt the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>First, we look at the results of finding temporal counterparts in <ref type="bibr">[1987,</ref><ref type="bibr">1991]</ref>. The average scores for each method are shown in <ref type="table" target="#tab_5">Table 2</ref>. <ref type="table">Table 1</ref> shows detailed results for few example queries.</p><p>The main finding is that all our methods outper- form the baselines when measured by MRR and by the precisions at different ranks. In the follow- ing subsections we discuss the results in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Context change over time</head><p>The first observation is that the task is quite diffi- cult as evidenced by extremely poor performance of the bag of words approach (BOW). The correct answers in BOW approach are usually found at ranks 10k-30k (recall that the vocabulary size is 360k). This suggests little overlap in the contexts of query and counterpart terms. The fact that all our methods outperform the baselines suggests that the across-time transformation is helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Using local context graph</head><p>We can observe from <ref type="table" target="#tab_5">Table 2</ref> that, in general, us- ing the local context graph improves the results. The best performing approach, LT-Lex, improves GT method, which uses only global similarity matching, by 24% when measured using MRR. It increases the precision at certain levels of top ranks, especially, at the top 1, where it boosts the performance by 44%. LT-Lex uses the hyper- nyms of query as reference points in the local graph. This suggests that using generalized con- text terms as reference points is most helpful for finding correct temporal counterparts. On the other hand, LT-Cooc and LT-Clust usually fail to improve GT. It may be because the term co-oc- currence and semantic clustering approaches de- tect less general terms that tend to capture too de- tailed information which is then poorly related to the temporal counterpart. For example, LT-Cooc detects {music, Apple, computer, digital, iTunes} as the reference points of the query iPod. While music is shared by iPod's counterpart (walkman) and Apple can be considered analogical to Sony, other terms (i.e., computer, digital, iTunes) are ra- ther too specific and unique for iPod.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Using neural network model</head><p>When comparing the results of LSI-Com and LSI-Tran in <ref type="table" target="#tab_5">Table 2</ref>, we can see that using the transformation does not help LSI to enhance the performance but, on the contrary, it makes the re- sults worse.   <ref type="bibr">present: 2002-2007; past: 1987-1991</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Yet, as discussed above, applying the transfor- mation is good idea in the case of the Neural Net- work Model. We believe the reason for this is be- cause it is difficult to perform the global transfor- mation between topics underling the dimensions of LSI, in contrast to transforming "semantic di- mensions" of Neural Network Model. <ref type="figure" target="#fig_3">Fig. 4</ref> shows MRR results for different numbers of Common Frequent Terms (CFTs) when apply- ing GT method. Note that the level of 0.10% (the first point) corresponds to using 658 stop words as seed pairs. As mentioned before, 5% of CFTs al- lows to obtain the best results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of the number of CFTs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Searching from past to present</head><p>We next analyze the case of searching from the past to the present. This scenario may apply to the case of a user (perhaps, an older person) who pos- sesses knowledge about the past term but does not know its modern counterparts. <ref type="table" target="#tab_7">Table 3</ref> shows the performance. We can see that, again, all our approaches outperform all the baselines using all the measures. LT-Lex is the best performing approach, when measured by MRR and P@1 and P@20. LT-Cooc this time re- turns the best results at P@5 and P@10.   <ref type="bibr">present (present: 2002-2007; past: 1987-1991</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The objective of testing the search from the past to present is to prove our methods work in both directions. As for now, we can only conclude the performance is asymmetrical. Yet, we might spec- ulate that, along with the increase in distance, searching from past to present could be harder due to present world becoming relatively more diverse when seen from the distant past.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Results using different time period</head><p>Finally, we perform additional experiment using another target time period <ref type="bibr">[1992,</ref><ref type="bibr">1996]</ref> to verify whether our approach is still superior on different target time. For the experiment we use the best performing baseline listed in <ref type="table" target="#tab_5">Table 2</ref>, LSI-Com, and the best proposed approach, LT-Lex, as well as GT. The results are shown in <ref type="table" target="#tab_9">Tables 4 and 5</ref>. LT-Lex outperforms the other baselines in both the search from the present to the past <ref type="table" target="#tab_9">(Table 4)</ref> and from the past to the present ( <ref type="table" target="#tab_11">Table 5</ref>). Note that since the query-answers pairs for <ref type="bibr">[1992,</ref><ref type="bibr">1996]</ref> are different than ones for <ref type="bibr">[1987,</ref><ref type="bibr">1991]</ref>   <ref type="bibr">present: 2002-2007; past: 1992-1996</ref>   <ref type="bibr">(present: 2002-2007; past: 1992-1996</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Confidence of Results</head><p>The approach described in this paper will al- ways try to output some matching terms to a query in the target time period. However in some cases, no term corresponding to the one in the base time existed in the target time (e.g. when the semantic concept behind the term was not yet born or, on the contrary, it has already felt out of use). For ex- ample, junk mail may not have any equivalent in texts created around 1800s. A simple solution to this problem would be to use Eqs. 2 and 4 to serve as measures of confidence behind each result in order to decide whether the found counterparts should or not be shown to users. Note however that the scores returned by Eqs. 2 and 4 need to be first normalized according to the distance between the target time and the base time periods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>652</head><p>Temporal changes in word meaning have been an important topic of study within historical linguis- tics <ref type="bibr" target="#b0">(Aitchison, 2001;</ref><ref type="bibr" target="#b5">Campbell 2004;</ref><ref type="bibr" target="#b15">Labov, 2010;</ref><ref type="bibr" target="#b8">Hughes, 1988)</ref>. Some researchers employed computational methods for analyzing changes in word senses over time <ref type="bibr" target="#b18">(Mihalcea and Nastase, 2012;</ref><ref type="bibr" target="#b13">Kim et al., 2014;</ref><ref type="bibr" target="#b9">Jatowt and Duh, 2014;</ref><ref type="bibr" target="#b14">Kulkarni et al., 2015)</ref>. For example, <ref type="bibr" target="#b18">Mihalcea and Nastase (2012)</ref> classified words to one of three past epochs based on words' contexts. <ref type="bibr" target="#b13">Kim et al. (2014)</ref> and <ref type="bibr" target="#b14">Kulkarni et al. (2015)</ref> computed the degree of meaning change by applying neural net- works for word representation. <ref type="bibr" target="#b9">Jatowt and Duh (2014)</ref> used also sentiment analysis and word pair comparison for meaning change estimation. Our objective is different as we search for correspond- ing terms across time, and, in our case, temporal counterparts can have different syntactic forms.</p><p>Some works considered computing term simi- larity across time ( <ref type="bibr" target="#b10">Kalurachchi et al., 2010;</ref><ref type="bibr" target="#b11">Kanhabua et al. 2010;</ref><ref type="bibr" target="#b28">Tahmasebi et al. 2012</ref><ref type="bibr" target="#b1">, Berberich et al. 2009</ref>). <ref type="bibr" target="#b10">Kalurachchi et al. (2010)</ref> pro- posed to discover semantically identical tempo- rally altering concepts by applying association rule mining, assuming that the concepts referred by similar events (verbs) are semantically related. <ref type="bibr" target="#b11">Kanhabua et al. (2010)</ref> discovered the change of terms through the comparison of temporal Wik- ipedia snapshots. <ref type="bibr" target="#b1">Berberich et al. (2009)</ref> ap- proached the problem by introducing a HMM model and measuring the across-time sematic similarity between two terms by comparing the contexts captured by co-occurrence measures. <ref type="bibr" target="#b28">Tahmasebi et al. (2012)</ref> improved their approach by first detecting the periods of name change and then by analyzing the contexts during the change periods to find the temporal co-references of dif- ferent names. There are important differences be- tween those works and ours. First, the previous works mainly focused on detecting changes of the names of the same, single entity over time. For ex- ample, the objective was to look for the previous name of Pope Benedict (i.e. Joseph Ratzinger) or the previous name of St. Petersburg (i.e. Lenin- grad). Second, these approaches relied on apply- ing the co-occurrence statistics according to the intuition that if two terms share similar contexts, then these terms are semantically similar. In our work, we do not require the context to be literally same but to have the same meaning.</p><p>Transfer <ref type="bibr">Learning (Pan et al., 2010</ref>) is related to some extent to our work. It has been mainly used in tasks such as POS tagging <ref type="bibr" target="#b3">(Blitzer et al., 2006</ref>), text classification ( <ref type="bibr" target="#b2">Blitzer et al., 2007;</ref><ref type="bibr" target="#b17">Ling et al., 2008;</ref><ref type="bibr" target="#b29">Wang et al., 2011;</ref>, learning to rank <ref type="bibr">(Cai et al., 2011;</ref><ref type="bibr" target="#b7">Gao et al., 2010;</ref><ref type="bibr" target="#b30">Wang et al., 2009</ref>) and content-based retrieval ( <ref type="bibr" target="#b12">Kato et al., 2012</ref>). The temporal correspondence problem can be also understood as a transfer learning as it is a search process that uses samples in the base time for inferring correspondent in- stances existing in the target time. However, the difference is that we do not only consider the structural correspondence but we also utilize the semantic similarity across time.</p><p>The idea of distance-preserving projections is also used in automatic translation ( <ref type="bibr" target="#b20">Mikolov et al., 2013b)</ref>. Our research problem is however more difficult and is still unexplored. In the traditional language translation, languages usually share same concepts, while in the across-time transla- tion concepts evolve and thus may be similar but not always same. Furthermore, the lack of training data is another key problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>This work approaches the problem of finding tem- poral counterparts as a way to build a "bridge" across different times. Knowing corresponding terms across time can have direct usage in sup- porting search within longitudinal document col- lections or be helpful for constructing evolution timelines. We first discuss the key challenge of the temporal counterpart detection -the fact that contexts of terms change, too. We then propose the global correspondence method using transfor- mation between two vector spaces. Based on this, we then introduce more refined approach of com- puting the local correspondence. Through experi- ments we demonstrate that the local correspond- ence using hypernyms outperforms both the base- lines and the global correspondence approach.</p><p>In the future, we plan to test our approaches over longer time spans and to design the way to automatically "explain" temporal counterparts by outputting "evidence" terms for clarifying the similarity between the counterparts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Conceptual view of the across-time transformation by matching similar relative geometric positions in each space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>:</head><label></label><figDesc>local graph of q, Sq F B W = top k corresponding terms of q (by Eq. 2) FF = {top k corresponding terms of each f in reference points FB={ f0, f1, …, fu}} (by Eq. 2) for w = W[1:k] do: sum_cos = 0 # total graph similarity score for F = FF[1:u] do: max_cos = 0 # current maximum similar- ity for c = F[1:k] do: find c which maximizes current graph similarity end for sum_cos += max_cos end for end for sort W by sum_cos of each w in W. Output: sorted W as ranked list of temporal counterparts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The concept of computing semantic and relational similarity in matching local graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of MRR for GT method depending on number of used CFTs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>and D(T T ) by first collecting CFTs as training pairs and then learning M using</head><label></label><figDesc></figDesc><table>T T 
1. Construct word representation model for 
corpus in the base time, D(T B ), and in the 
target time, D(T T ). (Section 2.1) 
2. Construct transformation matrix M be-
tween D(T B ) Eq. 1. (Section 2.2) 
3. Rank the words in target time by their cor-
respondence scores (Eq. 2) 
Output: ranked list of temporal counterparts 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>.</head><label></label><figDesc></figDesc><table>base time 
(e.g. 2003-2007) 

ipod 

mp3 
music 

apple 

target time 
(e.g. 1987-1991) 

music 
cassette 

walkman 

sony 

semantic similarity 

relational similarity </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of searching from present to past 
(</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 : Average scores of searching from past to</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head></head><label></label><figDesc>, their results cannot be directly compared.</figDesc><table>Method 
MRR P@1 
P@5 P@10 P@20 
LSI-Com 0.115 10.6 
14.9 
21.3 
23.4 
GT 
0.132 
8.5 
27.7 
40.4 
53.2 
LT-Lex 
0.169 10.6 
34.1 
48.9 
55.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results of searching from present to past 
(</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Method 
MRR P@1 
P@5 P@10 P@20 
LSI-Com 0.148 11.6 
18.6 
23.3 
30.2 
GT 
0.184 11.6 
23.3 
30.2 
44.2 
LT-Lex 
0.212 
14 
28 
32.6 
44.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 5 : Results of searching from past to present</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> We have experimented with hyponyms and coordinate terms used as reference points and found the results are worse than when using hypernyms.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Makoto P. Kato for valuable comments. This work was supported in part by Grants-in-Aid for Scientific Research (Nos. 15H01718, 15K12158) from MEXT of Japan and by the JST Research Promotion Program Sakigake: "Analyz-ing Collective Memory and Developing Methods for Knowledge Extraction from Historical Docu-ments".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language Change, Progress or Decay?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aitchison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bridging the Terminology Gap in Web Archive Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Bedathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sozio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WebDB&apos;09</title>
		<meeting>of WebDB&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bollywood, Boom-boxes and Blenders: Domain Adaption for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biographies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaption with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 conference on empirical methods in natural language processing. Association for Computational Linguistics (EMNLP)</title>
		<meeting>the 2006 conference on empirical methods in natural language processing. Association for Computational Linguistics (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Relevant knowledge helps in choosing right teacher: active query selection for ranking adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Campbell</surname></persName>
		</author>
		<title level="m">Historical Linguistics</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving Information Retrieval with Latent Semantic Indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the American Society for Information Science</title>
		<meeting>the 51st Annual Meeting of the American Society for Information Science</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to rank only using training data from related domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Words in Time: A Social History of the English Vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hughes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<pubPlace>Basil Blackwell</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A framework for analyzing semantic change of words across time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jatowt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of JCDL</title>
		<meeting>of JCDL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporating Terminology Evolution for Query Translation in Text Retrieval with Association Rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalurachchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Varde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bedathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the 19th ACM international Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1789" to="1792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting Time-based Synonyms in Searching Document Archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanhabua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nørvåg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th annual joint conference on Digital libraries (JCDL)</title>
		<meeting>the 10th annual joint conference on Digital libraries (JCDL)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Content-based Retrieval for Heterogeneous Domains: Domain Adaption by Relative Aggregation Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ohshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 35th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal Analysis of Language through Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-I</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hanaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science</title>
		<meeting>the ACL 2014 Workshop on Language Technologies and Computational Social Science</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="61" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistically Significant Detection of Linguistic Change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="625" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Principles of Linguistic Change (Social Factors)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Labov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Wiley-Blackwell</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Quantifying the evolutionary dynamics of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lieberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">449</biblScope>
			<biblScope unit="page" from="713" to="716" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spectral domain-transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="488" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Word Epoch Disambiguation: Finding How Words Change Over Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nastase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="259" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed Representation of Phrases and Their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-speed Detection of Ontological Knowledge and Bi-directional LexicoSyntactic Patterns from the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ohshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="205" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Frequency of word-use predicts rates of lexical evolution throughout Indo-European history</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pargel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">D</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">449</biblScope>
			<biblScope unit="page" from="717" to="720" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation. California Univ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">For Cognitive Science</title>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The New York Times Annotated Corpus Overview. The New York Times Company, Research and Development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sandhaus</surname></persName>
		</author>
		<ptr target="https://catalog.ldc.up-enn.edu/docs/LDC2008T19/new_york_times_an-notated_corpus.pdf" />
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comparison of document clustering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD workshop on text mining</title>
		<meeting>of KDD workshop on text mining</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="page" from="525" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">NEER: An Unsupervised Method for Named Entity Evolution Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tahmasebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gossen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanhabua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Holzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Risse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Coling</title>
		<meeting>of Coling</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2553" to="2568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-language web page classification via dual knowledge transfer using nonnegative matrix tri-factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="933" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Heterogeneous cross domain ranking in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management (CIKM)</title>
		<meeting>the 18th ACM conference on Information and knowledge management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="987" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Topic-bridged plsa for cross-domain text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 31st annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="627" to="634" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
