<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Sparse Topical Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">WuHan University</orgName>
								<address>
									<settlement>WuHan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">WuHan University</orgName>
								<address>
									<settlement>WuHan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Applied Informatics</orgName>
								<orgName type="institution">Victoria University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Applied Informatics</orgName>
								<orgName type="institution">Victoria University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuzheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Science</orgName>
								<orgName type="institution">RMIT University</orgName>
								<address>
									<settlement>Melbourne</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">WuHan University</orgName>
								<address>
									<settlement>WuHan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">WuHan University</orgName>
								<address>
									<settlement>WuHan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Sparse Topical Coding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2332" to="2340"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2332</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Topic models with sparsity enhancement have been proven to be effective at learning discriminative and coherent latent topics of short texts, which is critical to many scientific and engineering applications. However, the extensions of these models require carefully tailored graphi-cal models and re-deduced inference algorithms , limiting their variations and applications. We propose a novel sparsity-enhanced topic model, Neural Sparse Topical Coding (NSTC) base on a sparsity-enhanced topic model called Sparse Topical Coding (STC). It focuses on replacing the complex inference process with the back propagation, which makes the model easy to explore extensions. Moreover, the external semantic information of words in word embeddings is incorporated to improve the representation of short texts. To illustrate the flexibility offered by the neu-ral network based framework, we present three extensions base on NSTC without re-deduced inference algorithms. Experiments on Web Snippet and 20Newsgroups datasets demonstrate that our models out-perform existing methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models with sparsity enhancement have proven to be effective tools for exploratory analy- sis of the overload of short text content. The latent representations learned by these models are cen- tral to many applications. However, these mod- els have trouble to rapidly explore variations for the approximate inference methods of them. Even subtle variations on models can increase the com- plexity of the inference methods, which is espe- cially apparent for non-conjugate models.</p><p>With the development of deep learning, many works combine topic models with neural language model to overcome the computation complexity of topic models <ref type="bibr" target="#b8">(Larochelle and Lauly, 2012a;</ref><ref type="bibr" target="#b2">Cao et al., 2015;</ref><ref type="bibr" target="#b15">Tian et al., 2016</ref>). Most of these meth- ods adopt multiple neural network layers to model the generation process of each document. Nev- ertheless, these methods yield the same poor per- formance in short texts as traditional topic mod- els. There are also many works introducing new techniques such as word embeddings to traditional topic models. Word embeddings has proven to be effective at capturing syntactic and semantic infor- mation of words. Many works ( <ref type="bibr" target="#b3">Das et al., 2015;</ref><ref type="bibr" target="#b6">Hu and Tsujii, 2016;</ref><ref type="bibr" target="#b10">Li et al., 2016)</ref> have shown that the additional semantics in word embeddings can enhance the performance of traditional topic models. Yet these models have the same trouble in making extensions as traditional topic models.</p><p>Base on the above observations, we propose Neural Sparse Topical Coding (NSTC) by jointly utilizing word embeddings and neural network with a sparsity-enhanced topic model, Sparse Top- ical Coding (STC). We adopt neural network to model the generation process of STC to simplify the complex inference and improve flexibility, and incorporate external semantics provided by word embeddings to improve the overall accuracy. To illustrate the dramatic flexibility offered by the end-to-end neural network, we present three ex- tensions base on NSTC. Our proposed models all benefit from both sides: 1) when compared with the neural based topic models, which stuck in the sparseness of word co-occurrence informa- tion, they show how the sparsity mechanism and word embeddings enrich the features and improve the performance; 2) while with topic models with sparsity enhancement, our models present how the black-box inference method of neural network ac-celerates the training process and increases the flexibility. To evaluate the effectiveness of our models by conducting experiments on 20 News- groups and Web Snippet datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Topic models with sparsity enhancement: The performance of traditional topic models are com- promised by the sparse word co-occurrence in- formation when applied in short texts. To over- come the bottleneck, there have been many ef- forts to address the problem of sparsity in short texts. Based on traditional LDA, ( <ref type="bibr" target="#b16">Williamson et al., 2010</ref>) introduced a Spike and Slab prior to model the sparsity in finite and infinite latent topic structures of text. To consider the dual-sparsity of topics per document and terms per topic, ( <ref type="bibr" target="#b11">Lin et al., 2014</ref>) proposed a dual-sparse topic model that addresses the sparsity in both the topic mix- tures and the word usage. There are also some non-probabilistic sparse topic models, which can directly control the sparsity by imposing regular- izers. For example, the non-negative matrix fac- torization (NMF) <ref type="bibr" target="#b5">(Heiler and Schnörr, 2006</ref>) for- malized topic modeling as a problem of mini- mizing loss function regularized by lasso. Simi- larly, ( <ref type="bibr" target="#b17">Zhu and Xing, 2011</ref>) presented sparse top- ical coding (STC) by utilizing the Laplacian prior to directly control the sparsity of inferred repre- sentations. Additionally, ( <ref type="bibr" target="#b14">Peng et al., 2016</ref>) pre- sented sparse topical coding with sparse groups (STCSG) to find sparse word and document rep- resentations of texts. However, over complicated inference procedure of these sparse topic models make them difficult to rapidly explore variations.</p><p>Topic models with word embeddings: There are many works tried to incorporate word embed- dings with topic models to improve the perfor- mance. ( <ref type="bibr" target="#b3">Das et al., 2015)</ref> proposed a new tech- nique for topic modeling by treating the document as a collection of word embeddings and topics it- self as multivariate Gaussian distributions in the embedding space. However, the assumption that topics are unimodal in the embedding space is not appropriate, since topically related words can oc- cur distantly from each other in the embedding space. Therefore, ( <ref type="bibr" target="#b6">Hu and Tsujii, 2016)</ref> proposed latent concept topic model (LCTM), which mod- eled a topic as a distribution of concepts, where each concept defined another distribution of word vectors. ( <ref type="bibr" target="#b13">Nguyen et al., 2015)</ref> proposed Latent Feature Topic Modeling (LFTM), which extended LDA to incorporate word embeddings as latent features. ( <ref type="bibr" target="#b10">Li et al., 2016</ref>) focused on combing the local information of word embeddings and the global information of LDA, thus proposed a model TopicVec yielded by the variational infer- ence method. However, these models also have trouble to rapidly explore variations.</p><p>Neural Topic Models: There are also some ef- forts trying to combine topic models with neural networks to represent words and documents si- multaneously. (Larochelle and Lauly, 2012a) pro- posed a neural network topic model that is sim- ilarly inspired by the Replicated Softmax. ( <ref type="bibr" target="#b2">Cao et al., 2015)</ref> proposed a novel neural topic model (NTM) where the representation of words and documents are efficiently and naturally combined into a uniform framework. ( <ref type="bibr" target="#b3">Das et al., 2015</ref>) pro- posed a new technique for topic modeling by treat- ing the document as a collection of word embed- dings and topics itself as multivariate Gaussian distributions in the embedding space. To address the limitations of the bag-of-words assumption, ( <ref type="bibr" target="#b15">Tian et al., 2016)</ref> proposed Sentence Level Re- current Topic Model (SLRTM) by using a Recur- rent Neural Networks (RNN) based framework to model long range dependencies between words. Nevertheless, most of aforementioned works yield poor performance in short texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Sparse Topical Coding</head><p>Firstly, we define that D = {1, ..., M } is a doc- ument set with size M , T = {1, ..., K} is a topic collection with K topics, V = {1, .., N } is a vocabulary with N words, and w d = {w d,1 , ..., w d,|I| } is a vector of terms representing a document d, where I is the index of words in document d, and w d,n (n ∈ I) is the frequency of word n in document d. Moreover, we denote β ∈ R N ×K as a global topic dictionary for the whole document set with K bases, θ d ∈ R K is the document code of each document d and s d,n ∈ R K is the word code of each word n in each document d. To yield interpretable patterns, (θ, s, β) are constrained to be non-negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sparse Topical Coding</head><p>STC is a hierarchical non-negative matrix factor- ization for learning hierarchical latent representa- tions of input samples. In STC, each document and each word is represented as a low-dimensional code in topic space, which can be employed in many tasks. Based on the global topic dictionary β of all documents with K topic bases sampled from a uniform distribution, the generative process of each document d is described as follows:</p><formula xml:id="formula_0">1. Sample the document code θ d from a prior p(θ d ) ∼ Laplace(λ −1 ).</formula><p>2. For each observed word n in document d:</p><formula xml:id="formula_1">(a) Sample the word code s d,n from a conditional distribution p(s d,n |θ d ) ∼ supergaussian(θ d , γ −1 , ρ −1 ). (b) Sample the observed word count w d,n from a distribution p(w d,n |s d,n * β n ) ∼ P oisson(s d,n * β n )</formula><p>To achieve sparse word codes, STC defines</p><formula xml:id="formula_2">p(s d,n |θ d ) as a product of two component dis- tributions p(s d,n |θ d ) ∼ p(s d,n |θ d , γ)p(s d,n |ρ), where p(s d,n |θ d , γ) is an isotropic Gaussian dis- tribution, and p(s d,n |ρ) is a Laplace distribution. The composite distribution is super-Gaussian: p(s d,n |θ d ) ∝ exp(γ||s d,n θ d || 2 2 ρ||s d,n || 1 )</formula><p>. With the Laplace term, the composite distribution tends to yield sparse word codes. For the same purpose, the prior distribution p(θ d ) of document codes is a Laplace prior. Although STC has closed form coordinate descent equations for parameters (θ, s, β), it is inflexible for its complex inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Network View of Sparse Topical Coding</head><p>We devote to rebuild STC with a neural network to simplify it's inference process via BackPropoga- tion. After generating the topic dictionary from neural network, our model follows the generative story below for each document d:</p><p>1. For each word n in document d:</p><p>(a) Sample a latent variable word code</p><formula xml:id="formula_3">s d,n ∼ f g (d, n). (b) Sample the observed word count w d,n from p(w d,n |s d,n , β n ) ∼ P oisson(s d,n * β n )</formula><p>In our model, we have several assumptions: 1) To simplify our model and acclerate the infer- ence process, we collapse the document code from our model. As illuatrated in ( <ref type="bibr" target="#b0">Bai et al., 2013)</ref> and STC paper (Zhu and Xing, 2011), we can naturally generate each document code via a aggregation of all sampled word codes among all topics, after in- ferring the global topic dictionary and the word codes of words belong to each document:</p><formula xml:id="formula_4">θ d = D d=1 Nd n=1 s d,nk β kn / D d=1 Nd n=1 K k=1 s d,nk β kn ;</formula><p>2) We replace the composite super-Gaussian prior of the word codes and the uniform distri- bution of the topic dictionary with the neural net- work. In the topic dictionary neural network, we introduce the word semantic information via word embeddings to enrich the feature space for short texts;</p><p>3) Similar to STC, the observed word count is sampled from Poisson distribution, which is more appropriate for the discrete count data than other exponential family distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neural Sparse Topical Coding</head><p>In this section, we introduce the detailed layer structures of NSTC in <ref type="figure" target="#fig_0">Figure 1</ref>. We explicitly ex-</p><formula xml:id="formula_5">( , ) C d n ( , ) ( , ) ( ) C d n s d n n Word basis layer ( ) n Word code layer ( , ) s d n Topic dictionary Word code ,2 d W Lookup tableWE 1 ( ) relu WE W ,2 ( , ) ( ( ,:)) d s d n relu n W ( , ) d n Document d</formula><p>Word n plain each layer of NSTC below:</p><formula xml:id="formula_6">Input layer (n, d): A word n of document d ∈ D, where D is a document set.</formula><p>Word embedding layer (W E ∈ R N ×300 ): Sup- posing the word number of the vocabulary is N , this layer devotes to transform each word to a distributed embedding representation. Here, we adopt the pre-trained embeddings by GloVe based on a large Wikipedia dataset <ref type="bibr">1</ref> .</p><p>Word code layers (s d ∈ R N ×K ): These lay- ers generate the K-dimensional word code of in- put word n in document d.</p><formula xml:id="formula_7">s(d, n) = f s (d, n)<label>(1)</label></formula><p>where f s is a multilayer perceptron. In order to achieve interpretable word codes as in STC, we constrain s to be non-negative, therefore we apply the relu activation function on the output of the neural network. Although imposing non- negativity constraints could potentially result in sparser and more interpretable patterns, we exert l 1 norm regularization on s to further keep the sparse assumption. Topic dictionary layers (β ∈ R N ×K ): These layers aim at converting W E to a topic dictionary similar to the one in STC.</p><formula xml:id="formula_8">β(n) = f β (W E)<label>(2)</label></formula><p>where f β is a multilayer perceptron. We make a simplex projection among the output of topic dic- tionary neural network.We normalize each column of the dictionary via the simplex projection as fol- low:</p><formula xml:id="formula_9">β .k = project(β .k ), ∀k<label>(3)</label></formula><p>The simplex projection is the same as the sparse- max activation function in <ref type="bibr" target="#b12">(Martins and Astudillo, 2016)</ref>, providing the theoretical base of its em- ployment in a neural network trained with back- propagation. After the simplex projection, each column of the topic dictionary is promised to be sparse, non-negative and united. Score layer (C d,n ∈ R 1×1 ): NSTC outputs the matching score of a word n and a document d with the dot product of s(d, n) and β(n) in this layer. The output score is utilized to approximate the ob- served word count w d,n .</p><formula xml:id="formula_10">C(d, n) = s(d, n) * β(n)<label>(4)</label></formula><p>Given the count w d,n of word n in document d, we can directly use it to supervise the training pro- cess. According to the architecture of our model, for each word n and each document d, the cost function is:</p><formula xml:id="formula_11">L = l(w d,n , C(d, n)) + λ||s d,n || 1 (5)</formula><p>where l is the log-Poisson loss, λ is the regulariza- tion factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Extensions of NSTC</head><p>To future illustrate the benefits of a black box in- ference method, which allows rapidly explore new models, we present three variants of NSTC.</p><p>Deep l 1 Approximation. STC is based on the idea of sparse coding, in which the sparse code s of the input w can be obtained by solving the loss function for a given dictionary β. In <ref type="bibr" target="#b4">(Gregor and LeCun, 2010)</ref>, the parameterized encoder, named learned ISTA (LISTA) was proposed to ef- ficiently approximate the l 1 based sparse code. Based on the consideration, we present a enhanced NSTC via employing the deep l 1 regularized en- coder similar to LISTA, named NSTCE. We de- vise a feed-forward neural network as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, to efficiently approximate the l 1 based sparse code s of the input w.</p><formula xml:id="formula_12">F (w d ; W d , b d ) = relu(w d * W d + b d )<label>(6)</label></formula><p>The goal is to make the prediction of neural net- work predictor F after the fixed depth as close as possible to the optimal set of coefficients s * in Eq.4. To jointly optimizing all parameters with the dictionary β together, we add another term to the loss function in Eq.4, and enforce the representa- tion s to be as close as possible to the feed forward prediction ( <ref type="bibr" target="#b7">Kavukcuoglu et al., 2010)</ref>:</p><formula xml:id="formula_13">L =l(w d,n , C(d, n)) + λ||s d,n || 1 + α n ||s d − F (w d ; W d , b d )|| 2 2<label>(7)</label></formula><p>Minimizing the loss with respect to s produces a sparse representation that simultaneously recon- structs the word count and is not too different from the predicted representation. Group Sparse Regularization. Based on STC, ( <ref type="bibr" target="#b0">Bai et al., 2013</ref>) presented GSTC to discover document-level sparse or admixture proportion for short texts, in which the group sparse is employed to achieve sparse code at document level by taking into account the structure of bag of words. Here, we just need to add the group sparse regularization on the weight matrix, to make a neural network extension of GSTC, called NGSTC. We consider each column of s d as a group.</p><formula xml:id="formula_14">L = l(w d,n , C(d, n)) + λ K k=1 ||s d,.k || 2<label>(8)</label></formula><p>Sparse Group Lasso. Similar to GSTC, STCSG ( <ref type="bibr" target="#b14">Peng et al., 2016</ref>) was proposed to learn sparse word and document codes, which relaxes the normalization constraint of the inferred rep- resentations with sparse group lasso. Base on STCSG, we propose a novel neural topic model called NSTCSG. We imposse the sparse group lasso on the word code, and have the following cost function:</p><formula xml:id="formula_15">L = l(w d,n , C(d, n))+λ 1 ||s d,n || 1 +λ 2 K k=1 ||s d,.k || 2</formula><p>(9) These three models have the same neural network structures as NSTC, and can be trained end to end with out re-deduced mathematical inference. Moreover, group and sparse group sparsity can help reduce the intrinsic complexity of the model by eliminating neurons as shown in <ref type="figure" target="#fig_2">Figure 3</ref>, and thus can help obtain practical speed ups in deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Optimization</head><p>For the first two models with the lasso regular- izer, we can directly ulitize the end to end stochas- tic gradient descent (SGD) to perform optimiz- ing. The last two optimizing objectives of NGSTC and NSTCSG are formed as a combination of both smooth and non-smooth terms, they can be solved via proximal stochastic gradient descent. The proximal gradient algorithm first obtains the intermediate solution via SGD on the loss only, and then optimize for the regularization term via performing Euclidean projection of it to the solu- tion space, as in the following formulation:</p><formula xml:id="formula_16">min s t+1 d,n R(s t+1 d,n ) + 1 2 ||s t+1 d,n − s t+ 1 2 d,n || 2 2 (10)</formula><p>where R is the regularization, s t+ 1 2 d,n the intermedi- ate solution obtained by SGD, s t+1 d,n is the variable to obtain after the current iteration. For the group lasso, the above problem has the closed-form so- lution. The proximal operator for the group lasso regularizer in Eq.8 is given as follow:</p><formula xml:id="formula_17">prox GL (s d,nk ) = (1 − λ ||s d,.k || 2 ) + s d,nk (11)</formula><p>The proximal operator for the sparse group lasso regularizer in Eq.9 is given as follow:</p><formula xml:id="formula_18">prox SGL (s d,nk ) =(1 − λ 2 ||sign(s d,.k , λ 1 )|| 2 ) + sign(s d,nk , λ 1 )<label>(12)</label></formula><p>The detailed algorithm framework of NGSTC and NSTCSG is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training Algorithm for our models</head><p>Require:</p><formula xml:id="formula_19">a document d ∈ D 1: t = t + 1 2: repeat 3:</formula><p>Compute the partial derivatives of weight matrices,s, and β with a non-regularized objective;</p><p>4:</p><p>Update weight matrices, s, and β using SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Update s using proximal operator 6:</p><p>Update β using simplex projection. 7: until convergence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Setting</head><p>We perform our experiments on two benchmark datasets:</p><p>• 20Newsgroups: is comprised of 18775 newsgroup articles with 20 categories, and contains 60698 unique words 2 .</p><p>• Web Snippet: includes 12340 Web search snippets with 8 categories, we remove the words with fewer than 3 characters and with document frequency less than 3 in the dataset 3 .</p><p>We compare the performance of the NSTC with the following baselines:</p><p>• LDA ( <ref type="bibr" target="#b1">Blei et al., 2001)</ref>. A classical proba- bilistic topic model. We use the LDA pack- age 4 for its implementation. We use the set- tings with iteration number n = 2000, the Dirichlet parameter for distribution over top- ics α = 0.1 and the Dirichlet parameter for distribution over words η = 0.01.</p><p>• STC ( <ref type="bibr" target="#b17">Zhu and Xing, 2011)</ref>. It is a sparsity- enhanced non-probabilistic topic model. We use the code released by the authors <ref type="bibr">5</ref> . We set the regularization constants as λ = 0.3, ρ = 0.0001 and the maximum number of itera- tions of hierarchical sparse coding, dictionary learning as 100. Group lasso: when grouping weights from the the same input neuron into each group, the group sparsity has an effect of completely removing some neurons (highlighted in red). (c) Sparse group lasso: it combines the advantages of the former two formulations, which can remove some neurons and elements (highlighted in red).</p><p>• DocNADE ( <ref type="bibr" target="#b9">Larochelle and Lauly, 2012b</ref>). An unsupervised neural network topic model of documents and has shown that it is a com- petitive model both as a generative model and as a document representation learning algo- rithm <ref type="bibr">6</ref> . In DocNADE, the hidden size is 50, the learning rate is 0.0004 , the bath size is 64 and the max training number is 50000.</p><p>• GaussianLDA ( <ref type="bibr" target="#b3">Das et al., 2015)</ref>. A new technique for topic modeling by treating the document as a collection of word embed- dings and topics itself as multivariate Gaus- sian distributions in the embedding space 7 .</p><p>We use default values for the parameters.</p><p>• TopicVec ( <ref type="bibr" target="#b10">Li et al., 2016)</ref>. A model incorpo- rates generative word embedding model with LDA 8 . We also use default values for the pa- rameters.</p><p>Our three models are implemented in Python using TensorFlow <ref type="bibr">9</ref> . For both datasets, we use the pre- trained 300-dimensional word embeddings from Wikipedia by GloVe, and it is fixed during train- ing. For each out-of-vocab word, we sample a random vector from a normal distribution. In practice, we use a regular learning rate 0.00001 for both dataset. We set the regularization factor λ = 1, α = 1, λ 1 = 0.6, λ 2 = 0.4. In initial- ization, all weight matrices are randomly initial- ized with the uniformed distribution in the inter- val [0, 0.001] for web snippet, and [0, 0.0001] for 20Newsgroups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification Accuracy</head><p>We perform text classification tasks on Web Snip- pet dataset and 20Newsgroups. For the Web Snip- pet, we follow its original partition that consists of 10060 training documents and 2280 test doc- uments. On 20Newsgroups, we we keep 60% documents for training and 40% for testing as in ( <ref type="bibr" target="#b17">Zhu and Xing, 2011</ref>). We adopt the SVM as the classifier with the document representations learned by topic models. <ref type="figure" target="#fig_3">Figure 4</ref> reports the con- vergence curves of loss and accuracy over itera- tions. The results show that the loss and accu- racy of our method can achieve convergence af- ter almost 100 epochs on web snippets and 50 epochs on 20Newsgroups with appropriate learn- ing rate. <ref type="table">Table 1</ref> reports the classification ac- curacy on both datasets under different methods with different settings on the number of topics K = {50, 100, 150, 200, 250}. We can found that 1) The NSTCSG yields the highest accuracy, followed by NGSTC, NSTCE and NSTC which all outperform the DocNADE, GLDA, STC and LDA.</p><p>2) The embedding based models (NSTCSG, NGSTC, NSTCE, NSTC, DocNADE and GLDA) generate better document representations than STC and LDA separately, demonstrating the rep- resentative power of neural networks based on word embeddings. 3) Sparse models (NSTCSG, NGSTC, NSTCE, NSTC and STC) are superior to non-sparse models NTM and LDA separately. It indicates that sparse topic models are more suit- able to short documents. 4) The NSTCSG perform better than NGSTC, followed by NSTC, which il- lustrates both sparse group lasso and group lasso penalty are contribute to learning the document representations with clear semantic explanations.</p><p>5) The accuracies of DocNADE decrease with the increasing of the topic K. This is may because DocNADE may generate the document topic dis-tribution with many indistinct non-zeros due to the data sparsity caused by the increasing number of topics. Notice that LDA has the same performance on the web snippet dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sparse Ratio</head><p>We further compare the sparsity of the learned la- tent representations of words and documents from different models on Web Snippet.</p><p>Word code: For each word n, we compute the average word code and average sparsity ra- tio of them as in ( <ref type="bibr" target="#b17">Zhu and Xing, 2011)</ref>. <ref type="figure" target="#fig_4">Fig- ure 5a</ref> presents the average word sparse ratio of word codes discovered by different models for Web Snippet. Note that the NGSTC can not yield sparse word codes but sparse document codes. We can see that 1) The NSTCSG learns the sparsest word codes, followed by NSTC and STC, which perform much better than NTM and LDA. 2) The word codes discovered by LDA and NTM are very dense for lacking the mechanism to learn the focused topics. The sparsity in both models is mainly caused by the data scarcity.</p><p>3)The rep- resentations learned by sparse models (NSTCSG, NSTC and STC) are much sparser, which indi- cates each word concentrates on only a small num- ber of topics in these models, and therefore the word codes are more clear and semantically con- centrated. 4) Meanwhile, the sparse ratio of STC and NSTC are lower than NSTCSG. It proves the sparse group lasso penalty can easily allow to pro- vide networks with a high level of sparsity.</p><p>Document code: We further quantitatively evaluate the average sparse ratio on latent repre- sentations of documents from different models, as shown in <ref type="figure" target="#fig_4">Figure 5b</ref>. We can see that 1) The NSTCSG yields the highest sparsity ratio, fol- lowed by NGSTC and STC, which outperform NTM and LDA by a large margin. 2) The docu- ment codes discovered by LDA and NTM are still very dense, while the representations learned by sparse models (NSTC and STC) are much sparser. It indicates the sparse models can discover focused topics and obtain discriminative representations of documents. 3) Similar to the word code, NGSTC outperforms NGSTC and STC. It demonstrates that the sparse group lasso penalty can achieve bet- ter sparsity than group lasso and lasso. 4) The sparsity ratios of sparse models increase with the topic numbers. The possible reason is that the sparse models tend to learn the focused topic num- ber which approaches to the real topic number, and an increasing number of redundant topics can be discarded. 5) The NSTCSG inherits the advan- tages of NSTC and NGSTC, which can achieve the sparse topic representations of words and doc- uments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generative Model Evaluation</head><p>To further evaluate our models as a generative model of documents, we show the test document perplexity achieved by each topic model on the 20NewsGroups with 50 topic numbers in table 2. Notice that the topic number in TopicVec can not be specified to a fixed value, thus we follow the set in ( <ref type="bibr" target="#b10">Li et al., 2016</ref>) with 281 topics. In table 3, we show the top-9 words of learned focused top- ics in 20 Newsgroups datasets. For each topic, we list top-9 words according to their probabili- <ref type="table">Table 1</ref>: Classification accuracy of different models on Web snippet and 20NG, with different number of topic K settings. <ref type="table" target="#tab_0">Snippet  20NG  k  50  100  150  200  250  50  100  150  200  250</ref>     <ref type="figure" target="#fig_6">Figure 6</ref>, we also use the 2-dimensional t-SNE method to get the visu- alization of the learned latent representations for Web Snippet and 20Newsgroups Dataset with 200 topics. For Web Snippet, we sample 10% of the whole dataset. For 20newsgroups, we sample 30% of the dataset. It is obvious to see that all doc- uments are clustered into 8 and 20 distinct cate- gories. It proves the semantic effectiveness of the documents codes learned by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel neural sparsity- enhanced topic model NSTC, which improves STC by incorporating the neural network and word embeddings. Compared with other word embedding based and neural network based topic models, it overcomes the computation complex- ity of topic models, and improve the generation of representation over short documents. We present three variants of NSTC to illustrate the great flex- ibility of our framework. Experimental results demonstrate the effectiveness and efficiency of our models. For future work, we are interested in vari- ous extensions, including combining STC with au- toencoding variational Bayes (AVB).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Schematic overview of NSTC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Deep l 1 encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Lasso: the Lasso penalty removes elements without optimizing neuron-level considerations (highlighted in red). (b) Group lasso: when grouping weights from the the same input neuron into each group, the group sparsity has an effect of completely removing some neurons (highlighted in red). (c) Sparse group lasso: it combines the advantages of the former two formulations, which can remove some neurons and elements (highlighted in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The loss and accuracy curves with the iterations on two datasets,on different number of topic K settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The average sparsity ratio of word and document codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: T-SNE embeddings of learned document representations for Web Snippet and 20NewsGroups. Different colors mean different categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Perplexity on test 
dataset. 

Model 
20NG 
LDA 
1091 
STC 
611 
DocNADE 
896 
TopicVec 
650 
NSTC 
517 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Top Words of Learned Topics for 20Newsgroups. 

computer 
sport 
drug 
weapon space-flight 
computer 
hockey 
tobacco 
nuclear 
nasa 
windows 
games 
drug 
guns 
flyers 
ibm 
motorcycl fallacy 
crime 
space 
drive 
team 
aids 
booming 
air 
disk 
play 
hiv 
controller 
statelite 
system 
groups 
dades 
firearms 
send 
dos 
came 
illeg 
military 
launch 
key 
rom 
same 
wiring 
apartment 
hardware 
ball 
adict 
neutral 
la 

ties under the corresponding topic. It is obvious 
that the learned topics are clear and meaningful. 
Such as economics, hockey, games, play, ball in 
the topic about sport. In </table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/projects/glove/</note>

			<note place="foot" n="2"> http://www.qwone.com/ jason/20Newsgroups/ 3 http://jwebpro.sourceforge.net/data-web-snippets.tar.gz 4 https://pypi.python.org/pypi/lda 5 http://bigml.cs.tsinghua.edu.cn/ jun/stc.shtml/</note>

			<note place="foot" n="6"> https://github.com/huashiyiqike/TMBP/tree/master/DocN ADE 7 https://github.com/rajarshd/Gaussian LDA 8 https://github.com/askerlee/topicvec 9 https://www.tensorflow.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Science Foundation of China, under grant No.61472291 and grant No.61272110.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Group sparse topical coding: from code to topic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM international conference on Web search and data mining</title>
		<meeting>the sixth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gaussian lda for topic models with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning sparse representations by non-negative matrix factorization and sequential cone programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Heiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schnörr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1385" to="1407" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A latent concept topic model for robust topic inference using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 54th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">380</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast inference in sparse coding algorithms with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1010.3467</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A neural autoregressive topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2708" to="2716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative topic embedding: a continuous representation of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="666" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The dual-sparse topic model: mining focused topics and focused terms in short text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From softmax to sparsemax: A sparse model of attention and multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Astudillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1614" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="313" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse topical coding with sparse groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajia</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shuang Ouyang, Jimin Huang, and Gang Tian</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>WAIM</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sentence level recurrent topic model: Letting topics speak for themselves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1604.02038</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The ibp compound dirichlet process and its application to focused topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinead</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Heller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>abs/1202.3778</idno>
		<title level="m">Sparse topical coding. CoRR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
