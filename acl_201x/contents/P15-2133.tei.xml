<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Impact of Listener Gaze on Predicting Reference Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolina</forename><surname>Koleva</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Embodied Spoken Interaction Group</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Villalba</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Potsdam</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Staudte</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Embodied Spoken Interaction Group</orgName>
								<orgName type="institution">Saarland University</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Potsdam</orgName>
								<address>
									<settlement>Potsdam</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Impact of Listener Gaze on Predicting Reference Resolution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="812" to="817"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate the impact of listener&apos;s gaze on predicting reference resolution in situated interactions. We extend an existing model that predicts to which entity in the environment listeners will resolve a referring expression (RE). Our model makes use of features that capture which objects were looked at and for how long, reflecting listeners&apos; visual behavior. We improve a probabilistic model that considers a basic set of features for monitoring listeners&apos; movements in a virtual environment. Particularly, in complex referential scenes, where more objects next to the target are possible referents, gaze turns out to be beneficial and helps deciphering listen-ers&apos; intention. We evaluate performance at several prediction times before the listener performs an action, obtaining a highly significant accuracy gain.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Speakers tend to follow the listener's behavior in order to determine whether their communicated message was received and understood. This phe- nomenon is known as grounding, it is well estab- lished in the dialogue literature <ref type="bibr" target="#b2">(Clark, 1996)</ref>, and it plays an important role in collaborative tasks and goal-oriented conversations. Solving a col- laborative task in a shared environment is an ef- fective way of studying the alignment of commu- nication channels <ref type="bibr" target="#b1">(Clark and Krych, 2004;</ref><ref type="bibr" target="#b5">Hanna and Brennan, 2007)</ref>.</p><p>In situated spoken conversations ambiguous lin- guistic expressions are common, where additional modalities are available. While  studied instruction giving and following in virtual environments, <ref type="bibr" target="#b0">Brennan et al. (2013)</ref> ex- amined pedestrian guidance in outdoor real envi- ronments. Both studies investigate the interaction of human interlocutors but neither study exploits listeners' eye movements. In contrast,  designed a task in which a natural lan- guage generation (NLG) system gives instructions to a human player in virtual environment whose eye movements were tracked. They outperformed similar systems in both successful reference res- olution and listener confusion. <ref type="bibr" target="#b3">Engonopoulos et al. (2013)</ref> attempted to predict the resolution of an RE, achieving good performance by combining two probabilistic log-linear models: a semantic model P sem that analyzes the semantics of a given instruction, and an observational model P obs that inspects the player's behavior. However, they did not include listener's gaze. They observed that the accuracy for P obs reaches its highest point at a rel- atively late stage in an interaction. Similar obser- vations are reported by <ref type="bibr" target="#b7">Kennington and Schlangen (2014)</ref>: they compare listener gaze and an incre- mental update model (IUM) as predictors for the resolution of an RE, noting that gaze is more ac- curate before the onset of an utterance, whereas the model itself is more accurate afterwards.</p><p>In this paper we report on the extension of the P obs model to also consider listener's visual be- haviour. More precisely we implement features that encode listener's eye movement patterns and evaluate their performance on a multi-modal data collection. We show that such a model as it takes an additional communication channel pro- vides more accurate predictions especially when dealing with complex scenes. We also expand on concepts from the IUM, by applying the conclu- sions drawn from its behaviour to a dynamic task with a naturalistic interactive scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem definition</head><p>We address the research question of how to auto- matically predict an RE resolution, i.e., answer- ing the question of which entity in a virtual en- vironment has been understood by the listener af-ter receiving an instruction. While the linguistic material in instructions carries a lot of informa- tion, even completely unambiguous descriptions may be misunderstood. A robust NLG system should be capable of detecting misunderstandings and preventing its users from making mistakes.</p><p>Language comprehension is mirrored by inter- locutors' non verbal behavior, and this can help when decoding the listener's interpretation. Pre- cise automatic estimates may be crucial when de- veloping a real-time NLG system, as such a mech- anism would be more robust and capable at avoid- ing misunderstandings. As mentioned in section 1, <ref type="bibr" target="#b3">Engonopoulos et al. (2013)</ref> propose two statistical models to solve that problem: a semantic model P sem based on the linguistic content, and an ob- servation model P obs based on listener behavior features.</p><p>More formally, let's assume a system generates an expression r that aims to identify a target ob- ject o t among a set O of possible objects, i.e. those available in the scene view. Given the state of the world s at time point t, and the observed listener's behavior σ(t) of the user at time t ≥ t b (where t b denotes the end of an interaction), we estimated the conditional probability p(o p |r, s, σ(t)) that in- dicates how probable it is that the listener resolved r to o p . This probability can be also expressed as follows:</p><formula xml:id="formula_0">P (o p |r, s, σ(t)) ∝ P sem (o p |r, s)P obs (o p |σ(t)) P (o p )</formula><p>Following Engonopoulos et al. <ref type="formula">(2013)</ref> we make the simplifying assumption that the distribution of the probability among the possible targets is uni- form and obtain:</p><formula xml:id="formula_1">P (o p |r, s, σ(t)) ∝ P sem (o p |r, s)P obs (o p |σ(t))</formula><p>We expect an NLG system to compute and out- put an expression that maximizes the probability of o p . Due to the dynamic nature of our scenar- ios, we also require the probability value to be up- dated at certain time intervals throughout an in- teraction. Tracking the probability changes over time, an NLG system could proactively react to changes in its environment. <ref type="bibr" target="#b6">Henderson and Smith (2007)</ref> show that accounting for both fixation lo- cation and duration are key to identify a player's focus of attention. The technical contribution of this paper is to ex- tend the P obs model of <ref type="bibr" target="#b3">Engonopoulos et al. (2013)</ref> with gaze features to account for these variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Episodes and feature functions</head><p>The data for our experiment was obtained from the GIVE Challenge ( , an inter- active task in a 3D virtual environment in which a human player (instruction follower, IF) is navi- gated through a maze, locating and pressing but- tons in a predefined order aiming to unlock a safe. While pressing the wrong button in the sequences doesn't always have negative effects, it can also lead to restarting or losing the game. The IF re- ceives instructions from either another player or an automated system (instruction giver, IG). The IF's behavior was recorded every 200ms, along with the IG's instructions and the state of the virtual world. The result is an interaction cor- pus comprising over 2500 games and spanning over 340 hours of interactions. These interactions were mainly collected during the GIVE-2 and the GIVE-2.5 challenges. A laboratory study con- ducted by  comprises a data collection that contains eye-tracking records for the IF. Although the corpus contains both success- ful and unsuccessful games, we have decided to consider only the successful ones.</p><p>We define an episode over this corpus as a typ- ically short sequence of recorded behavior states, beginning with a manipulation instruction gener- ated by the IG and ending with a button press by the IF (at time point t b ). In order to make sure that the recorded button press is a direct response to the IG's instruction, an episode is defined such that it doesn't contain further utterances after the first one. Both the target intended by the IG (o t ) and the one selected by the IF (o p ) were recorded.  can be seen as a sequence of interaction states (s 1 , . . . , s n ), and each state has a set of visible objects ({o 1 , o 2 , o 3 , o 10 , o 12 }). We then compute the subset of fixated objects ({o 2 , o 3 , o 12 }). We update both sets of visible and fixated objects dy- namically in each interaction state with respect to the change in visual scene and the corresponding record of the listener's eye movements.</p><p>We developed feature functions over these episodes. Along with the episode's data, each function takes two parameters: an object o p for which the function is evaluated, and a parameter d seconds that defines how much of the episode's data is the feature allowed to analyze. Each feature looks only at the behavior that happens in the time interval −d to 0. Henceforth we refer to the value of a feature function over this interval as its value at time −d. The value of a feature function evalu- ated on episodes with length less than d seconds is undefined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prediction models</head><p>Given an RE uttered by an IG, the semantic model P sem estimates the probability for each possible object in the environment to have been understood as the referent, ranks all candidates and selects the most probable one in a current scene. This prob- ability represents the semantics of the utterance, and is evaluated at a single time point immediately after the instruction (e.g. "press the blue button") has been uttered. The model takes into account features that encode the presence or absence of ad- jectives carrying information about the spatial or color properties (like the adjective "blue"), along with landmarks appearing as post modifiers of the target noun.</p><p>In contrast to the semantic model, the observa- tional model P obs evaluates the changes in the vi- sual context and player's behavior after an instruc- tion has been received. The estimated probabil- ity is updated constantly before an action, as the listener in our task-oriented interactions is con- stantly in motion, altering the visual context. The model evaluates the distance of the listener posi- tion to a potential target, whether it is visible or not, and also how salient an object is in that par- ticular time window.</p><p>As we have seen above, eye movements pro- vide useful information indicating language com- prehension, and also how to map a semantic repre- sentation to an entity in a shared environment. In- terlocutors constantly interact with their surround- ing and point to specific entities with gestures. Gaze behaviour is also driven by the current state of an interaction. Thus, we extend the basic set of P obs features and implement eye-tracking fea- tures that capture gaze information. We call this the extended observational model P Eobs and con- sider the following additional features:</p><p>1. Looked at: feature counts the number of interaction states in which an object has been fixated at least once during the current episode.</p><p>2. Longest Sequence: detects the longest con- tinuous sequence of interaction states in which a particular object has been fixated.</p><p>3. Linear Distance: returns the euclidean dis- tance dist on screen between the gaze cursor and the center of an object. 5. Update Fixated Objects: expands the list of fixated objects in order to consider the IF's focus of attention. It successively searches in 10 pixel steps and stops as soon as an object is found (the threshold is 100 pixels). This feature evaluates to 1 if the list of fixated ob- jects is been expanded and 0 otherwise. When training our model at time −d train , we generate a feature matrix. Given a training episode, each possible (located in the same room) object o p is added as a new row, where each col- umn contains the value of a different feature func- tion for o p over this episode at time −d train . Fi- nally, the row based on the target selected by the IF is marked as a positive example. We then train a log-linear model, where the weights assigned to each feature function are learned via optimiza- tion with the L-BFGS algorithm. By training our model to correctly predict a target button based only on data observed up until −d train seconds before the actual action t b , we expect our model to reliably predict which button the user will select. Analogously, we define accuracy at testing time −d test as the percentage of correctly predicted tar- get objects when predicting over episodes at time −d test . This pair of training and test parameters is denoted as the tuple (d train , d test ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inv-Squared Distance: returns</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>814</head><p>We evaluated the performance of our improved model over data collected by  using the GIVE Challenge platform. Both training and testing were performed over a subset of the data obtained during a collection task involving worlds created by , designed to provide the task with varying levels of diffi- culty. This corpus provides recorded eye-tracking data, collected with a remote faceLAB system. In contrast, the evaluation presented by <ref type="bibr" target="#b3">Engonopoulos et al. (2013)</ref> uses only games collected for the GIVE 2 and GIVE 2.5 challenges, for which no eye-tracking data is available. Here, we do not in- vestigate the performance of P sem and concentrate on the direct comparison between P obs and P Eobs in order to find out if and when eye-tracking can improve the prediction of an RE resolution.</p><p>We further filtered our corpus in order to re- move noisy games following , considering only interactions for which the eye- tracker calibration detected inspection of either the target or another button object in at least 75% of all referential scenes in an interaction. The resulting corpus comprises 75 games, for a combined length of 8 hours. We extracted 761 episodes from this corpus, amounting to 47m 58s of recorded interac- tions, with an average length per episode of 3.78 seconds (σ = 3.03sec.). There are 261 episodes shorter than 2 sec., 207 in the 2-4 sec. range, 139 in the 4-6 sec. range, and 154 episodes longer than 6 sec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation and results</head><p>The accuracy of our probabilistic models depends on the parameters <ref type="figure">(d train , d test )</ref>. At different stages of an interaction the difficulty to predict an intended target varies as the visual context changes and in particular the number of visible objects. As the weights of the features are optimized at time −d train , it would be expected that testing also at time −d test = −d train yields the highest accu- racy. However, the difficulty to make a predic- tion decreases as t b − d test approaches t b , i.e. as the player moves towards the intended target. We expect that testing at −d train works best, but we need to be able to update continuously. Thus we also evaluate at other timepoints and test several combinations of the (d train , d test ) parameters.</p><p>Given the limited amount of eye-tracking data available in our corpus, we replaced the cross- corpora-challenge test setting from the original P obs study with a ten fold cross validation setup. As training and testing were performed over in- stances of a certain minimum length according to <ref type="bibr">(d train , d test )</ref>, we first removed all instances with length less than max(d train , d test ), and then per- form the cross validation split. In this way we ensure that the number of instances in the folds are not unbalanced. Moreover, each instance was classified as easy or hard depending on the num- ber of visible objects at time t b . An instance was considered easy if no more than three objects were visible at that point, or hard otherwise. For −d test = 0, 59.5% of all instances are considered hard, but this proportion decreases as −d test in- creases. At −d test = −6, the number of hard in- stances amounts to 72.7%.</p><p>We evaluated both the original P obs model and the P Eobs model on the same data set. We also cal- culated accuracy values for each feature function, in order to test whether a single function could out- perform P obs . We included as baselines two ver- sions of P obs using only the features InRoom and Visual Salience proposed by <ref type="bibr" target="#b3">Engonopoulos et al. (2013)</ref>.</p><p>The accuracy results on <ref type="figure" target="#fig_3">Figure 2</ref> show our ob- servations for −6 ≤ −d train ≤ −2 and −d train ≤ −d test ≤ 0. The graph shows that P Eobs performs similarly as P obs on the easy instances, i.e. the eye-tracking features are not contributing in those scenarios. However, P Eobs shows a consistent im- provement on the hard instances over P obs .</p><p>For each permutation of the training and test- ing parameters (d train , d test ), we obtain a set of episodes that fulfil the length criteria for the given parameters. We apply P obs and P Eobs on the ob- tained set of instances and measure two corre- sponding accuracy values. We compared the ac- curacy values of P obs and P Eobs over all 25 differ- ent (d train , d test ) pairs, using a paired samples t- test. The test indicated that the P Eobs performance (M = 83.72, SD = 3.56) is significantly better than the P obs performance (M = 79.33, SD = 3.89), (t(24) = 9.51, p &lt; .001, Cohen s d = 1.17). Thus eye-tracking features seem to be par- ticularly helpful for predicting to which entity an RE is resolved in hard scenes.</p><p>The results also show a peak in accuracy near the -3 seconds mark. We computed a 2x2 con- tingency table that contrasts correct and incorrect predictions for P obs and P Eobs , i.e. whether o i was classified as target object or not. Data for this ta- ble was collected from all episode judgements for models trained at times in the [−6 sec., −3 sec.] range and tested at -3 seconds. McNemar's test showed that the marginal row and column frequen- cies are significantly different (p &lt; 0.05). This peak is related to the average required time be- tween an utterance and the resulting target manip- ulation. This result shows that our model is more accurate precisely at points in time when we ex- pect fixations to a target object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we have shown that listener's gaze is useful by showing that accuracy improves over P obs in the context of predicting the resolution of an RE. In addition, we observed that our model P Eobs proves to be more robust than P obs when the time interval between the prediction (t b − d test ) and the button press (t b ) increases, i.e. gaze is especially beneficial in an early stage of an in- teraction. This approach shows significant ac- curacy improvement on hard referential scenes where more objects are visible.</p><p>We have also established that gaze is particu- larly useful when combined with some other sim- ple features, as the features that capture listeners visual behaviour are not powerful enough to out- perform even the simplest baseline. Gaze only benefits the model when it is added on top of fea- tures that capture the visual context, i.e. the current scene.</p><p>The most immediate future line of research is the combination of our P Eobs model with the se- mantic model P sem , in order to test the impact of the extended features in a combined model. If suc- cessful, such a model could provide reliable pre- dictions for a significant amount of time before an action takes place. This is of particular importance when it comes to designing a system that auto- matically generates and online outputs feedback to confirm correct and reject incorrect intentions.</p><p>Testing with users in real time is also an area for future research. An implementation of the P obs model is currently in the test phase, and an exten- sion for the P Eobs model would be the immediate next step. The model could be embedded in an NLG system to improve the automatic language generation in such scenarios.</p><p>Given that our work refers only to NLG sys- tems, there's no possible analysis of speaker's gaze. However, it may be interesting to ask whether a human IG could benefit from the pre- dictions of P Eobs . We could study whether pre- dictions based on the gaze (mis-)match between both interlocutors are more effective than simply presenting the IF's gaze to the IG and trusting the IG to correctly interpret this data. If such a sys- tem proved to be effective, it could point misun- derstandings to the IG before either of the partici- pants becomes aware of them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The structure of the interactions.</figDesc><graphic url="image-1.png" coords="2,307.28,544.59,204.10,156.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 depicts</head><label>1</label><figDesc>Figure 1 depicts the structure of an episode when eye-tracking data is available. Each episode</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy as a function of training and testing time.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was funded by the Cluster of Excel-lence on "Multimodal Computing and Interaction" of the German Excellence Initiative and the SFB 632 "Information Structure".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Entrainment on the move and in the lab: The walking around corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><forename type="middle">S</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karla</forename><forename type="middle">M</forename><surname>Batres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 35th Annual Conference of the Cognitive Science Society<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speaking while monitoring addressees for understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meredyth</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="81" />
			<date type="published" when="2004-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Using Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-05" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting the resolution of referring expressions from user behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Engonopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The give2 corpus of giving instructions in virtual environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gargett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantina</forename><surname>Garoufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Striegnitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)<address><addrLine>Valletta, Malta, May</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speakers&apos; eye gaze disambiguates referring expressions early during face-to-face conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><forename type="middle">E</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="596" to="615" />
			<date type="published" when="2007-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How are eye fixation durations controlled during scene viewing? further evidence from a scene onset delay paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="1055" to="1082" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparing listener gaze with predictions of an incremental reference resolution model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Kennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RefNet workshop on Psychological and Computational Models of Reference Comprehension and Production</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Report on the Second NLG Challenge on Generating Instructions in Virtual Environments (GIVE-2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Striegnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gargett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justine</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Oberlander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Natural Language Generation Conference (INLG)</title>
		<meeting>the 6th International Natural Language Generation Conference (INLG)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhancing referential success by tracking hearer gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Staudte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantina</forename><surname>Garoufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Crocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL &apos;12</title>
		<meeting>the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using listener gaze to augment speech generation in a virtual 3D environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Staudte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantina</forename><surname>Garoufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Crocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Meeting of the Cognitive Science Society (CogSci)</title>
		<meeting>the 34th Annual Meeting of the Cognitive Science Society (CogSci)<address><addrLine>Sapporo</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
