<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enforcing Structural Diversity in Cube-pruned Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Enforcing Structural Diversity in Cube-pruned Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="656" to="661"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we extend the cube-pruned dependency parsing framework of Zhang et al. (2012; 2013) by forcing inference to maintain both label and structural ambiguity. The resulting parser achieves state-of-the-art accuracies, in particular on datasets with a large set of dependency labels.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsers assign a syntactic depen- dency tree to an input sentence <ref type="bibr">(KÃ¼bler et al., 2009)</ref>, as exemplified in <ref type="figure">Figure 1</ref>. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs ( <ref type="bibr" target="#b15">McDonald et al., 2005</ref>), sibling or grand- parent arcs <ref type="bibr" target="#b14">(McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b4">Carreras, 2007</ref>) or higher-order substructures ( <ref type="bibr" target="#b10">Koo and Collins, 2010;</ref><ref type="bibr" target="#b12">Ma and Zhao, 2012)</ref>. As the scope of each feature function increases so does parsing complexity, e.g., o(n 5 ) for fourth-order dependency parsing <ref type="bibr" target="#b12">(Ma and Zhao, 2012)</ref>. This has led to work on approximate inference, typ- ically via pruning <ref type="bibr" target="#b0">(Bergsma and Cherry, 2010;</ref><ref type="bibr" target="#b18">Rush and Petrov, 2012;</ref><ref type="bibr" target="#b8">He et al., 2013)</ref> Recently, it has been shown that cube-pruning <ref type="bibr" target="#b5">(Chiang, 2007)</ref> can efficiently introduce higher- order dependencies in graph-based parsing <ref type="bibr" target="#b21">(Zhang and McDonald, 2012)</ref>. Cube-pruned dependency parsing runs standard bottom-up chart parsing us- ing the lower-order algorithms. Similar to k-best inference, each chart cell maintains a beam of k- best partial dependency structures. Higher-order features are scored when combining beams during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with higher-order fea- tures. However, <ref type="bibr" target="#b23">Zhang et al. (2013)</ref> observe state- of-the-art results when training accounts for errors that arise due to such approximations. In this work we extend the cube-pruning frame- work of Zhang et al. by observing that dependency parsing has two fundamental sources of ambiguity. The first, structural ambiguity, pertains to confu- sions about the unlabeled structure of the tree, e.g., the classic prepositional phrase attachment prob- lem. The second, label ambiguity, pertains to sim- ple label confusions, e.g., whether a verbal object is direct or indirect.</p><p>Distinctions between arc labels are frequently fine-grained and easily confused by parsing mod- els. For example, in the Stanford dependency label set <ref type="bibr" target="#b6">(De Marneffe et al., 2006</ref>), the labels TMOD (temporal modifier), NPADVMOD (noun- phrase adverbial modifier), IOBJ (indirect object) and DOBJ (direct object) can all be noun phrases that modify verbs to their right. In the context of cube-pruning, during inference, the system opts to maintain a large amount of label ambiguity at the expense of structural ambiguity. Frequently, the beam stores only label ambiguities and the result- ing set of trees have identical unlabeled structure. For example, in <ref type="figure">Figure 1</ref>, the aforementioned la- bel ambiguity around noun objects to the right of the verb (DOBJ vs. IOBJ vs. TMP) could lead one or more of the structural ambiguities falling out of the beam, especially if the beam is small.</p><p>To combat this, we introduce a secondary beam for each unique unlabeled structure. That is, we partition the primary (entire) beam into dis- joint groups according to the identity of unla- beled structure. By limiting the size of the sec- ondary beam, we restrict label ambiguity and en- force structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of <ref type="bibr" target="#b23">Zhang et al. (2013)</ref>. In </p><formula xml:id="formula_0">l = l + l1 (b) l = l1 + l2</formula><p>Figure 2: Structures and rules for parsing with the <ref type="bibr" target="#b7">(Eisner, 1996)</ref> algorithm. Solid lines show only the construction of right-pointing first-order de- pendencies. l is the predicted arc label. Dashed lines are the additional sibling modifier signatures in a generalized algorithm, specifically the previ- ous modifier in complete chart items.</p><p>particular, data sets with large label sets (and thus a large number of label confusions) typically see the largest jumps in accuracy. Finally, we show that the same result cannot be achieved by simply increasing the size of the beam, but requires ex- plicit enforcing of beam diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structural Diversity in Cube-Pruning</head><p>Our starting point is the cube-pruned dependency parsing model of <ref type="bibr" target="#b21">Zhang and McDonald (2012)</ref>. In that work, as here, inference is simply the Eis- ner first-order parsing model <ref type="bibr" target="#b7">(Eisner, 1996)</ref> shown in <ref type="figure">Figure 2</ref>. In order to score higher-order fea- tures, each chart item maintains a list of signa- tures, which represent subtrees consistent with the chart item. The stored signatures are the relevant portions of the subtrees that will be part of higher- order feature calculations. For example, to score features over adjacent arcs, we might maintain ad- ditional signatures, again shown in <ref type="figure">Figure 2</ref>. The scope of the signature adds asymptotic complexity to parsing. Even for second-order sib- lings, there will now be O(n) possible signatures per chart item. The result is that parsing com- plexity increases from O(n 3 ) to O(n 5 ). Instead of storing all signatures, <ref type="bibr" target="#b21">Zhang and McDonald (2012)</ref> store the current k-best in a beam. This re- sults in approximate inference, as some signatures may fall out of the beam before higher-order fea- tures can be scored. This general trick is known as cube-pruning and is a common approach to deal- ing with large hypergraph search spaces in ma- chine translation <ref type="bibr" target="#b5">(Chiang, 2007)</ref>.</p><p>Cube-pruned parsing is analogous to k-best parsing algorithmically. But there is a fundamen- tal difference. In k-best parsing, if two subtrees t a and t b belong to the same chart item, with t a l = 0 : 1 : 2 : ranking higher than t b , then an extension of t a through combing with a subtree t c from another chart item must also score higher than that of t b . This property is called the monotonicity property. Based on it, k-best parsing merges k-best subtrees in the following way: given two chart items with k-best lists to be combined, it proceeds on the two sorted lists monotonically from beginning to end to generate combinations. Cube pruning follows the merging procedure despite the loss of mono- tonicity due to the addition of higher-order feature functions over the signatures of the subtrees. The underlying assumption of cube pruning is that the true k-best results are likely in the cross-product space of top-ranked component subtrees. <ref type="figure" target="#fig_2">Figure 3</ref> shows that the space is the top-left corner of the grid in the binary branching cases. As mentioned earlier, the elements in chart item k-best lists are feature signatures of subtrees. We make a distinction between labeled signatures and unlabeled signatures. As feature functions are de- fined on sub-graphs of the dependency trees, a fea- ture signature is labeled if and only if feature func- tions draw information from both the arcs in the sub-graph and the labels on the arcs. Every la- beled signature projects to an unlabeled signature that ignores the arc labels.</p><p>The motivation for introducing unlabeled signa- tures for labeled parsing is to enforce structural di- versity. <ref type="figure" target="#fig_2">Figure 3</ref> illustrates the idea. In the top diagram, there is only one unlabeled signature in one of the two lists. This is likely to happen when there is label ambiguity so that all three labels have similar scores. In such cases, alternative tree struc- tures further down in the list that have the poten- tial to be scored higher when incorporating higher- order features, lose this opportunity due to prun- ing. By contrast, if we introduce structural diver- sity by limiting the number of label variants, such alternative structures can come out on top.</p><p>More formally, when the feature signatures of the subtrees include arc labels, the cardinality of the set of all possible signatures grows by a poly- nomial of the size of the label set. This factor has a diluting effect on the diversity of unlabeled signa- tures within the beam. The larger the label set is, the greater the chance label ambiguity will dom- inate the beam. Therefore, we introduce a sec- ond level of beam specifically for labeled signa- tures. We call it the secondary beam, relative to the primary beam, i.e., the entire beam. The sec- ondary beam limits the number of labeled signa- tures for each unlabeled signature, a projection of labeled signature, while the primary beam limits the total number of labeled signatures. To illus- trate this, consider an original primary beam of length b and a secondary beam length of s b . Let t j i represent the i th highest scoring labeled variant of unlabeled structure j. The table below shows a specific example of beam configurations for b = 4 for all possible values of s b . The original beam is the pathological case where all signatures have the same unlabeled projection. When s b = 1, all sig- natures in the beam now have a different unlabeled projection. When s b = 4, the beam reverts to the original without any structural diversity. Values between balance structural and label diversity. To achieve this in cube pruning, deeper explo- ration in the merging procedure becomes neces- sary. In this example, originally the merging pro- cedure stops when t 1 4 has been explored. When s b = 1, the exploration needs to go further from rank 4 to 9. When s b = 2, it needs to go from 4 to 6. When s b = 3, only one more step to rank 5 is necessary. The amount of additional compu- tation depends on the value of s b , the composi- tion of the incoming k-best lists, and the feature functions which determine feature signatures. To account for this we also compare to baselines sys- tems that simply increase the size of the beam to a comparable run-time.</p><formula xml:id="formula_1">beam original b = 4 b = 4 b = 4 b = 4 rank b=4 s b = 1 s b = 2 s b = 3 s b = 4 t 2 1 . . . .</formula><p>In our experiments we found that s b = b/2 is typically a good choice. As in most parsing sys- tems, beams are applied consistently during learn- ing and testing because feature weights will be ad- justed according to the diversity of the beam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We use the cube-pruned dependency parser of <ref type="bibr" target="#b23">Zhang et al. (2013)</ref> as our baseline system. To make an apples-to-apples comparison, we use the same online learning algorithm and the same fea- ture templates. The feature templates include first- to-third-order labeled features and valency fea- tures. More details of these features are described in <ref type="bibr" target="#b21">Zhang and McDonald (2012)</ref>. For online learn- ing, we apply the same violation-fixing strategy (so-called single-node max-violation) on MIRA and run 8 epochs of training for all experiments.</p><p>For English, we conduct experiments on the commonly-used constituency-to-dependency- converted Penn Treebank data sets. The first one, Penn-YM, was created by the Penn2Malt 1 soft- ware. The second one, Penn-S-2.0.5, used the Stanford dependency framework <ref type="bibr" target="#b6">(De Marneffe et al., 2006</ref>) by applying version 2.0.5 of the Stan- ford parser. The third one, Penn-S-3.3.0 was con- verted by version 3.3.0 of the Stanford parser. The train/dev/test split was standard: sections 2-21 for training; 22 for validation; and 23 for evaluation. Automatic POS tags for Penn-YM and Penn-S- 2.0.5 are provided by TurboTagger ( <ref type="bibr" target="#b13">Martins et al., 2013</ref>) with an accuracy of 97.3% on section 23. For Chinese, we use the CTB-5 dependency tree- bank which was converted from the original con- stituent treebank by <ref type="bibr" target="#b22">Zhang and Nivre (2011)</ref>   <ref type="table">Table 1</ref>: English and Chinese results for cube pruning dependency parsing with the enforcement of structural diversity. PENN-S and CTB-5 are significant at p &lt; 0.05. Penn-S-2.0.5 TurboParser result is from <ref type="bibr" target="#b13">Martins et al. (2013)</ref>. Following <ref type="bibr" target="#b9">Kong and Smith (2014)</ref>, we trained our models on Penn-S-3.3.0 with gold POS tags and evaluated with both non-gold (Stanford tagger) and gold tags. <ref type="table">Table 1</ref> shows the main results of the paper. Both the baseline and the new system keep a beam of size 6 for each chart cell. The difference is that the new system enforces structural diversity with the introduction of a secondary beam for la- bel variants. We choose the secondary beam that yields the highest LAS on the development data sets for Penn-YM, Penn-S-2.0.5 and CTB-5. In- deed we observe larger improvements for the data sets with larger label sets. Penn-S-2.0.5 has 49 la- bels and observes a 0.2% absolute improvement in LAS. Although CTB-5 has a small label set (18), we do see similar improvements for both UAS and LAS. There is a slight improvement for Penn-YM despite the fact that Penn-YM has the most com- pact label set (12). These results are the highest known in the literature. For the Penn-S-3.3.0 re- sults we can see that our model outperforms Tur- boPaser and is competitive with the Berkeley con- stituency parser ( <ref type="bibr" target="#b17">Petrov et al., 2006</ref>). In particu- lar, if gold tags are assumed, cube-pruning signif- icantly outperforms Berkeley. This suggests that joint tagging and parsing should improve perfor- mance further in the non-gold tag setting, as that is a differentiating characteristic of constituency parsers. <ref type="table" target="#tab_3">Table 2</ref> shows the results on the CoNLL 2006/2007 data sets <ref type="bibr" target="#b3">(Buchholz and Marsi, 2006;</ref><ref type="bibr" target="#b16">Nivre et al., 2007</ref>). For simplicity, we set the sec- ondary beam to 3 for all. We can see that over- all there is an improvement in accuracy and this is highly correlated with the size of the label set.</p><p>In order to examine the importance of balancing structural diversity and labeled diversity, we let the size of the secondary beam vary from one to the size of the primary beam. In <ref type="table" target="#tab_5">Table 3</ref>, we show the results of all combinations of beam settings of pri- mary beam sizes 4 and 6 for three data sets: Penn- YM, Penn-S-2.0.5, and CTB-5 respectively. In the table, we highlight the best results for each beam size and data set on the development data. For 5 of the total of 6 comparison groups -three lan-  guages times two primary beams -the best result is obtained by choosing a secondary beam size that is close to one half the size of the primary beam.</p><p>Contrasting <ref type="table" target="#tab_5">Table 1 and Table 3</ref>, the accuracy im- provements are consistent across the development set and the test set for all three data sets. A reasonable question is whether such improve- ments could be obtained by simply enlarging the beam in the baseline parser. The bottom row of <ref type="table" target="#tab_5">Table 3</ref> shows the parsing results for the three data sets when the beam is enlarged to 16. On Penn- S-2.0.5, the baseline with beam 16 is at roughly the same speed as the highlighted best system with primary beam 6 and secondary beam 3. On CTB- 5, the beam 16 baseline is 30% slower. <ref type="table" target="#tab_5">Table 3</ref> indicates that simply enlarging the beam -rela- tive to parsing speed -does not recover the wins of structural diversity on Penn-S-2.0.5 and CTB-5, though it does reduce the gap on Penn-S-2.0.5. On Penn-YM, the beam 16 baseline is slightly better than the new system, but 90% slower.   To better understand the behaviour of structural diversity pruning relative to increasing the beam, we looked at the unlabeled attachment F-score per dependency label in the Penn-S-2.0.5 development set 2 . <ref type="table">Table 4</ref> shows the 10 labels with the largest increase in attachment scores for structural diver- sity pruning relative to standard pruning. Impor- tantly, the biggest wins are primarily for labels in which unlabeled attachment is lower than average (93.99, 8 out of 10). Thus, diversity pruning gets most of its wins on difficult attachment decisions. Indeed, many of the relations represent clausal dependencies that are frequently structurally am- biguous. There are also cases of relatively short dependencies that can be difficult to attach. For instance, quantmod dependencies are typically ad- verbs occurring after verbs that modify quantities to their right. But these can be confused as ad- verbial modifiers of the verb to the left. These re- sults support our hypothesis that label ambiguity is causing hard attachment decisions to be pruned and that structural diversity can ameliorate this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Keeping multiple beams in approximate search has been explored in the past. In machine transla- tion, multiple beams are used to prune translation hypotheses at different levels of granularity ( <ref type="bibr" target="#b19">Zens and Ney, 2008)</ref>. However, the focus is improving the speed of translation decoder rather than im- proving translation quality through enforcement of hypothesis diversity. In parsing, Bohnet and Nivre (2012) and <ref type="bibr" target="#b2">Bohnet et al. (2013)</ref> propose a model for joint morphological analysis, part-of- speech tagging and dependency parsing using a <ref type="bibr">2</ref> Using eval.pl from <ref type="bibr" target="#b3">Buchholz and Marsi (2006</ref>  <ref type="table">Table 4</ref>: Unlabeled attachment F-score per de- pendency relation. The top 10 score increases for structural diversity pruning (beam 6 and la- bel beam of 3) over basic pruning (beam 16) are shown. Only labels with more than 100 instances in the development data are considered.</p><p>left-to-right beam. With a single beam, token level ambiguities (morphology and tags) dominate and dependency level ambiguity is suppressed. This is addressed by essentially keeping two beams. The first forces every tree to be different at the depen- dency level and the second stores the remaining highest scoring options, which can include outputs that differ only at the token level. The present work looks at beam diversity in graph-based dependency parsing, in particular la- bel versus structural diversity. It was shown that by keeping a diverse beam significant improve- ments could be achieved on standard benchmarks, in particular with respect to difficult attachment decisions. It is worth pointing out that other dependency parsing frameworks (e.g., transition- based parsing <ref type="bibr" target="#b20">(Zhang and Clark, 2008;</ref><ref type="bibr" target="#b22">Zhang and Nivre, 2011)</ref>) could also benefit from modeling structural diversity in search.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: A sample dependency parse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Merging procedure in cube pruning. The bottom shows that enforcing diversity in the k-best lists can give chance to a good structure at (2, 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>and use gold-standard POS tags as is standard.</figDesc><table>Berkeley Parser 

TurboParser 
Cube-pruned w/o diversity 
Cube-pruned w/ diversity 
UAS 
LAS 
UAS 
LAS 
UAS 
LAS 
UAS 
LAS 
PENN-YM 
-
-
93.07 
-
93.50 
92.41 
93.57 
92.48 
PENN-S-2.0.5 
-
-
92.82 
-
93.59 
91.17 
93.71 
91.37 
PENN-S-3.3.0 
93.31 
91.01 
92.20 89.67 
92.91 
90.52 
93.01 
90.64 
PENN-S-3.3.0-GOLD 93.65 
92.05 
93.56 91.99 
94.32 
92.90 
94.40 
93.02 
CTB-5 
-
-
-
-
87.78 
86.13 
87.96 
86.34 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results for languages from CoNLL 
2006/2007 shared tasks. When a language is in 
both years, the 2006 set is used. Languages are 
sorted by the number of unique arc labels. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Varying the degree of diversity by adjusting the secondary beam for labeled variants, with 
different primary beams. When the size of the secondary beam is equal to the primary beam, the parser 
degenerates to not enforcing structural diversity. In the opposite, when the secondary beam is smaller, 
there is more structural diversity and less label diversity. Results are on development sets. 

</table></figure>

			<note place="foot" n="1"> t 1 1 t 1 1 t 1 1 t 1 1 t 1 1 2 t 1 2 t 2 1 t 1 2 t 1 2 t 1 2 3 t 1 3 t 3 1 t 2 1 t 1 3 t 1 3 4 t 1 4 t 4 1 t 3 1 t 2 1 t 1 4 Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· beam cut-off Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â·</note>

			<note place="foot" n="1"> http://stp.lingfil.uu.se/â¼nivre/research/Penn2Malt.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and accurate arc filtering for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP/CoNLL</title>
		<meeting>of EMNLP/CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Joint morphological and syntactic analysis for richly inflected languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Boguslavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>RichÃ¡rd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hajic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">CoNLL-X shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Experiments with a higher-order projective dependency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the CoNLL Shared Task Session of EMNLP-CoNLL</title>
		<meeting>of the CoNLL Shared Task Session of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: an exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic feature selection for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>DaumÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An empirical comparison of parsing methods for stanford dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>ArXiv:1404.4314</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient third-order dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>KÃ¼bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fourth-order dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order nonprojective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The CoNLL 2007 shared task on dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>KÃ¼bler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL</title>
		<meeting>of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient multi-pass dependency pruning with vine parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improvements in dynamic programming beam search for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IWSLT</title>
		<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalized higher-order dependency parsing with cube pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-HLT</title>
		<meeting>of ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online learning for inexact hypergraph search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
