<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Twitter Sentiment Analysis with Topic-Based Mixture Modeling and Semi-Supervised Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhou</surname></persName>
							<email>l.zhou@thomsonreuters.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IBM Watson</orgName>
								<address>
									<addrLine>1101 Kitchawan Rd Yorktown Heights</addrLine>
									<postCode>10598</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Thomson Reuters</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Times Square New York</orgName>
								<address>
									<postCode>10036</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Twitter Sentiment Analysis with Topic-Based Mixture Modeling and Semi-Supervised Training</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="434" to="439"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we present multiple approaches to improve sentiment analysis on Twitter data. We first establish a state-of-the-art baseline with a rich feature set. Then we build a topic-based sentiment mixture model with topic-specific data in a semi-supervised training framework. The topic information is generated through topic modeling based on an efficient implementation of Latent Dirich-let Allocation (LDA). The proposed sentiment model outperforms the top system in the task of Sentiment Analysis in Twit-ter in SemEval-2013 in terms of averaged F scores.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social media, such as Twitter and Facebook, has attracted significant attention in recent years. The vast amount of data available online provides a unique opportunity to the people working on nat- ural language processing (NLP) and related fields. Sentiment analysis is one of the areas that has large potential in real-world applications. For ex- ample, monitoring the trend of sentiment for a spe- cific company or product mentioned in social me- dia can be useful in stock prediction and product marketing.</p><p>In this paper, we focus on sentiment analysis of Twitter data (tweets). It is one of the challenging tasks in NLP given the length limit on each tweet (up to 140 characters) and also the informal con- versation. Many approaches have been proposed previously to improve sentiment analysis on Twit- ter data. For example, <ref type="bibr" target="#b10">Nakov et al. (2013)</ref> provide an overview on the systems submitted to one of the SemEval-2013 tasks, Sentiment Analysis in Twit- ter. A variety of features have been utilized for sentiment classification on tweets. They include lexical features (e.g. word lexicon), syntactic fea- tures (e.g. Part-of-Speech), Twitter-specific fea- tures (e.g. emoticons), etc. However, all of these features only capture local information in the data and do not take into account of the global higher- level information, such as topic information.</p><p>Two example tweets are given below, with the word "offensive" appearing in both of them.</p><p>• Im gonna post something that might be offen- sive to people in Singapore.</p><p>• #FSU offensive coordinator Randy Sanders coached for Tennessee in 1st #BCS title game.</p><p>Generally "offensive" is used as a negative word (as in the first tweet), but it bears no sentiment in the second tweet when people are talking about a football game. Even though some local contextual features could be helpful to distinguish the two cases above, they still may not be enough to get the sentiment on the whole message correct. Also, the local features often suffer from the sparsity prob- lem. This motivates us to explore topic informa- tion explicitly in the task of sentiment analysis on Twitter data. There exists some work on applying topic in- formation in sentiment analysis, such as ( <ref type="bibr" target="#b8">Mei et al., 2007)</ref>, <ref type="bibr" target="#b1">(Branavan et al., 2008)</ref>, (Jo and Oh, 2011) and ( <ref type="bibr" target="#b5">He et al., 2012)</ref>. All these work are significantly different from what we propose in this work. Also they are conducted in a domain other than Twitter. Most recently, <ref type="bibr" target="#b11">Si et al. (2013)</ref> propose a continuous Dirichlet Process Mixture model for Twitter sentiment, for the purpose of stock prediction. Unfortunately there is no eval- uation on the accuracy of sentiment classification alone in that work. Furthermore, no standard train- ing or test corpus is used, which makes compari- son with other approaches difficult.</p><p>Our work is organized in the following way:</p><p>• We first propose a universal sentiment model that utilizes various features and resources. The universal model outperforms the top system submitted to the <ref type="bibr">SemEval-2013</ref><ref type="bibr">task (Mohammad et al., 2013</ref>, which was trained and tested on the same data. The universal model serves as a strong baseline and also provides an option for smoothing later.</p><p>• We introduce a topic-based mixture model for Twitter sentiment. The model is inte- grated in the framework of semi-supervised training that takes advantage of large amount of un-annotated Twitter data. Such a mixture model results in further improvement on the sentiment classification accuracy.</p><p>• We propose a smoothing technique through interpolation between universal model and topic-based mixture model.</p><p>• We also compare different approaches for topic modeling, such as cross-domain topic identification by utilizing data from newswire domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Universal Sentiment Classifier</head><p>In this section we present a universal topic- independent sentiment classifier to establish a state-of-the-art baseline. The sentiment labels are either positive, neutral or negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SVM Classifier</head><p>Support Vector Machine (SVM) is an effec- tive classifier that can achieve good performance in high-dimensional feature space. An SVM model represents the examples as points in space, mapped so that the examples of the different cate- gories are separated by a clear margin as wide as possible. In this work an SVM classifier is trained with LibSVM (Chang and Lin, 2011), a widely used toolkit. The linear kernel is found to achieve higher accuracy than other kernels in our initial ex- periments. The option of probability estimation in LibSVM is turned on so that it can produce the probability of sentiment class c given tweet x at the classification time, i.e. P (c|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Features</head><p>The training and testing data are run through tweet-specific tokenization, similar to that used in the CMU Twitter NLP tool ( <ref type="bibr" target="#b3">Gimpel et al., 2011</ref>).</p><p>It is shown in Section 5 that such customized tok- enization is helpful. Here are the features that we use for classification:</p><p>• Word N-grams: if certain N-gram (unigram, bigram, trigram or 4-gram) appears in the tweet, the corresponding feature is set to 1, otherwise 0. These features are collected from training data, with a count cutoff to avoid overtraining.</p><p>• Manual lexicons: it has been shown in other work ( <ref type="bibr" target="#b10">Nakov et al., 2013</ref>) that lexicons with positive and negative words are important to sentiment classification. In this work, we adopt the lexicon from Bing Liu ( <ref type="bibr" target="#b6">Hu and Liu, 2004</ref>) which includes about 2000 posi- tive words and 4700 negative words. We also experimented with the popular MPQA <ref type="bibr" target="#b12">(Wilson et al., 2005</ref>) lexicon but found no extra improvement on accuracies. A short list of Twitter-specific positive/negative words are also added to enhance the lexicons. We gen- erate two features based on the lexicons: total number of positive words or negative words found in each tweet.</p><p>• Emoticons: it is known that people use emoti- cons in social media data to express their emotions. A set of popular emoticons are col- lected from the Twitter data we have. Two features are created to represent the presence or absence of any positive/negative emoti- cons.</p><p>• Last sentiment word: a "sentiment word" is any word in the positive/negative lexicons mentioned above. If the last sentiment word found in the tweet is positive (or negative), this feature is set to 1 (or -1). If none of the words in the tweet is sentiment word, it is set to 0 by default.</p><p>• PMI unigram lexicons: in (Mohammad et al., 2013) two lexicons were automatically generated based on pointwise mutual infor- mation (PMI). One is NRC Hashtag Senti- ment Lexicon with 54K unigrams, and the other is Sentiment140 Lexicon with 62K un- igrams. Each word in the lexicon has an as- sociated sentiment score. We compute 7 fea- tures based on each of the two lexicons: (1) sum of sentiment score; (2) total number of positive words (with score s &gt; 1); (3) to- tal number of negative words (s &lt; −1); (4) maximal positive score; (5) minimal negative score; (6) score of the last positive words; (7) score of the last negative words. Note that for the second and third features, we ignore those with sentiment scores between -1 and 1, since we found that inclusion of those weak subjec- tive words results in unstable performance.</p><p>• PMI bigram lexicon: there are also 316K bi- grams in the NRC Hashtag Sentiment Lexi- con. For bigrams, we did not find the sen- timent scores useful. Instead, we only com- pute two features based on counts only: total number of positive bigrams; total number of negative bigrams.</p><p>• Punctuations: if there exists exclamation mark or question mark in the tweet, the fea- ture is set to 1, otherwise set to 0.</p><p>• Hashtag count: the number of hashtags in each tweet.</p><p>• Negation: we collect a list of negation words, including some informal words frequently observed in online conversations, such as "dunno" ("don't know"), "nvr" ("never"), etc. For any sentiment words within a win- dow following a negation word and not af- ter punctuations '.', ',', ';', '?', or '!', we re- verse its sentiment from positive to negative, or vice versa, before computing the lexicon- based features mentioned earlier. The win- dow size was set to 4 in this work.</p><p>• Elongated words: the number of words in the tweet that have letters repeated by at least 3 times in a row, e.g. the word "gooood".</p><p>3 Topic-Based Sentiment Mixture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Modeling</head><p>Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b0">Blei et al., 2003</ref>) is one of the widely adopted generative models for topic modeling. The fundamental idea is that a document is a mixture of topics. For each document there is a multinomial distribution over topics, and a Dirichlet prior Dir(α) is introduced on such distribution. For each topic, there is an- other multinomial distribution over words. One of the popular algorithms for LDA model parameter estimation and inference is Gibbs sampling <ref type="bibr" target="#b4">(Griffiths and Steyvers, 2004</ref>), a form of Markov Chain Monte Carlo. We adopt the efficient implementa- tion of Gibbs sampling as proposed in ( <ref type="bibr" target="#b13">Yao et al., 2009</ref>) in this work. Each tweet is regarded as one document. We conduct pre-processing by removing stop words and some of the frequent words found in Twitter data. Suppose that there are T topics in total in the training data, i.e. t 1 , t 2 , ..., t T . The posterior prob- ability of each topic given tweet x i is computed as in Eq. 1:</p><formula xml:id="formula_0">P t (t j |x i ) = C ij + α j T k=1 C ik + T α j<label>(1)</label></formula><p>where C ij is the number of times that topic t j is assigned to some word in tweet x i , usually aver- aged over multiple iterations of Gibbs sampling. α j is the j-th dimension of the hyperparameter of Dirichlet distribution that can be optimized during model estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentiment Mixture Model</head><p>Once we identify the topics for tweets in the train- ing data, we can split the data into multiple sub- sets based on topic distributions. For each subset, a separate sentiment model can be trained. There are many ways of splitting the data. For example, K-means clustering can be conducted based on the similarity between the topic distribution vec- tors or their transformed versions. In this work, we assign tweet x i to cluster j if P t (t j |x i ) &gt; τ or P t (t j |x i ) = max k P t (t k |x i ). Note that this is a soft clustering, with some tweets possibily as- signed to multiple topic-specific clusters. Similar to the universal model, we train T topic-specific sentiment models with LibSVM.</p><p>During classification on test tweets, we run topic inference and sentiment classification with multiple sentiment models. They jointly deter- mine the final probability of sentiment class c given tweet x i as the following in a sentiment mix- ture model:</p><formula xml:id="formula_1">P (c|x i ) = T j=1 P m (c|t j , x i )P t (t j |x i )<label>(2)</label></formula><p>where P m (c|t j , x i ) is the probability of sentiment c from topic-specific sentiment model trained on topic t j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Smoothing</head><p>Additionally, we also experiment with a smooth- ing technique through linear interpolation between the universal sentiment model and topic-based sentiment mixture model.</p><formula xml:id="formula_2">P (c|x i ) = θ × P U (c|x i ) + (1 − θ) × T j=1 P m (c|t j , x i )P t (t j |x i ) (3)</formula><p>where θ is the interpolation parameter and P U (c|x i ) is the probability of sentiment c given tweet x i from the universal sentiment model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Semi-supervised Training</head><p>In this section we propose an integrated frame- work of semi-supervised training that contains both topic modeling and sentiment classification. The idea of semi-supervised training is to take advantage of large amount low-cost un-annotated data (tweets in this case) to further improve the ac- curacy of sentiment classification. The algorithm is as follows: 7. Repeat from Step 3 until finishing a pre- determined number of iterations or no more data is added to D in Step 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data and Evaluation</head><p>We conduct experiments on the data from the task B of Sentiment Analysis in Twitter in SemEval- 2013. The distribution of positive, neutral and negative data is shown in <ref type="table">Table 1</ref>. The develop- ment set is used to tune parameters and features. The test set is for the blind evaluation. For semi-supervised training experiments, we explored two sets of additional data. The first one contains 2M tweets randomly sampled from the collection in January and February 2014. The other contains 74K news documents with 50M words collected during the first half year of 2013 from online newswire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set</head><p>For evaluation, we use macro averaged F score as in ( <ref type="bibr" target="#b10">Nakov et al., 2013)</ref>, i.e. average of the F scores computed on positive and negative classes only. Note that this does not make the task a binary classification problem. Any errors related to neu- tral class (false positives or false negatives) will negatively impact the F scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Universal Model</head><p>In <ref type="table" target="#tab_1">Table 2</ref>, we show the incremental improvement in adding various features described in Section 2, measured on the test set. In addition to the fea- tures, we also find SVM weighting on the training samples is helpful. Due to the skewness in class distribution in the training set, it is observed dur- ing error analysis on the development set that sub- jective (positive/negative) tweets are more likely to be classified as neutral tweets. The weights for positive, neutral and negative samples are set to be (1, 0.4, 1) based on the results on the develop- ment set. As shown in <ref type="table" target="#tab_1">Table 2</ref>, weighting adds a 2% improvement. With all features combined, the universal sentiment model achieves 69.7 on aver- age F score. The F score from the best system in <ref type="bibr">SemEval-2013</ref><ref type="bibr" target="#b9">(Mohammad et al., 2013</ref>) is also listed in the last row of  <ref type="table" target="#tab_1">Table 2</ref>: Results on the test set with universal sen- timent model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Topic-Based Mixture Model</head><p>For the topic-based mixture model and semi- supervised training, based on the experiments on the development set, we set the parameter τ used in soft clustering to 0.4, the data selection pa- rameter p to 0.96, and the interpolation parame- ter for smoothing θ to 0.3. We found no more noticeable benefits after two iterations of semi- supervised training. The number of topics is set to 100.</p><p>The results on the test set are shown <ref type="table" target="#tab_3">Table 3</ref>, with the topic information inferred from either Twitter data (second column) or newswire data (third column). The first row shows the per- formance of the universal sentiment model as a baseline. The second row shows the results from re-training the universal model by simply adding tweets selected from two iterations of semi-supervised training (about 100K). It serves as another baseline with more training data, for a fair comparison with the topic-based mixture modeling that uses the same amount of training data.</p><p>We also conduct an experiment by only consid- ering the most likely topic for each tweet when computing the sentiment probabilities. The results show that the topic-based mixture model outper- forms both the baseline and the one that considers the top topics only. Smoothing with the universal model adds further improvement in addition to the un-smoothed mixture model. With the topic in- formation inferred from Twitter data, the F score is 2 points higher than the baseline without semi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Tweet-topic News-topic  supervised training and 1.4 higher than the base- line with semi-supervised data. As shown in the third column in <ref type="table" target="#tab_3">Table 3</ref>, sur- prisingly, the model with topic information in- ferred from the newswire data works well on the Twitter domain. A 1.4 points of improvement can be obtained compared to the baseline. This pro- vides an opportunity for cross-domain topic iden- tification when data from certain domain is more difficult to obtain than others.</p><p>In <ref type="table">Table 4</ref>, we provide some examples from the topics identified in tweets as well as the newswire data. The most frequent words in each topic are listed in the table. We can clearly see that the top- ics are about phones, sports, sales and politics, re- spectively.</p><p>Tweet-1 Tweet-2 News-1</p><p>News- <ref type="table" target="#tab_1">2  phone  game  sales  party  call  great  stores  government  answer  play  online  election  question  team  retail  minister  service  win  store  political  text  tonight  retailer  prime  texting  super  business  state   Table 4</ref>: The most frequent words in example top- ics from tweets and newswire data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we presented multiple approaches for advanced Twitter sentiment analysis. We es- tablished a state-of-the-art baseline that utilizes a variety of features, and built a topic-based sen- timent mixture model with topic-specific Twitter data, all integrated in a semi-supervised training framework. The proposed model outperforms the top system in SemEval-2013. Further research is needed to continue to improve the accuracy in this difficult domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>Set training corpus D for sentiment classifi- cation to be the annotated training data D a ; 2. Train a sentiment model with current training corpus D; 3. Run sentiment classification on the un- annotated data D u with the current sentiment model and generate probabilities of sentiment classes for each tweet, P (c|x i ); 4. Perform data selection. For those tweets with P (c|x i ) &gt; p, add them to current training corpus D. The rest is used to replace the un- annotated corpus D u ; 5. Train a topic model on D, and store the topic inference model and topic distributions of each tweet; 6. Cluster data in D based on the topic distribu- tions from Step 5 and train a separate senti- ment model for each cluster. Replace current sentiment model with the new sentiment mix- ture model;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 for a comparison.</head><label>2</label><figDesc></figDesc><table>Model 

Avg. F score 
Baseline with word N-grams 
55.0 
+ tweet tokenization 
56.1 
+ manual lexicon features 
62.4 
+ emoticons 
62.8 
+ last sentiment word 
63.7 
+ PMI unigram lexicons 
64.5 
+ hashtag counts 
65.0 
+ SVM weighting 
67.0 
+ PMI bigram lexicons 
68.2 
+ negations 
69.0 
+ elongated words 
69.7 
Mohammad et al., 2013 
69.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of topic-based sentiment mixture 
model on SemEval test set. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning document-level semantic properties from free-text annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL-2008)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL-2008)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for Twitter: annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Science</title>
		<meeting>the National Academy of Science</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tracking sentiment and topic dynamics from social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International AAAI Conference on Weblogs and Social Media</title>
		<meeting>the 6th International AAAI Conference on Weblogs and Social Media</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ICWSM-2012</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Aspect and sentiment unification model for online review analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference in Web Search and Data Mining (WSDM-2011)</title>
		<meeting>ACM Conference in Web Search and Data Mining (WSDM-2011)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Topic sentiment mixture: modeling facets and opinions in weblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wondra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on World Wide Web</title>
		<meeting>International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation<address><addrLine>Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-14" />
			<biblScope unit="page" from="312" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SemEval-2013 Task 2: Sentiment Analysis in Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06-14" />
			<biblScope unit="page" from="312" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting topic based Twitter sentiment for stock prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotie</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-04" />
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
		<idno>HLT 05</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient methods for topic model inference on streaming document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
