<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Taylor&apos;s Law for Human Linguistic Sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 1138</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuru</forename><surname>Kobayashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Graduate School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">Research Center for Advanced Science and Technology</orgName>
								<orgName type="institution">University of Tokyo</orgName>
								<address>
									<addrLine>7-3-1 Hongo, Bunkyo-ku Tokyo</addrLine>
									<postCode>113-8656</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Tokyo</orgName>
								<address>
									<addrLine>4-6-1 Komaba, Meguro-ku Tokyo</addrLine>
									<postCode>153-8904</postCode>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Taylor&apos;s Law for Human Linguistic Sequences</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1138" to="1148"/>
							<date type="published">July 15-20, 2018. 2018. 1138</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Taylor&apos;s law describes the fluctuation characteristics underlying a system in which the variance of an event within a time span grows by a power law with respect to the mean. Although Taylor&apos;s law has been applied in many natural and social systems, its application for language has been scarce. This article describes a new quantification of Taylor&apos;s law in natural language and reports an analysis of over 1100 texts across 14 languages. The Tay-lor exponents of written natural language texts were found to exhibit almost the same value. The exponent was also compared for other language-related data, such as the child-directed speech, music, and programming language code. The results show how the Taylor exponent serves to quantify the fundamental structural complexity underlying linguistic time series. The article also shows the applicability of these findings in evaluating language models .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Taylor's law characterizes how the variance of the number of events for a given time and space grows with respect to the mean, forming a power law. It is a quantification method for the clustering behav- ior of a system. Since the pioneering studies of this concept <ref type="bibr" target="#b24">(Smith, 1938;</ref><ref type="bibr" target="#b31">Taylor, 1961)</ref>, a substan- tial number of studies have been conducted across various domains, including ecology, life science, physics, finance, and human dynamics, as well summarized in <ref type="bibr" target="#b12">(Eisler, Bartos, and Kertész, 2007</ref>  <ref type="bibr" target="#b8">Calif and Schmitt (2015)</ref> re- ported Taylor's law in wind energy data using a non-parametric regression. Those two papers also refer to research about Taylor's law in a wide range of fields. Despite such diverse application across do- mains, there has been little analysis based on Tay- lor's law in studying natural language. The only such report, to the best of our knowledge, is Ger- lach and <ref type="bibr" target="#b13">Altmann (2014)</ref>, but they measured the mean and variance by means of the vocabulary size within a document. This approach essen- tially differs from the original concept of Taylor analysis, which fundamentally counts the number of events, and thus the theoretical background of Taylor's law as presented in Eisler, <ref type="bibr" target="#b12">Bartos, and Kertész (2007)</ref> cannot be applied to interpret the results.</p><p>For the work described in this article, we ap- plied Taylor's law for texts, in a manner close to the original concept. We considered lexical fluctuation within texts, which involves the co- occurrence and burstiness of word alignment. The results can thus be interpreted according to the an- alytical results of Taylor's law, as described later. We found that the Taylor exponent is indeed a characteristic of texts and is universal across vari- ous kinds of texts and languages. These results are shown here for data including over 1100 single- author texts across 14 languages and large-scale newspaper data.</p><p>Moreover, we found that the Taylor expo- nents for other symbolic sequential data, includ- ing child-directed speech, programming language code, and music, differ from those for written nat- ural language texts, thus distinguishing different kinds of data sources. The Taylor exponent in this sense could categorize and quantify the structural complexity of language. The Chomsky hierarchy <ref type="bibr" target="#b9">(Chomsky, 1956</ref>) is, of course, the most important framework for such categorization. The Taylor ex- ponent is another way to quantify the complexity of natural language: it allows for continuous quan- tification based on lexical fluctuation.</p><p>Since the Taylor exponent can quantify and characterize one aspect of natural language, our findings are applicable in computational linguis- tics to assess language models. At the end of this article, in §5, we report how the most basic character-based long short-term memory (LSTM) unit produces texts with a Taylor exponent of 0.50, equal to that of a sequence of independent and identically distributed random variables (an i.i.d. sequence). This shows how such models are lim- ited in producing consistent co-occurrence among words, as compared with a real text. Taylor analy- sis thus provides a possible direction to reconsider the limitations of language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This work can be situated as a study to quan- tify the complexity underlying texts. As sum- marized in <ref type="bibr" target="#b29">(Tanaka-Ishii and Aihara, 2015)</ref>, mea- sures for this purpose include the entropy rate <ref type="bibr" target="#b28">(Takahira, Tanaka-Ishii, and Lukasz, 2016;</ref><ref type="bibr" target="#b5">Bentz et al., 2017)</ref> and those related to the scaling behav- iors of natural language. Regarding the latter, cer- tain power laws are known to hold universally in linguistic data. The most famous among these are Zipf's law <ref type="bibr" target="#b33">(Zipf, 1965)</ref> and <ref type="bibr">Heaps' law (Heaps, 1978)</ref>. Other, different kinds of power laws from Zipf's law are obtained through various methods of fluctuation analysis, but the question of how to quantify the fluctuation existing in language data has been controversial. Our work is situated as one such case of fluctuation analysis.</p><p>In real data, the occurrence timing of a particu- lar event is often biased in a bursty, clustered man- ner, and fluctuation analysis quantifies the degree of this bias. Originally, this was motivated by a study of how floods of the Nile River occur in clusters (i.e., many floods coming after an initial flood) <ref type="bibr" target="#b16">(Hurst, 1951)</ref>. Such clustering phenomena have been widely reported in both natural and so- cial domains <ref type="bibr" target="#b12">(Eisler, Bartos, and Kertész, 2007)</ref>.</p><p>Fluctuation analysis for language originates in <ref type="bibr" target="#b11">(Ebeling and Pöeschel, 1994)</ref>, which applied the approach to characters. That work corresponds to observing the average of the variances of each character's number of occurrences within a time span. Their method is strongly related to ours but different from two viewpoints: (1) Taylor analysis considers the variance with respect to the mean, rather than time; and (2) Taylor analysis does not average results over all elements. Because of these differences, the method in <ref type="bibr" target="#b11">(Ebeling and Pöeschel, 1994)</ref> cannot distinguish real texts from an i.i.d. process when applied to word sequences <ref type="bibr" target="#b27">(Takahashi and Tanaka-Ishii, 2018)</ref>.</p><p>Event clustering phenomena cause a sequence to resemble itself in a self-similar manner. There- fore, studies of the fluctuation underlying a se- quence can take another form of long-range corre- lation analysis, to consider the similarity between two subsequences underlying a time series. This approach requires a function to calculate the sim- ilarity of two sequences, and the autocorrelation function (ACF) is the main function considered. Since the ACF only applies to numerical data, both <ref type="bibr" target="#b0">Altmann, Pierrehumbert, and Motter (2009)</ref> and <ref type="bibr" target="#b30">Tanaka-Ishii and Bunde (2016)</ref> applied long-range correlation analysis by transforming text into in- tervals and showed how natural language texts are long-range correlated. Another recent work ( <ref type="bibr" target="#b18">Lin and Tegmark, 2016)</ref> proposed using mutual infor- mation instead of the ACF. Mutual information, however, cannot detect the long-range correlation underlying texts. All these works studied correla- tion phenomena via only a few texts and did not show any underlying universality with respect to data and language types. One reason is that anal- ysis methods for long-range correlation are non- trivial to apply to texts.</p><p>Overall, the analysis based on Taylor's law in the present work belongs to the former approach of fluctuation analysis and shows the law's vast ap- plicability and stability for written texts and even beyond, quantifying universal complexity under- lying human linguistic sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Measuring the Taylor Exponent</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proposed method</head><p>Given a set of elements W (words), let X = X 1 , X 2 , . . . , X N be a discrete time series of length N , where X i ∈ W for all i = 1, 2, . . . , N , i.e., each X i represents a word. For a given segment length ∆t ∈ N (a positive integer), a data sample X is segmented by the length ∆t. The number of occurrences of a specific word w k ∈ W is counted for every segment, and the mean µ k and standard deviation σ k across segments are obtained. Doing this for all word kinds w 1 , . . . , w |W | ∈ W gives the distribution of σ with respect to µ. Following a previous work <ref type="bibr" target="#b12">(Eisler, Bartos, and Kertész, 2007)</ref>, in this article Taylor's law is defined to hold when µ and σ are correlated by a power law in the fol- lowing way:</p><formula xml:id="formula_0">σ ∝ µ α .<label>(1)</label></formula><p>Experimentally, the Taylor exponent α is known to take a value within the range of 0.5 ≤ α ≤ 1.0 across a wide variety of domains as reported in <ref type="bibr" target="#b12">(Eisler, Bartos, and Kertész, 2007)</ref>, includ- ing finance, meteorology, agriculture, and biol- ogy. Mathematically, it is analytically proven that α = 0.5 for an i.i.d process, and the proof is in- cluded as Supplementary Material. On the other hand, α = 1.0 when all segments always contain the same proportion of the ele- ments of W . For example, suppose that W = {a, b}. If b always occurs twice as often as a in all segments (e.g., three a and six b in one segment, two a and four b in another, etc.), then both the mean and standard deviation for b are twice those for a, so the exponent is 1.0.</p><p>In a real text, this cannot occur for all W , so α &lt; 1.0 for natural language text. Never- theless, for a subset of words in W , this could happen, especially for a template-like sequence. For instance, consider a programming statement: while (i &lt; 1000) do i-. Here, the words while and do always occur once, whereas i al- ways occurs twice. This example shows that the exponent indicates how consistently words depend on each other in W , i.e., how words co-occur sys- tematically in a coherent manner, thus indicating that the Taylor exponent is partly related to gram- maticality.</p><p>To measure the Taylor exponent α, the mean and standard deviation are computed for every word kind 1 and then plotted in log-log coordi- nates. The number of points in this work was the number of different words. We fitted the points to a linear function in log-log coordinates by the least-squares method. We naturally took the loga- rithm of both cµ α and σ to estimate the exponent, because Taylor's law is a power law. The coeffi- cientˆccientˆ cientˆc, and exponentˆαexponentˆ exponentˆα are then estimated as the <ref type="bibr">1</ref> In this work, words are not lemmatized, e.g. "say," "said," and "says" are all considered different words. This was chosen so in this work because the Taylor exponent considers systematic co-occurrence of words, and idiomatic phrases should thus be considered in their original forms. following:</p><formula xml:id="formula_1">ˆ c, ˆ α = arg min c,α ϵ(c, α), ϵ(c, α) = 1 |W | |W | ∑ k=1 (log σ k − log cµ α k ) 2 .</formula><p>This fit function could be a problem depending on the distribution of errors between the data points and the regression line. As seen later, the er- ror distribution seems to differ with the kind of data: for a random source the error seems Gaus- sian, and so the above formula is relevant, whereas for real data, the distribution is biased. Chang- ing the fit function according to the data source, however, would cause other essential problems for fair comparison. Here, because <ref type="bibr" target="#b10">Cohen and Xu (2015)</ref> reported that most empirical works on Tay- lor's law used least-squares regression (including their own), this work also uses the above scheme 2 , with the error defined as ϵ(ˆ c, ˆ α).  <ref type="bibr">2</ref> The code for estimating the exponent is available from https://github.com/Group-TanakaIshii/ word_taylor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data</head><p>3 All texts above a size threshold (1 megabyte) were ex- tracted from the two archives, resulting in 1142 texts. <ref type="bibr">4</ref> Child Language Data Exchange System <ref type="bibr" target="#b19">(MacWhinney, 2000;</ref><ref type="bibr" target="#b6">Bol, 1995;</ref><ref type="bibr" target="#b17">Lieven, Salomo, and Tomasello, 2009;</ref><ref type="bibr" target="#b23">Rondal, 1985;</ref><ref type="bibr" target="#b3">Behrens, 2006;</ref><ref type="bibr" target="#b14">Gil and Tadmor, 2007;</ref><ref type="bibr">OshimaTakane et al., 1995;</ref><ref type="bibr" target="#b25">Smoczynska, 1985;</ref><ref type="bibr" target="#b1">An ¯ delkovi´cdelkovi´c, Ševa, and Moskovljevi´cMoskovljevi´c, 2001;</ref><ref type="bibr" target="#b4">Benedet et al., 2004;</ref><ref type="bibr" target="#b22">Plunkett and Strömqvist, 1992)</ref>  large representative archives, parsed, and stripped of natural language comments), and 12 pieces of musical data (long symphonies and so forth, trans- formed from MIDI into text with the software SMF2MML 5 , with annotations removed).</p><p>As for the randomized data listed in the last block, we took the text of Moby Dick and gen- erated 10 different shuffled samples and bigram- generated sequences. We also introduced LSTM- generated texts to consider the utility of our find- ings, as explained in §5. and (b)) and two multiple-author texts (newspa- pers, (c) and (d)), in English and Chinese, respec- tively. The segment size was ∆t = 5620 words 6 , i.e., each segment had 5620 words and the hori- zontal axis indicates the averaged frequency of a specific word within a segment of 5620 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Taylor Exponents for Real Data</head><p>The points at the upper right represent the most frequent words, whereas those at the lower left represent the least frequent. Although the plots exhibited different distributions, they could glob- ally be considered roughly aligned in a power-law manner. This finding is non-trivial, as seen in other analyses based on Taylor's law <ref type="bibr" target="#b12">(Eisler, Bartos, and Kertész, 2007</ref>). The exponent α was almost the same even though English and Chinese are differ- ent languages using different kinds of script. As explained in §3.1, the Taylor exponent in- dicates the degree of consistent co-occurrence among words. The value of 0.58 obtained here suggests that the words of natural language texts are not strongly or consistently coherent with re- spect to each other. Nevertheless, the value is well above 0.5, and for the real data listed in <ref type="table" target="#tab_1">Table 1</ref> (first to third blocks), not a single sample gave an exponent as low as 0.5.</p><p>Although the overall global tendencies in Fig- ure 1 followed power laws, many points deviated significantly from the regression lines. The words with the greatest fluctuation were often keywords. For example, among words in Moby Dick with large µ, those with the largest σ included whale, captain, and sailor, whereas those with the small- est σ included functional words such as to, that, and with.</p><p>The Taylor exponent depended only slightly on the data size. <ref type="figure" target="#fig_2">Figure 2</ref> shows this dependency To evaluate the exponent's dependence on the text size, parts of each text were taken and the exponents were calculated for those parts, with points taken log- arithmically. The window size was ∆t = 5620. As the text size grew, the Taylor exponent slightly decreased.</p><p>for the two largest data sets used, The New York Times (NYT, 1.5 billion words) and The Mainichi (24 years) newspapers. When the data size was in- creased, the exponent exhibited a slight tendency to decrease. For the NYT, the decrease seemed to have a lower limit, as the figure shows that the ex- ponent stabilized at around 10 7 words.</p><p>The reason for this decrease can be explained as follows. The Taylor exponent becomes larger when some words occur in a clustered manner. Making the text size larger increases the number of segments (since ∆t was fixed in this experiment). If the number of clusters does not increase as fast as the increase in the number of segments, then the number of clusters per segment becomes smaller, leading to a smaller exponent. In other words, the influence of each consecutive co-occurrence of a particular word decays slightly as the overall text size grows.</p><p>Analysis of different kinds of data showed how the Taylor exponent differed according to the data source. <ref type="figure" target="#fig_3">Figure 3</ref> shows plots for samples from enwiki8 (tagged Wikipedia), the child-directed speech of Thomas (taken from CHILDES), pro- gramming language data sets, and music. The dis- tributions appear different from those for the natu- ral language texts, and the exponents were signifi- cantly larger. This means that these data sets con- tained expressions with fixed forms much more frequently than did the natural language texts.   <ref type="table" target="#tab_1">Table 1</ref>. The first two boxes show results with an exponent of 0.50. These results were each obtained from 10 random samples of the randomized sequences. We will re- turn to these results in the next section.</p><p>The remaining boxes show results for real data. The exponents for texts from Project Gutenberg ranged from 0.53 to 0.68. <ref type="figure" target="#fig_6">Figure 5</ref> shows a his- togram of these texts with respect to the value ofˆα ofˆ ofˆα. The number of texts decreased significantly at a value of 0.63, showing that the distribution of the Taylor exponent was rather tight. The kinds of texts at the upper limit of exponents for Project Gutenberg included structured texts of fixed style, such as dictionaries, lists of histories, and Bibles.</p><p>The majority of texts were in English, followed by French and then other languages, as listed in <ref type="table" target="#tab_1">Table 1</ref>. Whether α distinguishes languages is a difficult question. The histogram suggests that Chinese texts exhibited larger values than did texts in Indo-European languages. We conducted a statistical test to evaluate whether this difference was significant as compared to English. Since the numbers of texts were very different, we used the non-parametric statistical test of the Brunner- Munzel method, among various possible methods, to test a null hypothesis of whether α was equal for the two distributions <ref type="bibr" target="#b7">(Brunner and Munzel, 2000</ref>). The p-value for Chinese was p = 1.24 × 10 −16 , thus rejecting the null hypothesis at the signifi- cance level of 0.01. This confirms that α was generally larger for Chinese texts than for En- glish texts. Similarly, the null hypothesis was re- jected for Finnish and French, but it was accepted for German and Japanese at the 0.01 significance level. Since Japanese was accepted despite its large difference from English, we could not con- clude whether the Taylor exponent distinguishes languages.</p><p>Turning to the last four columns of <ref type="figure" target="#fig_4">Figure 4</ref>, representing the enwiki8, child-directed speech (CHILDES), programming language, and music data, the Taylor exponents clearly differed from those of the natural language texts. Given the template-like nature of these four data sources, the results were somewhat expected. The kind of data thus might be distinguishable using the Tay- lor exponent. To confirm this, however, would re- quire assembling a larger data set. Applying this approach with Twitter data and adult utterances would produce interesting results and remains for our future work.</p><p>The Taylor exponent also differed according to ∆t, and <ref type="figure">Figure 6</ref> shows the dependence ofˆαofˆ ofˆα on ∆t. For each kind of data shown in <ref type="figure" target="#fig_4">Figure 4</ref>, the mean exponent is plotted for various ∆t. As reported in <ref type="bibr" target="#b12">(Eisler, Bartos, and Kertész, 2007)</ref>, the exponent is known to grow when the segment size gets larger. The reason is that words occur in a bursty, clustered manner at all length scales: no matter how large the segment size becomes, a segment will include either many or few in- stances of a given word, leading to larger variance growth. This phenomenon suggests how word co- occurrences in natural language are self-similar. The Taylor exponent is initially 0.5 when the seg- ment size is very small. This can be analytically explained as follows <ref type="bibr" target="#b12">(Eisler, Bartos, and Kertész, 2007)</ref>. Consider the case of ∆t=1. Let n be the frequency of a particular word in a segment. We have ⟨n⟩ ≪ 1.0, because the possibility of a spe- cific word appearing in a segment becomes very small. Because ⟨n⟩ 2 ≈ 0, σ 2 = ⟨n 2 ⟩ − ⟨n⟩ 2 ≈ ⟨n 2 ⟩. Because n = 1 or 0 (with ∆t=1), ⟨n 2 ⟩ = ⟨n⟩ = µ. Thus, σ 2 ≈ µ.</p><p>Overall, the results show the possibility of ap-  Each bar shows the number of texts with that value ofˆαofˆ ofˆα. Because of the skew of languages in the orig- inal conception of Project Gutenberg, the majority of the texts are in English, shown in blue, whereas texts in other languages are shown in other col- ors. The histogram shows how the Taylor expo- nent ranged fairly tightly around the mean, and natural language texts with an exponent larger than 0.63 were rare. plying Taylor's exponent to quantify the complex- ity underlying coherence among words. Grammat- ical complexity was formalized by Chomsky via the Chomsky hierarchy <ref type="bibr" target="#b9">(Chomsky, 1956</ref>), which describes grammar via rewriting rules. The con- straints placed on the rules distinguish four dif- ferent levels of grammar: regular, context-free, context-sensitive, and phrase structure. As indi- cated in <ref type="bibr" target="#b2">(Badii and Politi, 1997</ref>), however, this does not quantify the complexity on a continuous scale. For example, we might want to quantify the complexity of child-directed speech as com- pared to that of adults, and this could be addressed in only a limited way through the Chomsky hi- erarchy. Another point is that the hierarchy is sentence-based and does not consider fluctuation in the kinds of words appearing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation of Machine-Generated Text by the Taylor Exponent</head><p>The main contribution of this paper is the findings of Taylor's law behavior for real texts as presented thus far. This section explains the applicability of these findings, through results obtained with base- line language models.</p><p>As mentioned previously, i.i.d. mathematical processes have a Taylor exponent of 0.50. We show here that, even if a process is not trivially i.i.d., the exponent often takes a value of 0.50 <ref type="figure">Figure 6</ref>: Growth ofˆαofˆ ofˆα with respect to ∆t, aver- aged across data sets within each data kind. The plot labeled "random" shows the average for the two datasets of randomized text from Moby Dick (shuffled and bigrams, as explained in §5). Since this analysis required a large amount of compu- tation, for the large data sets (such as newspa- per and programming language data), 4 million words were taken from each kind of data and used here. When ∆t was small, the Taylor exponent was close to 0.5, as theoretically described in the main text. As ∆t was increased, the value ofˆαofˆ ofˆα grew. The maximum ∆t was about 10,000, or about one-tenth of the length of one long literary text. For the kinds of data investigated here, ˆ α grew almost linearly. The results show that, at a given ∆t, the Taylor exponent has some capabil- ity to distinguish different kinds of text data. for random processes, including texts produced by standard language models such as n-gram based models. A more complete work in this direction is reported in <ref type="bibr" target="#b27">(Takahashi and Tanaka-Ishii, 2018</ref>). <ref type="figure" target="#fig_7">Figure 7</ref> shows samples from each of two sim- ple random processes. <ref type="figure" target="#fig_7">Figure 7a</ref> shows the behav- ior of a shuffled text of Moby Dick. Obviously, Given that the Taylor exponent becomes larger for a sequence with words dependent on each other, as explained in §3, we would expect that a se- quence generated by an n-gram model would ex- hibit an exponent larger than 0.50. The simplest such model is the bigram model, so a sequence of 300,000 words was probabilistically generated using a bigram model of Moby Dick. <ref type="figure" target="#fig_7">Figure 7b</ref> shows the Taylor analysis, revealing that the expo- nent remained 0.50.</p><p>This result does not depend much on the qual- ity of the individual samples. The first and second box plots in <ref type="figure" target="#fig_4">Figure 4</ref> show the distribution of ex- ponents for 10 different samples for the shuffled and bigram-generated texts, respectively. The ex- ponents were all around 0.50, with small variance.</p><p>State-of-the-art language models are based on neural models, and they are mainly evaluated by perplexity and in terms of the performance of in- dividual applications. Since their architecture is complex, quality evaluation has become an is- sue. One possible improvement would be to use an evaluation method that qualitatively dif- fers from judging application performance. One such method is to verify whether the properties un- derlying natural language hold for texts generated by language models. The Taylor exponent is one such possibility, among various properties of nat- ural language texts.</p><p>As a step toward this approach, <ref type="figure" target="#fig_8">Figure 8</ref> shows two results produced by neural language mod- els. <ref type="figure" target="#fig_8">Figure 8a</ref> shows the result for a sam- ple of 2 million characters produced by a stan-dard (three-layer) stacked character-based LSTM unit that learned the complete works of Shake- speare. The model was optimized to minimize the cross-entropy with a stochastic gradient algo- rithm to predict the next character from the previ- ous 128 characters. See <ref type="bibr">(Takahashi and TanakaIshii, 2017)</ref> for the details of the experimental set- tings. The Taylor exponent of the generated text was 0.50. This indicates that the character-level language model could not capture or reproduce the word-level clustering behavior in text. This analysis sheds light on the quality of the language model, separate from the prediction accuracy.</p><p>The application of Taylor's law for a wider range of language models appears in <ref type="bibr" target="#b27">(Takahashi and Tanaka-Ishii, 2018)</ref>. Briefly, state-of-the- art word-level language models can generate text whose Taylor exponent is larger than 0.50 but smaller than that of the dataset used for train- ing. This indicates both the capability of modeling burstiness in text and the room for improvement. Also, the perplexity values correlate well with the Taylor exponents. Therefore, Taylor expo- nent can reasonably serve for evaluating machine- generated text.</p><p>In contrast to character-level neural language models, neural-network-based machine transla- tion (NMT) models are, in fact, capable of main- taining the burstiness of the original text. <ref type="figure" target="#fig_8">Fig- ure 8b</ref> shows the Taylor analysis for a machine- translated text of Les Misérables (from French to English), obtained from Google NMT ( <ref type="bibr" target="#b32">Wu et al., 2016)</ref>. We split the text into 5000-character por- tions because of the API's limitation (See <ref type="bibr" target="#b26">(Takahashi and Tanaka-Ishii, 2017</ref>) for the details). As is expected and desirable, the translated text re- tains the clustering behavior of the original text, as the Taylor exponent of 0.57 is equivalent to that of the original text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have proposed a method to analyze whether a natural language text follows Taylor's law, a scal- ing property quantifying the degree of consistent co-occurrence among words. In our method, a se- quence of words is divided into given segments, and the mean and standard deviation of the fre- quency of every kind of word are measured. The law is considered to hold when the standard devi- ation varies with the mean according to a power law, thus giving the Taylor exponent.</p><p>Theoretically, an i.i.d. process has a Taylor exponent of 0.5, whereas larger exponents indi- cate sequences in which words co-occur systemat- ically. Using over 1100 texts across 14 languages, we showed that written natural language texts fol- low Taylor's law, with the exponent distributed around 0.58. This value differed greatly from the exponents for other data sources: enwiki8 (tagged Wikipedia, 0.63), child-directed speech (CHILDES, around 0.68), and programming lan- guage and music data (around 0.79). These Taylor exponents imply that a written text is more com- plex than programming source code or music with regard to fluctuation of its components. None of the real data exhibited an exponent equal to 0.5. We conducted more detailed analysis varying the data size and the segment size.</p><p>Taylor's law and its exponent can also be ap- plied to evaluate machine-generated text. We showed that a character-based LSTM language model generated text with a Taylor exponent of 0.5. This indicates one limitation of that model.</p><p>Our future work will include an analysis using other kinds of data, such as Twitter data and adult utterances, and a study of how Taylor's law re- lates to grammatical complexity for different se- quences. Another direction will be to apply fluc- tuation analysis in formulating a statistical test to evaluate the structural complexity underlying a se- quence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 shows</head><label>1</label><figDesc>Figure 1 shows typical distributions for natural language texts, with two single-author texts ((a) 5 http://shaw.la.coocan.jp/smf2mml/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of Taylor's law for natural language texts. Moby Dick and Hong Lou Meng are representative of single-author texts, and the two newspapers are representative of multipleauthor texts, in English and Chinese, respectively. Each point represents a kind of word. The values of σ and µ for each word kind are plotted across texts within segments of size ∆t = 5620. The Taylor exponents obtained by the least-squares method were all around 0.58.</figDesc><graphic url="image-3.png" coords="5,76.37,157.48,104.77,77.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Taylor exponentˆαexponentˆ exponentˆα (vertical axis) calculated for the two largest texts: The New York Times and The Mainichi newspapers. To evaluate the exponent's dependence on the text size, parts of each text were taken and the exponents were calculated for those parts, with points taken logarithmically. The window size was ∆t = 5620. As the text size grew, the Taylor exponent slightly decreased.</figDesc><graphic url="image-5.png" coords="5,307.28,62.81,218.27,144.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of Taylor's law for alternative data sets listed in Table 1: enwiki8 (tag-annotated Wikipedia), Thomas (longest in CHILDES), Lisp source code, and the music of Bach's St Matthew Passion. These examples exhibited larger Taylor exponents than did typical natural language texts.</figDesc><graphic url="image-8.png" coords="6,76.37,167.45,104.77,77.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 summarizes</head><label>4</label><figDesc>Figure 4 summarizes the overall picture among the different data sources. The median and quantiles of the Taylor exponent were calculated for the different kinds of data listed in Table 1. The first two boxes show results with an exponent of 0.50. These results were each obtained from 10 random samples of the randomized sequences. We will return to these results in the next section. The remaining boxes show results for real data. The exponents for texts from Project Gutenberg ranged from 0.53 to 0.68. Figure 5 shows a histogram of these texts with respect to the value ofˆα ofˆ ofˆα. The number of texts decreased significantly at a value of 0.63, showing that the distribution of the Taylor exponent was rather tight. The kinds of texts at the upper limit of exponents for Project Gutenberg included structured texts of fixed style, such as dictionaries, lists of histories, and Bibles. The majority of texts were in English, followed by French and then other languages, as listed in Table 1. Whether α distinguishes languages is a difficult question. The histogram suggests that Chinese texts exhibited larger values than did texts in Indo-European languages. We conducted a statistical test to evaluate whether this difference was significant as compared to English. Since the numbers of texts were very different, we used the non-parametric statistical test of the Brunner</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Box plots of the Taylor exponents for different kinds of data. Each point represents one sample, and samples from the same kind of data are contained in each box plot. The first two boxes are for the randomized data, while the remaining boxes are for real data, including both the natural language texts and language-related sequences. Each box ranges between the quantiles, with the middle line indicating the median, the whiskers showing the maximum and minimum, and some extreme values lying beyond.</figDesc><graphic url="image-10.png" coords="7,117.35,62.81,362.84,200.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Histogram of Taylor exponents for long texts in Project Gutenberg (1129 texts). The legend indicates the languages, in frequency order. Each bar shows the number of texts with that value ofˆαofˆ ofˆα. Because of the skew of languages in the original conception of Project Gutenberg, the majority of the texts are in English, shown in blue, whereas texts in other languages are shown in other colors. The histogram shows how the Taylor exponent ranged fairly tightly around the mean, and natural language texts with an exponent larger than 0.63 were rare.</figDesc><graphic url="image-11.png" coords="7,77.46,360.60,207.35,207.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Taylor analysis of a shuffled text of Moby Dick and a randomized text generated by a bigram model. Both exhibited an exponent of 0.50.</figDesc><graphic url="image-13.png" coords="8,76.37,491.53,104.77,77.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Taylor analysis for two texts produced by standard neural language models: (a) a stacked LSTM model that learned the complete works of Shakespeare; and (b) a machine translation of Les Misérables (originally in French, translated into English), from a neural language model.</figDesc><graphic url="image-12.png" coords="8,72.00,62.91,218.27,161.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 lists</head><label>1</label><figDesc></figDesc><table>all the data used for this article. The 
data consisted of natural language texts, language-
related sequences, and randomized data, listed as 
different blocks in the table. The natural lan-
guage texts consisted of 1142 single-author long 
texts (first block, extracted from Project Guten-
berg and Aozora Bunko across 14 languages 3 , 
with the second block listing individual sam-
ples taken from Project Gutenberg together with 
the complete works of Shakespeare), and news-
papers (third block, from the Gigaword corpus, 
available from the Linguistic Data Consortium 
in English, Chinese, and other major languages). 
Other sequences appear in the fourth block: the 
enwiki8 100-MB dump dataset (consisting of 
tag-annotated text from English Wikipedia), the 
10 longest child-directed speech utterances in 
CHILDES data 4 (preprocessed by extracting only 
children's utterances), four program sources (in 
Lisp, Haskell, C++, and Python, crawled from 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 : Data we used in this article. For each dataset, length is the number of words, vocabulary is the number of different words. For detail of the data kind, see §3.2.</head><label>1</label><figDesc></figDesc><table>Texts 
Languagê 

Languagê 
α 

Number 
Length 
Vocabulary 

mean 
of samples 
Mean 
Min 
Max 
Mean 
Min 
Max 

English 
0.58 
910 
313127.4 
185939 
2488933 
17237.7 
7321 
69812 

French 
0.57 
66 
197519.3 
169415 
1528177 
22098.3 
14106 
57193 

Finnish 
0.55 
33 
197519.3 
149488 
396920 
33597.1 
26275 
47263 

Chinese 
0.61 
32 
629916.8 
315099 
4145117 
15352.9 
9153 
60950 

Dutch 
0.57 
27 
256859.2 
198924 
435683 
19159.1 
13880 
31595 

German 
0.59 
20 
236175.0 
184321 
331322 
24242.3 
11079 
37228 

Gutenberg 
Italian 
0.57 
14 
266809.0 
196961 
369326 
29103.5 
18641 
45032 

Spanish 
0.58 
12 
363837.2 
219787 
903051 
26111.1 
18111 
36507 

Greek 
0.58 
10 
159969.2 
119196 
243953 
22805.7 
15877 
31386 

Latin 
0.57 
2 
505743.5 
205228 
806259 
59667.5 
28739 
90596 

Portuguese 
0.56 
1 
261382.0 
261382 
261382 
24719.0 
24719 
24719 

Hungarian 
0.57 
1 
198303.0 
198303 
198303 
38384.0 
38384 
38384 

Tagalog 
0.59 
1 
208455.0 
208455 
208455 
26335.0 
26335 
26335 

Aozora 
Japanese 
0.59 
13 
616677.2 
105343 
2951320 
19760.0 
6620 
49100 

Moby Dick 
English 
0.58 
1 
254655.0 
254655 
254655 
20473.0 
20473 
20473 

Hong Lou Meng 
Chinese 
0.59 
1 
701256.0 
701256 
701256 
18451.0 
18451 
18451 

Les Miserables 
French 
0.57 
1 
691407.0 
690417 
690417 
31956.0 
31956 
31956 

Shakespeare (All) 
English 
0.59 
1 
1000238.0 
1000238 
1000238 
40840.0 
40840 
40840 

WSJ 
English 
0.56 
1 
22679513.0 
22679513 
22679513 
137467.0 
137467 
137467 

NYT 
English 
0.58 
1 
1528137194.0 
1528137194 
1528137194 
3155495.0 
3155495 
3155495 

People's Daily 
Chinese 
0.58 
1 
19420853.0 
19420853 
19420853 
172140.0 
172140 
172140 

Mainichi 
Japanese 
0.56 
24 (yrs) 
31321594.3 
24483331 
40270706 
145534.5 
127290 
169270 

enwiki8 
tag-annotated 
0.63 
1 
14647848.0 
14647848 
14647848 
1430791.0 
1430791 
1430791 

CHILDES 
various 
0.68 
10 
193434.0 
48952 
448772 
9908.0 
5619 
17893 

Programs 
-
0.79 
4 
34161018.8 
3697199 
68622162 
838907.8 
127653 
1545127 

Music 
-
0.79 
12 
135993.4 
76629 
215480 
9187.9 
907 
27043 

Moby Dick (shuffled) 
-
0.50 
10 
254655.0 
254655 
254655 
20473.0 
20473 
20473 

Moby Dick (bigram) 
-
0.50 
10 
300001.0 
300001 
300001 
16963.8 
16893 
17056 

3-layer stacked LSTM 
(English) 
0.50 
1 
256425.0 
256425 
256425 
50115.0 
50115 
50115 
(character-based) 

Neural MT 
(English) 
0.57 
1 
623235.0 
623235 
623235 
27370.0 
27370 
27370 

</table></figure>

			<note place="foot" n="6"> In comparison, Figure 6 shows the effect on the exponent of varying ∆t. As seen in that figure, larger ∆t increased the differences in exponent among different data sets, making the differences more distinguishable. Thus, ∆t had better be as large as possible while keeping µ and σ computable. For this article, we chose ∆t = 5620, which was one of the ∆t values used in Figure 6.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by JST Presto Grant Number JPMJPR14E5 and HITE funding. We thank Shuntaro Takahashi for offering his com-ments and providing the machine-generated data reported in §5.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Beyond word frequency: Bursts, lulls, and scaling in the temporal distributions of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">G</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">B</forename><surname>Pierrehumbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adilson</forename><forename type="middle">E</forename><surname>Motter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Serbian Corpus of Early Child Language. Laboratory for Experimental Psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>An ¯ Delkovi´cdelkovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nada</forename><surname>Darinka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmina</forename><surname>Ševa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moskovljevi´cmoskovljevi´c</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>Faculty of Philosophy, and Department of General Linguistics, Faculty of Philology, University of Belgrade</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Complexity: Hierarchical structures and scaling in physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remo</forename><surname>Badii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Politi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The input-output relationship in first language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Behrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2" to="24" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Benedet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celis</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Snow</surname></persName>
		</author>
		<title level="m">Spanish BecaCESNo Corpus. TalkBank</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The entropy of words-learnability and edxpressibvity across more than 1000 langauges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Alikaniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cysouw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Ferrer I Cancho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Implicational scaling in child language acquisition: the order of production of Dutch verb constructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">W</forename><surname>Bol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amsterdam Series in Child Language Development, chapter</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note>Amsterdam: Institute for General Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The nonparametric behrens-fisher problem: Asymtotic theory and a small-sample approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullrich</forename><surname>Munzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrical Journal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="17" to="25" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudy</forename><surname>Calif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><forename type="middle">G</forename><surname>Schmitt</surname></persName>
		</author>
		<title level="m">Taylor law in wind energy data. Resources</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="787" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Three models for the description of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="113" to="124" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random sampling of skewed distributions implies taylor&apos;s power law of fluctuation scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="7749" to="7754" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Entropy and long-range correlations in literary english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Ebeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Pöeschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Europhys. Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="241" to="246" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fluctuation scaling in complex systems: Taylor&apos;s law and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltán</forename><surname>Eisler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imre</forename><surname>Bartos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">János</forename><surname>Kertész</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Physics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="89" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling laws and fluctuations in the statistics of word frequencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Gerlach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">G</forename><surname>Altmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">113010</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The MPI-EVA Jakarta Child Language Database. A joint project of the Department of Linguistics, Max Planck Institute for Evolutionary Anthropology and the Center for Language and Culture Studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Tadmor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Atma Jaya Catholic University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Information Retrieval: Computational and Theoretical Aspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">S</forename><surname>Heaps</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Academic Press, Inc</publisher>
			<pubPlace>Orlando, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Long-term storage capacity of reservoirs. Transactions of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">E</forename><surname>Hurst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<publisher>American Society of Civil Engineers</publisher>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="770" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two-year-old children&apos;s production of multiword utterances: A usage-based analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Lieven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorothé</forename><surname>Salomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tomasello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="481" to="508" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Critical behavior from deep dynamics: A hidden dimension in natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tegmark</surname></persName>
		</author>
		<idno>abs/1606.06737</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Childes Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Macwhinney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Psychology Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long-range fractal correlations in literary corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">A</forename><surname>Montemurro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fractals</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="451" to="461" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuriko</forename><surname>Oshima-Takane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Macwhinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetosi</forename><surname>Sirai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Miyata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norio</forename><surname>Naka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>CHILDES manual for Japanese. Montreal: McGill University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The crosslinguistic study of language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Plunkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Strömqvist</surname></persName>
		</author>
		<editor>D. I. Slobin</editor>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Lawrence Erlbaum Associates</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="457" to="556" />
		</imprint>
	</monogr>
	<note>The acquisition of scandinavian languages</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adult-child interaction and the process of language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">A</forename><surname>Rondal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Praeger Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An empirical law describing hetero-geneity in the yields of agricultural crops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fairfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Agriculture Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The crosslinguistic study of language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Smoczynska</surname></persName>
		</author>
		<editor>D. I. Slobin</editor>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Lawrence Erlbaum Associates</publisher>
			<biblScope unit="page" from="595" to="686" />
		</imprint>
	</monogr>
	<note>The acquisition of polish</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Do neural nets learn statistical laws behind natural langauge? PLoS One</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuntaro</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Assesing language models with scaling properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuntaro</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08881</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Entropy rate estimates for natural language : A new extrapolation of compressed large-scale corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Takahira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debowski</forename><surname>Lukasz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Aihara</surname></persName>
		</author>
		<title level="m">Text constancy measures. Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="481" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Longrange memory in literary texts: On the universal clustering of the rare words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Bunde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS One. Online journal</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aggregation, variance and the mean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">732</biblScope>
			<biblScope unit="page" from="189" to="190" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<title level="m">Google s neural machine translation system: Bridging the gap between human and machine translation. arXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Human behavior and the principle of least effort: An introduction to human ecology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">K</forename><surname>Zipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
			<publisher>Hafner</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
