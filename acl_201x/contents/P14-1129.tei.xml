<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Robust Neural Network Joint Models for Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>10 Moulton St</addrLine>
									<postCode>02138</postCode>
									<settlement>Raytheon, Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>10 Moulton St</addrLine>
									<postCode>02138</postCode>
									<settlement>Raytheon, Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>10 Moulton St</addrLine>
									<postCode>02138</postCode>
									<settlement>Raytheon, Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>10 Moulton St</addrLine>
									<postCode>02138</postCode>
									<settlement>Raytheon, Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>10 Moulton St</addrLine>
									<postCode>02138</postCode>
									<settlement>Raytheon, Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>10 Moulton St</addrLine>
									<postCode>02138</postCode>
									<settlement>Raytheon, Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Robust Neural Network Joint Models for Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1370" to="1380"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
					<note>The views, opin-ions, and/or findings contained in this article are those of the author and should not be interpreted as representing the of-ficial views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Depart-ment of Defense.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent work has shown success in using neural network language models (NNLMs) as features in MT systems. Here, we present a novel formulation for a neural network joint model (NNJM), which augments the NNLM with a source context window. Our model is purely lexi-calized and can be integrated into any MT decoder. We also present several variations of the NNJM which provide significant additive improvements. Although the model is quite simple, it yields strong empirical results. On the NIST OpenMT12 Arabic-English condition , the NNJM features produce a gain of +3.0 BLEU on top of a powerful, feature-rich baseline which already includes a target-only NNLM. The NNJM features also produce a gain of +6.3 BLEU on top of a simpler baseline equivalent to Chi-ang&apos;s (2007) original Hiero implementation. Additionally, we describe two novel techniques for overcoming the historically high cost of using NNLM-style models in MT decoding. These techniques speed up NNJM computation by a factor of 10,000x, making the model as fast as a standard back-off LM.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, neural network models have be- come increasingly popular in NLP. Initially, these models were primarily used to create n-gram neu- ral network language models (NNLMs) for speech recognition and machine translation ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b22">Schwenk, 2010)</ref>. They have since been ex- tended to translation modeling, parsing, and many other NLP tasks.</p><p>In this paper we use a basic neural network ar- chitecture and a lexicalized probability model to create a powerful MT decoding feature. Specifi- cally, we introduce a novel formulation for a neu- ral network joint model (NNJM), which augments an n-gram target language model with an m-word source window. Unlike previous approaches to joint modeling ( <ref type="bibr" target="#b12">Le et al., 2012</ref>), our feature can be easily integrated into any statistical machine trans- lation (SMT) decoder, which leads to substantially larger improvements than k-best rescoring only. Additionally, we present several variations of this model which provide significant additive BLEU gains.</p><p>We also present a novel technique for training the neural network to be self-normalized, which avoids the costly step of posteriorizing over the entire vocabulary in decoding. When used in con- junction with a pre-computed hidden layer, these techniques speed up NNJM computation by a fac- tor of 10,000x, with only a small reduction on MT accuracy.</p><p>Although our model is quite simple, we obtain strong empirical results. We show primary results on the NIST OpenMT12 Arabic-English condi- tion. The NNJM features produce an improvement of +3.0 BLEU on top of a baseline that is already better than the 1st place MT12 result and includes a powerful NNLM. Additionally, on top of a sim- pler decoder equivalent to <ref type="bibr" target="#b4">Chiang's (2007)</ref> origi- nal Hiero implementation, our NNJM features are able to produce an improvement of +6.3 BLEU - as much as all of the other features in our strong baseline system combined.</p><p>We also show strong improvements on the NIST OpenMT12 Chinese-English task, as well as the DARPA BOLT (Broad Operational Language Translation) Arabic-English and Chinese-English conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Network Joint Model (NNJM)</head><p>Formally, our model approximates the probability of target hypothesis T conditioned on source sen- tence S. We follow the standard n-gram LM de- composition of the target, where each target word t i is conditioned on the previous n − 1 target words. To make this a joint model, we also condi- tion on source context vector S i :</p><formula xml:id="formula_0">P (T |S) ≈ Π |T | i=1 P (t i |t i−1 , · · · , t i−n+1 , S i )</formula><p>Intuitively, we want to define S i as the window that is most relevant to t i . To do this, we first say that each target word t i is affiliated with exactly one source word at index a i . S i is then the m-word source window centered at a i :</p><formula xml:id="formula_1">S i = s a i − m−1 2 , · · · , s a i , · · · , s a i + m−1 2</formula><p>This notion of affiliation is derived from the word alignment, but unlike word alignment, each target word must be affiliated with exactly one non-NULL source word. The affiliation heuristic is very simple:</p><p>(1) If t i aligns to exactly one source word, a i is the index of the word it aligns to. (2) If t i align to multiple source words, a i is the index of the aligned word in the middle. 1 (3) If t i is unaligned, we inherit its affiliation from the closest aligned word, with prefer- ence given to the right. <ref type="bibr">2</ref> An example of the NNJM context model for a Chinese-English parallel sentence is given in <ref type="figure">Fig- ure 1</ref>.</p><p>For all of our experiments we use n = 4 and m = 11. It is clear that this model is effectively an (n+m)-gram LM, and a 15-gram LM would be <ref type="bibr">1</ref> We arbitrarily round down. <ref type="bibr">2</ref> We have found that the affiliation heuristic is robust to small differences, such as left vs. right preference. far too sparse for standard probability models such as Kneser-Ney back-off <ref type="bibr" target="#b11">(Kneser and Ney, 1995)</ref> or Maximum Entropy <ref type="bibr" target="#b20">(Rosenfeld, 1996)</ref>. Fortu- nately, neural network language models are able to elegantly scale up and take advantage of arbi- trarily large context sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Network Architecture</head><p>Our neural network architecture is almost identi- cal to the original feed-forward NNLM architec- ture described in <ref type="bibr" target="#b1">Bengio et al. (2003)</ref>.</p><p>The input vector is a 14-word context vector (3 target words, 11 source words), where each word is mapped to a 192-dimensional vector us- ing a shared mapping layer. We use two 512- dimensional hidden layers with tanh activation functions. The output layer is a softmax over the entire output vocabulary.</p><p>The input vocabulary contains 16,000 source words and 16,000 target words, while the out- put vocabulary contains 32,000 target words. The vocabulary is selected by frequency-sorting the words in the parallel training data. Out-of- vocabulary words are mapped to their POS tag (or OOV, if POS is not available), and in this case P (P OS i |t i−1 , · · · ) is used directly without fur- ther normalization. Out-of-bounds words are rep- resented with special tokens &lt;src&gt;, &lt;/src&gt;, &lt;trg&gt;, &lt;/trg&gt;.</p><p>We chose these values for the hidden layer size, vocabulary size, and source window size because they seemed to work best on our data sets -larger sizes did not improve results, while smaller sizes degraded results. Empirical comparisons are given in Section 6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Network Training</head><p>The training procedure is identical to that of an NNLM, except that the parallel corpus is used instead of a monolingual corpus. Formally, we seek to maximize the log-likelihood of the train- ing data:</p><formula xml:id="formula_2">L = i log(P (x i ))</formula><p>where x i is the training sample, with one sample for every target word in the parallel corpus.</p><p>Optimization is performed using standard back propagation with stochastic gradient ascent ( <ref type="bibr" target="#b13">LeCun et al., 1998</ref>). Weights are randomly initial- ized in the range of <ref type="bibr">[−0.05, 0.05]</ref>. We use an ini- tial learning rate of 10 −3 and a minibatch size of <ref type="figure">Figure 1</ref>: Context vector for target word "the", using a 3-word target history and a 5-word source window (i.e., n = 4 and m = 5). Here, "the" inherits its affiliation from "money" because this is the first aligned word to its right. The number in each box denotes the index of the word in the context vector. This indexing must be consistent across samples, but the absolute ordering does not affect results.</p><p>128. <ref type="bibr">3</ref> At every epoch, which we define as 20,000 minibatches, the likelihood of a validation set is computed. If this likelihood is worse than the pre- vious epoch, the learning rate is multiplied by 0.5. The training is run for 40 epochs. The training data ranges from 10-30M words, depending on the condition. We perform a basic weight update with no L2 regularization or momentum. However, we have found it beneficial to clip each weight update to the range of [-0.1, 0.1], to prevent the training from entering degenerate search spaces ( <ref type="bibr" target="#b18">Pascanu et al., 2012</ref>).</p><p>Training is performed on a single Tesla K10 GPU, with each epoch (128*20k = 2.6M samples) taking roughly 1100 seconds to run, resulting in a total training time of ∼12 hours. Decoding is performed on a CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-Normalized Neural Network</head><p>The computational cost of NNLMs is a significant issue in decoding, and this cost is dominated by the output softmax over the entire target vocabu- lary. Even class-based approaches such as <ref type="bibr" target="#b12">Le et al. (2012)</ref> require a 2-20k shortlist vocabulary, and are therefore still quite costly.</p><p>Here, our goal is to be able to use a fairly large vocabulary without word classes, and to sim- ply avoid computing the entire output layer at de- code time. <ref type="bibr">4</ref> To do this, we present the novel technique of self-normalization, where the output layer scores are close to being probabilities with- out explicitly performing a softmax.</p><p>Formally, we define the standard softmax log likelihood as:</p><formula xml:id="formula_3">log(P (x)) = log e Ur(x) Z(x) = U r (x) − log(Z(x)) Z(x) = Σ |V | r =1 e U r (x)</formula><p>where x is the sample, U is the raw output layer scores, r is the output layer row corresponding to the observed target word, and Z(x) is the softmax normalizer.</p><p>If we could guarantee that log(Z(x)) were al- ways equal to 0 (i.e., Z(x) = 1) then at decode time we would only have to compute row r of the output layer instead of the whole matrix. While we cannot train a neural network with this guaran- tee, we can explicitly encourage the log-softmax normalizer to be as close to 0 as possible by aug- menting our training objective function:</p><formula xml:id="formula_4">L = i log(P (x i )) − α(log(Z(x i )) − 0) 2 = i log(P (x i )) − α log 2 (Z(x i ))</formula><p>In this case, the output layer bias weights are initialized to log(1/|V |), so that the initial net- work is self-normalized. At decode time, we sim- ply use U r (x) as the feature score, rather than log(P (x)). For our NNJM architecture, self- normalization increases the lookup speed during decoding by a factor of ∼15x. <ref type="table">Table 1</ref> shows the neural network training re- sults with various values of the free parameter α. In all subsequent MT experiments, we use α = 10 −1 .</p><p>We should note that <ref type="bibr" target="#b27">Vaswani et al. (2013)</ref> im- plements a method called Noise Contrastive Es- timation (NCE) that is also used to train self- normalized NNLMs. Although NCE results in faster training time, it has the downside that there <ref type="table">Table 1</ref>: Comparison of neural network likelihood for various α values. log(P (x)) is the average log-likelihood on a held-out set. | log(Z(x))| is the mean error in log-likelihood when using U r (x) directly instead of the true softmax probability log(P (x)). Note that α = 0 is equivalent to the standard neural network objective function.</p><formula xml:id="formula_5">Arabic BOLT Val α log(P (x)) | log(Z(x))| 0 −1.82 5.02 10 −2 −1.81 1.35 10 −1 −1.83 0.68 1 −1.91 0.28</formula><p>is no mechanism to control the degree of self- normalization. By contrast, our α parameter al- lows us to carefully choose the optimal trade-off between neural network accuracy and mean self- normalization error. In future work, we will thor- oughly compare self-normalization vs. NCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Pre-Computing the Hidden Layer</head><p>Although self-normalization significantly im- proves the speed of NNJM lookups, the model is still several orders of magnitude slower than a back-off LM. Here, we present a "trick" for pre- computing the first hidden layer, which further in- creases the speed of NNJM lookups by a factor of 1,000x. Note that this technique only results in a signif- icant speedup for self-normalized, feed-forward, NNLM-style networks with one hidden layer. We demonstrate in Section 6.6 that using one hidden layer instead of two has minimal effect on BLEU.</p><p>For the neural network described in Section 2.1, computing the first hidden layer requires mul- tiplying a 2689-dimensional input vector 5 with a 2689 × 512 dimensional hidden layer matrix. However, note that there are only 3 possible posi- tions for each target word, and 11 for each source word. Therefore, for every word in the vocabu- lary, and for each position, we can pre-compute the dot product between the word embedding and the first hidden layer. These are computed offline and stored in a lookup table, which is &lt;500MB in size.</p><p>Computing the first hidden layer now only re- quires 15 scalar additions for each of the 512 hidden rows -one for each word in the input 5 2689 = 14 words × 192 dimensions + 1 bias vector, plus the bias. This can be reduced to just 5 scalar additions by pre-summing each 11- word source window when starting a test sen- tence. If our neural network has only one hid- den layer and is self-normalized, the only remain- ing computation is 512 calls to tanh() and a sin- gle 513-dimensional dot product for the final out- put score. <ref type="bibr">6</ref> Thus, only ∼3500 arithmetic opera- tions are required per n-gram lookup, compared to ∼2.8M for self-normalized NNJM without pre- computation, and ∼35M for the standard NNJM. <ref type="bibr">7</ref>    <ref type="table" target="#tab_1">Table 2</ref> shows the speed of self-normalization and pre-computation for the NNJM. The decoding cost is based on a measurement of ∼1200 unique NNJM lookups per source word for our Arabic- English system. <ref type="bibr">8</ref> By combining self-normalization and pre- computation, we can achieve a speed of 1.4M lookups/second, which is on par with fast back- off LM implementations ( <ref type="bibr" target="#b26">Tanaka et al., 2013)</ref>. We demonstrate in Section 6.6 that using the self- normalized/pre-computed NNJM results in only a very small BLEU degradation compared to the standard NNJM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Network Speed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decoding with the NNJM</head><p>Because our NNJM is fundamentally an n-gram NNLM with additional source context, it can eas- ily be integrated into any SMT decoder. In this section, we describe the considerations that must be taken when integrating the NNJM into a hierar- chical decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Parsing</head><p>When performing hierarchical decoding with an n-gram LM, the leftmost and rightmost n − 1 words from each constituent must be stored in the state space. Here, we extend the state space to also include the index of the affiliated source word for these edge words. This does not noticeably in- crease the search space. We also train a separate lower-order n-gram model, which is necessary to compute estimate scores during hierarchical de- coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Affiliation Heuristic</head><p>For aligned target words, the normal affiliation heuristic can be used, since the word alignment is available within the rule. For unaligned words, the normal heuristic can also be used, except when the word is on the edge of a rule, because then the target neighbor words are not necessarily known.</p><p>In this case, we infer the affiliation from the rule structure. Specifically, if unaligned target word t is on the right edge of an arc that covers source span [s i , s j ], we simply say that t is affiliated with source word s j . If t is on the left edge of the arc, we say it is affiliated with s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Variations</head><p>Recall that our NNJM feature can be described with the following probability:</p><formula xml:id="formula_6">Π |T | i=1 P (t i |t i−1 , t i−2 , · · · , s a i , s a i −1 , s a i +1 , · · · )</formula><p>This formulation lends itself to several natural variations. In particular, we can reverse the trans- lation direction of the languages, as well as the di- rection of the language model. We denote our original formulation as a source- to-target, left-to-right model (S2T/L2R). We can train three variations using target-to-source (T2S) and right-to-left (R2L) models:</p><formula xml:id="formula_7">S2T/R2L Π |T | i=1 P (t i |t i+1 , t i+2 , · · · , s a i , s a i −1 , s a i +1 , · · · ) T2S/L2R Π |S| i=1 P (s i |s i−1 , s i−2 , · · · , t a i , t a i −1 , t a i +1 , · · · ) T2S/R2L Π |S| i=1 P (s i |s i+1 , s i+2 , · · · , t a i , t a i −1 , t a i +1 , · · · )</formula><p>where a i is the target-to-source affiliation, de- fined analogously to a i .</p><p>The T2S variations cannot be used in decoding due to the large target context required, and are thus only used in k-best rescoring. The S2T/R2L variant could be used in decoding, but we have not found this beneficial, so we only use it in rescor- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neural Network Lexical Translation</head><p>Model (NNLTM)</p><p>One issue with the S2T NNJM is that the prob- ability is computed over every target word, so it does not explicitly model NULL-aligned source words. In order to assign a probability to every source word during decoding, we also train a neu- ral network lexical translation model (NNLMT). Here, the input context is the 11-word source window centered at s i , and the output is the tar- get token t s i which s i aligns to. The probabil- ity is computed over every source word in the in- put sentence. We treat NULL as a normal target word, and if a source word aligns to multiple target words, it is treated as a single concatenated token. Formally, the probability model is:</p><formula xml:id="formula_8">Π |S| i=1 P (t s i |s i , s i−1 , s i+1 , · · · )</formula><p>This model is trained and evaluated like our NNJM. It is easy and computationally inexpensive to use this model in decoding, since only one neu- ral network computation must be made for each source word.</p><p>In rescoring, we also use a T2S NNLTM model computed over every target word:</p><formula xml:id="formula_9">Π |T | i=1 P (s t i |t i , t i−1 , t i+1 , · · · )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MT System</head><p>In this section, we describe the MT system used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MT Decoder</head><p>We use a state-of-the-art string-to-dependency hi- erarchical decoder <ref type="bibr" target="#b24">(Shen et al., 2010)</ref>. Our base- line decoder contains a large and powerful set of features, which include:</p><p>• Forward and backward rule probabilities We also perform 1000-best rescoring with the following features:</p><p>•</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5-gram Kneser-Ney LM • Recurrent neural network language model (RNNLM) (Mikolov et al., 2010)</head><p>Although we consider the RNNLM to be part of our baseline, we give it special treatment in the results section because we would expect it to have the highest overlap with our NNJM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training and Optimization</head><p>For Arabic word tokenization, we use the MADA- ARZ tokenizer <ref type="bibr" target="#b8">(Habash et al., 2013</ref>) for the BOLT condition, and the Sakhr 9 tokenizer for the NIST condition. For Chinese tokenization, we use a sim- ple longest-match-first lexicon-based approach.</p><p>For word alignment, we align all of the train- ing data with both GIZA++ <ref type="bibr" target="#b17">(Och and Ney, 2003)</ref> and NILE ( <ref type="bibr" target="#b19">Riesa et al., 2011)</ref>, and concatenate the corpora together for rule extraction.</p><p>For MT feature weight optimization, we use iterative k-best optimization with an Expected- BLEU objective function ( <ref type="bibr" target="#b21">Rosti et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We present MT primary results on Arabic-English and Chinese-English for the NIST OpenMT12 and DARPA BOLT conditions. We also present a set of auxiliary results in order to further analyze our features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">NIST OpenMT12 Results</head><p>Our NIST system is fully compatible with the OpenMT12 constrained track, which consists of 10M words of high-quality parallel training for Arabic, and 25M words for Chinese. <ref type="bibr">10</ref> The Kneser-Ney LM is trained on 5B words of data from English GigaWord.</p><p>For test, we use the "Arabic-To-English Original Progress Test" (1378 segments) and "Chinese-to-English Orig- inal Progress Test + OpenMT12 Current Test" (2190 segments), which consists of a mix of newswire and web data. 11 All test segments have 4 references. Our tuning set contains 5000 seg- ments, and is a mix of the MT02-05 eval set as well as held-out parallel training. <ref type="bibr">9</ref> http://www.sakhr.com <ref type="bibr">10</ref>   Results are shown in the second section of Ta- ble 3. On Arabic-English, the primary S2T/L2R NNJM gains +1.4 BLEU on top of our baseline, while the S2T NNLTM gains another +0.8, and the directional variations gain +0.8 BLEU more. This leads to a total improvement of +3.0 BLEU from the NNJM and its variations. Considering that our baseline is already +0.3 BLEU better than the 1st place result of MT12 and contains a strong RNNLM, we consider this to be quite an extraor- dinary improvement. <ref type="bibr">12</ref> For the Chinese-English condition, there is an improvement of +0.8 BLEU from the primary NNJM and +1.3 BLEU overall. Here, the base- line system is already +0.8 BLEU better than the best MT12 system. The smaller improvement on Chinese-English compared to Arabic-English is consistent with the behavior of our baseline fea- tures, as we show in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">"Simple Hierarchical" NIST Results</head><p>The baseline used in the last section is a highly- engineered research system, which uses a wide array of features that were refined over a num- ber of years, and some of which require linguis- tic resources. Because of this, the baseline BLEU scores are much higher than a typical MT system -especially a real-time, production engine which must support many language pairs. Therefore, we also present results using a simpler version of our decoder which emulates Chiang's original Hiero implementation <ref type="bibr" target="#b4">(Chiang, 2007)</ref>. Specifically, this means that we don't use dependency-based rule extraction, and our de- coder only contains the following MT features: (1) rule probabilities, (2) n-gram Kneser-Ney LM, (3) lexical smoothing, (4) target word count, (5) con- cat rule penalty.</p><p>Results are shown in the third section of <ref type="table" target="#tab_3">Table 3</ref>. The "Simple Hierarchical" Arabic-English system is -6.4 BLEU worse than our strong baseline, and would have ranked 10th place out of 11 systems in the evaluation. When the NNJM features are added to this system, we see an improvement of +6.3 BLEU, which would have ranked 1st place in the evaluation.</p><p>Effectively, this means that for Arabic-English, the NNJM features are equivalent to the combined improvements from the string-to-dependency model plus all of the features listed in Section 5.1.</p><p>For Chinese-English, the "Simple Hierarchical" system only degrades by -3.2 BLEU compared to our strongest baseline, and the NNJM features produce a gain of +2.1 BLEU on top of that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">BOLT Web Forum Results</head><p>DARPA BOLT is a major research project with the goal of improving translation of informal, dialec- tical Arabic and Chinese into English. The BOLT domain presented here is "web forum," which was crawled from various Chinese and Egyptian Inter- net forums by LDC. The BOLT parallel training consists of all of the high-quality NIST training, plus an additional 3 million words of translated forum data provided by LDC. The tuning and test sets consist of roughly 5000 segments each, with 2 references for Arabic and 3 for Chinese.</p><p>Results are shown in <ref type="table" target="#tab_4">Table 4</ref>. The baseline here uses the same feature set as the strong NIST sys- tem. On Arabic, the total gain is +2.6 BLEU, while on Chinese, the gain is +1.3 BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BOLT Test</head><p>Ar  <ref type="table" target="#tab_6">Table 5</ref> shows performance when our S2T/L2R NNJM is used only in 1000-best rescoring, com- pared to decoding. The primary purpose of this is as a comparison to <ref type="bibr" target="#b12">Le et al. (2012)</ref>, whose model can only be used in k-best rescoring.  We can see that the rescoring-only NNJM per- forms very well when used on top of a baseline without an RNNLM (+1.5 BLEU), but the gain on top of the RNNLM is very small (+0.3 BLEU). The gain from the decoding NNJM is large in both cases (+2.6 BLEU w/o RNNLM, +1.6 BLEU w/ RNNLM). This demonstrates that the full power of the NNJM can only be harnessed when it is used in decoding. It is also interesting to see that the RNNLM is no longer beneficial when the NNJM is used. <ref type="table">Table 6</ref> shows results using the S2T/L2R NNJM with various configurations. We can see that re- ducing the source window size, layer size, or vo- cab size will all degrade results. Increasing the sizes beyond the default NNJM has almost no ef- fect (102%). Also note that the target-only NNLM (i.e., Source Window=0) only obtains 33% of the improvements of the NNJM.</p><note type="other">-En Ch-En BLEU BLEU Baseline (w/o RNNLM) 40.2 30.6 Baseline (w/ RNNLM) 41.3 30.9 + S2T/L2R NNJM (Dec) 42.9 31.9 + S2T NNLTM (Dec) 43.2 31.9 + Other NNJMs (Resc) 43.9 32.2</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effect of k-best Rescoring Only</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BOLT Test</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Effect of Neural Network Configuration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BOLT Test</head><p>Ar  <ref type="table">Table 6</ref>: Results with different neural net- work architectures. The "default" NNJM in the second row uses these parameters: SW=11, L=192x512x512, V=32,000, A=tanh. All mod- els use a 3-word target history (i.e., 4-gram LM). "Layers" refers to the size of the word embedding followed by the hidden layers. "Vocab" refers to the size of the input and output vocabularies. "% Gain" is the BLEU gain over the baseline relative to the default NNJM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Effect of Speedups</head><p>All previous results use a self-normalized neural network with two hidden layers. In <ref type="table">Table 7</ref>, we compare this to using a standard network (with two hidden layers), as well as a pre-computed neu- ral network. <ref type="bibr">13</ref> The "Simple Hierarchical" base- line is used here because it more closely approx- imates a real-time MT engine. For the sake of speed, these experiments only use the S2T/L2R NNJM+S2T NNLTM. <ref type="bibr">13</ref> The difference in score for self-normalized vs. pre- computed is entirely due to two vs. one hidden layers.</p><p>Each result from <ref type="table">Table 7</ref> corresponds to a row in <ref type="table" target="#tab_1">Table 2</ref> of Section 2.4. We can see that go- ing from the standard model to the pre-computed model only reduces the BLEU improvement from +6.4 to +6.1, while increasing the NNJM lookup speed by a factor of 10,000x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BOLT Test</head><p>Ar  <ref type="table">Table 7</ref>: Results for the standard NNs vs. self- normalized NNs vs. pre-computed NNs.</p><p>In <ref type="table" target="#tab_1">Table 2</ref> we showed that the cost of unique lookups for the pre-computed NNJM is only ∼0.001 seconds per source word. This does not include the cost of n-gram creation or cached lookups, which amount to ∼0.03 seconds per source word in our current implementation. <ref type="bibr">14</ref> However, the n-grams created for the NNJM can be shared with the Kneser-Ney LM, which reduces the cost of that feature. Thus, the total cost in- crease of using the NNJM+NNLTM features in decoding is only ∼0.01 seconds per source word.</p><p>In future work we will provide more detailed analysis regarding the usability of the NNJM in a low-latency, high-throughput MT engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Although there has been a substantial amount of past work in lexicalized joint models <ref type="bibr" target="#b14">(Marino et al., 2006;</ref><ref type="bibr" target="#b5">Crego and Yvon, 2010)</ref>, nearly all of these papers have used older statistical techniques such as Kneser-Ney or Maximum Entropy. How- ever, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network's ability to semantically generalize ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) and learn non- linear relationships.</p><p>A number of recent papers have proposed meth- ods for creating neural network translation/joint models, but nearly all of these works have ob- tained much smaller BLEU improvements than ours. For each related paper, we will briefly con-trast their methodology with our own and summa- rize their BLEU improvements using scores taken directly from the cited paper. <ref type="bibr" target="#b0">Auli et al. (2013)</ref> use a fixed continuous-space source representation, obtained from LDA ( <ref type="bibr" target="#b2">Blei et al., 2003</ref>) or a source-only NNLM. Also, their model is recurrent, so it cannot be used in decod- ing. They obtain +0.2 BLEU improvement on top of a target-only NNLM (25.6 vs. 25.8).</p><p>Schwenk <ref type="formula">(2012)</ref> predicts an entire target phrase at a time, rather than a word at a time. He obtains +0.3 BLEU improvement (24.8 vs. 25.1). <ref type="bibr" target="#b28">Zou et al. (2013)</ref> estimate context-free bilingual lexical similarity scores, rather than using a large context. They obtain an +0.5 BLEU improvement on Chinese-English (30.0 vs. 30.5).</p><p>Kalchbrenner and Blunsom (2013) implement a convolutional recurrent NNJM. They score a 1000-best list using only their model and are able to achieve the same BLEU as using all 12 standard MT features (21.8 vs 21.7). However, additive re- sults are not presented.</p><p>The most similar work that we know of is <ref type="bibr" target="#b12">Le et al. (2012)</ref>. Le's basic procedure is to re-order the source to match the linear order of the target, and then segment the hypothesis into minimal bilin- gual phrase pairs. Then, he predicts each target word given the previous bilingual phrases. How- ever, Le's formulation could only be used in k- best rescoring, since it requires long-distance re- ordering and a large target context. Le's model does obtain an impressive +1.7 BLEU gain on top of a baseline without an NNLM <ref type="bibr">(25.8 vs. 27.5)</ref>. However, when compared to the strongest baseline which includes an NNLM, Le's best models (S2T + T2S) only obtain an +0.6 BLEU improvement <ref type="bibr">(26.9 vs. 27.5)</ref>. This is con- sistent with our rescoring-only result, which indi- cates that k-best rescoring is too shallow to take advantage of the power of a joint model. Le's model also uses minimal phrases rather than being purely lexicalized, which has two main downsides: (a) a number of complex, hand-crafted heuristics are required to define phrase boundaries, which may not transfer well to new languages, (b) the effective vocabulary size is much larger, which substantially increases data sparsity issues.</p><p>We should note that our best results use six sep- arate models, whereas all previous work only uses one or two models. However, we have demon- strated that we can obtain 50%-80% of the to- tal improvement with only one model (S2T/L2R NNJM), and 70%-90% with only two models (S2T/L2R NNJM + S2T NNLTM). Thus, the one and two-model conditions still significantly out- perform any past work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>We have described a novel formulation for a neural network-based machine translation joint model, along with several simple variations of this model. When used as MT decoding features, these models are able to produce a gain of +3.0 BLEU on top of a very strong and feature-rich baseline, as well as a +6.3 BLEU gain on top of a simpler system.</p><p>Our model is remarkably simple -it requires no linguistic resources, no feature engineering, and only a handful of hyper-parameters. It also has no reliance on potentially fragile outside algorithms, such as unsupervised word clustering. We con- sider the simplicity to be a major advantage. Not only does this suggest that it will generalize well to new language pairs and domains, but it also sug- gests that it will be straightforward for others to replicate these results.</p><p>Overall, we believe that the following factors set us apart from past work and allowed us to obtain such significant improvements:</p><p>1. The ability to use the NNJM in decoding rather than rescoring. 2. The use of a large bilingual context vector, which is provided to the neural network in "raw" form, rather than as the output of some other algorithm. 3. The fact that the model is purely lexicalized, which avoids both data sparsity and imple- mentation complexity. 4. The large size of the network architecture. 5. The directional variation models.</p><p>One of the biggest goals of this work is to quell any remaining doubts about the utility of neural networks in machine translation. We believe that there are large areas of research yet to be explored. For example, creating a new type of decoder cen- tered around a purely lexicalized neural network model. Our short term ideas include using more interesting types of context in our input vector (such as source syntax), or using the NNJM to model syntactic/semantic structure of the target.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>• 4 -</head><label>4</label><figDesc>gram Kneser-Ney LM • Dependency LM (Shen et al., 2010) • Contextual lexical smoothing (Devlin, 2009) • Length distribution (Shen et al., 2010) • Trait features (Devlin and Matsoukas, 2012) • Factored source syntax (Huang et al., 2013) • 7 sparse feature types, totaling 50k features (Chiang et al., 2009) • LM adaptation (Snover et al., 2008)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Speed of the neural network computa-
tion on a single CPU thread. "lookups/sec" is the 
number of unique n-gram probabilities that can be 
computed per second. "sec/word" is the amortized 
cost of unique NNJM lookups in decoding, per 
source word. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>We also make weak use of 30M-100M words of UN data + ISI comparable corpora, but this data provides almost</figDesc><table>no 
benefit. 
11 http://www.nist.gov/itl/iad/mig/openmt12results.cfm 

NIST MT12 Test 
Ar-En Ch-En 
BLEU BLEU 
OpenMT12 -1st Place 
49.5 
32.6 
OpenMT12 -2nd Place 
47.5 
32.2 
OpenMT12 -3rd Place 
47.4 
30.8 
· · · 
· · · 
· · · 
OpenMT12 -9th Place 
44.0 
27.0 
OpenMT12 -10th Place 
41.2 
25.7 
Baseline (w/o RNNLM) 
48.9 
33.0 
Baseline (w/ RNNLM) 
49.8 
33.4 
+ S2T/L2R NNJM (Dec) 
51.2 
34.2 
+ S2T NNLTM (Dec) 
52.0 
34.2 
+ T2S NNLTM (Resc) 
51.9 
34.2 
+ S2T/R2L NNJM (Resc) 
52.2 
34.3 
+ T2S/L2R NNJM (Resc) 
52.3 
34.5 
+ T2S/R2L NNJM (Resc) 
52.8 
34.7 
"Simple Hier." Baseline 
43.4 
30.1 
+ S2T/L2R NNJM (Dec) 
47.2 
31.5 
+ S2T NNLTM (Dec) 
48.5 
31.8 
+ Other NNJMs (Resc) 
49.7 
32.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Primary results on Arabic-English and 
Chinese-English NIST MT12 Test Set. The first 
section corresponds to the top and bottom ranked 
systems from the evaluation, and are taken from 
the NIST website. The second section corresponds 
to results on top of our strongest baseline. The 
third section corresponds to results on top of a 
simpler baseline. Within each section, each row 
includes all of the features from previous rows. 
BLEU scores are mixed-case. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Primary results on Arabic-English and 
Chinese-English BOLT Web Forum. Each row 
includes the aggregate features from all previous 
rows. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison of our primary NNJM in de-
coding vs. 1000-best rescoring. 

</table></figure>

			<note place="foot" n="3"> We do not divide the gradient by the minibatch size. For those who do, this is equivalent to using an initial learning rate of 10 −3 * 128 ≈ 10 −1. 4 We are not concerned with speeding up training time, as we already find GPU training time to be adequate.</note>

			<note place="foot" n="6"> tanh() is implemented using a lookup table. 7 3500 ≈ 5 × 512 + 2 × 513; 2.8M ≈ 2 × 2689 × 512 + 2 × 513; 35M ≈ 2 × 2689 × 512 + 2 × 513 × 32000. For the sake of a fair comparison, these all use one hidden layer. A second hidden layer adds 0.5M floating point operations. 8 This does not include the cost of duplicate lookups within the same test sentence, which are cached.</note>

			<note place="foot" n="12"> Note that the official 1st place OpenMT12 result was our own system, so we can assure that these comparisons are accurate.</note>

			<note place="foot" n="14"> In our decoder, roughly 95% of NNJM n-gram lookups within the same sentence are duplicates.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA,</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
	<note>October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">001 new features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="218" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Factored bilingual n-gram language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Josep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Traitbased hypothesis selection for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="528" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Lexical features for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Morphological analysis and disambiguation for dialectal arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="426" to="432" />
		</imprint>
	</monogr>
	<note>Ramy Eskander, and Nadi Tomeh</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Factored soft source syntactic constraints for hierarchical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="556" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Recurrent continuous translation models</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note>International Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuous space translation models with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Son</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">N-gram-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">E</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Josep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adrì A De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta R Costa-Jussà</forename><surname>Fonollosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="527" to="549" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<title level="m">On the difficulty of training recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature-rich language-independent syntax-based alignment for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to adaptive statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer, Speech and Language</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="187" to="228" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BBN system description for WMT10 system combination task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT/MetricsMATR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Continuous-space language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prague Bull. Math. Linguistics</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="137" to="146" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING (Posters)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1071" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">String-to-dependency statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="649" to="671" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language and translation model adaptation using comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="857" to="866" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An efficient language model using double-array structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhara</forename><surname>Toru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ya</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikio</forename><surname>Norimatsu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoding with largescale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
