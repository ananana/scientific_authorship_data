<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2182</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Hong Kong ‡ MoE Key Lab of High Confidence Software Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Hong Kong ‡ MoE Key Lab of High Confidence Software Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Hong Kong ‡ MoE Key Lab of High Confidence Software Technologies</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
						</author>
						<title level="a" type="main">Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2182" to="2192"/>
							<date type="published">July 15-20, 2018. 2018. 2182</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment , referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning policies for task-completion dialogue is often formulated as a reinforcement learning (RL) problem ( <ref type="bibr" target="#b39">Young et al., 2013</ref>; <ref type="bibr" target="#b7">Levin et al., 1997</ref>). However, applying RL to real-world dialogue sys- tems can be challenging, due to the constraint that an RL learner needs an environment to operate in. In the dialogue setting, this requires a dia- logue agent to interact with real users and adjust its policy in an online fashion, as illustrated in <ref type="figure" target="#fig_0">Fig- ure 1(a)</ref>. Unlike simulation-based games such as Atari games <ref type="bibr" target="#b15">(Mnih et al., 2015)</ref> and AlphaGo <ref type="bibr" target="#b26">(Silver et al., 2016a</ref><ref type="bibr" target="#b27">(Silver et al., , 2017</ref> where RL has made its greatest strides, task-completion dialogue systems may incur significant real-world cost in case of failure. Thus, except for very simple tasks <ref type="bibr" target="#b29">(Singh et al., 2002;</ref><ref type="bibr" target="#b2">Gaši´Gaši´c et al., 2010</ref><ref type="bibr" target="#b22">Pietquin et al., 2011;</ref><ref type="bibr" target="#b8">Li et al., 2016a;</ref><ref type="bibr" target="#b32">Su et al., 2016b</ref>), RL is too expensive to be applied to real users to train dialogue agents from scratch.</p><p>One strategy is to convert human-interacting di- alogue to a simulation problem (similar to Atari games), by building a user simulator using human conversational data ( <ref type="bibr" target="#b25">Schatzmann et al., 2007;</ref><ref type="bibr" target="#b10">Li et al., 2016b)</ref>. In this way, the dialogue agent can learn its policy by interacting with the simulator instead of real users <ref type="figure" target="#fig_0">(Figure 1(b)</ref>). The simulator, in theory, does not incur any real-world cost and can provide unlimited simulated experience for re- inforcement learning. The dialogue agent trained with such a user simulator can then be deployed to real users and further enhanced by only a small number of human interactions. Most of recent studies in this area have adopted this strategy ( <ref type="bibr" target="#b31">Su et al., 2016a;</ref><ref type="bibr" target="#b12">Lipton et al., 2016;</ref><ref type="bibr" target="#b40">Zhao and Eskenazi, 2016;</ref><ref type="bibr" target="#b38">Williams et al., 2017;</ref><ref type="bibr" target="#b1">Dhingra et al., 2017;</ref><ref type="bibr" target="#b13">Liu and Lane, 2017;</ref><ref type="bibr" target="#b20">Peng et al., 2017b;</ref><ref type="bibr" target="#b0">Budzianowski et al., 2017;</ref><ref type="bibr" target="#b19">Peng et al., 2017a</ref>).</p><p>However, user simulators usually lack the con- versational complexity of human interlocutors, and the trained agent is inevitably affected by bi- ases in the design of the simulator. <ref type="bibr" target="#b1">Dhingra et al. (2017)</ref> demonstrated a significant discrepancy in a simulator-trained dialogue agent when evalu- ated with simulators and with real users. Even more challenging is the fact that there is no uni- versally accepted metric to evaluate a user simula- tor ( <ref type="bibr" target="#b23">Pietquin and Hastie, 2013</ref>). Thus, it remains controversial whether training task-completion di- alogue agent via simulated users is a valid ap- proach.</p><p>We propose a new strategy of learning dialogue policy by interacting with real users. Compared to previous works <ref type="bibr" target="#b29">(Singh et al., 2002;</ref><ref type="bibr" target="#b8">Li et al., 2016a;</ref><ref type="bibr" target="#b32">Su et al., 2016b;</ref><ref type="bibr" target="#b18">Papangelis, 2012)</ref>, our di- alogue agent learns in a much more efficient way, using only a small number of real user interac- tions, which amounts to an affordable cost in many nontrivial dialogue tasks.</p><p>Our approach is based on the Dyna-Q frame- work <ref type="bibr" target="#b33">(Sutton, 1990)</ref> where planning is integrated into policy learning for task-completion dialogue. Specifically, we incorporate a model of the envi- ronment, referred to as the world model, into the dialogue agent, which simulates the environment and generates simulated user experience. During the dialogue policy learning, real user experience plays two pivotal roles: first, it can be used to im- prove the world model and make it behave more like real users, via supervised learning; second, it can also be used to directly improve the dialogue policy via RL. The former is referred to as world model learning, and the latter direct reinforcement learning. Dialogue policy can be improved ei- ther using real experience directly (i.e., direct re- inforcement learning) or via the world model in- directly (referred to as planning or indirect re- inforcement learning). The interaction between world model learning, direct reinforcement learn- ing and planning is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(c), fol- lowing the Dyna-Q framework <ref type="bibr" target="#b33">(Sutton, 1990</ref>).</p><p>The original papers on Dyna-Q and most its early extensions used tabular methods for both planning and learning <ref type="bibr" target="#b30">(Singh, 1992;</ref><ref type="bibr" target="#b21">Peng and Williams, 1993;</ref><ref type="bibr" target="#b16">Moore and Atkeson, 1993;</ref><ref type="bibr" target="#b6">Kuvayev and Sutton, 1996)</ref>. This table-lookup repre- sentation limits its application to small problems only. <ref type="bibr" target="#b35">Sutton et al. (2012)</ref> extends the Dyna ar- chitecture to linear function approximation, mak- ing it applicable to larger problems. In the dia- logue setting, we are dealing with a much larger action-state space. Inspired by <ref type="bibr" target="#b15">Mnih et al. (2015)</ref>, we propose Deep Dyna-Q (DDQ) by combining Dyna-Q with deep learning approaches to repre- senting the state-action space by neural networks (NN).</p><p>By employing the world model for planning, the DDQ method can be viewed as a model-based RL approach, which has drawn growing interest in the research community. However, most model-based RL methods ( <ref type="bibr" target="#b36">Tamar et al., 2016;</ref><ref type="bibr" target="#b28">Silver et al., 2016b;</ref><ref type="bibr" target="#b4">Gu et al., 2016;</ref><ref type="bibr">Racanì ere et al., 2017</ref>) are developed for simulation-based, synthetic prob- lems (e.g., games), but not for human-in-the-loop, real-world problems. To these ends, our main con- tributions in this work are two-fold:</p><p>• We present Deep Dyna-Q, which to the best of our knowledge is the first deep RL frame- work that incorporates planning for task- completion dialogue policy learning.</p><p>• We demonstrate that a task-completion dia- logue agent can efficiently adapt its policy on the fly, by interacting with real users via RL. This results in a significant improvement in success rate on a nontrivial task.  As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>(c), starting with an initial dialogue policy and an initial world model (both trained with pre-collected human conversa- tional data), the training of the DDQ agent con- sists of three processes: (1) direct reinforcement learning, where the agent interacts with a real user, collects real experience and improves the dialogue policy; (2) world model learning, where the world model is learned and refined using real experience; and (3) planning, where the agent improves the di- alogue policy using simulated experience.</p><p>Although these three processes conceptually can occur simultaneously in the DDQ agent, we implement an iterative training procedure, as shown in Algorithm 1, where we specify the or- der in which they occur within each iteration. In what follows, we will describe these processes in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Direct Reinforcement Learning</head><p>In this process (lines 5-18 in Algorithm 1) we use the DQN method ( <ref type="bibr" target="#b15">Mnih et al., 2015</ref>) to improve the dialogue policy based on real experience. We consider task-completion dialogue as a Markov Decision Process (MDP), where the agent inter-acts with a user in a sequence of actions to ac- complish a user goal. In each step, the agent ob- serves the dialogue state s, and chooses the action a to execute, using an -greedy policy that selects a random action with probability or otherwise fol- lows the greedy policy a = argmax a Q(s, a ; θ Q ). Q(s, a; θ Q ) which is the approximated value func- tion, implemented as a Multi-Layer Perceptron (MLP) parameterized by θ Q . The agent then re- ceives reward <ref type="bibr">3</ref> r, observes next user response a u , and updates the state to s . Finally, we store the experience (s, a, r, a u , s ) in the replay buffer D u . The cycle continues until the dialogue terminates.</p><p>We improve the value function Q(s, a; θ Q ) by adjusting θ Q to minimize the mean-squared loss function, defined as follows:</p><formula xml:id="formula_0">L(θ Q ) = E (s,a,r,s )∼D u [(y i − Q(s, a; θ Q )) 2 ] y i = r + γ max a Q (s , a ; θ Q ) (1)</formula><p>where γ ∈ [0, 1] is a discount factor, and Q (.) is the target value function that is only periodically updated (line 42 in Algorithm 1). By differentiat- ing the loss function with respect to θ Q , we arrive at the following gradient:</p><formula xml:id="formula_1">θ Q L(θ Q ) = E (s,a,r,s )∼D u [(r+ γ max a Q (s , a ; θ Q ) − Q(s, a; θ Q )) θ Q Q(s, a; θ Q )]<label>(2)</label></formula><p>As shown in lines 16-17 in Algorithm 1, in each iteration, we improve Q(.) using minibatch Deep Q-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Planning</head><p>In the planning process (lines 23-41 in Algo- rithm 1), the world model is employed to generate simulated experience that can be used to improve dialogue policy. K in line 24 is the number of planning steps that the agent performs per step of direct reinforcement learning. If the world model is able to accurately simulate the environment, a big K can be used to speed up the policy learn- ing. In DDQ, we use two replay buffers, D u for storing real experience and D s for simulated ex- perience. Learning and planning are accomplished</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Deep Dyna-Q for Dialogue Policy Learning</head><p>Require: N , , K, L, C, Z Ensure: Q(s, a; θQ), M (s, a; θM ) 1: initialize Q(s, a; θQ) and M (s, a; θM ) via pre-training on human conversational data 2: initialize Q (s, a; θ Q ) with θ Q = θQ 3: initialize real experience replay buffer D u using Reply Buffer Spiking (RBS), and simulated experience replay buffer D s as empty 4: for n=1:N do 5:</p><p># Direct Reinforcement Learning starts 6:</p><p>user starts a dialogue with user action a u 7:</p><p>generate an initial dialogue state s 8:</p><p>while s is not a terminal state do 9:</p><p>with probability select a random action a 10:</p><p>otherwise select a = argmax a Q(s, a ; θQ) 11:</p><p>execute a, and observe user response a u and re- ward r 12:</p><p>update dialogue state to s 13:</p><p>store (s, a, r, a u , s ) to D u 14:</p><formula xml:id="formula_2">s = s 15:</formula><p>end while 16:</p><p>sample random minibatches of (s, a, r, s ) from D u 17:</p><p>update θQ via Z-step minibatch Q-learning according to Equation <ref type="formula" target="#formula_1">(2)  18:</ref> # Direct Reinforcement Learning ends 19:</p><p># World Model Learning starts 20:</p><p>sample random minibatches of training samples</p><formula xml:id="formula_3">(s, a, r, a u , s ) from D u 21:</formula><p>update θM via Z-step minibatch SGD of multi-task learning 22:</p><p># World Model Learning ends 23:</p><p># Planning starts 24:</p><p>for k=1:K do 25: t = FALSE, l = 0 26: sample a user goal G 27: sample user action a u from G 28:</p><p>generate an initial dialogue state s 29:</p><formula xml:id="formula_4">while t is FALSE ∧ l ≤ L do 30:</formula><p>with probability select a random action a 31:</p><p>otherwise select a = argmax a Q(s, a ; θQ) 32:</p><p>execute a 33:</p><p>world model responds with a u , r and t 34:</p><p>update dialogue state to s 35: store (s, a, r, s ) to D s 36:</p><formula xml:id="formula_5">l = l + 1, s = s 37:</formula><p>end while 38:</p><p>sample random minibatches of (s, a, r, s ) from D s 39:</p><p>update θQ via Z-step minibatch Q-learning ac- cording to Equation (2) 40:</p><p>end for 41:</p><p># Planning ends 42:</p><p>every C steps reset θ Q = θQ 43: end for by the same DQN algorithm, operating on real ex- perience in D u for learning and on simulated ex- perience in D s for planning. Thus, here we only describe the way the simulated experience is gen- erated.</p><p>Similar to <ref type="bibr" target="#b25">Schatzmann et al. (2007)</ref>, at the be- ginning of each dialogue, we uniformly draw a user goal G = (C, R), where C is a set of con- straints and R is a set of requests (line 26 in Al- gorithm 1). For movie-ticket booking dialogues, constraints are typically the name and the date of the movie, the number of tickets to buy, etc. Requests can contain these slots as well as the location of the theater, its start time, etc. Ta- ble 3 presents some sampled user goals and di- alogues generated by simulated and real users, respectively. The first user action a u (line 27) can be either a request or an inform dialogue- act. A request, such as request(theater; moviename=batman), consists of a request slot and multiple ( 1) constraint slots, uni- formly sampled from R and C, respectively. An inform contains constraint slots only. The user action can also be converted to natural lan- guage via NLG, e.g., "which theater will show batman?"</p><p>In each dialogue turn, the world model takes as input the current dialogue state s and the last agent action a (represented as an one-hot vector), and generates user response a u , reward r, and a binary variable t, which indicates whether the di- alogue terminates (line 33). The generation is ac- complished using the world model M (s, a; θ M ), a MLP shown in <ref type="figure" target="#fig_2">Figure 3</ref>, as follows:</p><formula xml:id="formula_6">h = tanh(W h (s, a) + b h ) r = W r h + b r a u = softmax(W a h + b a ) t = sigmoid(W t h + b t )</formula><p>where (s, a) is the concatenation of s and a, and W and b are parameter matrices and vectors, re- spectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task-Specific Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">World Model Learning</head><p>In this process (lines 19-22 in Algorithm 1), M (s, a; θ M ) is refined via minibatch SGD using real experience in the replay buffer D u . As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, M (s, a; θ M ) is a multi-task neural net- work ( <ref type="bibr" target="#b14">Liu et al., 2015</ref>) that combines two classi- fication tasks of simulating a u and t, respectively, and one regression task of simulating r. The lower layers are shared across all tasks, while the top lay- ers are task-specific.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>We evaluate the DDQ method on a movie-ticket booking task in both simulation and human-in-the- loop settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Raw conversational data in the movie-ticket book- ing scenario was collected via Amazon Mechani- cal Turk. The dataset has been manually labeled based on a schema defined by domain experts, as shown in <ref type="table">Table 4</ref>, which consists of 11 dialogue acts and 16 slots. In total, the dataset contains 280 annotated dialogues, the average length of which is approximately 11 turns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dialogue Agents for Comparison</head><p>To benchmark the performance of DDQ, we have developed different versions of task-completion dialogue agents, using variations of Algorithm 1.</p><p>• A DQN agent is learned by standard DQN, implemented with direct reinforcement learn- ing only (lines 5-18 in Algorithm 1) in each epoch.</p><p>• The DDQ(K) agents are learned by DDQ of Algorithm 1, with an initial world model pre- trained on human conversational data, as de- scribed in Section 3. • The DQN(K) agents are learned by DQN, but with K times more real experiences than the DQN agent. DQN(K) is evaluated in the simulation setting only. Its performance can be viewed as the upper bound of its DDQ(K) counterpart, assuming that the world model in DDQ(K) perfectly matches real users.</p><p>Implementation Details All the models in these agents (Q(s, a; θ Q ), M (s, a; θ M )) are MLPs with tanh activations. Each policy network Q(.) has one hidden layer with 80 hidden nodes. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the world model M (.) contains two shared hidden layers and three task-specific hid- den layers, with 80 nodes in each. All the agents are trained by Algorithm 1 with the same set of hyper-parameters. -greedy is always applied for exploration. We set the discount factor γ = 0.95. The buffer sizes of both D u and D s are set to 5000. The target value function is updated at the end of each epoch. In each epoch, Q(.) and M (.) are refined using one-step (Z = 1) 16-tuple- minibatch update. <ref type="bibr">4</ref> In planning, the maximum length of a simulated dialogue is 40 (L = 40).</p><p>In addition, to make the dialogue training effi- cient, we also applied a variant of imitation learn- ing, called Reply Buffer Spiking (RBS) ( <ref type="bibr" target="#b12">Lipton et al., 2016</ref>). We built a naive but occasionally suc- cessful rule-based agent based on human conver- sational dataset (line 1 in Algorithm 1), and pre- filled the real experience replay buffer D u with 100 dialogues of experience (line 2) before train- ing for all the variants of agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Simulated User Evaluation</head><p>In this setting the dialogue agents are optimized by interacting with user simulators, instead of real users. Thus, the world model is learned to mimic user simulators. Although the simulator-trained agents are sub-optimal when applied to real users due to the discrepancy between simulators and real users, the simulation setting allows us to perform a detailed analysis of DDQ without much cost and to reproduce the experimental results easily. <ref type="bibr">4</ref> We found in our experiments that setting Z &gt; 1 im- proves the performance of all agents, but does not change the conclusion of this study: DDQ consistently outperforms DQN by a statistically significant margin. Conceptually, the optimal value of Z used in planning is different from that in direct reinforcement learning, and should vary according to the quality of the world model. The better the world model is, the more aggressive update (thus bigger Z) is being used in planning. We leave it to future work to investigate how to optimize Z for planning in DDQ.   User Simulator We adapted a publicly avail- able user simulator ( <ref type="bibr" target="#b10">Li et al., 2016b</ref>) to the task- completion dialogue setting. During training, the simulator provides the agent with a simulated user response in each dialogue turn and a reward sig- nal at the end of the dialogue. A dialogue is considered successful only when a movie ticket is booked successfully and when the information provided by the agent satisfies all the user's con- straints. At the end of each dialogue, the agent receives a positive reward of 2 * L for success, or a negative reward of −L for failure, where L is the maximum number of turns in each dialogue, and is set to 40 in our experiments. Furthermore, in each turn, the agent receives a reward of −1, so that shorter dialogues are encouraged. Read- ers can refer to Appendix B for details on the user simulator. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent Epoch = 100 Epoch = 200 Epoch = 300 Success Reward Turns Success Reward Turns Success Reward Turns</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The main simulation results are reported in <ref type="table">Table 1</ref> and Figures 4 and 5. For each agent, we report its results in terms of success rate, av- erage reward, and average number of turns (aver- aged over 5 repetitions of the experiments). Re- sults show that the DDQ agents consistently out- perform DQN with a statistically significant mar- gin. <ref type="figure">Figure 4</ref> shows the learning curves of differ- ent DDQ agents trained using different planning steps. Since the training of all RL agents started with RBS using the same rule-based agent, their performance in the first few epochs is very close. After that, performance improved for all values of K, but much more rapidly for larger values. Re- call that the DDQ(K) agent with K=0 is identical to the DQN agent, which does no planning but re- lies on direct reinforcement learning only. Without planning, the DQN agent took about 180 epochs (real dialogues) to reach the success rate of 50%,  <ref type="table">Table 2</ref>: The performance of different agents at training epoch = {100, 150, 200} in the human-in-the- loop experiments. The difference between the results of all agent pairs evaluated at the same epoch is statistically significant (p &lt; 0.01). (Success: success rate) and DDQ(10) took only 50 epochs. Intuitively, the optimal value of K needs to be determined by seeking the best trade-off between the quality of the world model and the amount of simulated experience that is useful for improv- ing the dialogue agent. This is a non-trivial opti- mization problem because both the dialogue agent and the world model are updated constantly during training and the optimal K needs to be adjusted accordingly. For example, we find in our experi- ments that at the early stages of training, it is fine to perform planning aggressively by using large amounts of simulated experience even though they are of low quality, but in the late stages of train- ing where the dialogue agent has been signif- icantly improved, low-quality simulated experi- ence is likely to hurt the performance. Thus, in our implementation of Algorithm 1, we use a heuris- tic <ref type="bibr">5</ref> to reduce the value of K in the late stages of training (e.g., after 150 epochs in <ref type="figure">Figure 4</ref>) to mit- igate the negative impact of low-qualify simulated experience. We leave it to future work how to op- timize the planning step size during DDQ training in a principled way. <ref type="figure" target="#fig_4">Figure 5</ref> shows that the quality of the world model has a significant impact on the agent's performance. The learning curve of DQN <ref type="formula">(10)</ref> indicates the best performance we can expect with a perfect world model. With a pre-trained world model, the performance of the DDQ agent improves more rapidly, although eventually, the DDQ and DDQ(rand-init θ M ) agents reach the same success rate after many epochs. The world model learning process is crucial to both the ef- ficiency of dialogue policy learning and the final performance of the agent. For example, in the early stages (before 60 epochs), the performances of DDQ and DDQ(fixed θ M ) remain very close to each other, but DDQ reaches a success rate almost <ref type="bibr">5</ref> The heuristic is not presented in Algorithm 1. Readers can refer to the released source code for details. 10% better than DDQ(fixed θ M ) after 400 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Human-in-the-Loop Evaluation</head><p>In this setting, five dialogue agents (i.e., DQN, DDQ(10), DDQ(10, rand-init θ M ), DDQ(5), and DDQ(5, rand-init θ M )) are trained via RL by in- teracting with real human users. In each dialogue session, one of the agents was randomly picked to converse with a user. The user was presented with a user goal sampled from the corpus, and was in- structed to converse with the agent to complete the task. The user had the choice of abandoning the task and ending the dialogue at any time, if she or he believed that the dialogue was unlikely to suc- ceed or simply because the dialogue dragged on for too many turns. In such cases, the dialogue ses- sion is considered failed. At the end of each ses- sion, the user was asked to give explicit feedback whether the dialogue succeeded (i.e., whether the movie tickets were booked with all the user con- straints satisfied). Each learning curve is trained with two runs, with each run generating 150 dia- logues (and K * 150 additional simulated dialogues when planning is applied). In total, we collected 1500 dialogue sessions for training all five agents.</p><p>The main results are presented in <ref type="table">Table 2</ref> and Simulation Sample Real User Sample movie-ticket booking user goal: { "request slots": { "constraint slots": { "ticket": "?" "numberofpeople":"2" "theater": "?"</p><p>"moviename": "deadpool" "starttime": "?"</p><p>"city": "seattle" "date": "?" } } }</p><p>movie-ticket booking user goal: { "request slots": { "constraint slots": { "ticket": "?"</p><p>"date":"this weekend" "theater": "?"</p><p>"numberofpeople": "1" "starttime": "?" "moviename": "batman" } } }   <ref type="figure" target="#fig_5">Figure 6</ref>, with each agent averaged over two in- dependent runs. The results confirm what we ob- served in the simulation experiments. The conclu- sions are summarized as below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DQN agent</head><p>• The DDQ agent significantly outperforms DQN, as demonstrated by the comparison be- tween DDQ(10) and DQN. <ref type="table" target="#tab_6">Table 3</ref> presents four example dialogues produced by two di- alogue agents interacting with simulated and human users, respectively. The DQN agent, after being trained with 100 dialogues, still behaved like a naive rule-based agent that re- quested information bit by bit in a fixed or- der. When the user did not answer the request explicitly (e.g., usr: which theater is available?), the agent failed to re- spond properly. On the other hand, with plan- ning, the DDQ agent trained with 100 real dialogues is much more robust and can com- plete 50% of user tasks successfully.</p><p>• A larger K leads to more aggressive planning and better results, as shown by DDQ(10) vs. DDQ(5).</p><p>• Pre-training world model with human con-versational data improves the learning effi- ciency and the agent's performance, as shown by DDQ(5) vs. DDQ(5, rand-init θ M ), and DDQ(10) vs. DDQ(10, rand-init θ M ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We propose a new strategy for a task-completion dialogue agent to learn its policy by interacting with real users. Compared to previous work, our agent learns in a much more efficient way, us- ing only a small number of real user interactions, which amounts to an affordable cost in many non- trivial domains. Our strategy is based on the Deep Dyna-Q (DDQ) framework where planning is in- tegrated into dialogue policy learning. The ef- fectiveness of DDQ is validated by human-in-the- loop experiments, demonstrating that a dialogue agent can efficiently adapt its policy on the fly by interacting with real users via deep RL.</p><p>One interesting topic for future research is ex- ploration in planning. We need to deal with the challenge of adapting the world model in a chang- ing environment, as exemplified by the domain ex- tension problem ( <ref type="bibr" target="#b12">Lipton et al., 2016</ref>). As pointed out by <ref type="bibr" target="#b34">Sutton and Barto (1998)</ref>, the general prob- lem here is a particular manifestation of the con- flict between exploration and exploitation. In a planning context, exploration means trying actions that may improve the world model, whereas ex- ploitation means trying to behave in the optimal way given the current model. To this end, we want the agent to explore in the environment, but not so much that the performance would be greatly de- graded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional Authors</head><p>Shang-Yu Su (National Taiwan University, Room 524, CSIE Bldg., No.</p><p>1, Sec. 4, Roo- sevelt Rd., <ref type="bibr">Taipei 10617, Taiwan.</ref> email: shangyusu.tw@gmail.com)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three strategies of learning task-completion dialogue policies via RL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the task-completion DDQ dialogue agent.</figDesc><graphic url="image-2.png" coords="3,179.86,63.26,108.80,176.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The world model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 :Figure 4 :</head><label>14</label><figDesc>Figure 4: Learning curves of the DDQ(K) agents with K = 2, 5, 10, 20. The DQN agent is identical to a DDQ(K) agent with K = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Learning curves of DQN, DDQ(10), DDQ(10, rand-init θ M ), DDQ(10, fixed θ M ), and DQN(10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Human-in-the-loop dialogue policy learning curves in four different agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Two sample dialogue sessions by DQN and DDQ(10) agents trained at epoch 100: Left: simu-
lated user experiments; Right: human-in-the-loop experiments. (agt: agent, usr: user) 

</table></figure>

			<note place="foot" n="1"> The source code of this work is available at https:// github.com/MiuLab/DDQ</note>

			<note place="foot" n="2"> In the dialogue scenario, actions are dialogue-acts, consisting of a single act and a (possibly empty) collection of (slot = value) pairs (Schatzmann et al., 2007).</note>

			<note place="foot" n="3"> In the dialogue scenario, reward is defined to measure the degree of success of a dialogue. In our experiment, for example, success corresponds to a reward of 80, failure to a reward of −40, and the agent receives a reward of −1 at each turn so as to encourage shorter dialogues.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Chris Brockett, Yun-Nung Chen, Michel Galley and Lihong Li for their in-sightful comments on the paper. We would like to acknowledge the volunteers from Microsoft Re-search for helping us with the human-in-the-loop experiments. This work was done when Baolin Peng and Shang-Yu Su were visiting Microsoft. Baolin Peng is in part supported by Innovation and Technology Fund (6904333), and General Re-search Fund of Hong Kong (12183516).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Annotation Schema</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B User Simulator</head><p>In the task-completion dialogue setting, the entire conversation is around a user goal implicitly, but the agent knows nothing about the user goal ex- plicitly and its objective is to help the user to ac- complish this goal. Generally, the definition of user goal contains two parts:</p><p>• inform slots contain a number of slot-value pairs which serve as constraints from the user.</p><p>• request slots contain a set of slots that user has no information about the values, but wants to get the values from the agent dur- ing the conversation. ticket is a default slot which always appears in the request slots part of user goal. To make the user goal more realistic, we add some constraints in the user goal: slots are split into two groups. Some of slots must appear in the user goal, we called these elements as Required slots. In the movie-booking scenario, it includes moviename, theater, starttime, date, num- berofpeople; the rest slots are Optional slots, for example, theater chain, video format etc.</p><p>We generated the user goals from the labeled dataset mentioned in Section 3.1, using two mech- anisms. One mechanism is to extract all the slots (known and unknown) from the first user turns (ex- cluding the greeting user turn) in the data, since usually the first turn contains some or all the re- quired information from user. The other mech- anism is to extract all the slots (known and un- known) that first appear in all the user turns, and then aggregate them into one user goal. We dump these user goals into a file as the user-goal database. Every time when running a dialogue, we randomly sample one user goal from this user goal database.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Subdomain modelling for dialogue management with hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06210</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards end-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaussian processes for fast policy optimisation of pomdp-based dialogue managers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">Young</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics</title>
		<meeting>the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="201" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On-line policy optimisation of spoken dialogue systems via live interaction with human subjects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Jurčíček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="312" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Continuous deep q-learning with model-based acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2829" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-domain joint semantic frame parsing using bi-directional RNN-LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 17th Annual Meeting of the International Speech Communication Association</title>
		<meeting>The 17th Annual Meeting of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modelbased reinforcement learning with an approximate, learned model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Kuvayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Yale Workshop on Adaptive and Learning Systems. Citeseer</title>
		<meeting>the Ninth Yale Workshop on Adaptive and Learning Systems. Citeseer</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning dialogue strategies within the markov decision process framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Pieraccini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
<note type="report_type">Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<idno type="arXiv">arXiv:1611.09823</idno>
		<title level="m">Dialogue learning with human-in-the-loop</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A user simulator for task-completion dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end taskcompletion neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The 8th International Joint Conference on Natural Language Processing</title>
		<meeting>the The 8th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="733" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05081</idno>
		<title level="m">Efficient exploration for dialogue policy learning with bbq networks &amp; replay buffer spiking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative policy learning in end-to-end trainable task-oriented neural dialog models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting>2017 IEEE Workshop on Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prioritized sweeping: Reinforcement learning with less data and less time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher G Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="130" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03777</idno>
		<title level="m">Neural belief tracker: Data-driven dialogue state tracking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comparative study of reinforcement learning techniques on dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Papangelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adversarial advantage actor-critic model for taskcompletion dialogue policy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11277</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2221" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient learning and planning within the dyna framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Behavior</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="437" to="454" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sample efficient online learning of optimal dialogue policies with kalman temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Olivier Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthilkumar</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandramohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI ProceedingsInternational Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1878</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A survey on metrics for the evaluation of user simulations. The knowledge engineering review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Hastie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagination-augmented agents for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théophane</forename><surname>Sébastienracanì Ere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrì</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puigdomènech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5694" to="5705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Agenda-based user simulation for bootstrapping a pomdp dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2007; Companion Volume, Short Papers</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dulacarnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barreto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08810</idno>
		<title level="m">The predictron: Endto-end learning and planning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimizing dialogue management with reinforcement learning: Experiments with the njfun system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="105" to="133" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reinforcement learning with a hierarchy of abstract models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">202</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojasbarahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02689</idno>
		<title level="m">Continuously learning neural dialogue management</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On-line active reward learning for policy optimisation in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojasbarahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07669</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard S Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh international conference on machine learning</title>
		<meeting>the seventh international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Introduction to reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dyna-style planning with linear function approximation and prioritized sweeping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Csaba</forename><surname>Richard S Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alborz</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael P</forename><surname>Geramifard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.3285</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2154" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantically conditioned LSTM-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2015</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hybrid code networks: Practical and efficient end-to-end dialog control with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavosh</forename><surname>Jason D Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02560</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
