<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-Task Oriented Textual Entailment via Deep Explorations of Inter-Sentence Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CIS, LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-Task Oriented Textual Entailment via Deep Explorations of Inter-Sentence Interactions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="540" to="545"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>540</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This work deals with SCITAIL, a natural entailment challenge derived from a multi-choice question answering problem. The premises and hypotheses in SCITAIL were generated with no awareness of each other, and did not specifically aim at the entailment task. This makes it more challenging than other entailment data sets and more directly useful to the end-task-question answering. We propose DEISTE (deep explorations of inter-sentence interactions for textual entailment) for this entail-ment task. Given word-to-word interactions between the premise-hypothesis pair (P , H), DEISTE consists of: (i) a parameter-dynamic convolution to make important words in P and H play a dominant role in learnt representations; and (ii) a position-aware attentive convolution to encode the representation and position information of the aligned word pairs. Experiments show that DEISTE gets ≈5% improvement over prior state of the art and that the pretrained DEISTE on SCI-TAIL generalizes well on RTE-5. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Textual entailment (TE) is a fundamental prob- lem in natural language understanding and has been studied intensively recently using multiple benchmarks -PASCAL RTE challenges ( <ref type="bibr" target="#b5">Dagan et al., 2006</ref><ref type="bibr" target="#b6">Dagan et al., , 2013</ref>, Paragraph-Headline ( <ref type="bibr" target="#b1">Burger and Ferro, 2005</ref>), SICK ( <ref type="bibr" target="#b13">Marelli et al., 2014</ref>) and SNLI <ref type="bibr" target="#b0">(Bowman et al., 2015</ref>). In particular, SNLI -while much easier than earlier datasets <ref type="bibr">1</ref> https://github.com/yinwenpeng/SciTail Premise P Pluto rotates once on its axis every 6.39 Earth days. 0 Once per day, the earth rotates about its axis. 1 It rotates on its axis once every 60 Earth days. 0 Earth orbits Sun, and rotates once per day about axis. 1 <ref type="table">Table 1</ref>: Examples of four premises for the hy- pothesis "Earth rotates on its axis once times in one day" in SCITAIL dataset. Right column (la- bel): "1" means entail, "0" otherwise.</p><p>-has generated much work based on deep neu- ral networks due to its large size. However, these benchmarks were mostly derived independently of any NLP problems. <ref type="bibr">2</ref> Therefore, the premise- hypothesis pairs were composed under the con- straint of predefined rules and the language skills of humans. As a result, while top-performing sys- tems push forward the state-of-the-art, they do not necessarily learn to support language inferences that emerge commonly and naturally in real NLP problems.</p><p>In this work, we study SCITAIL ( <ref type="bibr" target="#b11">Khot et al., 2018</ref>), an end-task oriented challenging entail- ment benchmark. SCITAIL is reformatted from a multi-choice question answering problem. All hypotheses H were obtained by rewriting (ques- tion, correct answer) pairs; all premises P are rel- evant web sentences collected by an Information retrieval (IR) method; then (P , H) pairs are an- notated via crowdsourcing. <ref type="table">Table 1</ref> shows exam- ples. By this construction, a substantial perfor- mance gain on SCITAIL can be turned into bet- ter QA performance ( <ref type="bibr" target="#b11">Khot et al., 2018)</ref>. <ref type="bibr" target="#b11">Khot et al. (2018)</ref> report that SCITAIL challenges neu- ral entailment models that show outstanding per- formance on SNLI, e.g., Decomposable Attention Model ( <ref type="bibr" target="#b15">Parikh et al., 2016)</ref> and Enhanced LSTM ( <ref type="bibr" target="#b3">Chen et al., 2017)</ref>.</p><p>We propose DEISTE for SCITAIL. Given word-to-word inter-sentence interactions between   For any word in one of (P , H), how to find the best aligned word in the other sentence, so that we know their connection is indicative of the final decision. (c) For a window of words in P or H, whether the locations of their best aligned words in the other sentence provides clues. As <ref type="figure" target="#fig_0">Figure 1</ref> il- lustrates, the premise "in this incident, the cop (C) shot (S) the thief (T )" is more likely to entail the hypothesis "</p><formula xml:id="formula_0">ˆ C . . . ˆ S . . . ˆ T " than " ˆ T . . . ˆ S . . . ˆ C" wherê</formula><p>X is the word that best matches X.</p><p>Our model DEISTE is implemented in convo- lutional neural architecture ( <ref type="bibr" target="#b12">LeCun et al., 1998</ref>). Specifically, DEISTE consists of (i) a parameter- dynamic convolution for exploration strategy (a) given above; and (ii) a position-aware atten- tive convolution for exploration strategies (b) and (c). In experiments, DEISTE outperforms prior top systems by ≈5%. Perhaps even more interestingly, the pretrained model over SCI- TAIL generalizes well on RTE-5.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>To start, a sentence S (S ∈ {P, H}) is represented as a sequence of n s hidden states, e.g.,</p><formula xml:id="formula_1">p i , h i ∈ R d (i = 1, 2, . . . , |n p/h |), forming a feature map S ∈ R d×|ns| ,</formula><p>where d is the dimensionality of hid- den states. <ref type="figure" target="#fig_1">Figure 2</ref> depicts the basic principle of DEISTE in modeling premise-hypothesis pair (P , H) with feature maps P and H, respectively.</p><p>First, P and H have fine-grained interactions I ∈ R np×n h by comparing any pair of (p i ,h j ):</p><formula xml:id="formula_2">I[i, j] = cosine(p i , h j )<label>(1)</label></formula><p>We now elaborate DEISTE's exploration strategies (a), (b) and (c) of the interaction results I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Parameter-dynamic convolution</head><p>Intuitively, important words should be expressed more intensively than other words in the learnt representation of a sentence. However, the impor- tance of words within a specific sentence might not depend on the sentence itself. E.g., <ref type="bibr" target="#b21">Yin and Schütze (2017b)</ref> found that in question-aware an- swer sentence selection, words well matched are more important; while in textual entailment, words hardly matched are more important. In this work, we try to make the semantics of those important words dominate in the output rep- resentations of a convolution encoder.</p><p>Given a local window of hidden states in the feature map P, e.g., three adjacent ones p i−1 , p i and p i+1 , a conventional convolution learns a higher-level representation r for this trigram:</p><formula xml:id="formula_3">r = tanh(W · [p i−1 , p i , p i+1 ] + b) (2)</formula><p>where W ∈ R d×3d and b ∈ R d . For simplicity, we neglect the bias term b and split the multiplication of big matrices -W · [p i−1 , p i , p i+1 ] -into three parts, then r can be formulated as:</p><formula xml:id="formula_4">r = tanh(W −1 · p i−1 ⊕ W 0 · p i ⊕ W +1 · p i+1 ) = tanh(ˆ p i−1 ⊕ ˆ p i ⊕ ˆ p i+1 )</formula><p>where ⊕ means element-wise addition; W −1 , W 0 , W +1 ∈ R d×d , and their concatenation equals to the W in Equation 2; ˆ p i is set to be W 0 · p i , sô p i can be seen as the projection of p i in a new space by parameters W 0 ; finally the three projected outputs contribute equally in the addition:</p><formula xml:id="formula_5">ˆ p i−1 ⊕ ˆ p i ⊕ ˆ p i+1 . The convolution encoder shares parameters [W −1 , W 0 , W +1 ]</formula><p>in all trigrams, so we cannot expect those parameters to express the importance ofˆpofˆ ofˆp i−1 , ˆ p i orˆporˆ orˆp i+1 in the output representation r. Instead, we formulate the idea as follows:</p><formula xml:id="formula_6">m i = tanh(α i−1 ˆ p i−1 ⊕ α i ˆ p i ⊕ α i+1ˆpi+1ˆ i+1ˆp i+1 )</formula><p>in which the three scalars α i−1 , α i and α i+1 indi- cate the importance scores ofˆpofˆ ofˆp i−1 , ˆ p i andˆpandˆ andˆp i+1 respectively. In our work, we adopt:</p><formula xml:id="formula_7">α i = 1.0 1.0 + max(I[i, :])<label>(3)</label></formula><p>Since</p><formula xml:id="formula_8">α i ˆ p i = α i W 0 · p i = W 0,i · p i</formula><p>, we notice that the original shared parameter W 0 is mapped to a dynamic parameter W 0,i , which is specific to the input p i . We refer to this as parameter- dynamic convolution, which enables our system DEISTE to highlight important words in higher- level representations.</p><p>Finally, a max-pooling layer is stacked over {m i } to get the representation for the pair (P , H), denoted as r dyn .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Position-aware attentive convolution</head><p>Our position-aware attentive convolution, shown in <ref type="figure" target="#fig_4">Figure 3</ref>, aims to encode the representations as well as the positions of the best word alignments.</p><p>Representation. Given the interaction scores in I, the representatioñ p i of all soft matches for hidden state p i in P is the weighted average of all hidden states in H: For p i , we first retrieve the index x i of the best- matched word in H by:</p><formula xml:id="formula_9">˜ p i = j softmax(I[i, :]) j · h j<label>(4)</label></formula><note type="other">methods dev test Majority Class 50.4 60.4 Hypothesis only 66.9 65.1 Premise only 72.6 73.4 NGram model 65.0 70.6 ESIM-600D 70.5 70.6 Decomp-Att 75.4 72.3 DGEM 79.6 77.3 AttentiveConvNet 79.3 78.1 DEISTE 82.4 82.1 w/o dyn-conv 80.2 79.1 w/o representation 76.3 74.9 w/o position 82.1 81.3</note><formula xml:id="formula_10">x i = argmax(I[i, :])<label>(5)</label></formula><p>then embed the concrete {x i } into randomly- initialized continuous space:</p><formula xml:id="formula_11">z i = M[x i ]<label>(6)</label></formula><p>where M ∈ R n h ×dm ; n h is the length of H; d m is the dimensionality of position embeddings. Now </p><formula xml:id="formula_12">n i = tanh(W · [c i−1 , c i , c i+1 , ˜ p i ] + b) (7)</formula><p>As <ref type="figure" target="#fig_4">Figure 3</ref> shows, the position-aware attentive convolution finally stacks a standard max-pooling layer over {n i } to get the representation for the pair (P , H), denoted as r pos .</p><p>Overall, our DEISTE will generate a represen- tation r dyn through the parameter-dynamic convo- lution, and generate a representation r pos through the position-aware attentive convolution. Finally the concatenation -[r dyn , r pos ] -is fed to a logis- tic regression classifier.   <ref type="table">Table 3</ref>: DEISTE vs. baselines on SNLI. DEISTE SCITAIL has exactly the same system layout and hyperparameters as the one reported on SCI- TAIL in <ref type="table" target="#tab_1">Table 2</ref>; DEISTE tune : tune hyperparame- ters on SNLI dev. State-of-the-art refers to ( <ref type="bibr" target="#b16">Peters et al., 2018)</ref>. Ensemble results are not considered.  <ref type="bibr" target="#b20">Yin and Schütze, 2017a</ref>): Our top-performing textual entailment system on SNLI dataset, equipped with RNN-style attention mech- anism in convolution. <ref type="bibr">4</ref> In addition, to check if SCITAIL can be eas- ily resolved by features from only premises or hypotheses (like the problem of SNLI shown by <ref type="bibr" target="#b9">Gururangan et al. (2018)</ref>), we put a vanilla CNN (convolution&amp;max-pooling) over merely hypothe- sis or premise to derive the pair label. <ref type="table" target="#tab_1">Table 2</ref> presents results on SCITAIL. (i) Our model DEISTE has a big improvement (∼ 5%) over DGEM, the best baseline. Interestingly, At- tentiveConvNet performs very competitively, sur- passing DGEM by 0.8% on test. These two results show the effectiveness of attentive convolution. DEISTE, equipped with a parameter-dynamic convolution and a more advanced position-aware attentive convolution, clearly gets a big plus. (ii) The ablation shows that all three aspects we ex- plore from the inter-sentence interactions con- tribute; "position" encoding is less important than "dyn-conv" and "representation". Without "repre- sentation", the system performs much worse. This is in line with the result of AttentiveConvNet base- line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>To further study the systems and datasets, <ref type="table">Table  3</ref> gives performance of DEISTE and baselines on SNLI. We see that DEISTE gets competitive per- formance on SNLI.</p><p>Comparing <ref type="table" target="#tab_1">Tables 2 and 3</ref>, the baselines "hy- pothesis only" and "premise only" show anal- ogous while different phenomena between SCI- TAIL and SNLI. On one hand, both SNLI and SCITAIL can get a relatively high number by looking at only one of {premise, hypothesis} - "premise only" gets 73.4% accuracy on SCITAIL, even higher than two more complicated baselines (ESIM-600D and Decomp-Att), and "hypothesis only" gets 68.7% accuracy on SNLI which is more than 30% higher than the "majority" and "premise only" baselines. Notice the contrast: SNLI "prefers" hypothesis, SCITAIL "prefers" premise. For SNLI, this is not surprising as the crowd-workers tend to construct the hypotheses in SNLI by some regular rules ( <ref type="bibr" target="#b9">Gururangan et al., 2018)</ref>. The phenomenon in SCITAIL is left to ex- plore in future work.</p><p>Error Analysis. <ref type="table" target="#tab_3">Table 4</ref> gives examples of er- rors. We explain them as follows.</p><p>Language conventions: The pair #1 uses dash "-" to indicate a definition sentence for "Front"; The pair #2 has "A (or B)" to denote the equiva- lence between A and B. This challenge is expected to be handled by rules.</p><p>Ambiguity: The pair #3 looks like having a sim- ilar challenge with the pair #2. We guess the anno- tators treat "· · · a vertebral column or backbone" and " · · · the backbone (or vertebral column)" as the same convention, which may be debatable.</p><p>Complex discourse relation: The premise in # (Premise P , Hypothesis H) Pair G/P Challenge 1 (P ) Front -The boundary between two different air masses. 1/0 language conventions (H) In weather terms, the boundary between two air masses is called front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>(P ) . . . the notochord forms the backbone (or vertebral column). 1/0 language conventions (H) Backbone is another name for the vertebral column.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>(P ) · · · animals with a vertebral column or backbone and animals without one. 1/0 ambiguity (H) Backbone is another name for the vertebral column.   the pair #4 has an "or" structure. In the pair #5, "a molecule made of · · · " defines the con- cept "Ethane" instead of the "hydrocarbon". Both cases require the model to be able to comprehend the discourse relation. Knowledge beyond text: The main challenge in the pair #6 is to distinguish between "weight" and "force", which requires more physical knowledge that is beyond the text described here and beyond the expressivity of word embeddings.</p><p>Transfer to RTE-5. One main motivation of exploring this SCITAIL problem is that this is an end-task oriented TE task. A natural question thus is how well the trained model can be trans- ferred to other end-task oriented TE tasks. In <ref type="table" target="#tab_4">Table 5</ref>, we take the models pretrained on SCI- TAIL and SNLI and test them on RTE-5. Clearly, the model pretrained on SNLI has not learned any- thing useful for RTE-5 -its performance of 46.0% is even worse than the majority baseline. The model pretrained on SCITAIL, in contrast, demon- strates much more promising generalization per- formance: 60.2% vs. 46.0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Learning automatically inter-sentence word-to- word interactions or alignments was first stud- ied in recurrent neural networks <ref type="bibr" target="#b8">(Elman, 1990)</ref>. <ref type="bibr" target="#b17">Rocktäschel et al. (2016)</ref> employ neural word-to- word attention for SNLI task. <ref type="bibr" target="#b18">Wang and Jiang (2016)</ref> propose match-LSTM, an extension of the attention mechanism in <ref type="bibr" target="#b17">(Rocktäschel et al., 2016)</ref>, by more fine-grained matching and accumulation. <ref type="bibr" target="#b4">Cheng et al. (2016)</ref> present a new LSTM equipped with a memory tape. Other work following this attentive matching idea includes Bilateral Multi- Perspective Matching model ( <ref type="bibr" target="#b19">Wang et al., 2017)</ref>, Enhanced LSTM ( <ref type="bibr" target="#b2">Chen et al., 2016)</ref> etc.</p><p>In addition, convolutional neural networks ( <ref type="bibr" target="#b12">LeCun et al., 1998)</ref>, equipped with attention mecha- nisms, also perform competitively in TE. <ref type="bibr" target="#b22">Yin et al. (2016)</ref> implement the attention in pooling phase so that important hidden states will be pooled with higher probabilities. <ref type="bibr" target="#b20">Yin and Schütze (2017a)</ref> fur- ther develop the attention idea in CNNs, so that a RNN-style attention mechanism is integrated into the convolution filters. This is similar with our position-aware attentive convolution. We instead explored a way to make use of position informa- tion of alignments to do reasoning.</p><p>Attention mechanisms in both RNNs and CNNs make use of sentence interactions. Our work achieves a deep exploration of those interactions, in order to guide representation learning in TE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>This work proposed DEISTE to deal with an end- task oriented textual entailment task -SCITAIL. Our model enables a comprehensive utilization of inter-sentence interactions. DEISTE outperforms competitive systems by big margins.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The motivation of considering alignment positions in TE. The same color in (premise, hypothesis) means the two words are best aligned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The basic principles of DEISTE in modeling the pair (P , H) the pair (P , H), DEISTE pursues three deep exploration strategies of these interactions. (a) How to express the importance of a word, and let it play a dominant role in learnt representations. (b) For any word in one of (P , H), how to find the best aligned word in the other sentence, so that we know their connection is indicative of the final decision. (c) For a window of words in P or H, whether the locations of their best aligned words in the other sentence provides clues. As Figure 1 illustrates, the premise "in this incident, the cop (C) shot (S) the thief (T )" is more likely to entail the hypothesis " ˆ C. .. ˆ S. .. ˆ T " than " ˆ T. .. ˆ S. .. ˆ C" wherê X is the word that best matches X.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Position-aware attentive convolution in modeling the pair (P , H)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, the three positions [i − 1, i, i + 1] in P concatenate vector-wisely original hidden states [p i−1 , p i , p i+1 ] with position embeddings [z i−1 , z i , z i+1 ], getting a new sequence of hidden states: [c i−1 , c i , c i+1 ]. As a result, a position i in P has hidden state c i , left context c i−1 , right context c i+1 and the representation of soft-aligned words in H, i.e., ˜ p i . Then a convolution works at posi- tion i in P as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>methods</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Dataset.</head><label></label><figDesc>SCITAIL 3 (Khot et al., 2018) is for tex- tual entailment in binary classification: entailment or neutral. Accuracy is reported. Training setup. All words are initialized by 300D Word2Vec (Mikolov et al., 2013) embed- dings, and are fine-tuned during training. The whole system is trained by AdaGrad (Duchi et al., 2011). Other hyperparameter values include: learning rate 0.01, d m =50 for position embeddings M, hidden size 300, batch size 50, filter width 3. Baselines. (i) Decomposable Attention Model (Decomp-Att) (Parikh et al., 2016): Develop at- tention mechanisms to decompose the problem into subproblems to solve in parallel. (ii) En- hanced LSTM (ESIM) (Chen et al., 2017): En- hance LSTM by encoding syntax and semantics from parsing information. (iii) Ngram Overlap: An overlap baseline, considering unigrams, one- skip bigrams and one-skip trigrams. (iv) DGEM (Khot et al., 2018): A decomposed graph entail- ment model, the current state-of-the-art. (v) At- tentiveConvNet (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4 (</head><label>4</label><figDesc>P ) Heterotrophs get energy and carbon from living plants or animals ( consumers ) or from dead organic matter ( decomposers ). 0/1 discourse relation (H) Mushrooms get their energy from decomposing dead organisms. 5 (P ) Ethane is a simple hydrocarbon, a molecule made of two carbon and six hydrogen atoms. 0/1 discourse relation (H) Hydrocarbons are made of one carbon and four hydrogen atoms. 6 (P ) . . . the SI unit. . . for force is the Newton (N) and is defined as (kg·m/s −2 ). 0/1 beyond text (H) Newton (N) is the SI unit for weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : DEISTE vs. baselines on SCITAIL</head><label>2</label><figDesc></figDesc><table>Position. For a trigram [p i−1 , p i , p i+1 ] in P , 
knowing where its best-matched words are located 
in H is helpful in TE, as discussed in Section 1. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Error cases of DEISTE in SCITAIL. "· · · ": truncated text. "G/P": gold/predicted label. 

dev test 
Majority baseline 50.0 50.0 
State-of-the-art 
-
73.5 
training data 
SNLI 
47.1 46.0 
SCITAIL 
60.5 60.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Train on different TE datasets, test accu-
racy on two-way RTE-5. State-of-the-art refers to 
(Iftene and Moruz, 2009) 

</table></figure>

			<note place="foot" n="2"> RTE-{5,6,7} is an exception to this rule.</note>

			<note place="foot" n="3"> Please refer to (Khot et al., 2018) for more details. 4 https://github.com/yinwenpeng/Attentive Convolution</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating an entailment corpus from news headlines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</title>
		<meeting>the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Enhancing and combining sequential and tree LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<idno>CoRR abs/1609.06038</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="551" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PASCAL Machine Learning Challenges Workshop</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Recognizing Textual Entailment: Models and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zanzoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">UAIC participation at RTE5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Iftene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mihai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TAC</title>
		<meeting>TAC</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SciTail: A textual entailment dataset from science question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR abs/1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Kočisk`y, and Phil Blunsom</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4144" to="4150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno>abs/1710.00519</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Taskspecific attentive pooling of phrase alignments contributes to sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="699" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ABCNN: attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
