<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
							<email>dagan@cs.biu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat-Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1200" to="1211"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1200</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Revealing the implicit semantic relation between the constituents of a noun-compound is important for many NLP applications. It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations. Most existing paraphrasing methods lack the ability to generalize, and have a hard time interpreting infrequent or new noun-compounds. We propose a neural model that generalizes better by representing paraphrases in a continuous space, generalizing for both unseen noun-compounds and rare paraphrases. Our model helps improving performance on both the noun-compound paraphrasing and classification tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Noun-compounds hold an implicit semantic rela- tion between their constituents. For example, a 'birthday cake' is a cake eaten on a birthday, while 'apple cake' is a cake made of apples. Interpreting noun-compounds by explicating the relationship is beneficial for many natural language understand- ing tasks, especially given the prevalence of noun- compounds in English <ref type="bibr" target="#b17">(Nakov, 2013)</ref>.</p><p>The interpretation of noun-compounds has been addressed in the literature either by classifying them to a fixed inventory of ontological relation- ships (e.g. <ref type="bibr" target="#b19">Nastase and Szpakowicz, 2003)</ref> or by generating various free text paraphrases that de- scribe the relation in a more expressive manner (e.g. <ref type="bibr" target="#b9">Hendrickx et al., 2013)</ref>.</p><p>Methods dedicated to paraphrasing noun- compounds usually rely on corpus co-occurrences of the compound's constituents as a source of ex- plicit relation paraphrases (e.g. <ref type="bibr">Wubben, 2010;</ref><ref type="bibr">Versley, 2013)</ref>. Such methods are unable to gen- eralize for unseen noun-compounds. Yet, most noun-compounds are very infrequent in text <ref type="bibr" target="#b12">(Kim and Baldwin, 2007)</ref>, and humans easily interpret the meaning of a new noun-compound by general- izing existing knowledge. For example, consider interpreting parsley cake as a cake made of pars- ley vs. resignation cake as a cake eaten to cele- brate quitting an unpleasant job.</p><p>We follow the paraphrasing approach and pro- pose a semi-supervised model for paraphras- ing noun-compounds. Differently from previ- ous methods, we train the model to predict ei- ther a paraphrase expressing the semantic rela- tion of a noun-compound (predicting '[w 2 ] made of [w 1 ]' given 'apple cake'), or a missing con- stituent given a combination of paraphrase and noun-compound (predicting 'apple' given 'cake made of [w 1 ]'). Constituents and paraphrase tem- plates are represented as continuous vectors, and semantically-similar paraphrase templates are em- bedded in proximity, enabling better generaliza- tion. Interpreting 'parsley cake' effectively re- duces to identifying paraphrase templates whose "selectional preferences" ( <ref type="bibr" target="#b25">Pantel et al., 2007</ref>) on each constituent fit 'parsley <ref type="bibr">' and 'cake'.</ref> A qualitative analysis of the model shows that the top ranked paraphrases retrieved for each noun-compound are plausible even when the con- stituents never co-occur (Section 4). We evalu- ate our model on both the paraphrasing and the classification tasks (Section 5). On both tasks, the model's ability to generalize leads to improved performance in challenging evaluation settings. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Noun-compound Classification</head><p>Noun-compound classification is the task con- cerned with automatically determining the seman- tic relation that holds between the constituents of a noun-compound, taken from a set of pre-defined relations.</p><p>Early work on the task leveraged information derived from lexical resources and corpora (e.g. <ref type="bibr" target="#b7">Girju, 2007;</ref><ref type="bibr" target="#b23">´ O Séaghdha and Copestake, 2009;</ref><ref type="bibr" target="#b35">Tratz and Hovy, 2010)</ref>. More recent work broke the task into two steps: in the first step, a noun- compound representation is learned from the dis- tributional representation of the constituent words (e.g. <ref type="bibr" target="#b16">Mitchell and Lapata, 2010;</ref><ref type="bibr">Zanzotto et al., 2010;</ref><ref type="bibr" target="#b32">Socher et al., 2012</ref>). In the second step, the noun-compound representations are used as fea- ture vectors for classification (e.g. <ref type="bibr" target="#b4">Dima and Hinrichs, 2015;</ref><ref type="bibr" target="#b3">Dima, 2016)</ref>. The datasets for this task differ in size, num- ber of relations and granularity level (e.g. <ref type="bibr" target="#b19">Nastase and Szpakowicz, 2003;</ref><ref type="bibr" target="#b12">Kim and Baldwin, 2007;</ref><ref type="bibr" target="#b35">Tratz and Hovy, 2010)</ref>. The decision on the re- lation inventory is somewhat arbitrary, and sub- sequently, the inter-annotator agreement is rela- tively low <ref type="bibr" target="#b12">(Kim and Baldwin, 2007)</ref>. Specifi- cally, a noun-compound may fit into more than one relation: for instance, in Tratz (2011), busi- ness zone is labeled as CONTAINED (zone con- tains business), although it could also be labeled as PURPOSE (zone whose purpose is business).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Noun-compound Paraphrasing</head><p>As an alternative to the strict classification to pre- defined relation classes, <ref type="bibr" target="#b18">Nakov and Hearst (2006)</ref> suggested that the semantics of a noun-compound could be expressed with multiple prepositional and verbal paraphrases. For example, apple cake is a cake from, made of, or which contains apples.</p><p>The suggestion was embraced and resulted in two SemEval tasks. SemEval 2010 task 9 ( <ref type="bibr" target="#b2">Butnariu et al., 2009</ref>) provided a list of plau- sible human-written paraphrases for each noun- compound, and systems had to rank them with the goal of high correlation with human judgments. In <ref type="bibr">task 4 (Hendrickx et al., 2013</ref>, systems were expected to provide a ranked list of paraphrases extracted from free text.</p><p>Various approaches were proposed for this task. Most approaches start with a pre-processing step of extracting joint occurrences of the constituents from a corpus to generate a list of candidate para- phrases. Unsupervised methods apply information extraction techniques to find and rank the most meaningful paraphrases ( <ref type="bibr" target="#b11">Kim and Nakov, 2011;</ref><ref type="bibr">Xavier and Lima, 2014;</ref><ref type="bibr" target="#b26">Pasca, 2015;</ref><ref type="bibr" target="#b27">Pavlick and Pasca, 2017)</ref>, while supervised approaches learn to rank paraphrases using various features such as co-occurrence counts <ref type="bibr">(Wubben, 2010;</ref><ref type="bibr" target="#b14">Li et al., 2010;</ref><ref type="bibr" target="#b33">Surtani et al., 2013;</ref><ref type="bibr">Versley, 2013)</ref> or the distributional representations of the noun- compounds (Van de <ref type="bibr" target="#b36">Cruys et al., 2013)</ref>.</p><p>One of the challenges of this approach is the ability to generalize. If one assumes that suffi- cient paraphrases for all noun-compounds appear in the corpus, the problem reduces to ranking the existing paraphrases. It is more likely, however, that some noun-compounds do not have any para- phrases in the corpus or have just a few. The ap- proach of Van de Cruys et al. (2013) somewhat generalizes for unseen noun-compounds. They represented each noun-compound using a compo- sitional distributional vector <ref type="bibr" target="#b16">(Mitchell and Lapata, 2010)</ref> and used it to predict paraphrases from the corpus. Similar noun-compounds are expected to have similar distributional representations and therefore yield the same paraphrases. For exam- ple, if the corpus does not contain paraphrases for plastic spoon, the model may predict the para- phrases of a similar compound such as steel knife.</p><p>In terms of sharing information between semantically-similar paraphrases, <ref type="bibr" target="#b22">Nulty and Costello (2010)</ref> and <ref type="bibr" target="#b33">Surtani et al. (2013)</ref> learned "is-a" relations between paraphrases from the co-occurrences of various paraphrases with each other. For example, the specific '[w 2 ] extracted from [w 1 ]' template (e.g. in the context of olive oil) generalizes to '[w 2 ] made from [w 1 ]'. One of the drawbacks of these systems is that they favor more frequent paraphrases, which may co-occur with a wide variety of more specific paraphrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Noun-compounds in other Tasks</head><p>Noun-compound paraphrasing may be considered as a subtask of the general paraphrasing task, whose goal is to generate, given a text fragment, additional texts with the same meaning. How- ever, general paraphrasing methods do not guar- antee to explicate implicit information conveyed in the original text. Moreover, the most notable source for extracting paraphrases is multiple trans- lations of the same text (Barzilay and McKeown,  2001; <ref type="bibr" target="#b6">Ganitkevitch et al., 2013;</ref><ref type="bibr" target="#b15">Mallinson et al., 2017)</ref>. If a certain concept can be described by an English noun-compound, it is unlikely that a translator chose to translate its foreign language equivalent to an explicit paraphrase instead. Another related task is Open Information Ex- traction ( <ref type="bibr" target="#b5">Etzioni et al., 2008)</ref>, whose goal is to ex- tract relational tuples from text. Most system fo- cus on extracting verb-mediated relations, and the few exceptions that addressed noun-compounds provided partial solutions. <ref type="bibr" target="#b24">Pal and Mausam (2016)</ref> focused on segmenting multi-word noun- compounds and assumed an is-a relation between the parts, as extracting (Francis Collins, is, NIH director) from "NIH director Francis Collins". Xavier and Lima (2014) enriched the corpus with compound definitions from online dictionaries, for example, interpreting oil industry as (industry, produces and delivers, oil) based on the Word- Net definition "industry that produces and delivers oil". This method is very limited as it can only interpret noun-compounds with dictionary entries, while the majority of English noun-compounds don't have them (Nakov, 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Paraphrasing Model</head><p>As opposed to previous approaches, that focus on predicting a paraphrase template for a given noun- compound, we reformulate the task as a multi- task learning problem (Section 3.1), and train the model to also predict a missing constituent given the paraphrase template and the other constituent. Our model is semi-supervised, and it expects as input a set of noun-compounds and a set of con- strained part-of-speech tag-based templates that make valid prepositional and verbal paraphrases. Section 3.2 details the creation of training data, and Section 3.3 describes the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-task Reformulation</head><p>Each training example consists of two constituents and a paraphrase (w 2 , p, w 1 ), and we train the model on 3 subtasks: (1) predict p given w 1 and w 2 , (2) predict w 1 given p and w 2 , and (3) predict w 2 given p and w 1 . <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates the pre- dictions for subtasks (1) (right) and (2) (left) for the training example (cake, made of, apple). Ef- fectively, the model is trained to answer questions such as "what can cake be made of?", "what can be made of apple?", and "what are the possible re- lationships between cake and apple?".</p><p>The multi-task reformulation helps learning bet- ter representations for paraphrase templates, by embedding semantically-similar paraphrases in proximity. Similarity between paraphrases stems either from lexical similarity and overlap between the paraphrases (e.g. 'is made of' and 'made of'), or from shared constituents, e.g.</p><formula xml:id="formula_0">'[w 2 ] involved in [w 1 ]' and '[w 2 ] in [w 1 ] industry' can share [w 1 ] = insurance and [w 2 ] = company.</formula><p>This allows the model to predict a correct paraphrase for a given noun-compound, even when the constituents do not occur with that paraphrase in the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Data</head><p>We collect a training set of (w 2 , p, w 1 , s) exam- ples, where w 1 and w 2 are constituents of a noun- compound w 1 w 2 , p is a templated paraphrase, and s is the score assigned to the training instance. <ref type="bibr">2</ref> We use the 19,491 noun-compounds found in the SemEval tasks datasets ( <ref type="bibr" target="#b2">Butnariu et al., 2009;</ref><ref type="bibr" target="#b9">Hendrickx et al., 2013)</ref> and in <ref type="bibr" target="#b34">Tratz (2011)</ref>. To ex- tract patterns of part-of-speech tags that can form noun-compound paraphrases, such as '[w 2 ] VERB PREP [w 1 ]', we use the SemEval task training data, but we do not use the lexical information in the gold paraphrases.</p><p>Corpus. Similarly to previous noun-compound paraphrasing approaches, we use the Google N- gram corpus <ref type="bibr" target="#b1">(Brants and Franz, 2006</ref>) as a source of paraphrases <ref type="bibr">(Wubben, 2010;</ref><ref type="bibr" target="#b14">Li et al., 2010;</ref><ref type="bibr" target="#b33">Surtani et al., 2013;</ref><ref type="bibr">Versley, 2013)</ref>. The cor- pus consists of sequences of n terms (for n ∈ {3, 4, 5}) that occur more than 40 times on the web. We search for n-grams following the ex- tracted patterns and containing w 1 and w 2 's lem- mas for some noun-compound in the set. We re- move punctuation, adjectives, adverbs and some determiners to unite similar paraphrases. For ex- ample, from the 5-gram 'cake made of sweet ap- ples' we extract the training example (cake, made of, apple). We keep only paraphrases that occurred at least 5 times, resulting in 136,609 instances.</p><p>Weighting. Each n-gram in the corpus is accom- panied with its frequency, which we use to assign scores to the different paraphrases. For instance, 'cake of apples' may also appear in the corpus, al- though with lower frequency than 'cake from ap- ples'. As also noted by <ref type="bibr" target="#b33">Surtani et al. (2013)</ref>, the shortcoming of such a weighting mechanism is that it prefers shorter paraphrases, which are much more common in the corpus (e.g. count('cake made of apples') count('cake of apples')). We overcome this by normalizing the frequencies for each paraphrase length, creating a distribution of paraphrases in a given length.</p><p>Negative Samples. We add 1% of negative sam- ples by selecting random corpus words w 1 and w 2 that do not co-occur, and adding an exam- ple (w 2 , [w 2 ] is unrelated to [w 1 ], w 1 , s n ), for some predefined negative samples score s n . Sim- ilarly, for a word w i that did not occur in a para- phrase p we add (w i , p, UNK, s n ) or (UNK, p, w i , s n ), where UNK is the unknown word. This may help the model deal with non-compositional noun-compounds, where w 1 and w 2 are unrelated, rather than forcibly predicting some relation be- tween them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model</head><p>For a training instance (w 2 , p, w 1 , s), we predict each item given the encoding of the other two.</p><p>Encoding. We use the 100-dimensional pre- trained GloVe embeddings ( <ref type="bibr" target="#b29">Pennington et al., 2014</ref>), which are fixed during training. In addi- tion, we learn embeddings for the special words For a missing component</p><formula xml:id="formula_1">x ∈ {[p], [w 1 ], [w 2 ]</formula><p>} surrounded by the sequences of words v 1:i−1 and v i+1:n , we encode the sequence using a bidirec- tional long-short term memory (bi-LSTM) net- work ( <ref type="bibr" target="#b8">Graves and Schmidhuber, 2005)</ref>, and take the ith output vector as representing the missing component:</p><formula xml:id="formula_2">bLS(v 1:i , x, v i+1:n ) i .</formula><p>In bi-LSTMs, each output vector is a concate- nation of the outputs of the forward and backward LSTMs, so the output vector is expected to con- tain information on valid substitutions both with respect to the previous words v 1:i−1 and the sub- sequent words v i+1:n .</p><p>Prediction. We predict a distribution of the vo- cabulary of the missing component, i.e. to predict w 1 correctly we need to predict its index in the word vocabulary V w , while the prediction of p is from the vocabulary of paraphrases in the training set, V p . We predict the following distributions:</p><formula xml:id="formula_3">ˆ p = softmax(W p · bLS( w 2 , [p], w 1 ) 2 ) ˆ w 1 = softmax(W w · bLS( w 2 , p 1:n , [w 1 ]) n+1 ) ˆ w 2 = softmax(W w · bLS([w 2 ], p 1:n , w 1 ) 1 )<label>(1)</label></formula><p>where W w ∈ R |Vw|×2d , W p ∈ R |Vp|×2d , and d is the embeddings dimension. During training, we compute cross-entropy loss for each subtask using the gold item and the pre- diction, sum up the losses, and weight them by the instance score. During inference, we predict the missing components by picking the best scoring index in each distribution: <ref type="bibr">3</ref> ˆ p i = argmax(ˆ p)</p><formula xml:id="formula_4">ˆ w 1i = argmax( ˆ w 1 ) ˆ w 2i = argmax( ˆ w 2 )<label>(2)</label></formula><p>The subtasks share the pre-trained word embed- dings, the special embeddings, and the biLSTM parameters. Subtasks (2) and (3) also share W w , the MLP that predicts the index of a word.</p><p>[ <ref type="bibr">w1]</ref> [  <ref type="table">Table 1</ref>: Examples of top ranked predicted components using the model: predicting the paraphrase given w 1 and w 2 (left), w 1 given w 2 and the paraphrase (middle), and w 2 given w 1 and the paraphrase (right). Implementation Details. The model is imple- mented in DyNet ( <ref type="bibr" target="#b21">Neubig et al., 2017</ref>). We dedi- cate a small number of noun-compounds from the corpus for validation. We train for up to 10 epochs, stopping early if the validation loss has not im- proved in 3 epochs. We use Momentum SGD <ref type="bibr" target="#b20">(Nesterov, 1983)</ref>, and set the batch size to 10 and the other hyper-parameters to their default values.</p><note type="other">w2] Predicted Paraphrases [w2] Paraphrase Predicted [w1] Paraphrase [w1] Predicted [w2</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Qualitative Analysis</head><p>To estimate the quality of the proposed model, we first provide a qualitative analysis of the model outputs. <ref type="table">Table 1</ref> displays examples of the model outputs for each possible usage: predicting the paraphrase given the constituent words, and pre- dicting each constituent word given the paraphrase and the other word. The examples in the table are from among the top 10 ranked predictions for each component- pair. We note that most of the (w 2 , paraphrase, w 1 ) triplets in the table do not occur in the training data, but are rather generalized from similar exam- ples. For example, there is no training instance for "company in the software industry" but there is a "firm in the software industry" and a company in many other industries.</p><p>While the frequent prepositional paraphrases are often ranked at the top of the list, the model also retrieves more specified verbal paraphrases. To illustrate paraphrase similarity we compute a t-SNE projection (Van Der Maaten, 2014) of the embeddings of all the paraphrases, and draw a sample of 50 paraphrases in <ref type="figure" target="#fig_2">Figure 2</ref>. The projec- tion positions semantically-similar but lexically- divergent paraphrases in proximity, likely due to many shared constituents. For instance, 'with', 'from', and 'out of' can all describe the relation between food words and their ingredients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation: Noun-Compound Interpretation Tasks</head><p>For quantitative evaluation we employ our model for two noun-compound interpretation tasks. The main evaluation is on retrieving and ranking para- phrases ( §5.1). For the sake of completeness, we also evaluate the model on classification to a fixed inventory of relations ( §5.2), although it wasn't de- signed for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Paraphrasing</head><p>Task Definition. The general goal of this task is to interpret each noun-compound to multiple prepositional and verbal paraphrases. In SemEval 2013 Task 4, 4 the participating systems were asked to retrieve a ranked list of paraphrases for each noun-compound, which was automatically evaluated against a similarly ranked list of para- phrases proposed by human annotators.</p><p>Model. For a given noun-compound w 1 w 2 , we first predict the k = 250 most likely paraphrases:</p><formula xml:id="formula_5">ˆ p 1 , ..., ˆ p k = argmax k ˆ p, wherê</formula><p>p is the distribution of paraphrases defined in Equation 1.</p><p>While the model also provides a score for each paraphrase (Equation 1), the scores have not been optimized to correlate with human judgments. We therefore developed a re-ranking model that re- ceives a list of paraphrases and re-ranks the list to better fit the human judgments.</p><p>We follow Herbrich (2000) and learn a pair- wise ranking model. The model determines which of two paraphrases of the same noun-compound should be ranked higher, and it is implemented as an SVM classifier using scikit-learn <ref type="bibr" target="#b28">(Pedregosa et al., 2011</ref>). For training, we use the available training data with gold paraphrases and ranks pro- vided by the SemEval task organizers. We extract the following features for a paraphrase p: is its confidence score. The last feature incorpo- rates the original model score into the decision, as to not let other considerations such as preposition frequency in the training set take over. During inference, the model sorts the list of paraphrases retrieved for each noun-compound ac- cording to the pairwise ranking. It then scores each paraphrase by multiplying its rank with its original model score, and prunes paraphrases with final score &lt; 0.025. The values for k and the threshold were tuned on the training set.</p><formula xml:id="formula_6">1.</formula><p>Evaluation Settings. The SemEval 2013 task provided a scorer that compares words and n- grams from the gold paraphrases against those in the predicted paraphrases, where agreement on a prefix of a word (e.g. in derivations) yields a partial scoring. The overall score assigned to each system is calculated in two different ways. The 'isomorphic' setting rewards both precision and recall, and performing well on it requires ac- curately reproducing as many of the gold para- phrases as possible, and in much the same order. The 'non-isomorphic' setting rewards only preci- sion, and performing well on it requires accurately reproducing the top-ranked gold paraphrases, with no importance to order.</p><p>Baselines. We compare our method with the published results from the SemEval task. The SemEval 2013 baseline generates for each noun- compound a list of prepositional paraphrases in an arbitrary fixed order. It achieves a moder- ately good score in the non-isomorphic setting by generating a fixed set of paraphrases which are both common and generic. The MELODI sys- tem performs similarly: it represents each noun- compound using a compositional distributional vector <ref type="bibr" target="#b16">(Mitchell and Lapata, 2010)</ref> which is then used to predict paraphrases from the corpus. The performance of MELODI indicates that the sys- tem was rather conservative, yielding a few com- mon paraphrases rather than many specific ones. SFS and IIITH, on the other hand, show a more balanced trade-off between recall and precision.</p><p>As a sanity check, we also report the results of a baseline that retrieves ranked paraphrases from the training data collected in Section 3.2. This base- line has no generalization abilities, therefore it is expected to score poorly on the recall-aware iso- morphic setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method isomorphic non-isomorphic</head><p>Baselines SFS <ref type="bibr">(Versley, 2013)</ref> 23.1 17.9 IIITH ( <ref type="bibr" target="#b33">Surtani et al., 2013)</ref> 23.1 25.8 MELODI <ref type="table" target="#tab_4">(Van de Cruys et al., 2013)</ref> 13.0 54.8 SemEval 2013 <ref type="bibr">Baseline (Hendrickx et al., 2013)</ref> 13   Results. <ref type="table" target="#tab_4">Table 2</ref> displays the performance of the proposed method and the baselines in the two eval- uation settings. Our method outperforms all the methods in the isomorphic setting. In the non- isomorphic setting, it outperforms the other two systems that score reasonably on the isomorphic setting (SFS and IIITH) but cannot compete with the systems that focus on achieving high precision.</p><p>The main advantage of our proposed model is in its ability to generalize, and that is also demonstrated in comparison to our baseline per- formance. The baseline retrieved paraphrases only for a third of the noun-compounds (61/181), ex- pectedly yielding poor performance on the isomor- phic setting. Our model, which was trained on the very same data, retrieved paraphrases for all noun- compounds. For example, welfare system was not present in the training data, yet the model pre- dicted the correct paraphrases "system of welfare benefits", "system to provide welfare" and others.</p><p>Error Analysis. We analyze the causes of the false positive and false negative errors made by the model. For each error type we sample 10 noun- compounds. For each noun-compound, false pos- itive errors are the top 10 predicted paraphrases which are not included in the gold paraphrases, while false negative errors are the top 10 gold paraphrases not found in the top k predictions made by the model. <ref type="table" target="#tab_5">Table 3</ref> displays the manu- ally annotated categories for each error type.</p><p>Many false positive errors are actually valid paraphrases that were not suggested by the hu- man annotators (error 1, "discussion by group"). Some are borderline valid with minor grammati- cal changes (error 6, "force of coalition forces") or too specific (error 2, "life of women in commu- nity" instead of "life in community"). Common prepositional paraphrases were often retrieved al- though they are incorrect (error 3). We conjec- ture that this error often stem from an n-gram that does not respect the syntactic structure of the sen- tence, e.g. a sentence such as "rinse away the oil from baby 's head" produces the n-gram "oil from baby".</p><p>With respect to false negative examples, they consisted of many long paraphrases, while our model was restricted to 5 words due to the source of the training data (error 1, "holding done in the case of a share"). Many prepositional paraphrases consisted of determiners, which we conflated with the same paraphrases without determiners (error 2, "mutation of a gene"). Finally, in some para- phrases, the constituents in the gold paraphrase appear in inflectional forms (error 3, "holding of shares" instead of "holding of share").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Classification</head><p>Noun-compound classification is defined as a mul- ticlass classification problem: given a pre-defined set of relations, classify w 1 w 2 to the relation that holds between w 1 and w 2 . Potentially, the cor- pus co-occurrences of w 1 and w 2 may contribute to the classification, e.g. '[w 2 ] held at [w 1 ]' in- dicates a TIME relation. <ref type="bibr" target="#b35">Tratz and Hovy (2010)</ref> in- cluded such features in their classifier, but ablation tests showed that these features had a relatively small contribution, probably due to the sparseness of the paraphrases. Recently, <ref type="bibr" target="#b31">Shwartz and Waterson (2018)</ref> showed that paraphrases may con- tribute to the classification when represented in a continuous space.</p><p>Model. We generate a paraphrase vector repre- sentation par(w 1 w 2 ) for a given noun-compound w 1 w 2 as follows. We predict the indices of the k most likely paraphrases:</p><formula xml:id="formula_7">ˆ p 1 , ..., ˆ p k = argmax k ˆ p, wherê</formula><p>p is the distribution on the paraphrase vo- cabulary V p , as defined in Equation 1. We then encode each paraphrase using the biLSTM, and average the paraphrase vectors, weighted by their confidence scores inˆpinˆ inˆp:</p><formula xml:id="formula_8">par(w 1 w 2 ) = k i=1ˆpˆpi=1ˆ i=1ˆpi=1ˆpˆ i=1ˆpˆp i · V p ˆ p i k i=1ˆpˆpi=1ˆ i=1ˆpi=1ˆpˆ i=1ˆpˆp i (3)</formula><p>We train a linear classifier, and represent w 1 w 2 in a feature vector f (w 1 w 2 ) in two variants: para- phrase: f (w 1 w 2 ) = par(w 1 w 2 ), or integrated: concatenated to the constituent word embeddings</p><formula xml:id="formula_9">f (w 1 w 2 ) = [ par(w 1 w 2 ), w 1 , w 2 ].</formula><p>The classifier type (logistic regression/SVM), k, and the penalty are tuned on the validation set. We also pro- vide a baseline in which we ablate the paraphrase component from our model, representing a noun- compound by the concatenation of its constituent embeddings f (w 1 w 2 ) = [</p><formula xml:id="formula_10">w 1 , w 2 ] (distributional).</formula><p>Datasets. We evaluate on the Tratz (2011) dataset, which consists of 19,158 instances, la- beled in 37 fine-grained relations (Tratz-fine) or 12 coarse-grained relations (Tratz-coarse).</p><p>We report the performance on two different dataset splits to train, test, and validation: a ran- dom split in a 75:20:5 ratio, and, following con- cerns raised by <ref type="bibr" target="#b3">Dima (2016)</ref> about lexical mem- orization ( <ref type="bibr" target="#b13">Levy et al., 2015</ref>), on a lexical split in which the sets consist of distinct vocabularies. The lexical split better demonstrates the scenario in which a noun-compound whose constituents have not been observed needs to be interpreted based on similar observed noun-compounds, e.g. inferring the relation in pear tart based on apple cake and other similar compounds. We follow the random and full-lexical splits from <ref type="bibr" target="#b31">Shwartz and Waterson (2018</ref>   <ref type="bibr" target="#b31">Shwartz and Waterson, 2018)</ref>: a neural classification model that learns an LSTM-based representation of the joint occur- rences of w 1 and w 2 in a corpus (i.e. observed paraphrases), and integrates distributional infor- mation using the constituent embeddings.</p><p>Results. <ref type="table" target="#tab_7">Table 4</ref> displays the methods' perfor- mance on the two versions of the Tratz (2011) dataset and the two dataset splits. The paraphrase model on its own is inferior to the distributional model, however, the integrated version improves upon the distributional model in 3 out of 4 settings, demonstrating the complementary nature of the distributional and paraphrase-based methods. The contribution of the paraphrase component is espe- cially noticeable in the lexical splits. As expected, the integrated method in Shwartz and <ref type="bibr" target="#b31">Waterson (2018)</ref>, in which the paraphrase representation was trained with the objective of classification, performs better than our integrated model. The superiority of both integrated models in the lexical splits confirms that paraphrases are beneficial for classification.    <ref type="table" target="#tab_9">Table 5</ref> provides examples of noun-compounds that were correctly classified by the integrated model while being incorrectly classified by the dis- tributional model. For each noun-compound, we provide examples of top ranked paraphrases which are indicative of the gold label relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Compositionality Analysis</head><p>Our paraphrasing approach at its core assumes compositionality: only a noun-compound whose meaning is derived from the meanings of its con- stituent words can be rephrased using them. In §3.2 we added negative samples to the train- ing data to simulate non-compositional noun- compounds, which are included in the classifi- cation dataset ( §5.2). We assumed that these compounds, more often than compositional ones would consist of unrelated constituents (spelling bee, sacred cow), and added instances of random unrelated nouns with '[w 2 ] is unrelated to [w 1 ]'. Here, we assess whether our model succeeds to recognize non-compositional noun-compounds.</p><p>We used the compositionality dataset of Reddy et al. (2011) which consists of 90 noun- compounds along with human judgments about their compositionality in a scale of 0-5, 0 be- ing non-compositional and 5 being compositional. For each noun-compound in the dataset, we pre- dicted the 15 best paraphrases and analyzed the er- rors. The most common error was predicting para- phrases for idiomatic compounds which may have a plausible concrete interpretation or which origi- nated from one. For example, it predicted that sil- ver spoon is simply a spoon made of silver and that monkey business is a business that buys or raises monkeys. In other cases, it seems that the strong prior on one constituent leads to ignoring the other, unrelated constituent, as in predicting "wedding made of diamond". Finally, the "unrelated" para- phrase was predicted for a few compounds, but those are not necessarily non-compositional (ap- plication form, head teacher). We conclude that the model does not address compositionality and suggest to apply it only to compositional com- pounds, which may be recognized using compo- sitionality prediction methods as in Reddy et al. (2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented a new semi-supervised model for noun-compound paraphrasing. The model differs from previous models by being trained to predict both a paraphrase given a noun-compound, and a missing constituent given the paraphrase and the other constituent. This results in better general- ization abilities, leading to improved performance in two noun-compound interpretation tasks. In the future, we plan to take generalization one step fur- ther, and explore the possibility to use the biL- STM for generating completely new paraphrase templates unseen during training. mEval 2013). Association for Computational Lin- guistics, Atlanta, Georgia, USA, pages 144-147. http://www.aclweb.org/anthology/S13-2026. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the model predictions for w 1 and p given the triplet (cake, made of, apple). The model predicts each component given the encoding of the other two components, successfully predicting 'apple' given 'cake made of [w 1 ]', while predicting '[w 2 ] containing [w 1 ]' for 'cake [p] apple'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>[w 1 ], [w 2 ], and [p], which are used to represent a missing component, as in "cake made of [w 1 ]", "[w 2 ] made of apple", and "cake [p] apple".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A t-SNE map of a sample of paraphrases, using the paraphrase vectors encoded by the biLSTM, for example bLS([w 2 ] made of [w 1 ]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The list often contains multiple semantically- similar paraphrases, such as '[w 2 ] involved in [w 1 ]' and '[w 2 ] in [w 1 ] industry'. This is a result of the model training objective (Section 3) which positions the vectors of semantically-similar para- phrases close to each other in the embedding space, based on similar constituents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Example</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Laurens Van Der Maaten. 2014 .</head><label>2014</label><figDesc>Accelerating t-sne using tree-based algorithms. Journal of machine learning research 15(1):3221-3245. Yannick Versley. 2013. Sfs-tue: Compound para- phrasing with a language model and discriminative reranking. In Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). volume 2, pages 148-152. Sander Wubben. 2010. Uvt: Memory-based pairwise ranking of paraphrasing verbs. In Proceedings of the 5th International Workshop on Semantic Eval- uation. Association for Computational Linguistics, pages 260-263.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of the proposed method and the baselines on the SemEval 2013 task. 

Category 
% 

False Positive 
(1) Valid paraphrase missing from gold 
44% 
(2) Valid paraphrase, slightly too specific 
15% 
(3) Incorrect, common prepositional paraphrase 
14% 
(4) Incorrect, other errors 
14% 
(5) Syntactic error in paraphrase 
8% 
(6) Valid paraphrase, but borderline grammatical 5% 

False Negative 
(1) Long paraphrase (more than 5 words) 
30% 
(2) Prepositional paraphrase with determiners 
25% 
(3) Inflected constituents in gold 
10% 
(4) Other errors 
35% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Categories of false positive and false neg- ative predictions along with their percentage.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Baselines. We report the results of 3 baselines 
representative of different approaches: 
1) Feature-based (Tratz and Hovy, 2010): we re-
implement a version of the classifier with features 
from WordNet and Roget's Thesaurus. 
2) Compositional (Dima, 2016): a neural archi-
tecture that operates on the distributional represen-
tations of the noun-compound and its constituents. 
Noun-compound representations are learned with 

Dataset &amp; Split 
Method 
F1 

Tratz 
fine 
Random 

Tratz and Hovy (2010) 
0.739 
Dima (2016) 
0.725 
Shwartz and Waterson (2018) 0.714 
distributional 
0.677 
paraphrase 
0.505 
integrated 
0.673 

Tratz 
fine 
Lexical 

Tratz and Hovy (2010) 
0.340 
Dima (2016) 
0.334 
Shwartz and Waterson (2018) 0.429 
distributional 
0.356 
paraphrase 
0.333 
integrated 
0.370 

Tratz 
coarse 
Random 

Tratz and Hovy (2010) 
0.760 
Dima (2016) 
0.775 
Shwartz and Waterson (2018) 0.736 
distributional 
0.689 
paraphrase 
0.557 
integrated 
0.700 

Tratz 
coarse 
Lexical 

Tratz and Hovy (2010) 
0.391 
Dima (2016) 
0.372 
Shwartz and Waterson (2018) 0.478 
distributional 
0.370 
paraphrase 
0.345 
integrated 
0.393 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Classification results. For each dataset 
split, the top part consists of baseline methods and 
the bottom part of methods from this paper. The 
best performance in each part appears in bold. 

the Full-Additive (Zanzotto et al., 2010) and Ma-
trix (Socher et al., 2012) models. We report the 
results from Shwartz and Waterson (2018). 
3) Paraphrase-based (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Examples of noun-compounds that were correctly classified by the integrated model while being 
incorrectly classified by distributional, along with top ranked indicative paraphrases. 

Analysis. To analyze the contribution of the 
paraphrase component to the classification, we fo-
cused on the differences between the distributional 
and integrated models on the Tratz-Coarse lexical 
split. Examination of the per-relation F 1 scores 
revealed that the relations for which performance 
improved the most in the integrated model were 
TOPICAL (+11.1 F 1 points), OBJECTIVE (+5.5), AT-
TRIBUTE (+3.8) and LOCATION/PART WHOLE (+3.5). 
</table></figure>

			<note place="foot" n="1"> The code is available at github.com/vered1986/panic</note>

			<note place="foot" n="2"> We refer to &quot;paraphrases&quot; and &quot;paraphrase templates&quot; interchangeably. In the extracted templates, [w2] always precedes [w1], probably because w2 is normally the head noun.</note>

			<note place="foot" n="3"> In practice, we pick the k best scoring indices in each distribution for some predefined k, as we discuss in Section 5.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by an Intel ICRI-CI grant, the Israel Science Foundation grant 1951/17, the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1), and Theo Hoffenberg. Vered is also supported by the Clore Scholars Pro-gramme <ref type="bibr">(2017)</ref>, and the AI2 Key Scientific Chal-lenges <ref type="bibr">Program (2017</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extracting paraphrases from a parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R. Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P01-1008" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Web 1t 5-gram version 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Franz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 9: The interpretation of noun compounds using paraphrasing verbs and prepositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Butnariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009)</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009)<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="100" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Dima</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/W16-1604</idno>
		<ptr target="https://doi.org/10.18653/v1/W16-1604" />
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP, Association for Computational Linguistics, chapter On the Compositionality and Semantic Interpretation of English Noun Compounds</title>
		<meeting>the 1st Workshop on Representation Learning for NLP, Association for Computational Linguistics, chapter On the Compositionality and Semantic Interpretation of English Noun Compounds</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic noun compound interpretation using deep neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corina</forename><surname>Dima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Hinrichs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IWCS</title>
		<imprint>
			<biblScope unit="page">173</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving the interpretation of noun phrases with cross-linguistic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="568" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 4: Free paraphrases of noun compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Szpakowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veale</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/S13-2025" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large margin rank boundaries for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in large margin classifiers pages</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="115" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Largescale noun compound interpretation using bootstrapping and the web as a corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D11-1060" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="648" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpreting noun compounds using bootstrapping and sense collocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the Pacific Association for Computational Linguistics</title>
		<meeting>Conference of the Pacific Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Do supervised distributional methods really learn lexical inference relations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Remus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N15-1098" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ucd-goggle: A hybrid system for noun compound paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandra</forename><surname>Lopez-Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="230" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Paraphrasing revisited with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers. Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="881" to="893" />
		</imprint>
	</monogr>
	<note>Valencia</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the interpretation of noun compounds: Syntax, semantics, and entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="291" to="330" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using verbs to characterize noun-noun relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marti</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence: Methodology, Systems, and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring noun-modifier semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth international workshop on computational semantics (IWCS-5)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="285" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate o (1/k2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurii</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet Mathematics Doklady</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
	</analytic>
	<monogr>
		<title level="m">The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ucd-pn: Selecting general paraphrases using conditional probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Nulty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fintan</forename><surname>Costello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics</title>
		<meeting>the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="234" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using lexical and relational similarity to classify semantic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´o</forename><surname>Diarmuid´odiarmuid´</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Copestake</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E09-1071" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009). Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the ACL (EACL 2009). Association for Computational Linguistics<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="621" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Demonyms and compound relational nouns in nominal open ie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harinder</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mausam</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W16-1307" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Automated Knowledge Base Construction. Association for Computational Linguistics</title>
		<meeting>the 5th Workshop on Automated Knowledge Base Construction. Association for Computational Linguistics<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="35" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonaventura</forename><surname>Coppola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N/N07/N07-1071" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference. Association for Computational Linguistics</title>
		<meeting>the Main Conference. Association for Computational Linguistics<address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
	<note>Human Language Technologies</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interpreting compound noun phrases using web search queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/N15-1037</idno>
		<ptr target="https://doi.org/10.3115/v1/N15-1037" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identifying 1950s american jazz musicians: Fine-grained isa extraction via modifier composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/P17-1192" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2099" to="2109" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An empirical study on compositionality in compound nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/I11-1024" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing. Asian Federation of Natural Language Processing<address><addrLine>Chiang Mai, Thailand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Olive oil is made of olives, baby oil is made for babies: Interpreting noun compounds using paraphrases in a neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Waterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting><address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D12-1110" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Iiit-h: A corpus-driven co-occurrence based probabilistic model for noun compound paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><surname>Surtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpita</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urmi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soma</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="153" to="157" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (* SEM)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Semantically-enriched parsing for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>University of Southern California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A taxonomy, dataset, and classifier for automatic noun compound interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P10-1070" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Melodi: A supervised distributional approach for free paraphrasing of noun compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stergos</forename><surname>Afantenos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation<address><addrLine>Se</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
