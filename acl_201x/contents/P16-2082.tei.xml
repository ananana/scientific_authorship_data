<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Relational Knowledge into Word Representations using Subspace Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<postCode>10598</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Araki</surname></persName>
							<email>junaraki@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Relational Knowledge into Word Representations using Subspace Regularization</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="506" to="511"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Incorporating lexical knowledge from semantic resources (e.g., WordNet) has been shown to improve the quality of distributed word representations. This knowledge often comes in the form of rela-tional triplets (x, r, y) where words x and y are connected by a relation type r. Existing methods either ignore the relation types, essentially treating the word pairs as generic related words, or employ rather restrictive assumptions to model the rela-tional knowledge. We propose a novel approach to model relational knowledge based on low-rank subspace regulariza-tion, and conduct experiments on standard tasks to evaluate its effectiveness.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed word representations, also known as word embeddings, are low-dimensional vector representations for words that capture semantic as- pects ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b19">Pennington et al., 2014;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013a</ref>). The algorithms for learn- ing the word embeddings rely on distributional hypothesis <ref type="bibr" target="#b12">(Harris, 1954</ref>) that words occurring in similar contexts tend to have similar meanings. Word embeddings have been shown to capture interesting linguistic regularities by simple vec- tor arithmetic (e.g., v(king)-v(man)+v(woman)≈ v(queen)) ( <ref type="bibr" target="#b18">Mikolov et al., 2013c</ref>). They have also been used to derive downstream features for vari- ous NLP tasks, such as named entity recognition, chunking, dependency parsing, sentiment analy- sis, paraphrase detection and machine translation <ref type="bibr" target="#b24">(Turian et al., 2010;</ref><ref type="bibr" target="#b6">Dhillon et al., 2011;</ref><ref type="bibr" target="#b0">Bansal et al., 2014;</ref><ref type="bibr" target="#b15">Maas et al., 2011;</ref><ref type="bibr" target="#b22">Socher et al., 2011;</ref><ref type="bibr" target="#b27">Zou et al., 2013)</ref>. Their promise as semantic word representations has led to increasing research ef- forts on improving their quality.</p><p>To this end, researchers have attempted to incor- porate lexical knowledge into word embeddings by using additional regularization or loss terms in the learning objective. This lexical knowledge is often available in the form of triplets {(w i , r, w j )}, where the words w i and w j are connected by rela- tion type r. These methods can be broadly classi- fied into two categories. First family of methods use a (over-)generalized notion of similarity be- tween words and ignore the type of relations, es- sentially treating the two words as generic similar words ( <ref type="bibr" target="#b26">Yu and Dredze, 2014;</ref><ref type="bibr" target="#b7">Faruqui et al., 2015;</ref><ref type="bibr" target="#b14">Liu et al., 2015)</ref>. This places an implicit restric- tion on the types of relations that can be used with these methods. Second family of methods model each relation type by a distinct operator. <ref type="bibr">Bordes et al. (2013)</ref> assumed a distinct relation vector r for every relation and minimize the distance between the translated first word and the second word, i.e., d(w i + r, w j ) for every triplet (w i , r, w j ). <ref type="bibr" target="#b27">Socher et al. (2013)</ref> proposed a neural tensor network which uses a distinct tensor operator for every re- lation. These methods were used to learn entity and relation embeddings from a large collection of relation triplets for the task of knowledge base completion. Since these methods did not use any co-occurrence information from a text corpus, all entities were required to appear at least once in the training data, ruling out generalization to un- seen entities <ref type="bibr">1</ref> . More recently, <ref type="bibr" target="#b25">Xu et al. (2014)</ref> combined the training objective of SKIP-GRAM ( <ref type="bibr" target="#b16">Mikolov et al., 2013a</ref>) with the training objec- tive of <ref type="bibr">(Bordes et al., 2013</ref>) to incorporate lexical knowledge into word embeddings. <ref type="bibr" target="#b9">Fried and Duh (2014)</ref> combine the training objective of <ref type="bibr">(Bordes et al., 2013</ref>) with that of neural language model <ref type="bibr">(Collobert et al., 2011</ref>) using alternating direction method of multipliers <ref type="bibr" target="#b4">(Boyd et al., 2011</ref>).</p><p>Constant translation model ( <ref type="bibr">Bordes et al., 2013;</ref><ref type="bibr" target="#b25">Xu et al., 2014;</ref><ref type="bibr" target="#b9">Fried and Duh, 2014</ref>) (referred as CTM from now on), although an important step in modeling relational knowledge, makes a rather restrictive assumption requiring all triplets (w i , r, w j ) pertaining to a relation type r to sat- isfy w i + r ≈ w j , ∀(i, j). This restriction can be severe when learning from a large text corpus since vector representation of a word also needs to respect a huge set of co-occurrence instances with other words. CTM is also not suitable for (i) modeling symmetric relations (e.g., synonyms, antonyms), and (ii) modeling transitive relations (e.g., synonyms, hypernyms). In this paper, we propose a novel formulation for modeling the rela- tional knowledge which addresses these issues by relaxing the constant translation assumption and modeling each relation by a low-rank subspace, i.e., all the word pairs pertaining to a relation are assumed to lie in a low-rank subspace. We demon- strate effectiveness of the learned word represen- tations on the tasks of knowledge-base completion and word analogy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Subspace-regularized word embedding</head><p>Although our proposed framework for relational modeling is general enough to use with any ex- isting word embedding method, we work with Word2Vec model ( <ref type="bibr" target="#b16">Mikolov et al., 2013a</ref>) in this paper for illustrating our ideas and later for em- pirical evaluations. Word2Vec is a neural network model trained on sequence of words and its hid- den layer activations can be read out as the word representations. Two variants were proposed in ( <ref type="bibr" target="#b16">Mikolov et al., 2013a</ref>) -SKIP-GRAM, which maximizes the log likelihood of the local context words given the target word, and CBOW, which maximizes the log likelihood of the target word given its local context. More specifically, CBOW maximizes the objective</p><formula xml:id="formula_0">1 T T t=1 log p(w t |w t+c t−c ) = 1 T T t=1 exp(w t v t ) w∈V exp(w v t )<label>(1)</label></formula><p>where w t+c t−c represents the words (or tokens) in the local context window around the t'th word (or token) and v t = −c≤i≤c,i =0 w t+i can be seen as the average context vector. The vectors w, w ∈ R d denote the input and output embed- dings for word w, respectively. The input embed- dings are taken as the final word representations. Negative sampling was proposed to efficiently op- timize Eq. 1 ( <ref type="bibr" target="#b17">Mikolov et al., 2013b</ref>). We report empirical results with CBOW since it was compu- tationally faster than SKIP-GRAM while giving similar results in our early explorations.</p><p>We assume access to relational knowledge in the form of triplets R k = {(w i , r k , w j )} ∀1 ≤ k ≤ m, where words w i and w j are connected by relation r k and R k is the set of all triplets corre- sponding to relation r k with |R k | = n k . This form of knowledge is commonly available from Knowl- edge Bases like WordNet <ref type="bibr">(Fellbaum, 1998)</ref>. Our framework is suitable for both symmetric relations where words can be interchanged (e.g., synonyms) and asymmetric relations which have a directional nature (e.g., hypernyms).</p><p>Let d ij = (w j − w i ) ∈ R d denote the dif- ference vector for the triplet (w i , r k , w j ) which points from the vector of word w i to that of word w j . Let us construct a matrix D k ∈ R d×n k by stacking the difference vectors corresponding to all the triplets in relation r k , i.e.,</p><formula xml:id="formula_1">D k = [· · · d ij · · · ] ∀{(i, j) : (w i , r k , w j ) ∈ R k }.<label>(2)</label></formula><p>To incorporate this relational knowledge into word embeddings, we enforce an approximate low-rank constraint on D k assuming</p><formula xml:id="formula_2">D k ≈ U k A k ,<label>(3)</label></formula><p>where U k ∈ R d×p , p d is the relation ba- sis whose linear span contains all the difference vectors pertaining to relation r k . For p = 2, this assumption implies that all the difference vectors pertaining to a relation lie in a 2-D plane. For</p><formula xml:id="formula_3">p = 1, it reduces to D k ≈ u k α T k , u k ∈ R d , α k ∈ R n k ,</formula><p>implying that all the difference vectors for a relation are collinear. In this paper, we mainly study the rank-1 model (p=1) since it seems to be a natural starting point for evaluating the idea of subspace-regularized relational modeling. The study of higher rank models will potentially re- quire a careful exploration of various structural regularizers for reconstruction matrix A k as well as a different evaluation scheme. We leave this study for future work.</p><p>Rank-1 subspace regularization can also be motivated from the fact that word embeddings are able to capture some linguistic regulari- ties ( <ref type="bibr" target="#b18">Mikolov et al., 2013c</ref>) along certain direc- tions in the vector space. For example, the dif- ference vector for word pair (king, queen) is ap- proximately aligned with the difference vector for (man,woman), encoding the gender relation. The direction of the difference vectors carries signifi- cant information for these regularities which is ev- ident from the success of cosine similarity metrics in the word analogy problems ( ). CTM that assumes w i + u k = w j ∀(w i , r k , w j ) ∈ R enforces an additional equal length constraint on the difference vectors, which may be rather re- strictive, especially when the word vectors are also influenced by co-occurrence statistics (apart from relational knowledge). Moreover, it may face fol- lowing challenges in relational modeling:</p><p>• It does not have a natural interpretation for modeling symmetric relations (e.g., syn- onyms, antonyms) that allow interchangeabil- ity of words in a given relation triplet (i.e.,</p><formula xml:id="formula_4">(w i , r k , w j ) ⇐⇒ (w j , r k , w i ))</formula><p>. Having a con- stant translation of u k ∈ R d from the first word to the second word leads to contradiction.</p><p>• It is also not natural for modeling relations with transitive property (i.e., (w i , r k , w j ) ∧ (w j , r k , w l ) =⇒ (w i , r k , w l )), again leading to contradictions. Common examples of such re- lations are synonyms and hypernyms.</p><p>The proposed rank-1 subspace relation model naturally allows for modeling such relations by doing away with the constant length restriction on the difference vectors. Our empirical evaluations verify that this relaxation indeed leads to improved quality of word vectors with respect to capturing linguistic regularities.</p><p>We incorporate the proposed relational model into the learning objective for word vectors by reg- ularizing the matrix of difference vectors towards a rank-1 matrix. We impose a nonnegativity con- straint on the reconstruction coefficients α k if re- lation r k is asymmetric. This respects the unidi- rectional nature of asymmetric relations. To en- sure uniqueness of solution for u k and α k , we constrain u k 2 = 1. Leaving α k completely free can end up creating spurious relations between any two words that are arbitrarily far but whose differ- ence vector is directionally aligned with any of the relation basis vectors {u k } m k=1 . To avoid this, we further impose a upper limit of c on the absolute value of elements of α k . We minimize the follow- ing joint objective for word vectors {w i } |V | i=1 and relation parameters {u k , α k } m k=1 :</p><formula xml:id="formula_5">− 1 T T t=1 log p(w t |w t+c t−c ) + λ 2 |R| m k=1 D k − u k α k 2 F s.t. α k ≥ 0 ∀ asymmetric r k , u k 2 = 1, |(α k ) l | ≤ c.<label>(4)</label></formula><p>where D k is the matrix of difference vectors as de- fined earlier and λ is the regularization parameter. The first term in the objective takes into account the co-occurrence information text corpus while the second term incorporates the relational knowl- edge.</p><p>Optimizing for word vectors: We adopt parallel asynchronous stochastic gradient descent (SGD) with negative sampling approach of ( <ref type="bibr" target="#b17">Mikolov et al., 2013b</ref>). The model parameters for optimiza- tion are input embeddings (weights connecting input and hidden layer) and output embeddings (weights connecting hidden and output layer). In- put embeddings are taken as the final word em- beddings. Each computing thread works with a predefined segment of the text corpus and updates parameters that are stored in a shared memory. In each gradient step of CBOW, a thread samples a tar- get word and its local context window and updates the parameters of the neural network. It can be seen as sampling one of the f t (·), t = 1, 2, . . . , T and taking a gradient step with it. A small num- ber of random target words are also sampled for the same context, treating them as negative ex- amples for the gradient update. In the CBOW ar- chitecture, representations for context words are directly encoded as columns of the linear weight matrix W ∈ R d×|V | that maps input bag-of-words layer to the hidden layer. The columns of W are taken as the word embeddings for the correspond- ing words in the vocabulary V . The reader is re- ferred to <ref type="bibr" target="#b17">(Mikolov et al., 2013b;</ref>) for more details on the optimization procedure for CBOW. If a word appears in the set of relation triplets R, our regularization term gets activated. Since we place the regularizer only on input embeddings, the following gradient updates due to the regularization term act only on input embeddings.</p><formula xml:id="formula_6">w i ←− w i − η λ |R|   j:(w i ,r k ,w j )∈R w i − w j + u k α k ij + j:(w j ,r k ,w i )∈R w i − w j − u k α k ji   ,<label>(5)</label></formula><p>where η is the learning rate, and α k ij denotes the element of α k corresponding to the column of matrix D k which contains difference vector (w j − w i ) (and similarly for α k ji ). The modifi- cations in the learning rate as the SGD progresses are kept same as in the original implementation of CBOW 2 .</p><p>Optimization for u k and α k : Instead of hav- ing stochastic gradient updates, we adopt an asynchronous batch update strategy for relation basis {u k } m k=1 and reconstruction coefficients {α k } m k=1 . We launch one compute thread that keeps solving the batch optimization problem for {u k } m k=1 and {α k } m k=1 in an infinite loop until the optimization for word embeddings finishes. The batch optimization problem for a symmetric rela- tion r k is:</p><formula xml:id="formula_7">min u k ,α k D k − u k α k 2 F , s.t. u k 2 = 1, |α k | ≤ c.<label>(6)</label></formula><p>where D k ∈ R d×n k is the matrix of difference vectors for all triplets corresponding to relation r k as defined in Eq. 2. Without the absolute value constraint on α k , this problem can be ex- actly solved by SVD. We follow an alternating optimization procedure for solving this problem. We initialize u k to the top left singular vector of D k and then alternate between solving two least- squares sub-problems for u k and α k respectively with the corresponding constraints. For asymmet- ric relations, there is an additional nonnegativ- ity constraint on α k . We use projected gradient descent to solve these constrained least-squares problems.</p><p>Relation English Wikipedia for training which contains ap- proximately 4.8 million articles and 2 billion to- kens. We lowercase all the text and and tokenize using Stanford NLP tokenizer. We use two datasets for evaluating the proposed method. Google word analogy data <ref type="bibr" target="#b16">(Mikolov et al., 2013a</ref>) contains 19544 analogy relations (14 relation types -5 semantic, 9 syntactic) of the form a:b::c:d constructed from 550 unique rela- tion triplets. We use this data only for evaluation (test phase). WordRep ( ) contains a large collection of relation triplets (44584 triplets in total, 25 relation types -18 semantic, 7 syntac- tic) extracted from WordNet, Wikipedia and Dic- tionary. For each relation type, we randomly split the triplets in 4 : 1 ratio with larger split used for training and smaller split used for test. We make sure that there is no word overlap between train- ing and test triplets. We also remove triplets con- taining words from Google Analogy data from the training set.</p><note type="other">-type RELCONST RELSUB capital-cities 48.15 59.26 currency 58.33 50.00 city-in-state 17.88 18.94 gender 44.44 50.00 similar-to 5.44 7.26 made-of 0 0 has-context 10.00 8.26 is-a 1.35 1.83 part-of 17.50 19.00 instance-of 8.40 12.98 derived-from 9.14 10.27 antonym 20.00 20.62 entails 0 4.35 causes 0 0 member-of 13.43 26.87 related-to 0 0 attribute 11.76 8.82 SEMANTIC 7.47 8.44 adjective-to-adverb 10.14 47.83 plural-verbs 61.25 71.77 plural-nouns 66.70 71.89 comparative 70.00 75.00 superlative 66.67 77.78 nationality 85.71 85.71 past-tense 42.20 66.84 present-participle 45.76 47.62 SYNTACTIC 54.88 65.38 TOTAL 24.61 29.03</note><p>We compare the proposed RELSUB model with two methods: (i) CBOW <ref type="bibr" target="#b16">(Mikolov et al., 2013a)</ref>, and (ii) RELCONST which is based on constant translation model for relations which was origi- nally proposed in ( <ref type="bibr">Bordes et al., 2013</ref>) for embed- ding knowledge-bases and was recently used by  are set equal to the vector of all 1's and norm constraint on u k are removed. This enables us to directly test the merit of the pro- posed rank-1 subspace relational model over that of constant translational model in the same regu- larization framework. Note that this objective is similar in spirit to ( <ref type="bibr" target="#b25">Xu et al., 2014</ref>) in the sense that it also uses a constant translation model for relations. However, <ref type="bibr" target="#b25">Xu et al. (Xu et al., 2014</ref>) employ a maximum margin objective on the rela- tion triplets as originally proposed in <ref type="bibr">(Bordes et al., 2013)</ref>. It encourages the loss (measured in terms of 2 distance) for true relation triplets to be smaller than the loss for randomly corrupted re- lation triplets. Instead of a maximum margin ob- jective for relational knowledge, our model uses a simpler regularization based objective. We could not obtain the implementation of RC-NET ( <ref type="bibr" target="#b25">Xu et al., 2014</ref>) due to copyright issues cited by its au- thors. We also cannot compare with approaches that use only knowledge-base for training <ref type="bibr" target="#b7">(Faruqui et al., 2015</ref>) since they do not learn or modify the embeddings of unseen words and our evaluation triplets do not overlap with training triplets.</p><p>We use the CBOW implementation in publicly available Word2Vec code 3 for our experiments. Our vocabulary has 400k words and we use a dimensionality of 300 for embeddings. For all other parameters, we use default values that the Word2Vec code comes with including a context window size of 5 tokens to each side, 5 neg- ative samples per positive sample for negative sampling technique, etc. For both RELSUB and RELCONST, we set the regularization parameter to λ |R| = 1e −4 in all our experiments. We set the up- per limit c in Eq. 4 to 1. The parameters were not fine tuned rigorously but these values seemed to work reasonably well for us. We do total 5 epochs of SGD over the text corpus for all methods.</p><p>In knowledge-base completion task, we want to predict the missing word of a relation triplet. For a triplet (x, r, y), we assume that x (first word) and <ref type="bibr">3</ref> https://code.google.com/p/word2vec/ r (relation type) are observed and the task is to predict the missing word y. We restrict the search for the missing word to the most frequent 300k words (75% of the vocabulary). The missing word is predicted to be the closest word along the rank-1 subspace spanned by the relation vector (restricted by c in Eq. 4). For RELCONST, the missing word is predicted by translating the first word by the re- lation vector and then searching for nearest word. The accuracy results on WordRep data are shown in <ref type="table" target="#tab_0">Table 1</ref>. Relaxing the constant translation to rank-1 subspace assumption results in significant improvements on this task.</p><p>In the analogy task, we want to predict the miss- ing word in an analogy tuple a:b::c:?. We use the Google word-analogy data ( <ref type="bibr" target="#b16">Mikolov et al., 2013a</ref>) for this evaluation. We observe consider- able gains with RELSUB over CBOW for seman- tic categories. The accuracy of knowledge reg- ularized methods on syntactic categories is a lit- tle worse than CBOW and only slightly better than RELCONST, which is contrary to our observation on the knowledge-base completion task. This is due to the fact that analogy task uses the differ- ence vector (b − a) instead of the learned relation vector which is assumed to be unknown.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>WordRep data: Accuracy on knowledge-
base completion 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Google analogy data: Accuracy on word 
analogy task 

(Xu et al., 2014) for learning word embeddings. 
Our objective for RELCONST is same as Eq. 4 ex-
cept that {α k } m 
k=1 </table></figure>

			<note place="foot" n="1"> There exists work on relation extraction and knowledgebase completion that combines structured relation triplets and logical rules with unstructured text using various forms of latent variable models (Riedel et al., 2013; Chang et al., 2014; Toutanova et al., 2015; Rocktäschel et al., 2015).</note>

			<note place="foot" n="3"> Empirical Observations We report preliminary evaluations of the proposed model (termed as RELSUB) on the tasks of word analogy and knowledge base completion. We use 2 https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Concluding Remarks</head><p>We proposed a novel framework for modeling relational knowledge in word embeddings using rank-1 subspace regularization. Our model can be seen as a generalization of the constant trans-lational model for relations <ref type="bibr">(Bordes et al., 2013;</ref><ref type="bibr" target="#b25">Xu et al., 2014</ref>). In the future, we would like to study the interplay between word frequencies and the strength of regularization, and perform an ex-haustive empirical evaluation. The study of higher rank subspaces for relation modeling is also an im-portant future direction.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2014</title>
		<meeting>ACL 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="809" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2013</title>
		<meeting>NIPS 2013</meeting>
		<imprint>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view learning of word embeddings via CCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2011</title>
		<meeting>NIPS 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2015</title>
		<meeting>NAACL-HLT 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4369</idno>
		<title level="m">Incorporating both distributional and relational semantics in word representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.1640</idno>
		<title level="m">Wordrep: A benchmark for research on learning word representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving mikolov et al.&apos;s negativesampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Israel</forename><surname>Ramat-Gan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning semantic word embeddings based on ordinal knowledge constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2011</title>
		<meeting>ACL 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2013</title>
		<meeting>ICLR 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL</title>
		<meeting>HLTNAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Injecting logical background knowledge into embeddings for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<editor>HLT-NAALC</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2011</title>
		<meeting>NIPS 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2010</title>
		<meeting>ACL 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">RCNET: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2014</title>
		<meeting>CIKM 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2014</title>
		<meeting>ACL 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2013</title>
		<meeting>EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
