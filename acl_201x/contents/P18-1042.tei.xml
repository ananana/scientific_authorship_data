<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARANMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
							<email>jwieting@cs.cmu.edu, kgimpel@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PARANMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="451" to="462"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>451</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We describe PARANMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al. (2017). Our hope is that PARANMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use PARANMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While many approaches have been developed for generating or finding paraphrases ( <ref type="bibr">Barzilay and McKeown, 2001;</ref><ref type="bibr" target="#b11">Lin and Pantel, 2001;</ref><ref type="bibr" target="#b23">Dolan et al., 2004</ref>), there do not exist any freely- available datasets with millions of sentential para- phrase pairs. The closest such resource is the Paraphrase Database (PPDB; <ref type="bibr">Ganitkevitch et al., 2013)</ref>, which was created automatically from bilingual text by pivoting over the non-English language <ref type="bibr">(Bannard and Callison-Burch, 2005</ref>). PPDB has been used to improve word embed- dings ( <ref type="bibr">Faruqui et al., 2015;</ref><ref type="bibr" target="#b17">Mrkši´Mrkši´c et al., 2016)</ref>. However, PPDB is less useful for learning sen- tence embeddings .</p><p>In this paper, we describe the creation of a dataset containing more than 50 million sentential paraphrase pairs. We create it automatically by scaling up the approach of . We use neural machine translation (NMT) to translate the Czech side of a large Czech-English parallel corpus. We pair the English translations with the English references to form paraphrase pairs. We call this dataset PARANMT-50M. It contains examples illustrating a broad range of paraphrase phenomena; we show examples in Sec- tion 3. PARANMT-50M has the potential to be useful for many tasks, from linguistically con- trolled paraphrase generation, style transfer, and sentence simplification to core NLP problems like machine translation.</p><p>We show the utility of PARANMT-50M by us- ing it to train paraphrastic sentence embeddings using the learning framework of <ref type="bibr" target="#b31">Wieting et al. (2016b)</ref>. We primarily evaluate our sentence em- beddings on the SemEval semantic textual similar- ity (STS) competitions from 2012-2016. Since so many domains are covered in these datasets, they form a demanding evaluation for a general purpose sentence embedding model. Our sentence embeddings learned from PARANMT-50M outperform all systems in every STS competition from 2012 to 2016. These tasks have drawn substantial participation; in 2016, for example, the competition attracted 43 teams and had 119 submissions. Most STS systems use curated lexical resources, the provided supervised training data with manually-annotated similari- ties, and joint modeling of the sentence pair. We use none of these, simply encoding each sentence independently using our models and computing cosine similarity between their embeddings.</p><p>We experiment with several compositional ar- chitectures and find them all to work well. We find benefit from making a simple change to learn- ing ("mega-batching") to better leverage the large training set, namely, increasing the search space of negative examples. In the supplementary, we evaluate on general-purpose sentence embedding tasks used in past work ( <ref type="bibr" target="#b8">Kiros et al., 2015;</ref><ref type="bibr">Conneau et al., 2017)</ref>, finding our embeddings to per- form competitively.</p><p>Finally, in Section 6, we briefly report re- sults showing how PARANMT-50M can be used for paraphrase generation. A standard encoder- decoder model trained on PARANMT-50M can generate paraphrases that show effects of "canon- icalizing" the input sentence. In other work, fully described by <ref type="bibr">Iyyer et al. (2018)</ref>, we used PARANMT-50M to generate paraphrases that have a specific syntactic structure (represented as the top two levels of a linearized parse tree).</p><p>We release the PARANMT-50M dataset, our trained sentence embeddings, and our code. PARANMT-50M is the largest collection of sen- tential paraphrases released to date. We hope it can motivate new research directions and be used to create powerful NLP models, while adding a robustness to existing ones by incorporating para- phrase knowledge. Our paraphrastic sentence em- beddings are state-of-the-art by a significant mar- gin, and we hope they can be useful for many ap- plications both as a sentence representation func- tion and as a general similarity metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We discuss work in automatically building para- phrase corpora, learning general-purpose sentence embeddings, and using parallel text for learning embeddings and similarity functions.</p><p>Paraphrase discovery and generation. Many methods have been developed for generating or finding paraphrases, including using multiple translations of the same source material ( <ref type="bibr">Barzilay and McKeown, 2001</ref>), using distributional similar- ity to find similar dependency paths <ref type="bibr" target="#b11">(Lin and Pantel, 2001</ref>), using comparable articles from mul- tiple news sources ( <ref type="bibr" target="#b23">Dolan et al., 2004;</ref><ref type="bibr">Dolan and Brockett, 2005;</ref><ref type="bibr" target="#b23">Quirk et al., 2004</ref>), aligning sentences between standard and Simple English Wikipedia ( <ref type="bibr">Coster and Kauchak, 2011</ref>), crowd- sourcing ( <ref type="bibr" target="#b36">Xu et al., 2014</ref><ref type="bibr" target="#b35">Xu et al., , 2015</ref><ref type="bibr">Jiang et al., 2017)</ref>, using diverse MT systems to translate a single source sentence ( <ref type="bibr" target="#b27">Suzuki et al., 2017)</ref>, and using tweets with matching URLs ( <ref type="bibr" target="#b9">Lan et al., 2017)</ref>.</p><p>The most relevant prior work uses bilingual cor- pora. <ref type="bibr">Bannard and Callison-Burch (2005)</ref> used methods from statistical machine translation to find lexical and phrasal paraphrases in parallel text. <ref type="bibr">Ganitkevitch et al. (2013)</ref> scaled up these techniques to produce the Paraphrase Database (PPDB). Our goals are similar to those of PPDB, which has likewise been generated for many lan- guages ( <ref type="bibr">Ganitkevitch and Callison-Burch, 2014</ref>) since it only needs parallel text. In particular, we follow the approach of , who used NMT to translate the non-English side of par- allel text to get English-English paraphrase pairs. We scale up the method to a larger dataset, pro- duce state-of-the-art paraphrastic sentence embed- dings, and release all of our resources.</p><p>Sentence embeddings. Our learning and eval- uation setting is the same as that of our re- cent work that seeks to learn paraphrastic sen- tence embeddings that can be used for downstream tasks <ref type="bibr">(Wieting et al., 2016b,a;</ref>. We trained mod- els on noisy paraphrase pairs and evaluated them primarily on semantic textual similarity (STS) tasks. Prior work in learning general sentence embeddings has used autoencoders <ref type="bibr" target="#b26">(Socher et al., 2011;</ref><ref type="bibr">Hill et al., 2016)</ref>, encoder-decoder architec- tures ( <ref type="bibr" target="#b8">Kiros et al., 2015;</ref><ref type="bibr">Gan et al., 2017)</ref>, and other sources of supervision and learning frame- works ( <ref type="bibr" target="#b10">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b22">Pham et al., 2015;</ref><ref type="bibr" target="#b5">Arora et al., 2017;</ref><ref type="bibr" target="#b18">Pagliardini et al., 2017;</ref><ref type="bibr">Conneau et al., 2017</ref>).</p><p>Parallel text for learning embeddings. Prior work has shown that parallel text, and resources built from parallel text like NMT systems and PPDB, can be used for learning embeddings for words and sentences. Several have used PPDB as a knowledge resource for training or improving embeddings <ref type="bibr">(Faruqui et al., 2015;</ref><ref type="bibr" target="#b32">Wieting et al., 2015;</ref><ref type="bibr" target="#b17">Mrkši´Mrkši´c et al., 2016)</ref>. NMT architectures and training settings have been used to obtain bet- ter embeddings for words ( <ref type="bibr">Hill et al., 2014a,b)</ref> and words-in-context ( <ref type="bibr" target="#b15">McCann et al., 2017)</ref>. <ref type="bibr">Hill et al. (2016)</ref> evaluated the encoders of English- to-X NMT systems as sentence representations.  adapted trained NMT mod- els to produce sentence similarity scores in seman- tic evaluations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The PARANMT-50M Dataset</head><p>To create our dataset, we used back-translation of bitext ( . We used a Czech- English NMT system to translate Czech sentences  from the training data into English. We paired the translations with the English references to form English-English paraphrase pairs. We used the pretrained Czech-English model from the NMT system of . Its training data includes four sources: Common Crawl, CzEng 1.6 ( <ref type="bibr">Bojar et al., 2016)</ref>, Europarl, and News Commentary. We did not choose Czech due to any particular linguistic properties.  found little difference among Czech, German, and French as source languages for back- translation. There were much larger differences due to data domain, so we focus on the question of domain in this section. We leave the question of investigating properties of back-translation of dif- ferent languages to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Choosing a Data Source</head><p>To assess characteristics that yield useful data, we randomly sampled 100K English reference trans- lations from each data source and computed statis- tics. <ref type="table">Table 1</ref> shows the average sentence length, the average inverse document frequency (IDF) where IDFs are computed using Wikipedia sen- tences, and the average paraphrase score for the two sentences. The paraphrase score is calcu- lated by averaging PARAGRAM-PHRASE embed- dings ( <ref type="bibr" target="#b31">Wieting et al., 2016b</ref>) for the two sentences in each pair and then computing their cosine sim- ilarity. The table also shows the entropies of the vocabularies and constituent parses obtained using the Stanford Parser ( ). <ref type="bibr">2</ref> Europarl exhibits the least diversity in terms of rare word usage, vocabulary entropy, and parse entropy. This is unsurprising given its formu- laic and repetitive nature. CzEng has shorter sen- tences than the other corpora and more diverse sentence structures, as shown by its high parse en- tropy. In terms of vocabulary use, CzEng is not particularly more diverse than Common Crawl and News Commentary, though this could be due to the prevalence of named entities in the latter two. In Section 5.3, we empirically compare these data sources as training data for sentence embed- dings. The CzEng corpus yields the strongest per- formance when controlling for training data size. Since its sentences are short, we suspect this helps ensure high-quality back-translations. A large por- tion of it is movie subtitles which tend to use a wide vocabulary and have a diversity of sentence structures; however, other domains are included as well. It is also the largest corpus, containing over 51 million sentence pairs. In addition to pro- viding a large number of training examples for downstream tasks, this means that the NMT sys- tem should be able to produce quality translations for this subset of its training data.</p><p>For all of these reasons, we chose the CzEng corpus to create PARANMT-50M. When doing so, we used beam search with a beam size of 12 and selected the highest scoring translation from the beam. It took over 10,000 GPU hours to back- translate the CzEng corpus. We show illustrative examples in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Manual Evaluation</head><p>We conducted a manual analysis of our dataset in order to quantify its noise level and assess how the</p><formula xml:id="formula_0">Para. Score # Avg. Tri. Paraphrase Fluency Range (M) Overlap 1 2 3 1 2 3 (-0.1, 0.2]</formula><p>4.0 0.00±0.0 <ref type="table">Table 3</ref>: Manual evaluation of PARANMT-50M. 100-pair samples were drawn from five ranges of the automatic paraphrase score (first column). Paraphrase strength and fluency were judged on a 1-3 scale and counts of each rating are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="92">6 2 1 5 94 (0.2, 0.4] 3.8 0.02±0.1 53 32 15 1 12 87 (0.4, 0.6] 6.9 0.07±0.1 22 45 33 2 9 89 (0.6, 0.8] 14.4 0.17±0.2 1 43 56 11 0 89 (0.8, 1.0] 18.0 0.35±0.2 1 13 86 3 0 97</head><p>noise can be ameliorated with filtering. Two na- tive English speakers annotated a sample of 100 examples from each of five ranges of the Para- phrase Score. <ref type="bibr">3</ref> We obtained annotations for both the strength of the paraphrase relationship and the fluency of the translations.</p><p>To annotate paraphrase strength, we adopted the annotation guidelines used by <ref type="bibr" target="#b4">Agirre et al. (2012)</ref>. The original guidelines specify six classes, which we reduced to three for simplicity. We combined the top two into one category, left the next, and combined the bottom three into the lowest cate- gory. Therefore, for a sentence pair to have a rat- ing of 3, the sentences must have the same mean- ing, but some unimportant details can differ. To have a rating of 2, the sentences are roughly equiv- alent, with some important information missing or that differs slightly. For a rating of 1, the sentences are not equivalent, even if they share minor details.</p><p>For fluency of the back-translation, we use the following: A rating of 3 means it has no grammat- ical errors, 2 means it has one to two errors, and 1 means it has more than two grammatical errors or is not a natural English sentence. <ref type="table">Table 3</ref> summarizes the annotations. For each score range, we report the number of pairs, the mean trigram overlap score, and the number of times each paraphrase/fluency label was present in the sample of 100 pairs. There is noise but it is largely confined to the bottom two ranges which together comprise only 16% of the entire dataset. In the highest paraphrase score range, 86% of the pairs possess a strong paraphrase relationship. The annotations suggest that PARANMT-50M con- tains approximately 30 million strong paraphrase pairs, and that the paraphrase score is a good indi-cator of quality. At the low ranges, we inspected the data and found there to be many errors in the sentence alignment in the original bitext. With re- gards to fluency, approximately 90% of the back- translations are fluent, even at the low end of the paraphrase score range. We do see an outlier at the second-highest range of the paraphrase score, but this may be due to the small number of annotated examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Sentence Embeddings</head><p>To show the usefulness of the PARANMT-50M dataset, we will use it to train sentence embed- dings. We adopt the learning framework from <ref type="bibr" target="#b31">Wieting et al. (2016b)</ref>, which was developed to train sentence embeddings from pairs in PPDB. We first describe the compositional sentence em- bedding models we will experiment with, then discuss training and our modification ("mega- batching").</p><p>Models. We want to embed a word sequence s into a fixed-length vector. We denote the tth word in s as s t , and we denote its word embedding by x t . We focus on three model families, though we also experiment with combining them in various ways. The first, which we call WORD, simply av- erages the embeddings x t of all words in s. This model was found by <ref type="bibr" target="#b31">Wieting et al. (2016b)</ref> to per- form strongly for semantic similarity tasks.</p><p>The second is similar to WORD, but instead of word embeddings, we average character trigram embeddings ( <ref type="bibr">Huang et al., 2013</ref>). We call this TRIGRAM. <ref type="bibr" target="#b30">Wieting et al. (2016a)</ref> found this to work well for sentence embeddings compared to other n-gram orders and to word averaging.</p><p>The third family includes long short-term mem- ory (LSTM) networks (Hochreiter and Schmidhu- ber, 1997). We average the hidden states to pro- duce the final sentence embedding. For regular- ization during training, we scramble words with a small probability ( . We also experiment with bidirectional LSTMs (BLSTM), averaging the forward and backward hidden states with no concatenation. 4</p><p>Training. The training data is a set S of para- phrase pairs s, s and we minimize a margin-</p><formula xml:id="formula_1">based loss (s, s ) = max(0, δ − cos(g(s), g(s )) + cos(g(s), g(t)))</formula><p>where g is the model (WORD, TRIGRAM, etc.), δ is the margin, and t is a "negative example" taken from a mini-batch during optimization. The intu- ition is that we want the two texts to be more sim- ilar to each other than to their negative examples. To select t we choose the most similar sentence in some set. For simplicity we use the mini-batch for this set, i.e., t = argmax</p><formula xml:id="formula_2">t :t ,··∈S b \{{s,s } cos(g(s), g(t ))</formula><p>where S b ⊆ S is the current mini-batch.</p><p>Modification: mega-batching. By using the mini-batch to select negative examples, we may be limiting the learning procedure. That is, if all potential negative examples in the mini-batch are highly dissimilar from s, the loss will be too easy to minimize. Stronger negative examples can be obtained by using larger mini-batches, but large mini-batches are sub-optimal for optimization. Therefore, we propose a procedure we call "mega-batching." We aggregate M mini-batches to create one mega-batch and select negative ex- amples from the mega-batch. Once each pair in the mega-batch has a negative example, the mega- batch is split back up into M mini-batches and training proceeds. We found that this provides more challenging negative examples during learn- ing as shown in Section 5.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We now investigate how best to use our generated paraphrase data for training paraphrastic sentence embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation</head><p>We evaluate sentence embeddings using the Sem- Eval semantic textual similarity (STS) tasks from 2012 to <ref type="bibr" target="#b4">(Agirre et al., 2012</ref>) and the STS Benchmark ( <ref type="bibr">Cer et al., 2017)</ref>. Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 means they are completely equivalent. As our test set, we report the average Pearson's r  The supplementary material contains a descrip- tion of a method to obtain a paraphrase lexicon from PARANMT-50M that is on par with that pro- vided by PPDB 2.0. We also evaluate our sen- tence embeddings on a range of additional tasks that have previously been used for evaluating sen- tence representations ( <ref type="bibr" target="#b8">Kiros et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>For training sentence embeddings on PARANMT- 50M, we follow the experimental procedure of <ref type="bibr" target="#b31">Wieting et al. (2016b)</ref>. We use PARAGRAM- SL999 embeddings ( <ref type="bibr" target="#b32">Wieting et al., 2015</ref>) to ini- tialize the word embedding matrix for all models that use word embeddings. We fix the mini-batch size to 100 and the margin δ to 0.4. We train all models for 5 epochs. For optimization we use Adam ( <ref type="bibr" target="#b7">Kingma and Ba, 2014</ref>) with a learning rate of 0.001. For the LSTM and BLSTM, we fixed the scrambling rate to 0.3. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dataset Comparison</head><p>We first compare parallel data sources. We evalu- ate the quality of a data source by using its back- translations paired with its English references as training data for paraphrastic sentence embed- dings. We compare the four data sources described in Section 3. We use 100K samples from each corpus and trained 3 different models on each: WORD, TRIGRAM, and LSTM. <ref type="table" target="#tab_4">Table 4</ref> shows that CzEng provides the best training data for all models, so we used it to create PARANMT-50M and for all remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filtering Method Model Avg. Translation Score 83.2 Trigram Overlap 83.1 Paraphrase Score</head><p>83.3 CzEng is diverse in terms of both vocabulary and sentence structure. It has significantly shorter sentences than the other corpora, and has much more training data, so its translations are ex- pected to be better than those in the other corpora.  found that sentence length was the most important factor in filtering quality training data, presumably due to how NMT qual- ity deteriorates with longer sentences. We suspect that better translations yield better data for training sentence embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Data Filtering</head><p>Since the PARANMT-50M dataset is so large, it is computationally demanding to train sentence em- beddings on it in its entirety. So, we filter the data to create a training set for sentence embeddings.</p><p>We experiment with three simple methods: (1) the length-normalized translation score from de- coding, (2) trigram overlap ( , and (3) the paraphrase score from Section 3. Tri- gram overlap is calculated by counting trigrams in the reference and translation, then dividing the number of shared trigrams by the total number in the reference or translation, whichever has fewer.</p><p>We filtered the back-translated CzEng data us- ing these three strategies. We ranked all 51M+ paraphrase pairs in the dataset by the filtering mea- sure under consideration and then split the data into tenths (so the first tenth contains the bottom 10% under the filtering criterion, the second con- tains those in the bottom 10-20%, etc.).</p><p>We trained WORD, TRIGRAM, and LSTM models for a single epoch on 1M examples sam- pled from each of the ten folds for each filter- ing criterion. We averaged the correlation on the STS2017 data across models for each fold. Ta- ble 5 shows the results of the filtering methods. Filtering based on the paraphrase score produces the best data for training sentence embeddings.</p><p>We randomly selected 5M examples from the top two scoring folds using paraphrase score fil-    tering, ensuring that we only selected examples in which both sentences have a maximum length of 30 tokens. 6 These resulting 5M examples form the training data for the rest of our experiments. Note that many more than 5M pairs from the dataset are useful, as suggested by our human evaluations in Section 3.2. We have experimented with dou- bling the training data when training our best sen- tence similarity model and found the correlation increased by more than half a percentage point on average across all datasets. <ref type="table" target="#tab_2">Table 6</ref> shows the impact of varying the mega- batch size M when training for 5 epochs on our 5M-example training set. For all models, larger mega-batches improve performance. There is a smaller gain when moving from 20 to 40, but all models show clear gains over M = 1. <ref type="table" target="#tab_8">Table 7</ref> shows negative examples with differ- ent mega-batch sizes M . We use the BLSTM model and show the negative examples (nearest neighbors from the mega-batch excluding the cur- rent training example) for three sentences. Using larger mega-batches improves performance, pre- sumably by producing more compelling negative examples for the learning procedure. This is likely more important when training on sentences than   <ref type="bibr" target="#b22">Pham et al., 2015)</ref> 63.9 GloVe ( <ref type="bibr" target="#b21">Pennington et al., 2014)</ref> 300 40.6 word2vec ( <ref type="bibr" target="#b16">Mikolov et al., 2013)</ref> 300 56.5 sent2vec ( <ref type="bibr" target="#b18">Pagliardini et al., 2017)</ref> 700 75.5 Related Work (Supervised) Dep. Tree LSTM ( <ref type="bibr" target="#b28">Tai et al., 2015)</ref> 71.2 Const. Tree LSTM ( <ref type="bibr" target="#b28">Tai et al., 2015)</ref> 71.9 CNN <ref type="bibr" target="#b25">(Shao, 2017)</ref> 78.4 <ref type="table">Table 9</ref>: Results on STS Benchmark test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Effect of Mega-Batching</head><note type="other">51.5 67.8 68.3 67.2 InferSent (SNLI) (Conneau et al., 2017) 4096 57.1 50.4 66.2 65.2 63.5 FastSent (Hill et al., 2016) 100 - - 63 - - DictRep (Hill et al., 2016) 500 - - 67 - - Related Work SkipThought (Kiros et al., 2015) 4800 - - 29 - - CPHRASE (Pham et al., 2015) - - - 65 - - CBOW (from Hill et al., 2016) 500 - - 64 - - BLEU (Papineni et al., 2002) - 39.2 29.5 42.8 49.8 47.4 METEOR (Denkowski and Lavie, 2014) - 53.4 47.6 63.7 68.8 61.8</note><p>prior work on learning from text snippets ( <ref type="bibr" target="#b32">Wieting et al., 2015</ref><ref type="bibr" target="#b31">Wieting et al., , 2016b</ref><ref type="bibr" target="#b22">Pham et al., 2015</ref>). <ref type="table" target="#tab_10">Table 8</ref> shows results on the 2012-2016 STS tasks and <ref type="table">Table 9</ref> shows results on the STS Benchmark. <ref type="bibr">7</ref> Our best models outperform all STS competition systems and all related work of which we are <ref type="bibr">7</ref> Baseline results are from http://ixa2.si.ehu. es/stswiki/index.php/STSbenchmark, except for the unsupervised InferSent result which we computed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Model Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Mean  <ref type="table">Table 10</ref>: The means (over all 25 STS competi- tion datasets) of the absolute differences in Pear- son's r between each pair of models.</p><p>aware on the 2012-2016 STS datasets. Note that the large improvement over BLEU and METEOR suggests that our embeddings could be useful for evaluating machine translation output. Overall, our individual models (WORD, TRI- GRAM, LSTM) perform similarly. Using 300 di- mensions appears to be sufficient; increasing di- mensionality does not necessarily improve corre- lation. When examining particular STS tasks, we found that our individual models showed marked differences on certain tasks. <ref type="table">Table 10</ref> shows the mean absolute difference in Pearson's r over all 25 datasets. The TRIGRAM model shows the largest differences from the other two, both of which use word embeddings. This suggests that TRIGRAM may be able to complement the other two by pro- viding information about words that are unknown to models that rely on word embeddings.</p><p>We experiment with two ways of combining models. The first is to define additive architectures <ref type="table">Target Syntax  Paraphrase  original</ref> with the help of captain picard, the borg will be prepared for everything.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(SBARQ(ADVP)(,)(S)(,)(SQ))</head><p>now, the borg will be prepared by picard, will it? (S(NP)(ADVP)(VP)) the borg here will be prepared for everything. original you seem to be an excellent burglar when the time comes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(S(SBAR)(,)(NP)(VP))</head><p>when the time comes, you'll be a great thief.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(S('')(UCP)('')(NP)(VP))</head><p>"you seem to be a great burglar, when the time comes." you said. <ref type="table">Table 11</ref>: Syntactically controlled paraphrases generated by the SCPN trained on PARANMT-50M.</p><p>that form the embedding for a sentence by adding the embeddings computed by two (or more) indi- vidual models. All parameters are trained jointly just like when we train individual models; that is, we do not first train two simple models and add their embeddings. The second way is to de- fine concatenative architectures that form a sen- tence embedding by concatenating the embed- dings computed by individual models, and again to train all parameters jointly.</p><p>In <ref type="table" target="#tab_10">Table 8</ref> and <ref type="table">Table 9</ref>, these combinations show consistent improvement over the individual mod- els as well as the larger LSTM and BLSTM. Con- catenating WORD and TRIGRAM results in the best performance on average across STS tasks, outperforming the best supervised systems from each year. We have released the pretrained model for these "WORD, TRIGRAM" embeddings. In ad- dition to providing a strong baseline for future STS tasks, these embeddings offer the advantages of being extremely efficient to compute and being ro- bust to unknown words.</p><p>We show the usefulness of PARANMT by also reporting the results of training the "WORD, TRI- GRAM" model on SimpWiki, a dataset of aligned sentences from Simple English and standard <ref type="bibr">English Wikipedia (Coster and Kauchak, 2011)</ref>. It has been shown useful for training sentence em- beddings in past work . However, <ref type="table" target="#tab_10">Table 8</ref> shows that training on PARANMT leads to gains in correlation of 3 to 6 points compared to SimpWiki.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Paraphrase Generation</head><p>In addition to powering state-of-the-art paraphras- tic sentence embeddings, our dataset is useful for paraphrase generation. We briefly describe two ef- forts in paraphrase generation here.</p><p>We have found that training an encoder-decoder model on PARANMT-50M can produce a para- phrase generation model that canonicalizes text. For this experiment, we used a bidirectional LSTM encoder and a two-layer LSTM decoder original overall, i that it's a decent buy, and am happy that i own it. paraphrase it's a good buy, and i'm happy to own it. original oh, that's a handsome women, that is. paraphrase that's a beautiful woman. with soft attention over the encoded states <ref type="bibr" target="#b6">(Bahdanau et al., 2015)</ref>. The attention computation consists of a bilinear product with a learned pa- rameter matrix. <ref type="table" target="#tab_1">Table 12</ref> shows examples of out- put generated by this model, showing how the model is able to standardize the text and correct grammatical errors. This model would be interest- ing to evaluate for automatic grammar correction as it does so without any direct supervision. Fu- ture work could also use this canonicalization to improve performance of models by standardizing inputs and removing noise from data.</p><p>PARANMT-50M has also been used for syntactically-controlled paraphrase generation; this work is described in detail by <ref type="bibr">Iyyer et al. (2018)</ref>. A syntactically controlled paraphrase net- work (SCPN) is trained to generate a paraphrase of a sentence whose constituent structure follows a provided parse template. A parse template con- tains the top two levels of a linearized parse tree. <ref type="table">Table 11</ref> shows example outputs using the SCPN. The paraphrases mostly preserve the semantics of the input sentences while changing their syntax to fit the target syntactic templates. The SCPN was used for augmenting training data and finding ad- versarial examples.</p><p>We believe that PARANMT-50M and future datasets like it can be used to generate rich para- phrases that improve the performance and robust- ness of models on a multitude of NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>One way to view PARANMT-50M is as a way to represent the learned translation model in a mono-lingual generated dataset. This raises the ques- tion of whether we could learn an effective sen- tence embedding model from the original parallel text used to train the NMT system, rather than re- quiring the intermediate step of generating a para- phrase training set.</p><p>However, while <ref type="bibr">Hill et al. (2016) and</ref> used trained NMT models to produce sentence similarity scores, their correlations are considerably lower than ours (by 10% to 35% ab- solute in terms of Pearson). It appears that NMT encoders form representations that do not neces- sarily encode the semantics of the sentence in a way conducive to STS evaluations. They must instead create representations suitable for a de- coder to generate a translation. These two goals of representing sentential semantics and produc- ing a translation, while likely correlated, evidently have some significant differences.</p><p>Our use of an intermediate dataset leads to the best results, but this may be due to our efforts in optimizing learning for this setting <ref type="bibr" target="#b31">(Wieting et al., 2016b;</ref>. Future work will be needed to develop learning frameworks that can leverage parallel text directly to reach the same or improved correlations on STS tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We described the creation of PARANMT-50M, a dataset of more than 50M English sentential para- phrase pairs. We showed how to use PARANMT- 50M to train paraphrastic sentence embeddings that outperform supervised systems on STS tasks, as well as how it can be used for generating para- phrases for purposes of data augmentation, robust- ness, and even grammar correction.</p><p>The key advantage of our approach is that it only requires parallel text. There are hundreds of millions of parallel sentence pairs, and more are being generated continually. Our procedure is immediately applicable to the wide range of lan- guages for which we have parallel text.</p><p>We release PARANMT-50M, our code, and pretrained sentence embeddings, which also ex- hibit strong performance as general-purpose rep- resentations for a multitude of tasks. We hope that PARANMT-50M, along with our embeddings, can impart a notion of meaning equivalence to im- prove NLP systems for a variety of tasks. We are actively investigating ways to apply these two new resources to downstream applications, including machine translation, question answering, and ad- ditional paraphrase generation tasks.</p><p>the International Conference on Learning Represen- tations.</p><p>Colin <ref type="bibr">Bannard and Chris Callison-Burch. 2005</ref>. Para- phrasing with bilingual parallel corpora. In Pro- ceedings of the 43rd Annual Meeting on Association for Computational Linguistics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regina</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>6 :</head><label>6</label><figDesc>Pearson's r × 100 on STS2017 with dif- ferent mega-batch sizes M . original sir, i'm just trying to protect. negative examples: M = 1 i mean, colonel... M = 20 i only ask that the baby be safe. M = 40 just trying to survive. on instinct. original i'm looking at him, you know? M = 1 they know that i've been looking for her. M = 20 i'm keeping him. M = 40 i looked at him with wonder. original i'il let it go a couple of rounds. M = 1 sometimes the ball doesn't go down. M = 20 i'll take two. M = 40 i want you to sit out a couple of rounds, all right?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Example paraphrase pairs from PARANMT-50M, where each consists of an English reference 
translation and the machine translation of the Czech source sentence (not shown). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 6 shows</head><label>6</label><figDesc>re- sults for different values of M , showing consis- tently higher correlations with larger M values.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Pearson's r × 100 on STS2017 when 
training on 100k pairs from each back-translated 
parallel corpus. CzEng works best for all models. 

over each year of the STS tasks from 2012-2016. 
We use the small (250-example) English dataset 
from SemEval 2017 (Cer et al., 2017) as a devel-
opment set, which we call STS2017 below. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Pearson's r × 100 on STS2017 for the 
best training fold across the average of WORD, 
TRIGRAM, and LSTM models for each filtering 
method. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Negative examples for various mega-
batch sizes M with the BLSTM model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Pearson's r × 100 on the STS tasks of our models and those from related work. We compare to 
the top performing systems from each SemEval STS competition. Note that we are reporting the mean 
correlations over domains for each year rather than weighted means as used in the competitions. Our 
best performing overall model (WORD, TRIGRAM) is in bold. 

Dim. Corr. 
Our Work (Unsupervised) 
WORD 
300 79.2 
TRIGRAM 
300 79.1 
LSTM 
300 78.4 
WORD + TRIGRAM (addition) 
300 79.9 
WORD + TRIGRAM + LSTM (addition) 
300 79.6 
WORD, TRIGRAM (concatenation) 
600 79.9 
WORD, TRIGRAM, LSTM (concatenation) 900 79.2 
Related Work (Unsupervised) 
InferSent (AllSNLI) (Conneau et al., 2017) 4096 70.6 
C-PHRASE (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 12 :</head><label>12</label><figDesc>Examples from our paraphrase gener- ation model that show the ability to canonicalize text and correct grammatical errors.</figDesc><table></table></figure>

			<note place="foot" n="1"> Dataset, code, and embeddings are available at https: //www.cs.cmu.edu/ ˜ jwieting.</note>

			<note place="foot" n="2"> To mitigate sparsity in the parse entropy, we used only the top two levels of each parse tree.</note>

			<note place="foot" n="3"> Even though the similarity score lies in [−1, 1], most observed scores were positive, so we chose the five ranges shown in Table 3.</note>

			<note place="foot" n="4"> Unlike Conneau et al. (2017), we found this to outperform max-pooling for both semantic similarity and general sentence embedding tasks.</note>

			<note place="foot" n="5"> As in our prior work (Wieting and Gimpel, 2017), we found that scrambling significantly improves results, even with our much larger training set. But while we previously used a scrambling rate of 0.5, we found that a smaller rate of 0.3 worked better when training on PARANMT-50M, presumably due to the larger training set.</note>

			<note place="foot" n="6"> Wieting et al. (2017) found that sentence length cutoffs were effective for filtering back-translated parallel text.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers, the develop-ers of <ref type="bibr">Theano (Theano Development Team, 2016)</ref>, the developers of PyTorch ( <ref type="bibr" target="#b20">Paszke et al., 2017)</ref>, and NVIDIA Corporation for donating GPUs used in this research. <ref type="bibr">Manaal</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>English, Spanish</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>SemEval-2015 task 2: Semantic textual similarity</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">*SEM 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalezagirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity</title>
		<meeting>the Main Conference and the Shared Task: Semantic Textual Similarity</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the First Joint Conference on Lexical and Computational Semantics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A continuously growing dataset of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuwei</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1224" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discovery of inference rules for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="342" to="360" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Paraphrasing revisited with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="881" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd</title>
		<meeting>52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Counter-fitting word vectors to linguistic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="142" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Nghia The Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monolingual machine translation for paraphrase generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<meeting>EMNLP 2004<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="142" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nematus: a toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadejde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">HCTI at SemEval-2017 task 1: Use convolutional neural network to evaluate semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="130" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Building a non-trivial paraphrase corpus using multiple machine translation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yui</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyuki</forename><surname>Kajiwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, Student Research Workshop</title>
		<meeting>ACL 2017, Student Research Workshop<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1504" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<title level="m">From paraphrase database to compositional paraphrase model and back. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Revisiting recurrent networks for paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2078" to="2088" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning paraphrastic sentence embeddings from back-translated bitext</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Mallinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="274" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
		<title level="m">SemEval-2015 task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Extracting lexically divergent paraphrases from Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="435" to="448" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
