<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Cross-Domain Word Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
							<email>danushka.bollegala@ maehara.takanori@ k keniti@ liverpool.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Liverpool Shizuoka University National Institute of Informatics JST</orgName>
								<orgName type="institution" key="instit2">ERATO</orgName>
								<orgName type="institution" key="instit3">Kawarabayashi Large Graph Project</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Liverpool Shizuoka University National Institute of Informatics JST</orgName>
								<orgName type="institution" key="instit2">ERATO</orgName>
								<orgName type="institution" key="instit3">Kawarabayashi Large Graph Project</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Liverpool Shizuoka University National Institute of Informatics JST</orgName>
								<orgName type="institution" key="instit2">ERATO</orgName>
								<orgName type="institution" key="instit3">Kawarabayashi Large Graph Project</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Cross-Domain Word Representation Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="730" to="740"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Meaning of a word varies from one domain to another. Despite this important domain dependence in word semantics , existing word representation learning methods are bound to a single domain. Given a pair of source-target domains, we propose an unsupervised method for learning domain-specific word representations that accurately capture the domain-specific aspects of word semantics. First, we select a subset of frequent words that occur in both domains as pivots. Next, we optimize an objective function that enforces two constraints: (a) for both source and target domain documents, pivots that appear in a document must accurately predict the co-occurring non-pivots, and (b) word representations learnt for pivots must be similar in the two domains. Moreover, we propose a method to perform domain adaptation using the learnt word representations. Our proposed method significantly outperforms competitive baselines including the state-of-the-art domain-insensitive word representations , and reports best sentiment classification accuracies for all domain-pairs in a benchmark dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning semantic representations for words is a fundamental task in NLP that is required in nu- merous higher-level NLP applications <ref type="bibr" target="#b11">(Collobert et al., 2011</ref>). Distributed word representations have gained much popularity lately because of their accuracy as semantic representations for words ( <ref type="bibr" target="#b24">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b32">Pennington et al., 2014</ref>). However, the meaning of a word often varies from one domain to another. For exam- ple, the phrase lightweight is often used in a posi- tive sentiment in the portable electronics domain because a lightweight device is easier to carry around, which is a positive attribute for a portable electronic device. However, the same phrase has a negative sentiment assocition in the movie domain because movies that do not invoke deep thoughts in viewers are considered to be lightweight ( <ref type="bibr" target="#b9">Bollegala et al., 2014</ref>). However, existing word rep- resentation learning methods are agnostic to such domain-specific semantic variations of words, and capture semantics of words only within a single domain. To overcome this problem and capture domain-specific semantic orientations of words, we propose a method that learns separate dis- tributed representations for each domain in which a word occurs.</p><p>Despite the successful applications of dis- tributed word representation learning meth- ods ( <ref type="bibr" target="#b32">Pennington et al., 2014;</ref><ref type="bibr" target="#b11">Collobert et al., 2011;</ref><ref type="bibr" target="#b24">Mikolov et al., 2013a</ref>) most existing ap- proaches are limited to learning only a single representation for a given word <ref type="bibr" target="#b33">(Reisinger and Mooney, 2010)</ref>. Although there have been some work on learning multiple prototype representa- tions ( <ref type="bibr" target="#b19">Huang et al., 2012;</ref><ref type="bibr" target="#b30">Neelakantan et al., 2014</ref>) for a word considering its multiple senses, such methods do not consider the semantics of the do- main in which the word is being used.</p><p>If we can learn separate representations for a word for each domain in which it occurs, we can use the learnt representations for domain adapta- tion tasks such as cross-domain sentiment clas- sification ( <ref type="bibr" target="#b6">Bollegala et al., 2011b</ref>), cross-domain POS tagging ( <ref type="bibr" target="#b35">Schnabel and Schütze, 2013)</ref>, cross- domain dependency parsing ( <ref type="bibr" target="#b23">McClosky et al., 2010)</ref>, and domain adaptation of relation extrac- tors ( <ref type="bibr" target="#b7">Bollegala et al., 2013a;</ref><ref type="bibr" target="#b8">Bollegala et al., 2013b;</ref><ref type="bibr" target="#b5">Bollegala et al., 2011a;</ref><ref type="bibr" target="#b20">Jiang and Zhai, 2007a;</ref><ref type="bibr" target="#b21">Jiang and Zhai, 2007b)</ref>.</p><p>We introduce the cross-domain word represen-tation learning task, where given two domains, (referred to as the source (S) and the target (T )) the goal is to learn two separate representations w S and w T for a word w respectively from the source and the target domain that capture domain- specific semantic variations of w. In this paper, we use the term domain to represent a collection of documents related to a particular topic such as user-reviews in Amazon for a product category (e.g. books, dvds, movies, etc.). However, a do- main in general can be a field of study (e.g. biol- ogy, computer science, law, etc.) or even an entire source of information (e.g. twitter, blogs, news articles, etc.). In particular, we do not assume the availability of any labeled data for learning word representations.</p><p>This problem setting is closely related to unsu- pervised domain adaptation <ref type="bibr" target="#b3">(Blitzer et al., 2006</ref>), which has found numerous useful applications such as, sentiment classification and POS tagging. For example, in unsupervised cross-domain sen- timent classification <ref type="bibr" target="#b3">(Blitzer et al., 2006;</ref><ref type="bibr" target="#b4">Blitzer et al., 2007)</ref>, we train a binary sentiment classifier using positive and negative labeled user reviews in the source domain, and apply the trained clas- sifier to predict sentiment of the target domain's user reviews. Although the distinction between the source and the target domains is not important for the word representation learning step, it is impor- tant for the domain adaptation tasks in which we subsequently evaluate the learnt word representa- tions. Following prior work on domain adapta- tion ( <ref type="bibr" target="#b3">Blitzer et al., 2006</ref>), high-frequent features (unigrams/bigrams) common to both domains are referred to as domain-independent features or piv- ots. In contrast, we use non-pivots to refer to fea- tures that are specific to a single domain.</p><p>We propose an unsupervised cross-domain word representation learning method that jointly optimizes two criteria: (a) given a document d from the source or the target domain, we must ac- curately predict the non-pivots that occur in d us- ing the pivots that occur in d, and (b) the source and target domain representations we learn for piv- ots must be similar. The main challenge in domain adaptation is feature mismatch, where the features that we use for training a classifier in the source domain do not necessarily occur in the target do- main. Consequently, prior work on domain adap- tation ( <ref type="bibr" target="#b3">Blitzer et al., 2006;</ref><ref type="bibr" target="#b31">Pan et al., 2010</ref>) learn lower-dimensional mappings from non-pivots to pivots, thereby overcoming the feature mismatch problem. Criteria (a) ensures that word represen- tations for domain-specific non-pivots in each do- main are related to the word representations for domain-independent pivots. This relationship en- ables us to discover pivots that are similar to tar- get domain-specific non-pivots, thereby overcom- ing the feature mismatch problem.</p><p>On the other hand, criteria (b) captures the prior knowledge that high-frequent words common to two domains often represent domain-independent semantics. For example, in sentiment classifica- tion, words such as excellent or terrible would ex- press similar sentiment about a product irrespec- tive of the domain. However, if a pivot expresses different semantics in source and the target do- mains, then it will be surrounded by dissimilar sets of non-pivots, and reflected in the first crite- ria. Criteria (b) can also be seen as a regulariza- tion constraint imposed on word representations to prevent overfitting by reducing the number of free parameters in the model. Our contributions in this paper can be summa- rized as follows.</p><p>• We propose a distributed word representa- tion learning method that learns separate representations for a word for each do- main in which it occurs. To the best of our knowledge, ours is the first-ever domain-sensitive distributed word represen- tation learning method.</p><p>• Given domain-specific word representations, we propose a method to learn a cross-domain sentiment classifier. Experimental results for cross-domain senti- ment classification on a benchmark dataset show that the word representations learnt using the pro- posed method statistically significantly outper-form a state-of-the-art domain-insensitive word representation learning method ( <ref type="bibr" target="#b32">Pennington et al., 2014)</ref>, and several competitive baselines. In par- ticular, our proposed cross-domain word represen- tation learning method is not specific to a par- ticular task such as sentiment classification, and in principle, can be in applied to a wide-range of domain adaptation tasks. Despite this task- independent nature of the proposed method, it achieves the best sentiment classification accu- racies on all domain-pairs, reporting statistically comparable results to the current state-of-the-art unsupervised cross-domain sentiment classifica- tion methods <ref type="bibr" target="#b31">(Pan et al., 2010;</ref><ref type="bibr" target="#b3">Blitzer et al., 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Representing the semantics of a word using some algebraic structure such as a vector (more gener- ally a tensor) is a common first step in many NLP tasks <ref type="bibr" target="#b38">(Turney and Pantel, 2010)</ref>. By applying al- gebraic operations on the word representations, we can perform numerous tasks in NLP, such as com- posing representations for larger textual units be- yond individual words such as phrases <ref type="bibr" target="#b27">(Mitchell and Lapata, 2008)</ref>. Moreover, word representa- tions are found to be useful for measuring se- mantic similarity, and for solving proportional analogies ( <ref type="bibr" target="#b26">Mikolov et al., 2013c</ref>). Two main ap- proaches for computing word representations can be identified in prior work ( <ref type="bibr" target="#b1">Baroni et al., 2014</ref>): counting-based and prediction-based.</p><p>In counting-based approaches ( <ref type="bibr" target="#b0">Baroni and Lenci, 2010)</ref>, a word w is represented by a vec- tor w that contains other words that co-occur with w in a corpus. Numerous methods for selecting co-occurrence contexts such as proximity or de- pendency relations have been proposed <ref type="bibr" target="#b38">(Turney and Pantel, 2010)</ref>. Despite the numerous suc- cessful applications of co-occurrence counting- based distributional word representations, their high dimensionality and sparsity are often prob- lematic in practice. Consequently, further post- processing steps such as dimensionality reduction, and feature selection are often required when us- ing counting-based word representations.</p><p>On the other hand, prediction-based approaches first assign each word, for example, with a d- dimensional real-vector, and learn the elements of those vectors by applying them in an auxiliary task such as language modeling, where the goal is to predict the next word in a given sequence. The dimensionality d is fixed for all the words in the vocabulary, and, unlike counting-based word rep- resentations, is much smaller (e.g. d ∈ <ref type="bibr">[10,</ref><ref type="bibr">1000]</ref> in practice) compared to the vocabulary size. The neural network language model (NNLM) ( <ref type="bibr" target="#b2">Bengio et al., 2003</ref>) uses a multi-layer feed-forward neu- ral network to predict the next word in a sequence, and uses backpropagation to update the word vec- tors such that the prediction error is minimized.</p><p>Although NNLMs learn word representations as a by-product, the main focus on language modeling is to predict the next word in a sen- tence given the previous words, and not learn- ing word representations that capture semantics. Moreover, training multi-layer neural networks using large text corpora is time consuming. To overcome those limitations, methods that specif- ically focus on learning word representations that model word co-occurrences in large corpora have been proposed <ref type="bibr" target="#b24">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b29">Mnih and Kavukcuoglu, 2013;</ref><ref type="bibr" target="#b19">Huang et al., 2012;</ref><ref type="bibr" target="#b32">Pennington et al., 2014</ref>). Unlike the NNLM, these methods use all the words in a contextual win- dow in the prediction task. Methods that use one or no hidden layers are proposed to improve the scalability of the learning algorithms. For example, the skip-gram model ( <ref type="bibr" target="#b25">Mikolov et al., 2013b</ref>) predicts the words c that appear in the local context of a word w, whereas the continu- ous bag-of-words model (CBOW) predicts a word w conditioned on all the words c that appear in w's local context ( <ref type="bibr" target="#b24">Mikolov et al., 2013a</ref>). Meth- ods that use global co-occurrences in the entire corpus to learn word representations have shown to outperform methods that use only local co- occurrences ( <ref type="bibr" target="#b19">Huang et al., 2012;</ref><ref type="bibr" target="#b32">Pennington et al., 2014</ref>). Overall, prediction-based methods have shown to outperform counting-based meth- ods ( <ref type="bibr" target="#b1">Baroni et al., 2014</ref>).</p><p>Despite their impressive performance, existing methods for word representation learning do not consider the semantic variation of words across different domains. However, as described in Sec- tion 1, the meaning of a word vary from one do- main to another, and must be considered. To the best of our knowledge, the only prior work study- ing the problem of word representation variation across domains is due to <ref type="bibr" target="#b9">Bollegala et al. (2014)</ref>. Given a source and a target domain, they first se- lect a set of pivots using pointwise mutual infor- mation, and create two distributional representa-tions for each pivot using their co-occurrence con- texts in a particular domain. Next, a projection matrix from the source to the target domain feature spaces is learnt using partial least squares regres- sion. Finally, the learnt projection matrix is used to find the nearest neighbors in the source domain for each target domain-specific features. However, unlike our proposed method, their method does not learn domain-specific word representations, but simply uses co-occurrence counting when cre- ating in-domain word representations.</p><p>Faralli et al. <ref type="formula" target="#formula_0">(2012)</ref> proposed a domain-driven word sense disambiguation (WSD) method where they construct glossaries for several domain us- ing a pattern-based bootstrapping technique. This work demonstrates the importance of considering the domain specificity of word senses. However, the focus of their work is not to learn representa- tions for words or their senses in a domain, but to construct glossaries. It would be an interesting fu- ture research direction to explore the possibility of using such domain-specific glossaries for learning domain-specific word representations. <ref type="bibr" target="#b30">Neelakantan et al. (2014)</ref> proposed a method that jointly performs WSD and word embedding learning, thereby learning multiple embeddings per word type. In particular, the number of senses per word type is automatically estimated. How- ever, their method is limited to a single domain, and does not consider how the representations vary across domains. On the other hand, our proposed method learns a single representation for a partic- ular word for each domain in which it occurs.</p><p>Although in this paper we focus on the mono- lingual setting where source and target domains belong to the same language, the related setting where learning representations for words that are translational pairs across languages has been stud- ied ( <ref type="bibr">Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b22">Klementiev et al., 2012;</ref><ref type="bibr" target="#b17">Gouws et al., 2015)</ref>. Such representa- tions are particularly useful for cross-lingual in- formation retrieval <ref type="bibr" target="#b12">(Duc et al., 2010</ref>). It will be an interesting future research direction to extend our proposed method to learn such cross-lingual word representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cross-Domain Representation Learning</head><p>We propose a method for learning word represen- tations that are sensitive to the semantic variations of words across domains. We call this problem cross-domain word representation learning, and provide a definition in Section 3.1. Next, in Sec- tion 3.2, given a set of pivots that occurs in both a source and a target domain, we propose a method for learning cross-domain word representations. We defer the discussion of pivot selection meth- ods to Section 3.4. In Section 3.5, we propose a method for using the learnt word representations to train a cross-domain sentiment classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Let us assume that we are given two sets of docu- ments D S and D T respectively for a source (S) and a target (T ) domain. We do not consider the problem of retrieving documents for a domain, and assume such a collection of documents to be given. Then, given a particular word w, we define cross-domain representation learning as the task of learning two separate representations w S and w T capturing w's semantics in respectively the source S and the target T domains.</p><p>Unlike in domain adaptation, where there is a clear distinction between the source (i.e. the do- main on which we train) vs. the target (i.e. the domain on which we test) domains, for represen- tation learning purposes we do not make a distinc- tion between the two domains. In the unsupervised setting of the cross-domain representation learn- ing that we study in this paper, we do not assume the availability of labeled data for any domain for the purpose of learning word representations. As an extrinsic evaluation task, we apply the trained word representations for classifying sentiment re- lated to user-reviews (Section 3.5). However, for this evaluation task we require sentiment-labeled user-reviews from the source domain.</p><p>Decoupling of the word representation learn- ing from any tasks in which those representations are subsequently used, simplifies the problem as well as enables us to learn task-independent word representations with potential generic applicabil- ity. Although we limit the discussion to a pair of domains for simplicity, the proposed method can be easily extended to jointly learn word represen- tations for more than two domains. In fact, prior work on cross-domain sentiment analysis show that incorporating multiple source domains im- proves sentiment classification accuracy on a tar- get domain ( <ref type="bibr" target="#b6">Bollegala et al., 2011b;</ref><ref type="bibr" target="#b16">Glorot et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proposed Method</head><p>To describe our proposed method, let us denote a pivot and a non-pivot feature respectively by c and w. Our proposed method does not depend on a specific pivot selection method, and can be used with all previously proposed methods for selecting</p><note type="other">pivots as explained later in Section 3.4. A pivot c is represented in the source and target domains respectively by vectors c S ∈ R n and c T ∈ R n . Likewise, a source specific non-pivot w is repre- sented by w S in the source domain, whereas a tar- get specific non-pivot w is represented by w T in the target domain. By definition, a non-pivot oc- curs only in a single domain. For notational conve- nience we use w to denote non-pivots in both do- mains when the domain is clear from the context. We use C S , W S , C T , and W T to denote the sets of word representation vectors respectively for the source pivots, source non-pivots, target pivots, and target non-pivots.</note><p>Let us denote the set of documents in the source and the target domains respectively by D S and D T . Following the bag-of-features model, we as- sume that a document D is represented by the set of pivots and non-pivots that occur in D (w ∈ d and c ∈ d). We consider the co-occurrences of a pivot c and a non-pivot w within a fixed- size contextual window in a document. Following prior work on representation learning ( <ref type="bibr" target="#b24">Mikolov et al., 2013a)</ref>, in our experiments, we set the win- dow size to 10 tokens, without crossing sentence boundaries. The notation (c, w) ∈ d denotes the co-occurrence of a pivot c and a non-pivot w in a document d.</p><p>We learn domain-specific word representations by maximizing the prediction accuracy of the non- pivots w that occur in the local context of a pivot c. The hinge loss, L(C S , W S ), associated with predicting a non-pivot w in a source document d ∈ D S that co-occurs with pivots c is given by:</p><formula xml:id="formula_0">d∈D S (c,w)∈d w * ∼p(w) max 0, 1 − cS wS + cS w * S<label>(1)</label></formula><p>Here, w * S is the source domain representation of a non-pivot w * that does not occur in d. The loss function given by Eq. 1 requires that a non-pivot w that co-occurs with a pivot c in the document d is assigned a higher ranking score as measured by the inner-product between c S and w S than a non- pivot w * that does not occur in d. We randomly sample k non-pivots from the set of all source do- main non-pivots that do not occur in d as w * .</p><p>Specifically, we use the marginal distribution of non-pivots p(w), estimated from the corpus counts, as the sampling distribution. We raise p(w) to the 3/4-th power as proposed by <ref type="bibr" target="#b24">Mikolov et al. (2013a)</ref>, and normalize it to unit probabil- ity mass prior to sampling k non-pivots w * per each co-occurrence of (c, w) ∈ d. Because non- occurring non-pivots w * are randomly sampled, prior work on noise contrastive estimation has found that it requires more negative samples than positive samples to accurately learn a prediction model <ref type="bibr" target="#b29">(Mnih and Kavukcuoglu, 2013)</ref>. We exper- imentally found k = 5 to be an acceptable trade- off between the prediction accuracy and the num- ber of training instances.</p><p>Likewise, the loss function L(C T , W T ) for pre- dicting non-pivots using pivots in the target do- main is given by:</p><formula xml:id="formula_1">d∈D T (c,w)∈d w * ∼p(w) max 0, 1 − cT wT + cT w * T<label>(2)</label></formula><p>Here, w * denotes target domain non-pivots that do not occur in d, and are randomly sampled from p(w) following the same procedure as in the source domain. The source and target loss functions given re- spectively by Eqs. 1 and 2 can be used on their own to independently learn source and target domain word representations. However, by definition, piv- ots are common to both domains. We use this property to relate the source and target word repre- sentations via a pivot-regularizer, R(C S , C T ), de- fined as:</p><formula xml:id="formula_2">R(C S , C T ) = 1 2 K i=1 ||c (i) S − c (i) T || 2<label>(3)</label></formula><p>Here, ||x|| represents the l 2 norm of a vector x, and c (i) is the i-th pivot in a total collection of K pivots. Word representations for non-pivots in the source and target domains are linked via the pivot regularizer because, the non-pivots in each domain are predicted using the word representations for the pivots in each domain, which in turn are reg- ularized by Eq. 3. The overall objective function, L(C S , W S , C T , W T ), we minimize is the sum 1 of the source and target loss functions, regularized via Eq. 3 with coefficient λ, and is given by:</p><formula xml:id="formula_3">L(C S , W S , ) + L(C T , W T ) + λR(C S , C T ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Word representations of pivots c and non-pivots w in the source (c S , w S ) and the target (c T , w T ) do- mains are parameters to be learnt in the proposed method. To derive parameter updates, we compute the gradients of the overall loss function in Eq. 4 w.r.t. to each parameter as follows:</p><formula xml:id="formula_4">∂L ∂wS = 0 if cS (wS − w * S ) ≥ 1 −cS otherwise (5) ∂L ∂w * S = 0 if cS (wS − w * S ) ≥ 1 cS otheriwse (6) ∂L ∂wT = 0 if cT (wT − w * T ) ≥ 1 −cT otherwise (7) ∂L ∂w * T = 0 if cT (wT − w * T ) ≥ 1 cT otherwise (8) ∂L ∂cS = λ(cS − cT ) if cS (wS − w * S ) ≥ 1 w * S − wS + λ(cS − cT ) otherwise (9) ∂L ∂cT = λ(cT − cS ) if cT (wT − w * T ) ≥ 1 w * T − wT + λ(cT − cS ) otherwise<label>(10)</label></formula><p>Here, for simplicity, we drop the arguments inside the loss function and write it as L. We use mini batch stochastic gradient descent with a batch size of 50 instances. AdaGrad ( <ref type="bibr" target="#b13">Duchi et al., 2011</ref>) is used to schedule the learning rate. All word repre- sentations are initialized with n dimensional ran- dom vectors sampled from a zero mean and unit variance Gaussian. Although the objective in Eq. 4</p><p>is not jointly convex in all four representations, it is convex w.r.t. the representation of a partic- ular feature (pivot or non-pivot) when the repre- sentations for all the other features are held fixed.</p><p>In our experiments, the training converged in all cases with less than 100 epochs over the dataset. The rank-based predictive hinge loss (Eq. 1) is inspired by the prior work on word represen- tation learning for a single domain <ref type="bibr" target="#b11">(Collobert et al., 2011</ref>). However, unlike the multilayer neu- ral network in <ref type="bibr" target="#b11">Collobert et al. (2011)</ref>, the pro- posed method uses a computationally efficient sin- gle layer to reduce the number of parameters that must be learnt, thereby scaling to large datasets. Similar to the skip-gram model ( <ref type="bibr" target="#b24">Mikolov et al., 2013a</ref>), the proposed method predicts occurrences of contexts (non-pivots) w within a fixed-size con- textual window of a target word (pivot) c.</p><p>Scoring the co-occurrences of two words c and w by the bilinear form given by the inner-product is similar to prior work on domain-insensitive word-representation learning <ref type="bibr" target="#b28">(Mnih and Hinton, 2008;</ref><ref type="bibr" target="#b24">Mikolov et al., 2013a</ref>). However, unlike those methods that use the softmax function to convert inner-products to probabilities, we directly use the inner-products without any further trans- formations, thereby avoiding computationally ex- pensive distribution normalizations over the entire vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pivot Selection</head><p>Given two sets of documents D S , D T respec- tively for the source and the target domains, we use the following procedure to select pivots and non-pivots. First, we tokenize and lemmatize each document using the Stanford CoreNLP toolkit 2 . Next, we extract unigrams and bigrams as features for representing a document. We remove features listed as stop words using a standard stop words list. Stop word removal increases the effective co- occurrence window size for a pivot. Finally, we remove features that occur less than 50 times in the entire set of documents.</p><p>Several methods have been proposed in the prior work on domain adaptation for selecting a set of pivots from a given pair of domains such as the minimum frequency of occurrence of a fea- ture in the two domains, mutual information (MI), and the entropy of the feature distribution over the documents ( <ref type="bibr" target="#b31">Pan et al., 2010</ref>). In our preliminary experiments, we discovered that a normalized ver- sion of the PMI (NPMI) <ref type="bibr" target="#b10">(Bouma, 2009)</ref> to work consistently well for selecting pivots from differ- ent pairs of domains. NPMI between two features x and y is given by:</p><formula xml:id="formula_5">NPMI(x, y) = log p(x, y) p(x)p(y) 1 − log(p(x, y))<label>(11)</label></formula><p>Here, the joint probability p(x, y), and the marginal probabilities p(x) and p(y) are estimated using the number of co-occurrences of x and y in the sentences in the documents. Eq. 11 normalizes both the upper and lower bounds of the PMI.</p><p>We measure the appropriateness of a feature as a pivot according to the score given by:</p><p>score(x) = min (NPMI(x, S), NPMI(x, T )) .</p><p>(12) We rank features that are common to both domains in the descending order of their scores as given by Eq. 12, and select the top N P features as pivots. We rank features x that occur only in the source domain by NPMI(x, S), and select the top ranked N S features as source-specific non-pivots. Like- wise, we rank the features x that occur only in the target domain by NPMI(x, T ), and select the top ranked N T features as target-specific non-pivots.</p><p>The pivot selection criterion described here dif- fers from that of <ref type="bibr" target="#b3">Blitzer et al. (2006;</ref><ref type="bibr" target="#b4">2007)</ref>, where pivots are defined as features that behave similarly both in the source and the target domains. They compute the mutual information between a feature (i.e. unigrams or bigrams) and the sentiment labels using source domain labeled reviews. This method is useful when selecting pivots that are closely as- sociated with positive or negative sentiment in the source domain. However, in unsupervised domain adaptation we do not have labeled data for the tar- get domain. Therefore, the pivots selected using this approach are not guaranteed to demonstrate the same sentiment in the target domain as in the source domain. On the other hand, the pivot se- lection method proposed in this paper focuses on identifying a subset of features that are closely as- sociated with both domains.</p><p>It is noteworthy that our proposed cross-domain word representation learning method (Section 3.2) does not assume any specific pivot/non-pivot se- lection method. Therefore, in principle, our pro- posed word representation learning method could be used with any of the previously proposed pivot selection methods. We defer a comprehensive evaluation of possible combinations of pivot selec- tion methods and their effect on the proposed word representation learning method to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Cross-Domain Sentiment Classification</head><p>As a concrete application of cross-domain word representations, we describe a method for learning a cross-domain sentiment classifier using the word representations learnt by the proposed method. Existing word representation learning methods that learn from only a single domain are typi- cally evaluated for their accuracy in measuring se- mantic similarity between words, or by solving word analogy problems. Unfortunately, such gold standard datasets capturing cross-domain seman- tic variations of words are unavailable. Therefore, by applying the learnt word representations in a cross-domain sentiment classification task, we can conduct an indirect extrinsic evaluation.</p><p>The train data available for unsupervised cross- domain sentiment classification consists of unla- beled data for both the source and the target do- mains as well as labeled data for the source do- main. We train a binary sentiment classifier using those train data, and apply it to classify sentiment of the target test data.</p><p>Unsupervised cross-domain sentiment classifi- cation is challenging due to two reasons: feature- mismatch, and semantic variation. First, the sets of features that occur in source and target domain documents are different. Therefore, a sentiment classifier trained using source domain labeled data is likely to encounter unseen features during test time. We refer to this as the feature-mismatch problem. Second, some of the features that occur in both domains will have different sentiments as- sociated with them (e.g. lightweight). Therefore, a sentiment classifier trained using source domain labeled data is likely to incorrectly predict simi- lar sentiment (as in the source) for such features. We call this the semantic variation problem. Next, we propose a method to overcome both problems using cross-domain word representations.</p><p>Let us assume that we are given a set</p><formula xml:id="formula_6">{(x (i) S , y (i) )} n i=1 of n labeled reviews x (i)</formula><p>S for the source domain S. For simplicity, let us consider binary sentiment classification where each review x (i) is labeled either as positive (i.e. y (i) = 1) or negative (i.e. y (i) = −1). Our cross-domain bi- nary sentiment classification method can be eas- ily extended to multi-class classification. First, we lemmatize each word in a source domain labeled review x  S by a binary-valued fea- ture vector. Next, we train a binary linear clas- sifier, θ, using those feature vectors. Any binary classification algorithm can be used for this pur- pose. We use θ(z) to denote the weight learnt by the classifier for a feature z. In our experiments, we used l 2 regularized logistic regression.</p><p>At test time, we represent a test target review by a binary-valued vector h using a the set of un- igrams and bigrams extracted from that review. Then, the activation score, ψ(h), of h is defined by:</p><formula xml:id="formula_7">ψ(h) = c∈h c ∈θ θ(c )f (c S , cS )+ w∈h w ∈θ θ(w )f (w S , wT ) (13)</formula><p>Here, f is a similarity measure between two vec- tors. If ψ(h) &gt; 0, we classify h as positive, and negative otherwise. Eq. 13 measures the similarity between each feature in h against the features in the classification model θ. For pivots c ∈ h, we use the the source domain representations to mea- sure similarity, whereas for the (target-specific) non-pivots w ∈ h, we use their target domain rep- resentations. We experimented with several pop- ular similarity measures for f and found cosine similarity to perform consistently well. We can in- terpret Eq. 13 as a method for expanding a test tar- get document using nearest neighbor features from the source domain labeled data. It is analogous to query expansion used in information retrieval to improve document recall <ref type="bibr" target="#b14">(Fang, 2008)</ref>. Alterna- tively, Eq. 13 can be seen as a linearly-weighted additive kernel function over two feature spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>For train and evaluation purposes, we use the Amazon product reviews collected by <ref type="bibr" target="#b4">Blitzer et al. (2007)</ref> for the four product categories: books (B), DVDs (D), electronic items (E), and kitchen appliances (K). There are 1000 positive and 1000 negative sentiment labeled reviews for each do- main. Moreover, each domain has on average 17, 547 unlabeled reviews. We use the standard split of 800 positive and 800 negative labeled re- views from each domain as training data, and the rest (200+200) for testing. For validation purposes we use movie (source) and computer (target) do- mains, which were also collected by <ref type="bibr" target="#b4">Blitzer et al. (2007)</ref>, but not part of the train/test domains. Experiments conducted using this validation dataset revealed that the performance of the pro- posed method is relatively insensitive to the value of the regularization parameter λ ∈ [10 −3 , 10 3 ]. For the non-pivot prediction task we generate pos- itive and negative instances using the procedure described in Section 3.2. As a typical example, we have 88, 494 train instances from the books source domain and 141, 756 train instances from the target domain (1:5 ratio between positive and negative instances in each domain). The number of pivots and non-pivots are set to N P = N S = N T = 500.</p><p>In <ref type="figure" target="#fig_3">Figure 1</ref>, we compare the proposed method against two baselines (NA, InDomain), current state-of-the-art methods for unsupervised cross- domain sentiment classification (SFA, SCL), word representation learning (GloVe), and cross- domain similarity prediction (CS). The NA (no- adapt) lower baseline uses a classifier trained on source labeled data to classify target test data with- out any domain adaptation. The InDomain base- line is trained using the labeled data for the target domain, and simulates the performance we can ex- pect to obtain if target domain labeled data were available. Spectral Feature Alignment (SFA) ( <ref type="bibr" target="#b31">Pan et al., 2010)</ref> and Structural Correspondence Learn- ing (SCL) <ref type="bibr" target="#b4">(Blitzer et al., 2007)</ref> are the state-of- the-art methods for cross-domain sentiment clas- sification. However, those methods do not learn word representations.</p><p>We use Global Vector Prediction (GloVe) <ref type="bibr" target="#b32">(Pennington et al., 2014)</ref>, the current state-of-the- art word representation learning method, to learn word representations separately from the source and target domain unlabeled data, and use the learnt representations in Eq. 13 for sentiment clas- sification. In contrast to the joint word representa- tions learnt by the proposed method, GloVe sim- ulates the level of performance we would obtain by learning representations independently. CS de- notes the cross-domain vector prediction method proposed by <ref type="bibr" target="#b9">Bollegala et al. (2014)</ref>. Although CS can be used to learn a vector-space transla- tion matrix, it does not learn word representations. Vertical bars represent the classification accuracies (i.e. percentage of the correctly classified test in- stances) obtained by a particular method on target domain's test data, and Clopper-Pearson 95% bi- nomial confidence intervals are superimposed.</p><p>Differences in data pre-processing (tokeniza- tion/lemmatization), selection (train/test splits), feature representation (unigram/bigram), pivot se- lection (MI/frequency), and the binary classifica- tion algorithms used to train the final classifier make it difficult to directly compare results pub- lished in prior work. Therefore, we re-run the orig- inal algorithms on the same processed dataset un- der the same conditions such that any differences reported in <ref type="figure" target="#fig_3">Figure 1</ref> can be directly attributable to the domain adaptation, or word-representation learning methods compared.</p><p>All methods use l 2 regularized logistic regres- sion as the binary sentiment classifier, and the reg- ularization coefficients are set to their optimal val- ues on the validation dataset. SFA, SCL, and CS use the same set of 500 pivots as used by the pro- posed method selected using NPMI (Section 3.4). Dimensionality n of the representation is set to 300 for both GloVe and the proposed method.</p><p>From <ref type="figure" target="#fig_3">Fig. 1</ref> we see that the proposed method reports the highest classification accuracies in all 12 domain pairs. Overall, the improvements of the proposed method over NA, GloVe, and CS are sta- tistically significant, and is comparable with SFA, and SCL. The proposed method's improvement over CS shows the importance of predicting word representations instead of counting. The improve- ment over GloVe shows that it is inadequate to simply apply existing word representation learn- ing methods to learn independent word represen- tations for the source and target domains.</p><p>We must consider the correspondences between the two domains as expressed by the pivots to jointly learn word representations. As shown in <ref type="figure" target="#fig_4">Fig. 2</ref>, the proposed method reports superior ac- curacies over GloVe across different dimension- alities. Moreover, we see that when the dimen- sionality of the representations increases, initially accuracies increase in both methods and saturates after 200 − 600 dimensions. However, further increasing the dimensionality results in unstable and some what poor accuracies due to overfit- ting when training high-dimensional representa- tions. Although our word representations learnt by the proposed method are not specific to senti- ment classification, the fact that it clearly outper- forms SFA and SCL in all domain pairs is encour- aging, and implies the wider-applicability of the proposed method for domain adaptation tasks be- yond sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed an unsupervised method for learning cross-domain word representations using a given set of pivots and non-pivots selected from a source and a target domain. Moreover, we proposed a do- main adaptation method using the learnt word rep- resentations.</p><p>Experimental results on a cross-domain senti- ment classification task showed that the proposed method outperforms several competitive baselines and achieves best sentiment classification accura- cies for all domain pairs. In future, we plan to apply the proposed method to other types of do- main adaptation tasks such as cross-domain part- of-speech tagging, named entity recognition, and relation extraction.</p><p>Source code and pre-processed data etc. for this publication are publicly available 3 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Although word representation learning meth- ods have been used for various related tasks in NLP such as similarity measure- ment (Mikolov et al., 2013c), POS tag- ging (Collobert et al., 2011), dependency parsing (Socher et al., 2011a), machine trans- lation (Zou et al., 2013), sentiment classifica- tion (Socher et al., 2011b), and semantic role labeling (Roth and Woodsend, 2014), to the best of our knowledge, word representations methods have not yet been used for cross- domain sentiment classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>S</head><label></label><figDesc>, and extract unigrams and bigrams as features to represent x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Accuracies obtained by different methods for each source-target pair in cross-domain sentiment classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy vs. dimensionality of the representation.</figDesc></figure>

			<note place="foot" n="1"> Weighting the source and target loss functions by the respective dataset sizes did not result in any significant increase in performance. We believe that this is because the benchmark dataset contains approximately equal numbers of documents for each domain.</note>

			<note place="foot" n="2"> http://nlp.stanford.edu/software/ corenlp.shtml</note>

			<note place="foot" n="3"> www.csc.liv.ac.uk/ ˜ danushka/prj/darep</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributional memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relation adaptation: Learning to extract novel relations with minimum supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2205" to="2210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL/HLT</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining for analogous tuples from an entity-relation graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Kusumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2064" to="2070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Minimally supervised novel relation extraction using latent relational mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="419" to="432" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to predict distributions of words across domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="613" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Normalized (pointwsie) mutual information in collocation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerlof</forename><surname>Bouma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of GSCL</title>
		<meeting>of GSCL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using relational similarity between word pairs for latent relational search on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Nguyen Tuan Duc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="196" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A re-examination of query expansion using lexical resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="139" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new minimally-supervised framework for domain word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1411" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilingual distributed representations without word alignment</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2007</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A two-stage approach to domain adaptation for statistical classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM 2007</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="401" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic domain adaptation for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL/HLT</title>
		<meeting>of NAACL/HLT</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="28" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representation in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACLHLT</title>
		<meeting>of ACLHLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1059" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW</title>
		<meeting>of WWW</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glove: global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffery</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Composition of word representations improves semantic role labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="407" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards robust cross-domain domain adaptation for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNLP</title>
		<meeting>of IJCNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="198" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff Chiung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<title level="m">Parsing natural scenes and natural language with recursive neural networks. In ICML&apos;11</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Aritificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP&apos;13</title>
		<meeting>of EMNLP&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
