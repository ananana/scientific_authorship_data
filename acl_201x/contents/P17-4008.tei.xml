<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hafez: an Interactive Poetry Generation System</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Priyadarshi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
						</author>
						<title level="a" type="main">Hafez: an Interactive Poetry Generation System</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
						<meeting>ACL 2017, System Demonstrations <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="43" to="48"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-4008</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Hafez is an automatic poetry generation system that integrates a Recurrent Neural Network (RNN) with a Finite State Accep-tor (FSA). It generates sonnets given arbitrary topics. Furthermore, Hafez enables users to revise and polish generated poems by adjusting various style configurations. Experiments demonstrate that such &quot;pol-ish&quot; mechanisms consider the user&apos;s intention and lead to a better poem. For evaluation , we build a web interface where users can rate the quality of each poem from 1 to 5 stars. We also speed up the whole system by a factor of 10, via vocabulary pruning and GPU computation, so that adequate feedback can be collected at a fast pace. Based on such feedback, the system learns to adjust its parameters to improve poetry quality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automated poetry generation is attracting increas- ing research effort. Researchers approach the problem by using grammatical and semantic tem- plates <ref type="bibr" target="#b4">(Oliveira, 2009</ref><ref type="bibr" target="#b5">(Oliveira, , 2012</ref> or treating the gen- eration task as a translation/summarization task ( <ref type="bibr" target="#b10">Zhou et al., 2009;</ref><ref type="bibr" target="#b3">He et al., 2012;</ref><ref type="bibr">Yan et al., 2013;</ref><ref type="bibr" target="#b9">Zhang and Lapata, 2014;</ref><ref type="bibr" target="#b8">Yi et al., 2016;</ref><ref type="bibr" target="#b6">Wang et al., 2016;</ref><ref type="bibr" target="#b2">Ghazvininejad et al., 2016)</ref>. How- ever, such poetry generation systems face these challenges:</p><p>1. Difficulty of evaluating poetry quality. Au- tomatic evaluation methods, like BLEU, can- not judge the rhythm, meter, creativity or syn- tactic/semantic coherence, and furthermore, there is no test data in most cases. Subjective * *equal contributions evaluation requires evaluators to have rela- tively high literary training, so systems will receive limited feedback during the develop- ment phase. <ref type="bibr">1</ref> 2. Inability to adjust the generated poem.</p><p>When poets compose a poem, they usually need to revise and polish the draft from differ- ent aspects (e.g., word choice, sentiment, al- literation, etc.) for several iterations until sat- isfaction. This is a crucial step for poetry cre- ation. However, given a user-supplied topic or phrase, most existing automated systems can only generate different poems by using different random seeds, providing no other support for the user to polish the generated poem in a desired direction.</p><p>3. Slow generation speed. Generating a poem may require a heavy search procedure. For example, the system of <ref type="bibr" target="#b2">Ghazvininejad et al. (2016)</ref> needs 20 seconds for a four-line poem. Such slow speed is a serious bottleneck for a smooth user experience, and prevents the large-scale collection of feedback for system tuning.</p><p>This work is based on our previous poetry gen- eration system called Hafez ( <ref type="bibr" target="#b2">Ghazvininejad et al., 2016)</ref>, which generates poems in three steps: (1) search for related rhyme words given user- supplied topic, (2) create a finite-state acceptor (FSA) that incorporates the rhyme words and con- trols meter, and (3) use a recurrent neural network (RNN) to generate the poem string, guided by the FSA. We address the above-mentioned challenges with the following approaches:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic: Presidential elections</head><p>To hear the sound of communist aggression! I never thought about an exit poll, At a new Republican convention, On the other side of gun control. <ref type="table">Table 1</ref>: One poem generated in a 15-minute hu- man/computer interactive poetry contest.</p><p>1. We build a web interface 2 for our poem gen- eration system, and for each generated poem, the user can rate its quality from 1-star to 5- stars. Our logging system collects poems, re- lated parameters, and user feedback. Such crowd-sourcing enables us to obtain large amounts of feedback in a cheap and efficient way. Once we collect enough feedback, the system learns to find a better set of parame- ters and updates the system continuously.</p><p>2. We add additional weights during decoding to control the style of generated poem, in- cluding the extent of words repetition, allit- eration, word length, cursing, sentiment, and concreteness.</p><p>3. We increase speed by pre-calculation, pre- loading model parameters, and pruning the vocabulary. We also parallelize the computa- tion of FSA expansion, weight merging, and beam search, and we port them into a GPU. Overall, we can generate a four-line poem within 2 seconds, ten times faster than our previous CPU-based system.</p><p>With the web interface's style control and fast generation speed, people can generate creative po- ems within a short time. <ref type="table">Table 1</ref> shows one of the poems generated in a poetry mini-competition where 7 people are asked to use Hafez to generate poems within 15 minutes. We also conduct exper- iments on Amazon Mechanical Turk, which show: first, through style-control interaction, 71% users can find a better poem than the poem generated by the default configuration. Second, based on users' evaluation results, the system learns a new config- uration which generates better poems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Description</head><p>Figure 1: Overview of Hafez <ref type="figure">Figure 1</ref> shows an overview of Hafez. In the web interface, a user can input topic words or phrases and adjust the style configuration. This in- formation is then sent to our backend server, which is primarily based on our previously-described work ( <ref type="bibr" target="#b2">Ghazvininejad et al., 2016)</ref>. First, the back- end will use the topic words/phrases to find related rhyme word pairs by using a word2vec model and a pre-calculated rhyme-type dictionary. Given these rhyme word pairs, an FSA that encodes all valid word sequences is generated, where a valid word sequence follows certain type of meter and puts the rhyme word at the end of each line. This FSA, together with the user-supplied style config- uration, is then used to guide the Recurrent Neural Network (RNN) decoder to generate the rest of the poem. User can rate the generated poem using a 5-star system. Finally, the tuple (topic, style con- figuration, generated poem, star-rating) is pushed to the logging system. Periodically, a module will analyze the logs, learn a better style configuration and update it as the new default style configura- tion. <ref type="figure">Figure 2</ref> provides an example in action. The user has input the topic word "love" and left the style configuration as default. After they click the "Generate" button, a four-line poem is gen- erated and displayed. The user may not be sat- isfied with current generation, and may decide to add more positive sentiment and encourage a little bit of the alliteration. After they move the cor- responding slider bars and click the "Re-generate with the same rhyme words" button, a new poem is returned. This poem has more positive senti-ment ("A lonely part of you and me tonight" vs. "A lovely dream of you and me tonight") and more alliteration ("My merry little love", "The lucky lady" and "She sings the sweetest song" ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Example in Action</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Style Control</head><p>During the RNN's beam search, each beam cell records the current FSA state s. Its succeeding state is denoted as s suc . All the words over all the succeeding states forms a vocabulary V suc . To ex- pand the beam state b, we need to calculate a score for each word in V suc :</p><formula xml:id="formula_0">score(w, b) = score(b) + log P RN N (w) + i w i * f i (w); ∀w ∈ V suc (1)</formula><p>where log P RN N (w) is the log-probability of word w calculated by RNN. score(b) is the accumulated score of the already-generated words in beam state b . f i (w) is ith feature function and w i is the cor- responding weight.</p><p>To control the style, we design the following 8 features:</p><p>1. Encourage/discourage words. User can input words that they would like in the poem, or words to be banned. f (w) = I(w, V enc/dis ) where I(w, V ) = 1 if w is in the word list V , otherwise I(w, V ) = 0. w enc = 5 and w dis = −5.</p><p>2. Curse words. We pre-build a curse-word list V curse , and f (w) = I(w, V curse ).</p><p>3. Repetition. To control the extent of repeated words in the poem. For each beam, we record the current generated words V history , and f (w) = I(w, V history ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Alliteration. To control how often adjacent</head><p>non-function words start with the same con- sonant sound. In the beam cell, we also record the previous generated word w t−1 , and f (w t ) = 1 if w t and w t−1 share the same first consonant sound, otherwise it equals 0.</p><p>5. Word length. To control a preference for longer words in the generated poem. f (w) = length(w) 2 .</p><p>6. Topical words. For each user-supplied topic words, we generate a list of related words V topical . f (w) = I(w, V topical ).</p><p>7. Sentiment. We pre-build a word list together with its sentiment scores based on Senti- WordNet ( <ref type="bibr" target="#b0">Baccianella et al., 2010)</ref>. f (w) equals to w s sentiment score.</p><p>8. Concrete words. We pre-build a word list to- gether with a score to reflect its concreteness based on <ref type="bibr" target="#b1">Brysbaert et al. (2014)</ref>. f (w) equals to w's concreteness score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Speedup</head><p>To find the rhyming words related to the topic, we employ a word2vec model. Given a topic word or phrase w t ∈ V , we find related words w r based on the cosine distance:</p><formula xml:id="formula_1">w r = argmax wr∈V ⊆V cosine(e wr , e wt )<label>(2)</label></formula><p>where e w is the embedding of word w. Then we calculate the rhyme type of each related word w r to find rhyme pairs. To speed up this step, we carefully optimize the computation with these methods:</p><p>1. Pre-load all parameters into RAM. As we are aiming to accept arbitrary topics, the vo- cabulary V of word2vec model is very large (1.8M words and phrases). Pre-loading saves 3-4 seconds.</p><p>2. Pre-calculate the rhyme types for all words w ∈ V . During runtime, we use this dictio- nary to lookup the rhyme type.</p><p>3. Shrink V'. As every rhyme word/phrase pairs must be in the target vocabulary V RN N of the RNN, we further shrink V = V ∩ V RN N .</p><p>To speedup the RNN decoding step, we use GPU processing for all forward-propagation com- putations. For beam search, we port to GPU the two most time-consuming parts, calculating scores with Equation 1 and finding the top words based the score:</p><p>1. We warp all the computation needed in Equa- tion 1 into a single large GPU kernel launch.</p><p>2. With beam size B, to find the top k words, instead of using a heap sort on CPU with complexity O(B|V suc |logk), we do a global sort on GPU with complex- ity O(B|V suc |log(B|V suc |)) in one kernel launch. Even though the complexity in- creases, the computation time in practice re- duces quite a bit. Finally, our system can generate a 4-line poem within 2 seconds, which is 10 times faster than the previous CPU-based version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Learn a New Style Configuration</head><p>Except the fixed weight for encouragement and discouragement of words, all other 7 weights form our style configuration space:</p><formula xml:id="formula_2">W = {w i |i = 1..7}<label>(3)</label></formula><p>and we denote the default configuration as W d , in- tuitively selected. Users will usually start with W d to generate their first poem p d , and if they later adjust the style configuration and click "Re-generate with same rhyme words" button, the new poem p i will change the style accordingly, but still with the same rhyme words as p d . In the logging system, a unique hash h r is recorded to distinguish different rhyme word sets in the poem.</p><p>After proper processing, our logging system or- ganizes the data points as a dictionary D = {h r : <ref type="figure">W 1 , r 1 )</ref>, ..., (p n , W n , r n )]}. r d represents user's star rating for the poem with de- fault setting(p d ) while r i ∈ {1, ..., n} represents user's star rating for p 1 , ..., p n .</p><formula xml:id="formula_3">[(p d , W d , r d ), (p 1 ,</formula><p>To learn a new style configuration W new , we construct our training data D train = {h r : [(∆W 1 , ∆r 1 ), ..., (∆W n , ∆r n )]}, where ∆W i = W i − W d and ∆r i = r i − r d . Then, we fit a quadratic regression between the rating change ∆r and each weight change ∆w j ∈ ∆W indepen- dently:</p><formula xml:id="formula_4">∆r = a(∆w j ) 2 + b∆w j + c<label>(4)</label></formula><p>and the new weight w j,new is</p><formula xml:id="formula_5">w j,new = w j,d + argmax ∆w j ∆r<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Alexa Skill</head><p>To feel beauty, rhythm, and emotion in poetry, it is better to listen to a spoken version. With this User: Alexa, start Custom Poetry Alexa: Welcome to Custom Poetry. Please give me a topic. User: Mountains Alexa: A hundred thousand Morrison formation, An ancient crown of gold or mountain chains, Mountains from the land of elevation, A northern storm across the hills and plains. Alexa: Feel free to give me another topic. User: Lakes Alexa: ... Alexa: Feel free to give me another topic. User: Stop Alexa: Thank you for trying Custom Poetry. Have a nice day! <ref type="table">Table 2</ref>: Spoken onversation between a user and Alexa.</p><p>in mind, we also publish our system as an Ama- zon Alexa Skill ("Custom Poetry"), so that users can ask Alexa to compose and recite a poem on any topic. <ref type="table">Table 2</ref> shows a sample conversation between a user and Alexa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We design an Amazon Mechanical Turk task to ex- plore the effect of style options. In this task Turk- ers first use Hafez to generate a default poem on an arbitrary topic with the default style configura- tion, and rate it. Next, they are asked to adjust the style configurations to re-generate at least five dif- ferent adjusted poems with the same rhyme words, and rate them as well. Improving the quality of adjusted poems over the default poem is not re- quired for finishing the task, but it is encouraged. For each task, Turkers can select the best gener- ated poem, and if subsequent human judges (do- main experts) rank that poem as "great", a bonus reward will be assigned to that Turker. We gath- ered data from 62 completed HITs (Human Intel- ligence Tasks) for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Human-Computer Collaboration</head><p>This experiment tests whether human collabora- tion can help Hafez generate better poems.</p><p>In only 10% of the HITs, the reported best poem was generated by the default style options, i.e., the default poem. Additionally, in 71% of the HITs, users assign a higher star rating to at least one of the adjust poems than the default poem. On aver- age the best poems got +1.4 more stars compared to the default one.</p><p>However, poem creators might have a tendency to report a higher ranking for poems generated through the human/machine collaboration process. To sanity check the results we designed another task and asked 18 users to compare the default and the reported best poems. This experiment sec-onded the original rankings in 72% of the cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automatic tuning for quality</head><p>We learn new default configurations using the data gathered from Mechanical Turk. As we explained in section 2.4, we examine the effect of different feature weights like repetition and sentiment on star ranking scores. We aim to cancel out the effect of topic and rhyme words on our scoring function. We achieve this by plotting the score offset from the default poem for each topic and set of rhyme words. <ref type="figure" target="#fig_1">Figure 3</ref> shows the distribution of scores against topical, concreteness, sentiment and repe- tition weights. In each plot the zero weight rep- resents the default value. Each plot also shows a quadratic regression curve fit to its data.</p><p>In order to alter the style options toward gener- ating better default poems, we re-set each weight to the maximum of each quadratic curve. Hence, the new weights encourage more topical, less con- crete, more positive words and less repetition. It is notable that for sentiment, users prefer both more positive and more negative words to the initial neutral setting, but the preference is slightly biased towards positive words.</p><p>We update Hafez's default settings based on this analysis. We ask 29 users to compare poems gen- erated on the same topic and rhyme words using both old and new style settings. In 59% of the cases, users prefer the poem generated by the new setting.</p><p>We thus improve the default settings for gen- erating a poem, though this does not mean that the poems cannot be further improved by human collaboration. In most cases, a better poem can be generated by collaboration with the system (changing the style options) for the specific topic and set of rhyme words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>Figure 2: A poem generated with (a) default style configuration and (b) user-adjusted style configuration.</figDesc><graphic url="image-9.png" coords="4,111.79,243.41,368.49,203.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The distribution of poem star-ratings against normalized topical, concreteness, sentiment and repetition weights. Star ratings are computed as an offset from the version of the poem generated from default settings. We normalize all features weights by calculating their offset from the default values. The solid curve represents a quadratic regression fit to the data. To avoid overlapping points, we plot with a small amount of random noise added.</figDesc></figure>

			<note place="foot" n="1"> The Dartmouth Turing Tests in the Creative Arts (bit.ly/20WGLF3), in which human experts are employed to judge the generation quality, is held only once a year.</note>

			<note place="foot" n="2"> Live demo at http://52.24.230.241/poem/ advance/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We demonstrate Hafez, an interactive poetry gen-eration system. It enables users to generate poems about any topic, and revise generated texts through multiple style configurations. We speed up the system by vocabulary pruning and GPU computa-tion. Together with an easily-accessible web in-terface, we can collect large numbers of human evaluations in a short timespan, making automatic system tuning possible.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">Beth</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
		<title level="m">Concreteness ratings for 40 thousand generally known english word lemmas. Behavior research methods</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating topical poetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating Chinese classical poems with statistical machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic generation of poetry: an overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Seminar of Art, Music, Creativity and Artificial Intelligence</title>
		<meeting>1st Seminar of Art, Music, Creativity and Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PoeTryMe: a versatile platform for poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Creativity, Concept Invention, and General Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06274</idno>
		<title level="m">Chinese song iambics generation with neural attention-based model</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">2013. I, Poet: Automatic Chinese poetry composition through a generative summarization framework under constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqiang</forename><surname>Shou-De Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating chinese classical poems with RNN encoderdecoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01537</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating Chinese couplets and quatrain using a statistical approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Pacific Asia Conference on Language, Information and Computation</title>
		<meeting>Pacific Asia Conference on Language, Information and Computation</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
