<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Stochastic Decoder for Neural Machine Translation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Schulz</surname></persName>
							<email>phschulz@amazon.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Research †</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
							<email>w.aziz@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Research †</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<email>trevor.cohn@unimelb.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Amazon Research †</orgName>
								<orgName type="institution" key="instit1">University of Amsterdam</orgName>
								<orgName type="institution" key="instit2">University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Stochastic Decoder for Neural Machine Translation *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1243" to="1252"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The process of translation is ambiguous, in that there are typically many valid translations for a given sentence. This gives rise to significant variation in parallel corpora , however, most current models of machine translation do not account for this variation, instead treating the problem as a deterministic process. To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to account for local lexical and syntactic variation in parallel corpora. We provide an in-depth analysis of the pitfalls encountered in variational inference for training deep generative models. Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural architectures have taken the field of ma- chine translation by storm and are in the pro- cess of replacing phrase-based systems. Based on the encoder-decoder framework ) increasingly complex neural systems are being developed at the moment. These systems find new ways of extracting information from the source sentence and the target sentence prefix for example by using convolutions <ref type="bibr" target="#b8">(Gehring et al., 2017</ref>) or stacked self-attention layers ( <ref type="bibr" target="#b28">Vaswani et al., 2017)</ref>. These architectural changes have led to great performance improvements over classical RNN-based neural translation systems ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>).</p><p>Surprisingly, there have been almost no efforts to change the probabilistic model wich is used to train the neural architectures. A notable exception is the work of <ref type="bibr" target="#b30">Zhang et al. (2016)</ref> who introduce a sentence-level latent <ref type="bibr">Gaussian variable.</ref> In this work, we propose a more expressive latent variable model that extends the attention- based architecture of <ref type="bibr" target="#b1">Bahdanau et al. (2014)</ref>. Our model is motivated by the following observation: translations by professional translators vary across translators but also within a single translator (the same translator may produce different translations on different days, depending on his state of health, concentration etc.). Neural machine translation (NMT) models are incapable of capturing this variation, however. This is because their likeli- hood function incorporates the statistical assump- tion that there is one (and only one) output 1 for a given source sentence, i.e.,</p><formula xml:id="formula_0">P (y n 1 |x m 1 ) = n ∏ i=1 P (y i |x m 1 , y &lt;i ) .<label>(1)</label></formula><p>Our proposal is to augment this model with la- tent sources of variation that are able to represent more of the variation present in the training data. The noise sources are modelled as Gaussian ran- dom variables.</p><p>The contributions of this work are:</p><p>• The introduction of an NMT system that is capable of capturing word-level variation in translation data.</p><p>• A thorough discussions of issues encountered when training this model. In particular, we motivate the use of KL scaling as introduced by <ref type="bibr" target="#b4">Bowman et al. (2016)</ref> theoretically. <ref type="bibr">1</ref> Notice that from a statistical perspective the output of an NMT system is a distribution over target sentences and not any particular sentence. The mapping from the output dis- tribution to a sentence is performed by a decision rule (e.g. argmax decoding) which can be chosen independently of the NMT system.</p><p>• An empirical demonstration of the improve- ments achievable with the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>The NMT system upon which we base our exper- iments is based on the work of <ref type="bibr" target="#b1">Bahdanau et al. (2014)</ref>. The likelihood of the model is given in Equation (1). We briefly describe its architecture. Let x m 1 = (x 1 , . . . , x m ) be the source sentence and y n 1 the target sentence. Let RNN (·) be any function computed by a recurrent neural network (we use a bi-LSTM for the encoder and an LSTM for the decoder). We call the decoder state at the ith target position t i ; 1 ≤ i ≤ n. The computation performed by the baseline system is summarised below.</p><p>[</p><formula xml:id="formula_1">h 1 , . . . , h m ] = RNN (x m 1 ) (2a) ˜ t i = RNN (t i−1 , y i−1 ) (2b) e ij = v ⊤ a tanh ( W a [ ˜ t i , h j ] ⊤ + b a ) (2c) α ij = exp (e ij ) ∑ m j=1 exp (e ij ) (2d) c i = m ∑ j=1 α ij h j (2e) t i = W t [ ˜ t i , c i ] ⊤ + b t (2f ) ϕ i = softmax(W o t i + b o ) (2g) The parameters {W a , W t , W o , b a , b t , b o , v a } ⊆ θ are learned during training.</formula><p>The model is trained using maximum likelihood estimation. This means that we employ a cross-entropy loss whose input is the probability vector returned by the softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Stochastic Decoder</head><p>This section introduces our stochastic decoder model for capturing word-level variation in trans- lation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Imagine an idealised translator whose translations are always perfectly accurate and fluent. If an MT system was provided with training data from such a translator, it would still encounter variation in that data. After all, there are several perfectly accurate and fluent translations for each source sentence. These can be highly different in both their lexical as well as their syntactic realisations.</p><p>In practice, of course, human translators' per- formance varies according to their level of educa- tion, their experience on the job, their familiarity with the textual domain and myriads of other fac- tors. Even within a single translator variation may occur due to level of stress, tiredness or status of health. That translation corpora contain variation is acknowledged by the machine translation com- munity in the design of their evaluation metrics which are geared towards comparing one machine- generated translation against several human trans- lations (see e.g. <ref type="bibr" target="#b18">Papineni et al., 2002</ref>).</p><p>Prior to our work, the only attempt at mod- elling the latent variation underlying these differ- ent translations was made by <ref type="bibr" target="#b30">Zhang et al. (2016)</ref> who introduced a sentence level Gaussian variable. Intuitively, however, there is more to latent varia- tion than a unimodal density can capture, for ex- ample, there may be several highly likely clusters of plausible variations. A cluster may e.g. consist of identical syntactic structures that differ in word choice, another may consist of different syntactic constructs such as active or passive constructions. Multimodal modelling of these variations is thus called for-and our results confirm this intuition.</p><p>An example of variation comes from free word order and agreement phenomena in morphologi- cally rich languages. An English sentence with rigid word order may be translated into several or- derings in German. However, all orderings need to respect the agreement relationship between the main verb and the subject (indicated by underlin- ing) as well as the dative case of the direct object (dashes) and the accusative of the indirect object (dots). The agreement requirements are fixed and independent of word order. Stochastically encoding the word order variation allows the model to learn the same agreement phe- nomenon from different translation variants as it does not need to encode the word order and agree- ment relationships jointly in the decoder state.</p><p>Further examples of VP and NP variation from an actual translation corpus are shown in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>We aim to address these word-level variation phenomena with a stochastic decoder model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>预计听证会将进⾏两天。</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VOM19981105_0700_0262</head><p>The hearing is expected to last two days. The hearing will last two days. The hearings are expected to last two days. It is expected that the hearing will go on for two days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>众议院共和党的起诉⼈则希望传唤莱温斯基等多达15个⼈出庭作证。 VOM19981230_0700_0515</head><p>However, the Republican complainant in the House wanted to summon 15 people including Lewinsky to testify in court. The prosecutor of Republican Party in House of Representative hoped to summons more than 15 persons, including Lewinsky, to court. The House of Representatives republican prosecution hopes to summon over fifteen witnesses includ- ing Monica Lewinsky to appear in court. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model formulation</head><p>The model contains a latent Gaussian variable for each target position. This variable depends on the previous latent states and the decoder state. Through the use of recurrent networks, the condi- tioning context does not need to be restricted and the likelihood factorises exactly.</p><formula xml:id="formula_2">P (y n 1 |x m 1 ) = ∫ dz n 0 p(z 0 |x m 1 )× n ∏ i=1 p(z i |z &lt;i , y &lt;i , x m 1 )P (y i |z i 1 , y &lt;i , x m 1 )<label>(3)</label></formula><p>As can be seen from Equation <ref type="formula" target="#formula_2">(3)</ref>, the model also contains a 0th latent variable that is meant to initialise the chain of latent variables based solely on the source sentence. Contrast this with the model of <ref type="bibr" target="#b30">Zhang et al. (2016)</ref> which uses only that 0th variable.</p><p>A graphical representation of the stochastic de- coder model is given in <ref type="figure" target="#fig_2">Figure 2a</ref>. Its generative story is as follows</p><formula xml:id="formula_3">Z 0 |x m 1 ∼ N (µ 0 , σ 2 0 ) (4a) Z i |z &lt;i , y &lt;i , x m 1 ∼ N (µ i , σ 2 i ) (4b) Y i |z i 0 , y &lt;i , x m 1 ∼ Cat(ϕ i )<label>(4c)</label></formula><p>where i = 1, . . . , n and both the Gaussian and the Categorical parameters are predicted by neural network architectures whose inputs vary per time step. This probabilistic formulation can be imple- mented with a multitude of different architectures. We present ours in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neural Architecture</head><p>Since the model contains latent variables and is parametrised by a neural network, it falls into the class of deep generative models (DGMs). We use a reparametrisation of the Gaussian variables ( <ref type="bibr" target="#b14">Kingma and Welling, 2014;</ref><ref type="bibr" target="#b20">Rezende et al., 2014;</ref><ref type="bibr" target="#b27">Titsias and Lázaro-Gredilla, 2014</ref>) to enable back- propagation inside a stochastic computation graph ( <ref type="bibr" target="#b21">Schulman et al., 2015)</ref>. In order to sample d- dimensional Gaussian variable z ∈ R d with mean µ and variance σ 2 , we first sample from a standard Gaussian distribution and then transform the sam- ple,</p><formula xml:id="formula_4">z = µ + σ ⊙ ϵ ϵ ∼ N (0, I) .<label>(5)</label></formula><p>Here µ, σ ∈ R d and ⊙ denotes element-wise multiplication (also known as Hadamard product).</p><p>See the supplement for details on the Gaussian reparametrisation.</p><p>We use neural networks with one hidden layer with a tanh activation to compute the mean and standard deviation of each Gaussian distribution. A softplus transformation is applied to the output of the standard deviation's network to ensure pos- itivity. Let us denote the functions that these net- works compute by f .</p><p>For the initial latent state z 0 we compute the mean and standard deviation as</p><formula xml:id="formula_5">µ 0 = f µ 0 (h m ) σ 0 = f σ 0 (h m ) .<label>(6)</label></formula><p>. .</p><formula xml:id="formula_6">x l 1 . z 0 . y 1 . z 1 . y 2 . z 2 . y 3 . z 3 (a)</formula><p>. </p><formula xml:id="formula_7">. z i−1 . z i . y i−1 . y i . y i+1 . . . .</formula><formula xml:id="formula_8">µ i = f µ (t i−1 , z i−1 ) σ i = f σ (t i−1 , z i−1 ) (7)</formula><p>Using these values, each latent variable is sam- pled according to Equation (5). The sampled latent variables are then used to modify the update of the decoder hidden state (Equation (2b)) as follows:</p><formula xml:id="formula_9">˜ t i = RNN (t i−1 , y i−1 , z i )<label>(8)</label></formula><p>The remaining computations stay unchanged. Notice that the latent values are used directly in up- dating the decoder state. This makes the decoder state a function of a random variable and thus the decoder state is itself random. Applying this ar- gument recursively shows that also the attention mechanism is random, making the decoder entirely stochastic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference and Training</head><p>We use variational inference (see e.g. <ref type="bibr" target="#b3">Blei et al., 2017</ref>) to train the model. In variational inference, we employ a variational distribution q(z) that ap- proximates the true posterior p(z|x) over the latent variables. The distribution q(z) has its own set of parameters λ that is disjoint from the set of model parameters θ. It is used to maximise the evidence lower bound (ELBO) which is a lower bound on the marginal likelihood p(x). The ELBO is max- imised with respect to both the model parameters θ and the variational parameters λ.</p><p>Most NLP models that use DGMs only use one latent variable (e.g. <ref type="bibr" target="#b4">Bowman et al., 2016)</ref>. Models that use several variables usually employ a mean field approximation under which all latent vari- ables are independent. This turns the ELBO into a sum of expectations (e.g. <ref type="bibr" target="#b31">Zhou and Neubig, 2017)</ref>. For our stochastic decoder we design a more flexi- ble approximation posterior family which respects the dependencies between the latent variables,</p><formula xml:id="formula_10">q(z n 0 ) = q(z 0 ) n ∏ i=1 q(z i |z &lt;i ) .<label>(9)</label></formula><p>Our stochastic decoder can be viewed as a stack of conditional DGMs ( <ref type="bibr" target="#b23">Sohn et al., 2015</ref>) in which the latent variables depend on one another. The ELBO thus consists of nested positional ELBOs,</p><formula xml:id="formula_11">ELBO 0 + E q(z 0 ) [ELBO 1 +E q(z 1 ) [ELBO 2 + . . .]] ,<label>(10)</label></formula><p>where for a given target position i the ELBO is</p><formula xml:id="formula_12">ELBO i = E q(z i ) [log p(y i |x m 1 , y &lt;i , z &lt;i , z i )] − KL (q(z i ) || p(z i |x m 1 , y &lt;i , z &lt;i )) .<label>(11)</label></formula><p>The first term is often called reconstruction or like- lihood term whereas the second term is called the KL term. Since the KL term is a function of two Gaussian distributions, and the Gaussian is an ex- ponential family, we can compute it analytically ( <ref type="bibr" target="#b17">Michalowicz et al., 2014</ref>), without the need for sampling. This is very similar to the hierarchical latent variable model of <ref type="bibr" target="#b20">Rezende et al. (2014)</ref>.</p><p>Following common practice in DGM research, we employ a neural network to compute the vari- ational distributions. To discriminate it from the generative model, we call this neural net the in- ference model. At training time both the source and target sentence are observed. We exploit this by endowing our inference model with a "look- ahead" mechanism. Concretely, samples from the inference network condition on the information available to the generation network (Section 3.3) and also on the target words that are yet to be pro- cessed by the generative decoder. This allows the latent distribution to not only encode information about the currently modelled word but also about the target words that follow it. The conditioning of the inference network is illustrated graphically in <ref type="figure" target="#fig_2">Figure 2b</ref>.</p><p>The inference network produces additional rep- resentations of the target sentence. One represen- tation encodes the target sentence bidirectionally (12a), in analogy to the source sentence encoding. The second representation is built by encoding the target sentence in reverse (12b). This reverse en- coding can be used to provide information about future context to the decoder. We use the sym- bols b and r for the bidirectional and reverse target encodings, respectively. In our experiments, we again use LSTMs to compute these encodings.</p><p>[</p><formula xml:id="formula_13">b 1 , . . . , b n ] = RNN (y n 1 ) (12a) [ r 1 , . . . , r n ] = RNN (y n 1 )<label>(12b)</label></formula><p>In analogy to the generative model (Section 3.3), the inference network uses single hidden layer net- works to compute the mean and standard devia- tions of the latent variable distributions. We denote these functions g and again employ different func- tions for the initial latent state and all other latent states.</p><formula xml:id="formula_14">µ 0 = g µ 0 (h m , b n ) (13a) σ 0 = g σ 0 (h m , b n ) (13b) µ i = g µ (t i−1 , z i−1 , r i , y i )<label>(13c)</label></formula><formula xml:id="formula_15">σ i = g σ (t i−1 , z i−1 , r i , y i )<label>(13d)</label></formula><p>As before, we use Equation (5) to sample from the variational distribution. During training, all samples are obtained from the inference network. Only at test time do we sample from the generator. Notice that since the inference network conditions on representations produced by the generator network, a naïve appli- cation of backpropagation would update parts of the generator network with gradients computed for the inference network. We prevent this by block- ing gradient flow from the inference net into the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis of the Training Procedure</head><p>The training procedure as outlined above does not work well empirically. This is because our model uses a strong generator. By this we mean that the generation model (that is the baseline NMT model) is a very good density model in and by it- self and does not need to rely on latent informa- tion to achieve acceptable likelihood values dur- ing training. DGMs with strong generators have a tendency to not make use of latent information <ref type="bibr" target="#b4">(Bowman et al., 2016)</ref>. This problem went ini- tially unnoticed because early DGMs ( <ref type="bibr" target="#b14">Kingma and Welling, 2014;</ref><ref type="bibr" target="#b20">Rezende et al., 2014</ref>) used weak generators 2 , i.e. models that made very strong in- dependence assumptions and were not able to cap- ture contextual information without making use of the information encoded by the latent variable.</p><p>Why DGMs would ignore the latent information can be understood by considering the KL-term of the ELBO. In order for the latent variable to be in- formative about the observed data, we need them to have high mutual information I(Z; Y ).</p><formula xml:id="formula_16">I(Z; Y ) = E p(z,y) [ log p(Z, Y ) p(Z)p(Y ) ]<label>(14)</label></formula><p>Observe that we can rewrite the mutual informa- tion as an expected KL divergence by applying the definition of conditional probability.</p><formula xml:id="formula_17">I(Z; Y ) = E p(y) [KL (p(Z|Y ) || p(Z))]<label>(15)</label></formula><p>Since we cannot compute the posterior p(z|y) exactly, we approximate it with the variational distribution q(z|y) (the joint is approximated by q(z|y)p(y) where the latter factor is the data dis- tribution). To the extent that the variational distri- bution recovers the true posterior, the mutual in- formation can be computed this way. In fact, if we take the learned prior p(z) to be an approxima- tion of the marginal ∫ q(z|y)p(y)dy it can easily be shown that the thus computed KL term is an upper bound on mutual information ( <ref type="bibr" target="#b0">Alemi et al., 2017</ref>).</p><p>The trouble is that the ELBO (Equation (11)) can be trivially maximised by setting the KL-term to 0 and maximising only the reconstruction term. This is especially likely at the beginning of train- ing when the variational approximation does not yet encode much useful information. We can only hope to learn a useful variational distribution if a) the variational approximation is allowed to move away from the prior and b) the resulting increase in the reconstruction term is higher than the increase in the KL-term (i.e. the ELBO increases overall).</p><p>Several schemes have been proposed to en- able better learning of the variational distribution ( <ref type="bibr" target="#b4">Bowman et al., 2016;</ref><ref type="bibr" target="#b13">Kingma et al., 2016;</ref><ref type="bibr" target="#b0">Alemi et al., 2017</ref>). Here we use KL scaling and increase the scale gradually until the original objective is recovered. This has the following effect: during the initial learning stage, the KL-term barely con- tributes to the objective and thus the updates to the variational parameters are driven by the signal from the reconstruction term and hardly restricted by the prior.</p><p>Once the scale factor approaches 1 the varia- tional distribution will be highly informative to the generator (assuming sufficiently slow increase of the scale factor). The KL-term can now be min- imised by matching the prior to the variational dis- tribution. Notice that up to this point, the prior has hardly been updated. Thus moving the varia- tional approximation back to the prior would likely reduce the reconstruction term since the standard normal prior is not useful for inference purposes. This is in stark contrast to <ref type="bibr" target="#b4">Bowman et al. (2016)</ref> whose prior was a fixed standard normal distri- bution. Although they used KL scaling, the KL term could only be decreased by moving the varia- tional approximation back to the fixed prior. This problem disappears in our model where priors are learned.</p><p>Moving the prior towards the variational ap- proximation has another desirable effect. The prior can now learn to emulate the variational "look- ahead" mechanism without having access to future contexts itself (recall that the inference model has access to future target tokens). At test time we can thus hope to have learned latent variable distribu- tions that encode information not only about the output at the current position but about future out- puts as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We report experiments on the IWSLT 2016 data set which contains transcriptions of TED talks and their respective translations. We trained models to <ref type="table" target="#tab_1">Data Arabic  Czech  French German   Train 224,125 114,389 220,399 196,883  Dev  6,746  5,326  5,937  6,996  Test  2,762  2,762  2,762  2,762   Table 1</ref>: Number of parallel sentence pairs for each language paired with English for IWSLT data.</p><p>translate from English into Arabic, Czech, French and German. The number of sentences for each language after preprocessing is shown in <ref type="table">Table 1</ref>. The vocabulary was split into 50,000 subword units using Google's sentence piece 3 software in its standard settings. As our baseline NMT sys- tems we use Sockeye <ref type="figure" target="#fig_1">(Hieber et al., 2017)</ref>  <ref type="bibr">4</ref> . Sock- eye implements several different NMT models but here we use the standard recurrent attentional model described in Section 2. We report baselines with and without dropout ( <ref type="bibr" target="#b24">Srivastava et al., 2014</ref>). For dropout a retention probability of 0.5 was used.</p><p>As a second baseline we use our own implemen- tation of the model of <ref type="bibr" target="#b30">Zhang et al. (2016)</ref> which contains a single sentence-level Gaussian latent variable (SENT). Our implementation differs from theirs in three aspects. First, we feed the last hid- den state of the bidirectional encoding into encod- ing of the source and target sentence into the in- ference network ( <ref type="bibr" target="#b30">Zhang et al. (2016)</ref> use the av- erage of all states). Second, the latent variable is smaller in size than the one used by <ref type="bibr" target="#b30">(Zhang et al., 2016)</ref>. 5 This was done to make their model and the stochastic decoder proposed here as similar as possible. Finally, their implementation was based on groundhog whereas ours builds on Sockeye.</p><p>Our stochastic decoder model (SDEC) is also built on top of the basic Sockeye model. It adds the components described in Sections 3 and 4. Recall that the functions that compute the means and stan- dard deviations are implemented by neural nets with a single hidden layer with tanh activation. The width of that layer is twice the size of the la- tent variable. In our experiments we tested differ- ent latent variable sizes and used KL scaling (see Section 4.1). The scale started from 0 and was in- creased by 1 /20,000 after each mini-batch. Thus, at iteration t the scale is min( t /20,000, 1).</p><p>All models use 1028 units for the LSTM hid-den state (or 512 for each direction in the bidirec- tional LSTMs) and 256 for the attention mechan- sim. Training is done with Adam ( <ref type="bibr" target="#b12">Kingma and Ba, 2015)</ref>. In decoding we use a beam of size 5 and output the most likely word at each position. We deterministically set all latent variables to their mean values during decoding. Monte Carlo decod- ing ( <ref type="bibr" target="#b7">Gal, 2016</ref>) is difficult to apply to our setting as it would require sampling entire translations.</p><p>Results We show the BLEU scores for all mod- els that we tested on the IWSLT data set in Ta- ble 2. The stochastic decoder dominates the Sock- eye baseline across all 4 languages, and outper- forms SENT on most languages. Except on Ger- man, there is a trend towards smaller latent vari- able sizes being more helpful. This is in line with findings by <ref type="bibr" target="#b5">Chung et al. (2015)</ref> and <ref type="bibr" target="#b6">Fraccaro et al. (2016)</ref> who also used relatively small latent vari- ables. This observation also implies that our model does not improve simply because it has more pa- rameters than the baseline. That the margin between the SDEC and SENT models is not large was to be expected for two reasons. First, <ref type="bibr" target="#b5">Chung et al. (2015)</ref> and <ref type="bibr" target="#b6">Fraccaro et al. (2016)</ref> have shown that stochastic RNNs lead to enormous improvements in modelling continu- ous sequences but only modest increases in perfor- mance for discrete sequences (such as natural lan- guage). Second, translation performance is mea- sured in BLEU score. We observed that SDEC of- ten reached better ELBO values than SENT indi- cating a better model fit. How to fully leverage the better modelling ability of stochastic RNNs when producing discrete outputs is a matter of future re- search.</p><p>Qualitative Analysis Finally, we would like to demonstrate that our model does indeed capture variation in translation. To this end, we randomly picked sentences from the IWSLT test set and had our model translate them several times, however, the values of the latent variables were sampled in- stead of fixed. Contrary to the BLEU-based evalu- ation, beam search was not used in this evaluation in order to avoid interaction between different la- tent variable samples. See <ref type="figure" target="#fig_3">Figure 3</ref> for examples of syntactic and lexical variation. It is important to note that we do not sample from the categori- cal output distribution. For each target position we pick the most likely word. A non-stochastic NMT system would always yield the same translation in this scenario. Interestingly, when we applied the sampling procedure to the SENT model it did not produce any variation at all, thus behaving like a deterministic NMT system. This supports our ini- tial point that the SENT model is likely insensitive to local variation, a problem that our model was designed to address. Like the model of <ref type="bibr" target="#b4">Bowman et al. (2016)</ref>, SENT presumably tends to ignore the latent variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The stochastic decoder is strongly influenced by previous work on stochastic RNNs. The first such proposal was made by <ref type="bibr" target="#b2">Bayer and Osendorfer (2015)</ref> who introduced i.i.d. Gaussian latent vari- ables at each output position. Since their model neglects any sequential dependence of the noise sources, it underperformed on several sequence modeling tasks. <ref type="bibr" target="#b5">Chung et al. (2015)</ref> made the la- tent variables depend on previous information by feeding the previous decoder state into the latent variable sampler. Their inference model did not make use of future elements in the sequence.</p><p>Using a "look-ahead" mechanism in the infer- ence net was proposed by <ref type="bibr" target="#b6">Fraccaro et al. (2016)</ref> who had a separate stochastic and deterministic RNN layer which both influence the output. Since the stochastic layer in their model depends on the deterministic layer but not vice versa, they could first run the deterministic layer at inference time and then condition the inference net's encoding of the future on the thus obtained features. Like us, they used KL scaling during training.</p><p>More recently, <ref type="bibr" target="#b9">Goyal et al. (2017)</ref> proposed an auxiliary loss that has the inference net predict fu- ture feature representations. This approach yields state-of-the-art results but is still in need of a the- oretical justification.</p><p>Within translation, <ref type="bibr" target="#b30">Zhang et al. (2016)</ref> were the first to incorporate Gaussian variables into an NMT model. Their approach only uses one sentence-level latent variable (corresponding to our z 0 ) and can thus not deal with word-level vari- ation directly. Concurrently to our work, <ref type="bibr" target="#b25">Su et al. (2018)</ref> have also proposed a recurrent latent vari- able model for NMT. Their approach differs from ours in that they do not use a 0 th latent variable nor a look-ahead mechanism during inference time. Furthermore, their underlying recurrent model is a GRU.</p><p>In the wider field of NLP, deep generative mod-  Source They undertook a study of autism prevalence in the general population.</p><p>SENT Sie haben eine Studie von Autismus in der allgemeinen Population übernommen. SDEC Sie entwarfen eine Studie von Autismus in der allgemeinen Bevölkerung. SDEC Sie führten eine Studie von Autismus in der allgemeinen Population ein. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We have presented a recurrent decoder for machine translation that uses word-level Gaussian variables to model underlying sources of variation observed in translation corpora. Our experiments confirm our intuition that modelling variation is crucial to the success of machine translation. The proposed model consistently outperforms strong baselines on several language pairs.</p><p>As this is the first work that systematically con- siders word-level variation in NMT, there are lots of research ideas to explore in the future. Here, we list the three which we believe to be most promis- ing.</p><p>• Latent factor models: our model only con- tains one source of variation per word. A latent factor model such as DARN ( <ref type="bibr" target="#b10">Gregor et al., 2014</ref>) would consider several sources simultaneously. This would also allow us to perform a better analysis of the model be- haviour as we could correlate the factors with observed linguistic phenomena.</p><p>• Richer prior and variational distributions:</p><p>The diagonal Gaussian is likely too simple a distribution to appropriately model the vari- ation in our data. Richer distributions com- puted by normalising flows <ref type="bibr" target="#b19">(Rezende and Mohamed, 2015;</ref><ref type="bibr" target="#b13">Kingma et al., 2016)</ref> will likely improve our model. • Extension to other architectures: Introduc- ing latent variables into non-autoregressive translation models such as the transformer ( <ref type="bibr" target="#b28">Vaswani et al., 2017)</ref> should increase their translation ability further.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Dich kann ich mir nicht nackt vorstellen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples from the multiple-translation Chinese corpus (LDC2002T01), where the translations come from different translators. These demonstrate the lexical variation of the verb and variation between passive and raising structures (top), and lexical variation on the agent NP (bottom). Both examples also exhibit appreciable length variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graphical representation of 2a the generative model and 2b the inference model. Black lines indicate generative parameters (θ) and red lines variational parameters (λ). Dashed red-black lines indicate that the inference model uses feature representations computed by the generative model as inputs. Through the recurrent net, the generative model (2a) also conditions its outputs on all previous latent assignments. We omit these arrows to avoid clutter. The inference model (2b) is only used at training time. Dots indicate further conditioning context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sampled translations from our model (SDEC) and the sentent-level latent variable model (SENT). The first SDEC example shows alternation between the German simple past and past perfect. The past perfect introduces a long range dependency between the main and auxiliary verb (underlined) that the model handles well. The second example shows variation in the lexical realisation of the verb. The second variant uses a particle verb and we again observe a long range dependency between the main verb and its particle (underlined).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU scores for different models on the IWSLT data for translation into English. Recall that 
all SDEC and SENT models used KL scaling during training. 

Source Coincidentally, at the same time, the first easy-to-use clinical tests for diagnosing autism 
were introduced. 

SENT Im gleichen Zeitraum wurden die ersten einfachen klinischen Tests für Diagnose getestet. 
SDEC Übrigens, zur gleichen Zeit, wurden die ersten einfache klinische Tests für die Diagnose 
von Autismus eingeführt. 
SDEC Übrigens, zur gleichen Zeit, waren die ersten einfache klinische Tests für die Diagnose von 
Autismus eingeführt worden. 

</table></figure>

			<note place="foot" n="2"> The term weak generator has first been coined by Alemi et al. (2017).</note>

			<note place="foot" n="3"> https://github.com/google/sentencepiece 4 https://github.com/awslabs/sockeye 5 We did, however, find that increasing the latent variable size actually hurt performance in our implementation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>Philip Schulz and Wilker Aziz were supported by the Dutch Organisation for Scientific Re-search (NWO) VICI Grant nr.</p><p>277-89-002. Trevor Cohn is the recipient of an Australian Re-search Council Future Fellowship (project number FT130101105).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An information theoretic analysis of deep latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Osendorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 28</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Søren Kaae Sø Nderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 29</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2199" to="2207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Uncertainty in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Z-forcing: Training stochastic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 30</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6716" to="6726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep autoregressive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. Bejing, China</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1242" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sockeye: A Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vilar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 29</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autoencoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Victor Michalowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Bucholtz</surname></persName>
		</author>
		<title level="m">Handbook of Differential Entropy</title>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient estimation using stochastic computation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 28</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3528" to="3536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hybrid convolutional variational autoencoder for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="627" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 28</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variational recurrent neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaojie</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 27</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Doubly stochastic Variational Bayes for nonconjugate inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1971" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 30</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Latent intention dialogue models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3732" to="3741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Variational neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-space variational encoder-decoders for semi-supervised labeled sequence transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="310" to="320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
