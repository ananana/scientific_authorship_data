<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
							<email>taku@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="66" to="75"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>66</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword seg-mentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robust-ness of NMT. We present a simple regu-larization method, subword regularization, which trains the model with multiple sub-word segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new sub-word segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) models ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b13">Luong et al., 2015;</ref><ref type="bibr" target="#b31">Wu et al., 2016;</ref><ref type="bibr" target="#b26">Vaswani et al., 2017</ref>) often oper- ate with fixed word vocabularies, as their training and inference depend heavily on the vocabulary size. However, limiting vocabulary size increases the amount of unknown words, which makes the translation inaccurate especially in an open vocabulary setting.</p><p>A common approach for dealing with the open vocabulary issue is to break up rare words into subword units <ref type="bibr" target="#b19">(Schuster and Nakajima, 2012;</ref><ref type="bibr" target="#b4">Chitnis and DeNero, 2015;</ref><ref type="bibr" target="#b21">Sennrich et al., 2016;</ref><ref type="bibr" target="#b31">Wu et al., 2016</ref> (BPE) ( <ref type="bibr" target="#b21">Sennrich et al., 2016</ref>) is a de facto standard subword segmentation algorithm ap- plied to many NMT systems and achieving top translation quality in several shared tasks <ref type="bibr" target="#b5">(Denkowski and Neubig, 2017;</ref><ref type="bibr" target="#b15">Nakazawa et al., 2017)</ref>. BPE segmentation gives a good balance between the vocabulary size and the decoding ef- ficiency, and also sidesteps the need for a special treatment of unknown words.</p><p>BPE encodes a sentence into a unique subword sequence. However, a sentence can be repre- sented in multiple subword sequences even with the same vocabulary. <ref type="table">Table 1</ref> illustrates an exam- ple. While these sequences encode the same input "Hello World", NMT handles them as completely different inputs. This observation becomes more apparent when converting subword sequences into id sequences (right column in <ref type="table">Table 1</ref>). These vari- ants can be viewed as a spurious ambiguity, which might not always be resolved in decoding process. At training time of NMT, multiple segmentation candidates will make the model robust to noise and segmentation errors, as they can indirectly help the model to learn the compositionality of words, e.g., "books" can be decomposed into "book" + "s".</p><p>In this study, we propose a new regulariza- tion method for open-vocabulary NMT, called subword regularization, which employs multiple subword segmentations to make the NMT model accurate and robust. Subword regularization con- sists of the following two sub-contributions:</p><p>• We propose a simple NMT training algo- rithm to integrate multiple segmentation can- didates. Our approach is implemented as an on-the-fly data sampling, which is not spe- cific to NMT architecture. Subword regular- ization can be applied to any NMT system without changing the model structure.</p><p>• We also propose a new subword segmenta- tion algorithm based on a language model, which provides multiple segmentations with probabilities. The language model allows to emulate the noise generated during the seg- mentation of actual data.</p><p>Empirical experiments using multiple corpora with different sizes and languages show that subword regularization achieves significant im- provements over the method using a single sub- word sequence. In addition, through experiments with out-of-domain corpora, we show that sub- word regularization improves the robustness of the NMT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation with</head><p>multiple subword segmentations 2.1 NMT training with on-the-fly subword sampling</p><p>Given a source sentence X and a target sentence Y , let x = (x 1 , . . . , x M ) and y = (y 1 , . . . , y N ) be the corresponding subword sequences seg- mented with an underlying subword segmenter, e.g., BPE. NMT models the translation probability P (Y |X) = P (y|x) as a target language sequence model that generates target subword y n condition- ing on the target history y &lt;n and source input se- quence x:</p><formula xml:id="formula_0">P (y|x; θ) = N ∏ n=1 P (y n |x, y &lt;n ; θ),<label>(1)</label></formula><p>where θ is a set of model parameters. A com- mon choice to predict the subword y n is to use a recurrent neural network (RNN) architecture. However, note that subword regularization is not specific to this architecture and can be applica- ble to other NMT architectures without RNN, e.g., <ref type="bibr" target="#b26">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b7">Gehring et al., 2017)</ref>. NMT is trained using the standard maximum likelihood estimation, i.e., maximizing the log-</p><formula xml:id="formula_1">likelihood L(θ) of a given parallel corpus D = {⟨X (s) , Y (s) ⟩} |D| s=1 = {⟨x (s) , y (s) ⟩} |D| s=1 , θ M LE = arg max θ L(θ) where, L(θ) = |D| ∑ s=1 log P (y (s) |x (s) ; θ). (2)</formula><p>We here assume that the source and target sen- tences X and Y can be segmented into multiple subword sequences with the segmentation proba- bilities P (x|X) and P (y|Y ) respectively. In sub- word regularization, we optimize the parameter set θ with the marginalized likelihood as <ref type="bibr">(3)</ref>.</p><formula xml:id="formula_2">L marginal (θ) = |D| ∑ s=1 E x∼P (x|X (s) ) y∼P (y|Y (s) ) [log P (y|x; θ)] (3)</formula><p>Exact optimization of <ref type="formula">(3)</ref> is not feasible as the number of possible segmentations increases expo- nentially with respect to the sentence length. We approximate (3) with finite k sequences sampled from P (x|X) and P (y|Y ) respectively.</p><formula xml:id="formula_3">L marginal (θ) ∼ = 1 k 2 |D| ∑ s=1 k ∑ i=1 k ∑ j=1 log P (y j |x i ; θ) x i ∼ P (x|X (s) ), y j ∼ P (y|Y (s) ).<label>(4)</label></formula><p>For the sake of simplicity, we use k = 1. Train- ing of NMT usually uses an online training for efficiency, in which the parameter θ is iteratively optimized with respect to the smaller subset of D (mini-batch). When we have a sufficient number of iterations, subword sampling is executed via the data sampling of online training, which yields a good approximation of (3) even if k = 1. It should be noted, however, that the subword sequence is sampled on-the-fly for each parameter update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoding</head><p>In the decoding of NMT, we only have a raw source sentence X. A straightforward approach for decoding is to translate from the best segmen- tation x * that maximizes the probability P (x|X), i.e., x * = argmax x P (x|X). Additionally, we can use the n-best segmentations of P (x|X) to incorporate multiple segmentation candidates. More specifically, given n-best segmentations (x 1 , . . . , x n ), we choose the best translation y * that maximizes the following score.</p><p>score(x, y) = log P (y|x)/|y| λ ,</p><p>where |y| is the number of subwords in y and λ ∈ R + is the parameter to penalize shorter sentences. λ is optimized with the development data. In this paper, we call these two algorithms one- best decoding and n-best decoding respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Subword segmentations with language model</head><p>3.1 Byte-Pair-Encoding (BPE)</p><p>Byte-Pair-Encoding (BPE) ( <ref type="bibr" target="#b21">Sennrich et al., 2016;</ref><ref type="bibr" target="#b19">Schuster and Nakajima, 2012</ref>) is a subword seg- mentation algorithm widely used in many NMT systems <ref type="bibr">1</ref> . BPE first splits the whole sentence into individual characters. The most frequent 2 adjacent pairs of characters are then consecutively merged until reaching a desired vocabulary size. Subword segmentation is performed by applying the same merge operations to the test sentence. An advantage of BPE segmentation is that it can effectively balance the vocabulary size and the step size (the number of tokens required to encode the sentence). BPE trains the merged operations only with a frequency of characters. Frequent sub- strings will be joined early, resulting in common words remaining as one unique symbol. Words consisting of rare character combinations will be split into smaller units, e.g., substrings or charac- ters. Therefore, only with a small fixed size of vocabulary (usually 16k to 32k), the number of re- quired symbols to encode a sentence will not sig- nificantly increase, which is an important feature for an efficient decoding.</p><p>One downside is, however, that BPE is based on a greedy and deterministic symbol replace- ment, which can not provide multiple segmenta- tions with probabilities. It is not trivial to apply BPE to the subword regularization that depends on segmentation probabilities P (x|X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unigram language model</head><p>In this paper, we propose a new subword seg- mentation algorithm based on a unigram language model, which is capable of outputing multiple sub- word segmentations with probabilities. The uni- gram language model makes an assumption that <ref type="bibr">1</ref> Strictly speaking, wordpiece model ( <ref type="bibr" target="#b19">Schuster and Nakajima, 2012</ref>) is different from BPE. We consider wordpiece as a variant of BPE, as it also uses an incremental vocabulary generation with a different loss function.</p><p>2 Wordpiece model uses a likelihood instead of frequency.</p><p>each subword occurs independently, and conse- quently, the probability of a subword sequence x = (x 1 , . . . , x M ) is formulated as the product of the subword occurrence probabilities p(x i ) 3 :</p><formula xml:id="formula_5">P (x) = M ∏ i=1 p(x i ),<label>(6)</label></formula><formula xml:id="formula_6">∀i x i ∈ V, ∑ x∈V p(x) = 1,</formula><p>where V is a pre-determined vocabulary. The most probable segmentation x * for the input sentence X is then given by</p><formula xml:id="formula_7">x * = arg max x∈S(X) P (x),<label>(7)</label></formula><p>where S(X) is a set of segmentation candidates built from the input sentence X. x * is obtained with the Viterbi algorithm <ref type="bibr" target="#b30">(Viterbi, 1967)</ref>. If the vocabulary V is given, subword occur- rence probabilities p(x i ) are estimated via the EM algorithm that maximizes the following marginal likelihood L assuming that p(x i ) are hidden vari- ables.</p><formula xml:id="formula_8">L = |D| ∑ s=1 log(P (X (s) )) = |D| ∑ s=1 log ( ∑ x∈S(X (s) ) P (x) )</formula><p>In the real setting, however, the vocabulary set V is also unknown. Because the joint optimization of vocabulary set and their occurrence probabili- ties is intractable, we here seek to find them with the following iterative algorithm.</p><p>1. Heuristically make a reasonably big seed vo- cabulary from the training corpus.</p><p>2. Repeat the following steps until |V| reaches a desired vocabulary size.</p><p>(a) Fixing the set of vocabulary, optimize p(x) with the EM algorithm. (b) Compute the loss i for each subword x i , where loss i represents how likely the likelihood L is reduced when the sub- word x i is removed from the current vo- cabulary. (c) Sort the symbols by loss i and keep top η % of subwords (η is 80, for example).</p><p>Note that we always keep the subwords consisting of a single character to avoid out-of-vocabulary.</p><p>There are several ways to prepare the seed vo- cabulary. The natural choice is to use the union of all characters and the most frequent substrings in the corpus <ref type="bibr">4</ref> . Frequent substrings can be enu- merated in O(T ) time and O(20T ) space with the Enhanced Suffix Array algorithm ( <ref type="bibr" target="#b16">Nong et al., 2009)</ref>, where T is the size of the corpus. Simi- lar to <ref type="bibr" target="#b21">(Sennrich et al., 2016</ref>), we do not consider subwords that cross word boundaries.</p><p>As the final vocabulary V contains all individual characters in the corpus, character-based segmen- tation is also included in the set of segmentation candidates S(X). In other words, subword seg- mentation with the unigram language model can be seen as a probabilsitic mixture of characters, subwords and word segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Subword sampling</head><p>Subword regularization samples one subword seg- mentation from the distribution P (x|X) for each parameter update. A straightforward approach for an approximate sampling is to use the l-best segmentations. More specifically, we first obtain l-best segmentations according to the probabil- ity P (x|X). l-best search is performed in lin- ear time with the Forward-DP Backward-A* al- gorithm <ref type="bibr" target="#b14">(Nagata, 1994)</ref>. One segmentation x i is then sampled from the multinomial distribution</p><formula xml:id="formula_9">P (x i |X) ∼ = P (x i ) α / ∑ l i=1 P (x i ) α ,</formula><p>where α ∈ R + is the hyperparameter to control the smooth- ness of the distribution. A smaller α leads to sam- ple x i from a more uniform distribution. A larger α tends to select the Viterbi segmentation.</p><p>Setting l → ∞, in theory, allows to take all pos- sible segmentations into account. However, it is not feasible to increase l explicitly as the num- ber of candidates increases exponentially with re- spect to the sentence length. In order to exactly sample from all possible segmentations, we use the Forward-Filtering and Backward-Sampling al- gorithm (FFBS) <ref type="bibr" target="#b20">(Scott, 2002</ref>), a variant of the dynamic programming originally introduced by Bayesian hidden Markov model training. In FFBS, all segmentation candidates are represented in a compact lattice structure, where each node de- notes a subword. In the first pass, FFBS computes a set of forward probabilities for all subwords in the lattice, which provide the probability of end- ing up in any particular subword w. In the second pass, traversing the nodes in the lattice from the end of the sentence to the beginning of the sen- tence, subwords are recursively sampled for each branch according to the forward probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">BPE vs. Unigram language model</head><p>BPE was originally introduced in the data com- pression literature <ref type="bibr" target="#b6">(Gage, 1994)</ref>. BPE is a vari- ant of dictionary (substitution) encoder that incre- mentally finds a set of symbols such that the total number of symbols for encoding the text is mini- mized. On the other hand, the unigram language model is reformulated as an entropy encoder that minimizes the total code length for the text. Ac- cording to Shannon's coding theorem, the optimal code length for a symbol s is − log p s , where p s is the occurrence probability of s. This is essen- tially the same as the segmentation strategy of the unigram language model described as <ref type="formula" target="#formula_7">(7)</ref>. BPE and the unigram language model share the same idea that they encode a text using fewer bits with a certain data compression principle (dictio- nary vs. entropy). Therefore, we expect to see the same benefit as BPE with the unigram language model. However, the unigram language model is more flexible as it is based on a probabilistic lan- guage model and can output multiple segmenta- tions with their probabilities, which is an essential requirement for subword regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Regularization by noise is a well studied tech- nique in deep neural networks. A well-known ex- ample is dropout ( <ref type="bibr" target="#b23">Srivastava et al., 2014</ref>), which randomly turns off a subset of hidden units dur- ing training. Dropout is analyzed as an ensemble training, where many different models are trained on different subsets of the data. Subword regu- larization trains the model on different data inputs randomly sampled from the original input sen- tences, and thus is regarded as a variant of ensem- ble training.</p><p>The idea of noise injection has previously been used in the context of Denoising Auto-Encoders (DAEs) ( <ref type="bibr" target="#b27">Vincent et al., 2008)</ref>, where noise is added to the inputs and the model is trained to re- construct the original inputs. There are a couple of studies that employ DAEs in natural language processing.</p><p>( <ref type="bibr" target="#b12">Lample et al., 2017;</ref><ref type="bibr" target="#b0">Artetxe et al., 2017</ref>) in- dependently propose DAEs in the context of sequence-to-sequence learning, where they ran- domly alter the word order of the input sentence and the model is trained to reconstruct the original sentence. Their technique is applied to an unsu- pervised machine translation to make the encoder truly learn the compositionality of input sentences.</p><p>Word dropout <ref type="bibr" target="#b9">(Iyyer et al., 2015</ref>) is a simple ap- proach for a bag-of-words representation, in which the embedding of a certain word sequence is sim- ply calculated by averaging the word embeddings. Word dropout randomly drops words from the bag before averaging word embeddings, and conse- quently can see 2 |X| different token sequences for each input X.</p><p>( <ref type="bibr" target="#b2">Belinkov and Bisk, 2017)</ref> explore the training of character-based NMT with a synthetic noise that randomly changes the order of characters in a word. ( <ref type="bibr" target="#b32">Xie et al., 2017</ref>) also proposes a robust RNN language model that interpolates random un- igram language model.</p><p>The basic idea and motivation behind subword regularization are similar to those of previous work. In order to increase the robustness, they in- ject noise to input sentences by randomly chang- ing the internal representation of sentences. How- ever, these previous approaches often depend on heuristics to generate synthetic noises, which do not always reflect the real noises on training and inference. In addition, these approaches can only be applied to source sentences (encoder), as they irreversibly rewrite the surface of sentences. Sub- word regularization, on the other hand, generates synthetic subword sequences with an underlying language model to better emulate the noises and segmentation errors. As subword regularization is based on an invertible conversion, we can safely apply it both to source and target sentences.</p><p>Subword regularization can also be viewed as a data augmentation. In subword regularization, an input sentence is converted into multiple invariant sequences, which is similar to the data augmen- tation for image classification tasks, for example, random flipping, distorting, or cropping.</p><p>There are several studies focusing on segmen- tation ambiguities in language modeling. Latent Sequence Decompositions (LSDs) ( <ref type="bibr" target="#b3">Chan et al., 2016</ref>) learns the mapping from the input and the output by marginalizing over all possible segmen- tations. LSDs and subword regularization do not assume a predetermined segmentation for a sen- tence, and take multiple segmentations by a sim- ilar marginalization technique. The difference is that subword regularization injects the multi- ple segmentations with a separate language model through an on-the-fly subword sampling. This ap- proach makes the model simple and independent from NMT architectures.</p><p>Lattice-to-sequence models ( <ref type="bibr" target="#b24">Su et al., 2017;</ref><ref type="bibr" target="#b22">Sperber et al., 2017)</ref> are natural extension of sequence-to-sequence models, which represent in- puts uncertainty through lattices. Lattice is en- coded with a variant of <ref type="bibr">TreeLSTM (Tai et al., 2015)</ref>, which requires changing the model archi- tecture. In addition, while subword regulariza- tion is applied both to source and target sentences, lattice-to-sequence models do not handle target side ambiguities.</p><p>A mixed word/character model ( <ref type="bibr" target="#b31">Wu et al., 2016)</ref> addresses the out-of-vocabulary problem with a fixed vocabulary. In this model, out-of- vocabulary words are not collapsed into a single UNK symbol, but converted into the sequence of characters with special prefixes representing the positions in the word. Similar to BPE, this model also encodes a sentence into a unique fixed se- quence, thus multiple segmentations are not taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setting</head><p>We conducted experiments using multiple corpora with different sizes and languages. <ref type="table">Table 2</ref> sum- marizes the evaluation data we used 5 6 7 8 9 10 . IWSLT15/17 and KFTT are relatively small cor- pora, which include a wider spectrum of languages with different linguistic properties. They can eval- uate the language-agnostic property of subword regularization. ASPEC and WMT14 (en↔de) are medium-sized corpora. WMT14 (en↔cs) is a rather big corpus consisting of more than 10M par- allel sentences.</p><p>We used GNMT ( <ref type="bibr" target="#b31">Wu et al., 2016)</ref> as the im- plementation of the NMT system for all exper- iments. We generally followed the settings and training procedure described in ( <ref type="bibr" target="#b31">Wu et al., 2016)</ref>, however, we changed the settings according to the corpus size. <ref type="table">Table 2</ref> shows the hyperparameters we used in each experiment. As common set- tings, we set the dropout probability to be 0.2. For parameter estimation, we used a combination of Adam ( <ref type="bibr" target="#b10">Kingma and Adam, 2014</ref>) and SGD algo- rithms. Both length normalization and converge penalty parameters are set to 0.2 (see section 7 in ( <ref type="bibr" target="#b31">Wu et al., 2016)</ref>). We set the decoding beam size to 4.</p><p>The data was preprocessed with Moses tok- enizer before training subword models. It should be noted, however, that Chinese and Japanese have no explicit word boundaries and Moses tokenizer does not segment sentences into words, and hence subword segmentations are trained almost from unsegmented raw sentences in these languages.</p><p>We used the case sensitive BLEU score ( <ref type="bibr" target="#b17">Papineni et al., 2002</ref>) as an evaluation metric. As the output sentences are not segmented in Chi- nese and Japanese, we segment them with char- acters and KyTea 11 for Chinese and Japanese re- spectively before calculating BLEU scores.</p><p>BPE segmentation is used as a baseline sys- tem. We evaluate three test systems with dif- ferent sampling strategies: (1) Unigram language model-based subword segmentation without sub- word regularization (l = 1), (2) with subword reg- ularization (l = 64, α = 0.1) and (3) (l = ∞, α = 0.2/0.5) 0.2: IWSLT, 0.5: others. These sam- pling parameters were determined with prelimi- nary experiments. l = 1 is aimed at a pure com- parison between BPE and the unigram language model. In addition, we compare one-best decod- ing and n-best decoding (See section 2.2). Be- cause BPE is not able to provide multiple segmen- tations, we only evaluate one-best decoding for BPE. Consequently, we compare 7 systems (1 + 3 × 2) for each language pair. <ref type="table">Table 3</ref> shows the translation experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>First, as can be seen in the table, BPE and un- igram language model without subword regular- ization (l = 1) show almost comparable BLEU scores. This is not surprising, given that both BPE and the unigram language model are based on data compression algorithms.</p><p>We can see that subword regularization (l &gt; 1) boosted BLEU scores quite impressively (+1 to 2 points) in all language pairs except for WMT14 (en→cs) dataset. The gains are larger especially in lower resource settings (IWSLT and KFTT). It can be considered that the positive effects of data augmentation with subword regularization worked better in lower resource settings, which is a com- mon property of other regularization techniques.</p><p>As for the sampling algorithm, (l = ∞ α = 0.2/0.5) slightly outperforms (l = 64, α = 0.1) on IWSLT corpus, but they show almost compara- ble results on larger data set. Detailed analysis is described in Section 5.5.</p><p>On top of the gains with subword regulariza- tion, n-best decoding yields further improvements in many language pairs. However, we should note that the subword regularization is mandatory for n-best decoding and the BLEU score is degraded in some language pairs without subword regular- ization (l = 1). This result indicates that the de- coder is more confused for multiple segmentations when they are not explored at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results with out-of-domain corpus</head><p>To see the effect of subword regularization on a more open-domain setting, we evaluate the sys- tems with out-of-domain in-house data consisting of multiple genres: Web, patents and query logs. Note that we did not conduct the comparison with KFTT and ASPEC corpora, as we found that the domains of these corpora are too specific 12 , and preliminary evaluations showed extremely poor BLEU scores (less than 5) on out-of-domain cor- pora. <ref type="table">Table 4</ref> shows the results. Compared to the gains obtained with the standard in-domain evalu- ations in <ref type="table">Table 3</ref>, subword regularization achieves significantly larger improvements (+2 points) in every domain of corpus. An interesting observa- tion is that we have the same level of improve- ments even on large training data sets (WMT14), which showed marginal or small gains with the in-domain data. This result strongly supports our claim that subword regularization is more useful for open-domain settings. Proposed (n-best decoding, n = 64)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with other segmentation algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Language pair  <ref type="table">Table 3</ref>: Main Results (BLEU(%)) (l: sampling size in SR, α: smoothing parameter). * indicates statistically significant difference (p &lt; 0.05) from baselines with bootstrap resampling <ref type="bibr" target="#b11">(Koehn, 2004)</ref>. The same mark is used in <ref type="table">Table 4</ref> and 6.</p><formula xml:id="formula_10">baseline (BPE) l = 1 l = 64 α = 0.1 l = ∞ α = 0.2/0.5 l = 1 l = 64 α = 0.1 l = ∞ α = 0.2/0.5 IWSLT15 en →</formula><p>( <ref type="bibr" target="#b21">Sennrich et al., 2016</ref>) and our unigram model with or without subword regularization. The BLEU scores of word, character and mixed word/character models are cited from ( <ref type="bibr" target="#b31">Wu et al., 2016)</ref>. As German is a morphologically rich language and needs a huge vocabulary for word models, subword-based algorithms perform a gain of more than 1 BLEU point than word model. Among subword-based algorithms, the unigram language model with subword regular- ization achieved the best BLEU score (25.04), which demonstrates the effectiveness of multiple subword segmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Impact of sampling hyperparameters</head><p>Subword regularization has two hyperparameters: l: size of sampling candidates, α: smoothing con- stant. <ref type="figure" target="#fig_0">Figure 1</ref> shows the BLEU scores of various hyperparameters on IWSLT15 (en → vi) dataset.</p><p>First, we can find that the peaks of BLEU scores against smoothing parameter α are different de- pending on the sampling size l. This is expected, because l = ∞ has larger search space than l = 64, and needs to set α larger to sample sequences close to the Viterbi sequence x * .</p><p>Another interesting observation is that α = 0.0 leads to performance drops especially on l = ∞. When α = 0.0, the segmentation probability P (x|X) is virtually ignored and one segmentation is uniformly sampled. This result suggests that bi- ased sampling with a language model is helpful to emulate the real noise in the actual translation.</p><p>In general, larger l allows a more aggressive regularization and is more effective for low re- source settings such as IWSLT. However, the es- timation of α is more sensitive and performance becomes even worse than baseline when α is ex- tremely small. To weaken the effect of regular- ization and avoid selecting invalid parameters, it might be more reasonable to use l = 64 for high resource languages. Although we can see in general that the opti- mal hyperparameters are roughly predicted with the held-out estimation, it is still an open question how to choose the optimal size l in subword sam- pling. <ref type="table">Table 6</ref> summarizes the BLEU scores with sub- word regularization either on source or target sen- tence to figure out which components (encoder or decoder) are more affected. As expected, we can see that the BLEU scores with single side regular- ization are worse than full regularization. How- ever, it should be noted that single side regular- ization still has positive effects. This result im- plies that subword regularization is not only help- ful for encoder-decoder architectures, but appli- cable to other NLP tasks that only use an either encoder or decoder, including text classification  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Results with single side regularization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we presented a simple regularization method, subword regularization 13 , for NMT, with no change to the network architecture. The central idea is to virtually augment training data with on-the-fly subword sampling, which helps to improve the accuracy as well as robustness of NMT models. In addition, for better subword sam- pling, we propose a new subword segmentation algorithm based on the unigram language model. Experiments on multiple corpora with different sizes and languages show that subword regulariza- tion leads to significant improvements especially on low resource and open-domain settings.</p><p>Promising avenues for future work are to ap- ply subword regularization to other NLP tasks based on encoder-decoder architectures, e.g., di- alog generation <ref type="bibr" target="#b28">(Vinyals and Le, 2015)</ref> and auto- matic summarization <ref type="bibr" target="#b18">(Rush et al., 2015)</ref>. Com- pared to machine translation, these tasks do not have enough training data, and thus there could be a large room for improvement with subword regularization. Additionally, we would like to ex- plore the application of subword regularization for machine learning, including Denoising Auto En- coder ( <ref type="bibr" target="#b27">Vincent et al., 2008)</ref> and Adversarial Train- ing ( <ref type="bibr" target="#b8">Goodfellow et al., 2015</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Effect of sampling hyperparameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 5 shows</head><label>5</label><figDesc></figDesc><table>the comparison on different 
segmentation algorithms: 
word, character, 
mixed word/character (Wu et al., 2016), BPE </table></figure>

			<note place="foot" n="3"> Target sequence y = (y1,. .. , yN ) can also be modeled similarly.</note>

			<note place="foot" n="4"> It is also possible to run BPE with a sufficient number of merge operations.</note>

			<note place="foot" n="5"> IWSLT15: http://workshop2015.iwslt.org/ 6 IWSLT17: http://workshop2017.iwslt.org/ 7 KFTT: http://www.phontron.com/kftt/ 8 ASPEC: http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ 9 WMT14: http://statmt.org/wmt14/ 10 WMT14(en↔de) uses the same setting as (Wu et al., 2016).</note>

			<note place="foot" n="11"> http://www.phontron.com/kytea</note>

			<note place="foot" n="12"> KFTT focuses on Wikipedia articles related to Kyoto, and ASPEC is a corpus of scientific paper domain. Therefore, it is hard to translate out-of-domain texts.</note>

			<note place="foot" n="13"> Implementation is available at https://github.com/google/sentencepiece</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Unsupervised neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11041</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXive preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02173</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXive preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03035</idno>
		<title level="m">Latent sequence decompositions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variablelength word encodings for neural translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chitnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2088" to="2093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stronger baselines for trustable results in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Workshop on Neural Machine Translation</title>
		<meeting>of Workshop on Neural Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new algorithm for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C Users J</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy Ba</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXive preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A stochastic japanese morphological analyzer using a forward-dp backward-a* nbest search algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overview of the 4th workshop on asian translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Higashiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideya</forename><surname>Mino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Asian Translation (WAT2017)</title>
		<meeting>the 4th Workshop on Asian Translation (WAT2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linear suffix array construction by almost pure inducedsorting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Nong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai Hong</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DCC</title>
		<meeting>of DCC</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian methods for hidden markov models: Recursive computing in the 21st century</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural lattice-to-sequence models for uncertain inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lattice-based recurrent neural network encoders for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>De Yi Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3302" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXive preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data noising as smoothing in neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lévy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
