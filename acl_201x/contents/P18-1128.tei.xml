<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Hitchhiker&apos;s Guide to Testing Statistical Significance in Natural Language Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rotem</forename><surname>Dror</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gili</forename><surname>Baumer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Segev</forename><surname>Shlomov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Hitchhiker&apos;s Guide to Testing Statistical Significance in Natural Language Processing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1383" to="1392"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1383</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opin-ion/theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion, we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results , statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied in NLP research in a statistically sound manner 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The field of Natural Language Processing (NLP) has recently made great progress due to the data revolution that has made abundant amounts of tex- tual data from a variety of languages and linguis- tic domains (newspapers, scientific journals, so- cial media etc.) available. This, together with the emergence of a new generation of computing re- sources and the related development of Deep Neu- ral Network models, have resulted in dramatic im- provements in the capabilities of NLP algorithms.</p><p>The extended reach of NLP algorithms has also resulted in NLP papers giving much more empha- sis to the experiment and result sections by show- ing comparisons between multiple algorithms on various datasets from different languages and do- mains. This emphasis on empirical results high- lights the role of statistical significance testing in NLP research: if we rely on empirical evalua- tion to validate our hypotheses and reveal the cor- rect language processing mechanisms, we better be sure that our results are not coincidental.</p><p>This paper aims to discuss the various aspects of proper statistical significance testing in NLP and to provide a simple and sound guide to the way this important tool should be used. We also discuss the particular challenges of statistical significance in the context of language processing tasks.</p><p>To facilitate a clear and coherent presentation, our (somewhat simplified) model of an NLP paper is one that presents a new algorithm and makes the hypothesis that this algorithm is better than a pre- vious strong algorithm, which serves as the base- line. This hypothesis is verified in experiments where the two algorithms are applied to the same datasets (test sets), reasoning that if one algorithm is consistently better than the other, hopefully with a sufficiently large margin, then it should also be better on future, currently unknown, datasets. Yet, the experimental differences might be coinciden- tal. Here comes statistical significance testing into the picture: we have to make sure that the prob- ability of falsely concluding that one algorithm is better than the other is very small.</p><p>We note that in this paper we do not deal with the problem of drawing valid conclusions from multiple comparisons between algorithms across a large number of datasets , a.k.a. replicability anal- ysis (see <ref type="bibr" target="#b9">(Dror et al., 2017)</ref>). Instead, our focus is on a single comparison: how can we make sure that the difference between the two algorithms, as observed in an individual comparison, is not coin- cidental. Statistical significance testing of each in- dividual comparison is the basic building block of replicability analysis -its accurate performance is a pre-condition for any multiple dataset analysis.</p><p>Statistical significance testing ( § 2) is a well re- searched problem in the statistical literature. How- ever, the unique structured nature of natural lan- guage data is reflected in specialized evaluation measures such as BLEU (machine translation, <ref type="bibr" target="#b25">(Papineni et al., 2002)</ref>), ROUGE (extractive summa- rization, <ref type="bibr" target="#b19">(Lin, 2004)</ref>), UAS and LAS (dependency parsing, <ref type="bibr">(Kübler et al., 2009)</ref>). The distribution of these measures is of great importance to statistical significance testing. Moreover, certain properties of NLP datasets and the community's evaluation standards also affect the way significance testing should be performed. An NLP-specific discussion of significance testing is hence in need.</p><p>In § 3 we discuss the considerations to be made in order to select the proper statistical significance test in NLP setups. We propose a simple deci- sion tree algorithm for this purpose, and survey the prominent significance tests -parametric and non- parametric -for NLP tasks and data.</p><p>In § 4 we survey the current evaluation and sig- nificance testing practices of the community. We provide statistics collected from the long papers of the latest ACL proceedings ( <ref type="bibr" target="#b2">Barzilay and Kan, 2017)</ref> as well as from the papers published in the TACL journal during 2017. Our analysis reveals that there is still a room for improvement in the way statistical significance is used in papers pub- lished in our top-tier publication venues. Particu- larly, a large portion of the surveyed papers do not test the significance of their results, or use incor- rect tests for this purpose.</p><p>Finally, in § 5 we discuss open issues. A par- ticularly challenging problem is that while most significance tests assume the test set consists of independent observations, most NLP datasets con- sist of dependent data points. For example, many NLP standard evaluation sets consist of sentences coming from the same source (e.g. newspaper) or document (e.g. newspaper article) or written by the same author. Unfortunately, the nature of these dependencies is hard to characterize, let alone to quantify. Another important problem is how to test significance when cross-validation, a popular eval- uation methodology in NLP papers, is performed.</p><p>Besides its practical value, we hope this paper will encourage further research into the role of statistical significance testing in NLP and on the questions that still remain open.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section we provide the required preliminar- ies for our discussion. We start with a formal def- inition of statistical significance testing and pro- ceed with an overview of the prominent evaluation measures in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Statistical Significance Testing</head><p>In this paper we focus on the setup where the per- formance of two algorithms, A and B, on a dataset X, is compared using an evaluation measure M.</p><p>Let us denote M(ALG, X) as the value of the evaluation measure M when algorithm ALG is applied to the dataset X. Without loss of gener- ality, we assume that higher values of the measure are better. We define the difference in performance between the two algorithms according to the mea- sure M on the dataset X as:</p><formula xml:id="formula_0">δ(X) = M(A, X) − M(B, X).<label>(1)</label></formula><p>In this paper we will refer to δ(X) as our test statistic. Using this notation we formulate the fol- lowing statistical hypothesis testing problem: 2</p><formula xml:id="formula_1">H 0 :δ(X) ≤ 0 H 1 :δ(X) &gt; 0.</formula><p>In order to decide whether or not to reject the null hypothesis, that is reaching the conclusion that δ(X) is indeed greater than 0, we usually compute a p−value for the test. The p−value is defined as the probability, under the null hypoth- esis H 0 , of obtaining a result equal to or more extreme than what was actually observed. For the one-sided hypothesis testing defined here, the p−value is defined as:</p><formula xml:id="formula_2">P r(δ(X) ≥ δ observed |H 0 ).</formula><p>Where δ observed is the performance difference be- tween the algorithms (according to M) when ap- plied to X. The smaller the p-value, the higher the significance, or, in other words, the stronger the indication provided by the data that the null- hypothesis, H 0 , does not hold. In order to de- cide whether H 0 should be rejected, the researcher should pre-define an arbitrary, fixed threshold value α, a.k.a the significance level. Only if p−value &lt; α then the null hypothesis is rejected.</p><p>In significance (or hypothesis) testing we con- sider two error types. Type I error refers to the case where the null hypothesis is rejected when it is actually true. Type II error refers to the case where the null hypothesis is not rejected although it should be. A common approach in hypothe- sis testing is to choose a test that guarantees that the probability of making a type I error is up- per bounded by the test significance level α, men- tioned above, while achieving the highest possible power: i.e. the lowest possible probability of mak- ing a type II error. In order to draw valid conclusions from the ex- periments formulated in § 2.1 it is crucial to ap- ply the correct statistical significance test. In § 3 we explain that the choice of the significance test is based, among other considerations, on the dis- tribution of the test statistics, δ(X). From equa- tion 1 it is clear that δ(X) depends on the evalu- ation measure M. We hence turn to discuss the evaluation measures employed in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation</head><p>In § 4 we analyze the (long) ACL and TACL 2017 papers, and observe that the most commonly used evaluation measures are the 12 measures that appear in <ref type="table">Table 1</ref>. Notice that seven of these measures: Accuracy, Precision, Recall, F-score, Pearson and Spearman correlations and Perplexity, are not specific to NLP. The other five measures: BLEU ( <ref type="bibr" target="#b25">Papineni et al., 2002</ref>), ROUGE <ref type="bibr" target="#b19">(Lin, 2004</ref>), METEOR (Banerjee and <ref type="bibr" target="#b1">Lavie, 2005</ref>), UAS and LAS <ref type="bibr">(Kübler et al., 2009)</ref>, are unique measures that were developed for NLP applica- tions. BLEU and METEOR are standard evalu- ation measures for machine translation, ROUGE for extractive summarization, and UAS and LAS for dependency parsing. While UAS and LAS are in fact accuracy measures, BLEU, ROUGE and METEOR are designed for tasks where there are several possible outputs -a characteristic property of several NLP tasks. In machine translation, for example, a sentence in one language can be trans- lated in multiple ways to another language. Conse- quently, BLEU takes an n-gram based approach on the surface forms, while METEOR considers only unigram matches but uses stemming and controls for synonyms. All 12 measures return a real number, either in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> or in R. Notice though that accuracy may reflect an average over a set of categorical scores (observations), e.g., in document-level binary sen- timent analysis where every document is tagged as either positive or negative. In other cases, the in- dividual observations are also continuous. For ex- ample, when comparing two dependency parsers, we may want to understand how likely it is, given our results, that one parser will do better than the other on a new sentence. In such a case we will consider the sentence-level UAS or LAS differ- ences between the two parsers on all the sentences in the test set. Such sentence level UAS or LAS scores -the individual observations to be consid- ered in the significance test -are real-valued.</p><p>With the basic concepts clarified, we are ready to discuss the considerations to be made when choosing a statistical significance test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Statistical Significance in NLP</head><p>The goal of this section is to detail the considera- tions involved in the selection of a statistical sig- nificance test for an NLP application. Based on these considerations we provide a practical recipe that can be applied in order to make a good choice. In order to make this paper a practical guide for the community, we also provide a short descrip- tion of the significance tests that are most relevant for NLP setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Parametric vs. Non-parametric Tests</head><p>As noted above, a major consideration in the se- lection of a statistical significance test is the dis- tribution of the test statistic, δ(X), under the null hypothesis. If the distribution is known, then the suitable test will come from the family of para- metric tests, that uses this distribution in order to achieve powerful results (i.e., low probability of making a type II error, see § 2). If the distribu- tion is unknown then any assumption made by a test may lead to erroneous conclusions and hence we should rely on non-parametric tests that do not make any such assumption. While non-parametric tests may be less powerful than their paramet- ric counterparts, they do not make unjustified as- sumptions and are hence statistically sound even when the test statistic distribution is unknown.</p><p>But how can one know the test statistic dis- tribution? One possibility is to apply tests de- signed to evaluate the distribution of a sample of observations. For example, the Shapiro-Wilk test <ref type="bibr" target="#b29">(Shapiro and Wilk, 1965</ref>) tests the null hypothesis that a sample comes from a normally distributed population, the Kolmogorov-Smirnov test quanti- fies the distance between the empirical distribu- tion function of the sample and the cumulative distribution function of the reference distribution, and the Anderson-Darling test <ref type="bibr" target="#b0">(Anderson and Darling, 1954</ref>) tests whether a given sample of data is drawn from a given probability distribution. As discussed below, there seems to be other heuris- tics that are used in practice but are not often men- tioned in research papers.</p><p>In what follows we discuss the prominent para- metric and non-parametric tests for NLP setups. Based on this discussion we end this section with a simple decision tree that aims to properly guide the significance test choice process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prominent Significance Tests</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Parametric Tests</head><p>Parametric significance tests assume that the test statistic is distributed according to a known dis- tribution with defined parameters, typically the normal distribution. While this assumption may be hard to verify (see discussion above), when it holds, these parametric tests have stronger statis- tical power compared to non-parametric tests that do not make this assumption <ref type="bibr" target="#b11">(Fisher, 1937)</ref>.</p><p>Here we discuss the prominent parametric test for NLP setups -the paired student's t-test. Paired Student's t-test This test assesses whether the population means of two sets of mea- surements differ from each other, and is based on the assumption that both samples come from a nor- mal distribution <ref type="bibr" target="#b11">(Fisher, 1937)</ref>.</p><p>In practice, t-test is often applied with evalua- tion measures such as accuracy, UAS and LAS, that compute the mean number of correct predic- tions per input example. When comparing two de- pendency parsers, for example, we can apply the test to check if the averaged difference of their UAS scores is significantly larger than zero, which can serve as an indication that one parser is better than the other.</p><p>Although we have not seen this discussed in NLP papers, we believe that the decision to use the t-test with these measures is based on the Cen- tral Limit Theorem (CLT). CLT establishes that, in most situations, when independent random vari- ables are added, their properly normalized sum tends toward a normal distribution even if the orig- inal variables themselves are not normally dis- tributed. That is, accuracy measures in structured tasks tend to be normally distributed when the number individual predictions (e.g. number of words in a sentence when considering sentence- level UAS) is large enough.</p><p>One case where it is theoretically justified to employ the t-test is described in <ref type="bibr" target="#b28">(Sethuraman, 1963)</ref>. The authors prove that for large enough data, the sampling distribution of a certain func- tion of the Pearson's correlation coefficient fol- lows the Student's t-distribution with n − 2 de- grees of freedom. With the recent surge in word similarity research with word embedding models, this result is of importance to our community.</p><p>For other evaluation measures, such as F-score, BLEU, METEOR and ROUGE that do not com- pute means, the common practice is to assume that they are not normally distributed <ref type="bibr" target="#b35">(Yeh, 2000;</ref><ref type="bibr" target="#b3">Berg-Kirkpatrick et al., 2012</ref>). We believe this issue requires a further investigation and suggest that it may be best to rely on the normality tests discussed in § 3.1 when deciding whether or not to employ the t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Non-parametric Tests</head><p>When the test statistic distribution is unknown, non-parametric significance testing should be used. The non-parametric tests that are com- monly used in NLP setups can be divided into two families that differ with respect to their statistical power and computational complexity.</p><p>The first family consists of tests that do not con- sider the actual values of the evaluation measures. The second family do consider the values of the measures: it tests repeatedly sample from the test data, and estimates the p-value based on the test statistic values in the samples. We refer to the first family as the family of sampling-free tests and to the second as the family of sampling-based tests.</p><p>The two families of tests reflect different pref- erences with respect to the balance between statistical power and computational efficiency. Sampling-free tests do not consider the evaluation measure values, only higher level statistics of the results such as the number of cases in which each of the algorithms performs better than the other. Consequently, their statistical power is lower than that of sampling-based tests that do consider the evaluation measure values. Sampling-based tests, however, compensate for the lack of distributional assumptions over the data with re-sampling -a computationally intensive procedure. Sampling- based methods are hence not the optimal candi- dates for very large datasets.</p><p>We consider here four commonly used sampling-free tests: the sign test and two of its variants, and the wilcoxon signed-rank test.</p><p>Sign test This test tests whether matched pair samples are drawn from distributions with equal medians. The test statistic is the number of exam- ples for which algorithm A is better than algorithm B, and the null hypothesis states that given a new pair of measurements (e.g. evaluations (a i , b i ) of the two algorithms on a new test example), then a i and b i are equally likely to be larger than the other ( <ref type="bibr" target="#b12">Gibbons and Chakraborti, 2011</ref>).</p><p>The sign test has limited practical implications since it only checks if algorithm A is better than B and ignores the extent of the difference. Yet, it has been used in a variety of NLP papers (e.g. ( <ref type="bibr" target="#b8">Collins et al., 2005;</ref><ref type="bibr" target="#b6">Chan et al., 2007;</ref><ref type="bibr" target="#b27">Rush et al., 2012)</ref>). The assumptions of this test is that the data samples are i.i.d, the differences come from a continuous distribution (not necessarily normal) and that the values are ordered.</p><p>The next test is a special case of the sign test for binary classification (or a two-tailed sign test).</p><p>McNemar's test <ref type="bibr" target="#b21">(McNemar, 1947)</ref> This test is designed for paired nominal observations (binary labels). The test is applied to a 2 × 2 contingency table, which tabulates the outcomes of two algo- rithms on a sample of n examples. The null hy- pothesis for this test states that the marginal prob- ability for each outcome (label one or label two) is the same for both algorithms. That is, when ap- plying both algorithms on the same data we would expect them to be correct/incorrect on the same proportion of items. Under the null hypothesis, with a sufficiently large number of disagreements between the algorithms, the test statistic has a dis- tribution of χ 2 with one degree of freedom. This test is appropriate for binary classification tasks, and has been indeed used in such NLP works (e.g. sentiment classificaiton, <ref type="bibr" target="#b4">(Blitzer et al., 2006;</ref><ref type="bibr" target="#b36">Ziser and Reichart, 2017)</ref>). The Cochran's Q test <ref type="bibr" target="#b7">(Cochran, 1950)</ref> generalizes the McNemar's test for multi-class classification setups.</p><p>The sign test and its variants consider only pair- wise ranks: which algorithm performs better on each test example. In NLP setups, however, we also have access to the evaluation measure val- ues, and this allows us to rank the differences be- tween the algorithms. The Wilcoxon signed-rank test makes use of such a rank and hence, while it does not consider the evaluation measure values, it is more powerful than the sign test and its variants.</p><p>Wilcoxon signed-rank test <ref type="bibr" target="#b32">(Wilcoxon, 1945)</ref> Like the sign test variants, this test is used when comparing two matched samples (e.g. UAS values of two dependency parsers on a set of sentences). Its null hypothesis is that the differences follow a symmetric distribution around zero. First, the ab- solute values of the differences are ranked. Then, each rank gets a sign according to the sign of the difference. The Wilcoxon test statistic sums these signed ranks. The test is actually applicable for most NLP setups and it has been used widely (e.g. ( <ref type="bibr" target="#b31">Søgaard et al., 2014;</ref><ref type="bibr" target="#b30">Søgaard, 2013;</ref><ref type="bibr" target="#b34">Yang and Mitchell, 2017)</ref>) due to its improved power com- pared to the sign test variants.</p><p>As noted above, sampling-free tests trade sta- tistical power for efficiency. Sampling-based methods take the opposite approach. This family includes two main methods: permuta- tion/randomization tests <ref type="bibr" target="#b23">(Noreen, 1989)</ref> and the paired bootstrap <ref type="bibr" target="#b10">(Efron and Tibshirani, 1994)</ref>.</p><p>Pitman's permutation test This test estimates the test statistic distribution under the null hypoth- esis by calculating the values of this statistic un- der all possible labellings (permutations) of the test set. The (two-sided) p-value of the test is calculated as the proportion of these permutations where the absolute difference was greater than or equal to the absolute value of the difference in the output of the algorithm.</p><p>Obviously, permutation tests are computation- ally intensive due to the exponentially large num- ber of possible permutations. In practice, ap- proximate randomization tests are used where a pre-defined limited number of permutations are drawn from the space of all possible permuta- tions, without replacements (see, e.g. ( <ref type="bibr" target="#b26">Riezler and Maxwell, 2005</ref>) in the context of machine trans- lation). The bootstrap test <ref type="bibr" target="#b10">(Efron and Tibshirani, 1994</ref>) is based on a closely related idea.</p><p>Paired bootstrap test This test is very similar to approximate randomization of the permutation test, with the difference that the sampling is done with replacements (i.e., an example from the orig- inal test data can appear more than once in a sam- ple). The idea of bootstrap is to use the samples as surrogate populations, for the purpose of approx- imating the sampling distribution of the statistic. The p-value is calculated in a similar manner to the permutation test.</p><p>Bootstrap was used with a variety of NLP tasks, including machine translation, text summarization and semantic parsing (e.g. <ref type="bibr" target="#b13">(Koehn, 2004;</ref><ref type="bibr" target="#b33">Wu et al., 2017;</ref><ref type="bibr" target="#b24">Ouchi et al., 2017)</ref>). The test is less effective for small test sets, as it as- sumes that the test set distribution does not deviate too much from the population distribution.</p><p>Clearly, Sampling-based methods are computa- tionally intensive and can be intractable for large datasets, even with modern computing power. In such cases, sampling-free methods form an avail- able alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Significance Test Selection</head><p>With the discussion of significance test families -parametric vs. non-parametric ( § 3.1), and the properties of the actual significance tests ( § 3.2) we are now ready to provide a simple recipe for significance test selection in NLP setups. The de- cision tree in <ref type="figure" target="#fig_0">Figure 1</ref> provides an illustration. If the distribution of the test statistic is known, then parametric tests are most appropri- ate. These tests are more statistically powerful and less computationally intensive compared to their non-parametric counterparts. The stronger statistical power of parametric tests stems from the stronger, parametric assumptions they make, while the higher computational demand of some non-parametric tests is the result of their sampling process.</p><note type="other">Does the test statistic come from a known distribution? Use a para- metric test Is the data size small ? Use bootstrap or random- ization test Use sampling- free non- parametric test</note><p>When the distribution of the test statistic is un- known, the first non-parametric family of choice is that of sampling-based tests. These tests con- sider the actual values of the evaluation measures and are not restricted to higher order properties (e.g. ranks) of the observed values -their statis- tical power is hence higher. As noted in ( <ref type="bibr" target="#b26">Riezler and Maxwell, 2005)</ref>, in the case where the distri- butional assumptions of the parametric tests are vi- olated, sampling-based tests have more statistical power than parametric tests.</p><p>Nonetheless, sampling-based tests are compu- tationally intensive -the exact permutation test, for example, requires the generation of all 2 n data permutations (where n is the number of points in the dataset). To overcome this, approximate ran- domization can be used, as was done, e.g., by <ref type="bibr" target="#b35">Yeh (2000)</ref> for test sets of more than 20 points. The other alternative for very large datasets are sampling-free tests that are less powerful but are computationally feasible.</p><p>In what follows we check whether recent ACL and TACL papers follow these guidelines.  We analyzed the long papers from the pro- ceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics (ACL17, ( <ref type="bibr" target="#b2">Barzilay and Kan, 2017)</ref>), a total of 196 pa- pers, and the papers from the Transactions of the Association of Computational Linguistics journal (TACL17), Volume 5, Issue 1, a total of 37 pa- pers. We have focused on empirical papers where at least one comparison between methods was per- formed.  <ref type="table" target="#tab_1">Statistical Test  ACL '17 TACL '17  Bootstrap  6  1  t-test  17  2  Wilcoxon  3  0  Chi square  3  1  Randomization  3  1  McNemar  2  3  Sign  2  3  Permutation  1  4</ref>  with an average of 2.34 (ACL) and 2.1 (TACL). <ref type="table">Table 1</ref> presents the most common of these evalu- ation measures. The lower part of <ref type="table" target="#tab_2">Table 2</ref> depicts the disturb- ing reality of statistical significance testing in our research community. Out of the 180 experimen- tal long papers of ACL 2017, only 63 papers in- cluded a statistical significance test. Moreover, out of these 63 papers 21 did not mention the name of the significance test they employed. Of the 42 papers that did mention the name of the signifi- cance test, 6 used the wrong test according to the considerations discussed in § 3. <ref type="bibr">3</ref> In TACL, where the review process is presumably more strict and of higher quality, out of 33 experimental papers,of 110 papers that used multiple datasets only 3 corrected for multiplicity (all using the Bonferroni correction). In TACL, the situation is slightly bet- ter with 4 papers correcting for multiplicity out of 19 that should have done that.</p><p>Regarding the statistical tests that were used in the papers that did report significance <ref type="table" target="#tab_4">(Table 3)</ref>, in ACL 2017 most of the papers used the Student's t-test that assumes the data is i.i.d and that the test statistics are normally distributed. As discussed in § 3 this is not the case in many NLP applications. Gladly, in TACL, t-test is not as prominent.</p><p>One final note is about the misuse of the word significant. We noticed that in a considerable number of papers this word was used as a syn- onym for words such as important, considerable, meaningful, substantial, major, notable etc. We believe that we should be more careful when us- ing this word, ideally keeping its statistical sense and using other, more general words to indicate a substantial impact.</p><p>We close this discussion with two important open issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Open Questions</head><p>In this section we would like to point on two issues that remain open even after our investigation. We hope that bringing these issues to the attention of the research community will encourage our fellow researchers to come up with appropriate solutions.</p><p>The first open issue is that of dependent obser- vations. An assumption shared by the statistical significance tests described in § 3, that are com- monly used in NLP setups, is that the data samples are independent and identically distributed. This assumption, however, is rarely true in NLP setups.</p><p>For example, the popular WSJ Penn Treebank corpus <ref type="bibr" target="#b20">(Marcus et al., 1993)</ref> consists of 2,499 ar- ticles from a three year Wall Street Journal (WSJ) collection of 98,732 stories. Obviously, some of the sentences included in the corpus come from the same article, were written by the same author or were reviewed before publication by the same editor. As another example, many sentences in the Europarl parallel corpus <ref type="bibr" target="#b14">(Koehn, 2005</ref>) that is very popular in the machine translation literature are taken from the same parliament discussion. An in- dependence assumption between the sentences in these corpora is not likely to hold.</p><p>This dependence between test examples vio- lates the conditions under which the theoretical guarantees of the various tests were developed. The impact of this phenomenon on our results is hard to quantify, partly because it is hard to quantify the nature of the dependence between test set examples in NLP datasets. Some papers are even talking about abandoning the null hy- pothesis statistical significance test approach due to this hard-to-meet assumption <ref type="bibr" target="#b15">(Koplenig, 2017;</ref><ref type="bibr" target="#b5">Carver, 1978;</ref><ref type="bibr" target="#b17">Leek et al., 2017)</ref>. In our opinion, this calls for a future col- laboration with statisticians in order to better un- derstand the extent to which existing popular sig- nificance tests are relevant for NLP, and to develop alternative tests if necessary.</p><p>Another issue that deserves some thought is that of cross-validation. To increase the validity of re- ported results, it is customary in NLP papers to create a number of random splits of the experimen- tal corpus into train, development and test portions (see <ref type="table" target="#tab_2">Table 2</ref>). For each such split (fold), the tested algorithms are trained and tuned on the training and development datasets, respectively, and their results on the test data are recorded. The final re- ported result is typically the average of the test set results across the splits. Some papers also report the fraction of the folds for which one algorithm was better than the others. While cross-validation is surely a desired practice, it is challenging to re- port statistical significance when it is employed. Particularly, the test sets of the different folds are obviously not independent -their content is even likely to overlap.</p><p>One solution we would like to propose here is based on replicability analysis ( <ref type="bibr" target="#b9">Dror et al., 2017)</ref>. This paper proposes a statistical sig- nificance framework for multiple comparisons performed with dependent test sets, using the K Bonf erroni estimator for the number of datasets with significant effect. One statistically sound way to test for significance when a cross-validation protocol is employed is hence to calculate the p- value for each fold separately, and then to per- form replicability analysis for dependent datasets with K Bonf erroni . Only if this analysis rejects the null hypothesis in all folds (or in more than a predefined threshold number of folds), the results should be declared significant. Here again, further statistical investigation may lead to additional, po- tentially better, solutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Decision tree for statistical significance test selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>4 Survey of ACL and TACL papers</head><label>4</label><figDesc></figDesc><table>General Statistics 
ACL '17 TACL '17 
Total number of pa-
pers 

196 
37 

# relevant (experimen-
tal) papers 

180 
33 

# different tasks 
36 
15 
# different evaluation 
measures 

24 
19 

Average number of 
measures per paper 

2.34 
2.1 

# papers that do not 
report significance 

117 
15 

# papers that report 
significance 

63 
18 

# papers that report 
significance but use 
the wrong statistical 
test 

6 
0 

# papers that report 
significance but do not 
mention the test name 

21 
3 

# papers that have to 
report replicability 

110 
19 

# papers that report 
replicability 

3 
4 

# papers that perform 
cross validation 

23 
5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistical significance statistics for em-
pirical ACL and TACL 2017 papers. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 presents</head><label>2</label><figDesc>the main results from our sur- vey. The top part of the table presents general statistics of our dataset. In both conference and journal papers, the variety of different NLP tasks is quite large: 36 tasks in ACL 2017 and 15 tasks in TACL. Interestingly, in almost every paper in our survey the researchers chose to analyze their results using more than one evaluation measure,</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Number of times each of the prominent 
statistical significance tests in ACL and TACL 
2017 papers was used. 42 ACL and 15 TACL pa-
pers reported the significance test name. 5 ACL 
papers mentioned an unrecognized test name. 

</table></figure>

			<note place="foot" n="1"> The code for all statistical tests detailed in this paper is found on: https://github.com/rtmdrr/ testSignificanceNLP.git</note>

			<note place="foot" n="2"> For simplicity we consider a one-sided hypothesis, it can be easily re-formulated as a double-sided hypothesis.</note>

			<note place="foot" n="15"> did not include statistical significance testing, and all the papers that report significance and mentioned the name of the test used a valid test. While this paper focuses on the correct choice of a significance test, we also checked whether the papers in our sample account for the effect of multiple hypothesis testing when testing statistical significance (see (Dror et al., 2017)). When testing multiple hypotheses, as in the case of comparing the participating algorithms across a large number of datasets, the probability of making one or more false claims may be very high, even if the probability of drawing an erroneous conclusion in each individual comparison is small. In ACL 2017, out 3 We considered the significance test to be inappropriate in three cases: 1. Using the t-test when the evaluation measure is not an average measure; 2. Using the t-test for a classification task (i.e. when the observations are categorical rather then continuous), even if the evaluation measure is an average measure; and 3. Using a Boostrap test with a small test set size.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We discussed the use of significance testing in NLP. We provided the main considerations for sig-nificance test selection, and proposed a simple test selection protocol. We then surveyed the state of significance testing in recent top venue papers and concluded with open issues. We hope this paper will serve as a guide for NLP researchers and, not less importantly, that it will encourage discus-sions and collaborations that will contribute to the soundness and correctness of our research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A test of goodness of fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Theodore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">268</biblScope>
			<biblScope unit="page" from="765" to="769" />
			<date type="published" when="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long papers)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th annual meeting of the association for computational linguistics</title>
		<meeting>the 55th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical investigation of statistical significance in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLPCoNLL</title>
		<meeting>EMNLPCoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The case against statistical significance testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Carver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Educational Review</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="378" to="399" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word sense disambiguation improves statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The comparison of percentages in matched samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William G Cochran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="256" to="266" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Clause restructuring for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivona</forename><surname>Kucerova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Replicability analysis for natural language processing: Testing significance with multiple datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rotem</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gili</forename><surname>Baumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Bogomolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="471" to="486" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The design of experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald Aylmer</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1937" />
		</imprint>
		<respStmt>
			<orgName>Oliver And Boyd; Edinburgh; London</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonparametric statistical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Dickinson Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Chakraborti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International encyclopedia of statistical science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="977" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MT summit</title>
		<meeting>the MT summit</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Against statistical significance testing in corpus linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koplenig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Corpus Linguistics and Linguistic Theory</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Five ways to fix statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Blakeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mcshane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Colquhoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven N</forename><surname>Michèle B Nuijten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">551</biblScope>
			<biblScope unit="issue">7682</biblScope>
			<biblScope unit="page" from="557" to="559" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling source syntax for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Note on the sampling error of the difference between correlated proportions or percentages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quinn</forename><surname>Mcnemar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="157" />
			<date type="published" when="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blakeley B Mcshane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tackett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.07588</idno>
		<title level="m">Abandon statistical significance</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Computer intensive methods for hypothesis testing: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eric W Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural modeling of multi-predicate interactions for japanese predicate argument structure analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On some pitfalls in automatic evaluation and significance testing for mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John T</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved parsing and pos tagging using inter-sentence consistency constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sethuraman</surname></persName>
		</author>
		<title level="m">The Advanced Theory of Statistics</title>
		<imprint>
			<publisher>JSTOR</publisher>
			<date type="published" when="1963" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Inference and Relationship</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An analysis of variance test for normality (complete samples)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanford</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin B</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="591" to="611" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimating effect size across datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">What&apos;s in a p-value in nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Héctor Martínez</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Individual comparisons by ranking methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics bulletin</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence-to-dependency neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Leveraging knowledge bases in lstms for improving machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">More accurate tests for the statistical significance of result differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural structural correspondence learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yftah</forename><surname>Ziser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
