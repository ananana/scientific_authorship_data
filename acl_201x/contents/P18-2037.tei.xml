<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Filtering and Mining Parallel Data in a Joint Multilingual Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
							<email>schwenk@dfb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Filtering and Mining Parallel Data in a Joint Multilingual Space</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="228" to="234"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>228</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We learn a joint multilingual sentence embedding and use the distance between sentences in different languages to filter noisy parallel data and to mine for parallel data in large news collections. We are able to improve a competitive baseline on the WMT&apos;14 English to German task by 0.3 BLEU by filtering out 25% of the training data. The same approach is used to mine additional bitexts for the WMT&apos;14 system and to obtain competitive results on the BUCC shared task to identify parallel sentences in comparable corpora. The approach is generic, it can be applied to many language pairs and it is independent of the architecture of the machine translation system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Parallel data, also called bitexts, is an important resource to train neural machine translation sys- tems (NMT). It is usually assumed that the qual- ity of the automatic translations increases with the amount of available training data. However, it was observed that NMT systems are more sensitive to noise than SMT systems, e.g. ( <ref type="bibr" target="#b3">Belinkov and Bisk, 2017)</ref>. Well known sources of parallel data are international organizations like the European Par- liament or the United Nations, or community pro- vided translations like the TED talks. In addi- tion, there are many texts on the Internet which are potential mutual translations, but which need to be identified and aligned. Typical examples are Wikipedia or news collections which report on the same facts in different languages. These collec- tions are usually called comparable corpora.</p><p>In this paper we propose an unified approach to filter noisy bitexts and to mine bitexts in huge monolingual texts. The main idea is to first learn a joint multilingual sentence embedding. Then, a threshold on the distance between two sentences in this joint embedding space can be used to fil- ter bitexts (distance between source and target sen- tences), or to mine for additional bitexts (pairwise distances between all source and target sentences). No additional features or classifiers are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The problem of how to select parts of bitexts has been addressed before, but mainly from the aspect of domain adaptation <ref type="bibr" target="#b1">(Axelrod et al., 2011;</ref><ref type="bibr" target="#b22">Santamaría and Axelrod, 2017)</ref>. It was successfully used in many phrase-based MT systems, but it was reported to be less successful for NMT (van der <ref type="bibr" target="#b27">Wees et al., 2017)</ref>. It should be stressed that do- main adaptation is different from filtering noisy training data. Data selection extracts the most rel- evant bitexts for the test set domain, but does not necessarily remove wrong translations, e.g. source and target sentences are both in-domain and well formed, but they are not mutual translations.</p><p>There is a huge body of research on mining bi- texts, e.g. by analyzing the name of WEB pages or links <ref type="bibr" target="#b21">(Resnik and Smith, 2003)</ref>. Another di- rection of research is to use cross-lingual informa- tion retrieval, e.g. ( <ref type="bibr" target="#b26">Utiyama and Isahara, 2003;</ref><ref type="bibr" target="#b18">Munteanu and Marcu, 2005;</ref><ref type="bibr" target="#b20">Rauf and Schwenk, 2009</ref>). There are some works which use joint em- beddings in the process of filtering or mining bi- texts. For instance, <ref type="bibr" target="#b10">Grégoire and Langlais (2017)</ref> first embed sentences into two separate spaces. Then, a classifier is learned on labeled data to de- cide whether sentences are parallel or not. Our ap- proach clearly outperforms this technique on the BUCC corpus (cf. section 4). <ref type="bibr" target="#b4">Bouamor and Sajjad (2018)</ref> use averaged multilingual word embed- dings to calculate a joint embedding of all sen-tences. However, distances between all sentences are only used to extract a set of potential mutual translation. The decision is based on a different system. In  NMT systems for Zh ↔ En are learned using a joint encoder. A sen- tence representation is obtained as the mean of the last encoder states. Noisy bitexts are filtered based on the distance. In all these works, embeddings are learned for two languages only, while we learn one joint embedding for up to nine languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multilingual sentence embeddings</head><p>We are aiming at an embedding of entire sen- tences in different languages into one joint space, with the goal that the distance in that space re- flects their semantic difference, independently of the language. There are several works on learning multilingual sentence embeddings which could be used for that purpose, i.e. ( <ref type="bibr">Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b29">Zhou et al., 2016;</ref><ref type="bibr" target="#b5">Chandar et al., 2013;</ref><ref type="bibr" target="#b17">Mogadala and Rettinger, 2016)</ref>.</p><p>In this paper, we extend our initial approach ( <ref type="bibr" target="#b23">Schwenk and Douze, 2017)</ref>. The underlying idea is to use multiple sequence encoders and decoders and to train them with N -way aligned corpora from the MT community. Instead of using one en- coder for each language as in the original paper, we use a shared encoder which handles all the in- put languages. Joint encoders (and decoders) have already been used in NMT <ref type="bibr" target="#b14">(Johnson et al., 2016)</ref>. In contrast to that work, we do not use a special in- put token to indicate the target language. Our joint encoder has no information at all on the encoded language, or what will be done with sentence rep- resentation.</p><p>We trained this architecture on nine languages 1 of the Europarl corpus with about 2M sentences each. We use BPE ( <ref type="bibr" target="#b25">Sennrich et al., 2016b</ref>) to learn one 20k joint vocabulary for all the nine lan- guages. <ref type="bibr">2</ref> The joint encoder is a 3-layer BLSTM. The word embeddings are of size 384 and the hidden layer of the BLSTM is 512-dimensional. The 1024 dimensional sentence embedding is ob- tained by max-pooling over the BLSTM outputs. Dropout is set to 0.1. These settings are identical to those reported in ( <ref type="bibr" target="#b23">Schwenk and Douze, 2017)</ref>, with the difference that we observe slight improve- ment by using a deeper network for the joint en- coder. Once the system is learned, all the BLSTM decoders are discarded and we only use the mul- tilingual BLSTM encoder to embed the sentences into the joint space.</p><p>A very similar approach was also proposed in <ref type="bibr" target="#b7">España-Bonet et al. (2017)</ref>. A joint NMT sys- tem with attention is trained on several languages pairs, similar to ( <ref type="bibr" target="#b14">Johnson et al., 2016)</ref>, including a special token to indicate the target language. Af- ter training, the sum of the encoder output states is used to obtain a fixed size sentence representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental evaluation: BUCC shared task on mining bitexts</head><p>Since 2017, the workshop on Building and Us- ing Comparable Corpora (BUCC) is organizing a shared task to evaluate the performance of ap- proaches to mine for parallel sentences in compa- rable corpora ( <ref type="bibr" target="#b30">Zweigenbaum et al., 2018)</ref>. <ref type="table">Table 1</ref> summarizes the available data, and <ref type="table" target="#tab_1">Table 2</ref> the official results. Roughly a 40th of the sentences are aligned. The best performing system "VIC" is based on the so-called STACC method which was shown to achieve state-of-the-art performance <ref type="bibr" target="#b8">(Etchegoyhen and Azpeitia, 2016)</ref>. It combines probabilistic dictionaries, search for similar sen- tences in both directions and a decision module which explores various features (common word prefixes, numbers, capitalized true-case tokens, etc). This STACC system was improved and adapted to the BUCC tasks with a word weight- ing scheme which is optimized on the monolin- gual corpora, and a named entity penalty. This task adaption substantially improved the generic STACC approach ( <ref type="bibr" target="#b2">Azpeitia et al., 2018</ref>). The sys- tems RALI (Grégoire and Langlais, 2017) and H2 (Bouamor and Sajjad, 2018) have been already described in section 2. NLP2CT uses a denois- ing auto-encoder and a maximum-entropy classi- fier ( <ref type="bibr" target="#b15">Leong et al., 2018</ref>). We applied our approach to all language pairs of the BUCC shared task (see <ref type="table">Table 3</ref> System en-fr en-de en-ru en-zh <ref type="table" target="#tab_1">VIC'17  79  84  - - RALI'17  20  - - - LIMSI'17  - - - 43  VIC'18  81  86  81  77  H2'18  76  - - - NLP2CT'</ref>18 - - - 56 embeddings from (Schwenk and Douze, 2017) for ru and zh, which were trained on the UN corpus.</p><p>The only task-specific adaptation is the optimiza- tion of the threshold on the distance in the multi- lingual joint space. Our system does not match the performance of the heavily tuned VIC system, but it is on-pair with H2 on en-fr, and outperforms all other approaches by a large margin. We would like to emphasize that our approach uses no additional features or classifiers, and that we apply the same approach to all language pairs. It is nice to see that the performance varies little for the languages. España-Bonet et al. <ref type="formula">(2017)</ref> have also evaluated their technique on the BUCC data, but results on the official test set are not provided. Also, their joint encoder uses the "news-commentary" corpus during training. This is likely to add an important bias since all the parallel sentences in the BUCC corpus are from the news-commentary corpus.</p><p>Since we learn multilingual embeddings for many languages in one joint space, we can mine for parallel data for any language pair. As an example, we have mined for French/German and Chinese/Russian bitexts, respectively. There are no reference alignments to optimize the threshold for this language pair. Based on the experiments with the other languages, we chose a value of 0.55.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>en-fr en-de en-ru en-zh P 81  <ref type="table">Table 3</ref>: Results on the BUCC test set of our ap- proach: Precision, Recall and F-measure (%). We also provide the optimal threshold.</p><p>In the annex, we provide examples of extracted parallel sentences for various values of the mul- tilingual distance. These examples show that our approach may wrongly align sentences which are mainly an enumeration of named entities, numer- ical values, etc. Many of these erroneous align- ments could be possibly excluded by some post- processing, e.g. comparing the number of named entities in each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental evaluation: improving WMT'14 En-De NMT systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baseline NMT systems</head><p>We have performed all our experiments with the freely available Sequence-to-Sequence Py- Torch toolkit from Facebook AI Research, 3 called fairseq-py. It implements a convolutional model which achieves very competitive results <ref type="bibr" target="#b9">(Gehring et al., 2017</ref>). We use this system to show the improvements obtained by filtering the stan- dard training data and by integrating additional mined data. We will freely share this data so that it can be used to train different NMT architectures. In this work, we focus on translating from En- glish into German using the WMT'14 data. This task was selected for two reasons:</p><p>• it is the de-facto standard to evaluate NMT systems and many comparable results are available, e.g. • only a limited amount of parallel training data is available (4.5M sentences). 2.1M are high quality human translations and 2.4M are crawled and aligned sentences (Common Crawl corpus).</p><p>As in other works, we use newstest-2014 as test set.</p><p>However, in order to follow the standard WMT evaluation setting, we use mteval-v14.pl on untokenized hypothesis to calculate case-sensitive BLEU scores.</p><p>Note that in some papers, BLEU is calculated with multi-bleu.perl on tokenized hypothesis. All our results are for one single system only.</p><p>We trained the fairseq-py system with de- fault parameters, but a slightly different pre-and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Human only All WMT'14 (Eparl+NC) (Eparl+NC+CC) #sents 2.1M 4.5M BLEU 21.87 24.75 <ref type="table" target="#tab_3">Table 4</ref>: Our baseline results on WMT'14 en-de.</p><p>post-processing scheme. In particular, we lower- case all data and use a 40k BPE vocabulary <ref type="bibr" target="#b25">(Sennrich et al., 2016b</ref>). Before scoring, the case of the hypothesis is restored using a recaser trained on the WMT German news data.  <ref type="bibr">4</ref> Please remember that the goal of this paper is not to set a new state-of-the-art in NMT on this data set, but to show relative im- provement with respect to a competitive baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Filtering Common Crawl</head><p>The Common Crawl corpus is provided by the or- ganizers of WMT'14. We do not know how this corpus was produced, but like all crawled corpora, it is inherently noisy. To filter that corpus, we first embed all the sentences into the joint space and calculate the cosine distance between the English source and the provided German translation. We then extract subsets of different sizes as a function of the threshold on this distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All</head><p>Commas &lt;50 words LID 2399k 2144k 2071k 1935k <ref type="table">Table 5</ref>: Pre-processing of the Common Crawl corpus before distance-based filtering.</p><p>After some initial experiments, it turned out that some additional steps are needed before cal- culating the distances (see <ref type="table">Table 5</ref>): 1) remove sentences with more than 3 commas. Those are indeed often enumerations of names, cities, etc. While such sentences maybe useful to train NMT systems, the multilingual distance is not very reli- able to distinguish list of named entities; 2) limit to sentences with less than 50 words; 3) perform LID on source and target sentences; These steps <ref type="bibr">4</ref> This version uses a subset of 2737 out of 3003 sentences. discarded overall 19% of the data. It is surpris- ing that almost 6% of the data seems to have the wrong source or target language. 5 <ref type="figure" target="#fig_1">Figure 1</ref> (pink curve) shows the amount of data as a function of the threshold on the multilingual distance. Some human inspection of the filtered corpus indicated that the translations start to be wrong for a threshold larger than 1.0. Therefore, we build NMT systems using a filtered version of Common Crawl for thresholds in the range of 0.8 to 1.2 (see <ref type="figure" target="#fig_1">Figure 1</ref>, green curve). It is good to see that the BLEU score increases when less but better data is used and then decreases again since we discard too much data. Best performance of 25.06 BLEU is achieved for a threshold of 1.0. This corresponds to a gain of 0.3 BLEU on top of a very competitive baseline (24.75→25.06), us- ing only 3.4M instead of the original 4.5M sen- tence pairs. We actually discard almost half of the Common Crawl data. For comparison, we also trained an NMT system using the pre-processed Common Crawl corpus of 1.9M sentences (cf. Ta- ble 5), but without distance-based filtering. This gives a BLEU score of 24.82, a small 0.07 change.</p><p>Aiming at a compromise between speed and full convergence, we trained all systems for 55 epochs which takes less than two days on 8 NVidia GPU100s. Longer training may improve the over- all results slightly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mining Parallel Data in WMT News</head><p>In the framework of the WMT evaluation, large news corpora are provided: 144M English and 187M German sentences (after removing sentence with more than 50 words). As in section 4, we embed all sentences into the joint space. For each source sentence, we search for the k-nearest sen- tences in the target language. We use k = 20 since it can happen that for the same source sen- tence, several possible translations are found (dif- ferent news sites reporting on the same fact with different wordings). This search has a complex- ity of O(N × M ), while filtering presumed paral- lel corpora is O(N ). In our case, 144M × 185M amounts to 26 peta distance calculations. This can be quite efficiently done with the highly optimized FAISS toolkit <ref type="bibr" target="#b13">(Johnson et al., 2017)</ref>.</p><p>To start, we trained NMT systems on the ex- tracted data only (see <ref type="table">Table 6</ref>, 3rd column). As with the Common Crawl corpus, we discarded sentences pairs with the wrong language and many commas. By varying the threshold on the distance between two sentences in the embedding space, we can extract various amounts of data. However, the larger the threshold, the more unlikely the sen- tences are translations. Training on 1M mined sen- tences gives a modest BLEU score of 4.18, which increases up to 7.77 when 4.3M sentences are ex- tracted. This result is well below an NMT system trained on "real parallel data".</p><p>We have observed that the length distribution of the mined sentences is very different of the one of the WMT'14 training corpora (see <ref type="figure" target="#fig_2">Figure 2</ref>). The average sentence length for all the WMT training corpora is 24, while it is only 8 words for our mined texts. On one hand, it could be of course that our distance based mining approach works badly for long sentences. But on the other hand, the longer the sentences, the more unlikely it is to find perfect translation in crawled news data. If  <ref type="table">Table 6</ref>: BLEU scores when training on the mined data only, adding it (at different thresholds) to the human translated training corpus (Eparl+NC) and to our best system using filtered Common Crawl.  we shuffle the Europarl corpus and consider it as a comparable corpus, our approach is able to ex- tract more than 95% of the translation pairs. It is also an open question how short sentences impact the training of NMT systems. Further research in those directions is needed. When adding our mined data to the Europarl and News Commentary corpora (2.1M sentences), we are able to achieve an improvement of 0.45 BLEU (21.87→22.32, 4th column of <ref type="table">Table 6</ref>). However, we observe no improvement when adding the mined data to our best system which uses the filtered Common Crawl data (5th column of <ref type="table">Table 6</ref>). It could be that some of our mined data is actually a subset of Common Crawl.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that a simple cosine distance in a joint multilingual sentence embedding space can be used to filter noisy parallel data and to mine for bitexts in large news collections. We were able to improve a competitive baseline on the WMT'14 English to German task by 0.3 BLEU by filtering out 25% of the training data. We will make the filtered and extracted data freely available, as well as a tool to filter noisy bitexts in nine languages.</p><p>There are many directions to extend this re- search, in particular to scale-up to larger corpora. We will apply it to the data mined by the European ParaCrawl project. <ref type="bibr">6</ref> The proposed multilingual sentence distance could be also used in MT con- fidence estimation, or to filter back-translations of monolingual data <ref type="bibr" target="#b24">(Sennrich et al., 2016a</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>Sennrich et al., 2016b; Chunga et al., 2016; Wu et al., 2016; Gehring et al., 2017; Ashish Vaswani et al., 2017);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Filtering the Common Crawl corpus: size of corpus (pink) and BLEU scores (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Number of sentences as a function of their length, for WMT'14 training corpora and the mined news texts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Official test set results of the 2017 and 
2018 BUCC shared tasks (F-scores). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 gives</head><label>4</label><figDesc></figDesc><table>our baseline results using the provided data as it 
is. We distinguish results when training on human 
labeled data only, i.e. Europarl and News Com-
mentary (2.1M sentences), and with all WMT'14 
training data, i.e. human + Common Crawl (total 
of 4.5M sentences). Gehring et al. (2017) report a 
tokenized BLEU score of 25.16 on a slightly dif-
ferent version of newstest-2014 as defined in (Lu-
ong et al., 2015). </table></figure>

			<note place="foot" n="1"> en, fr, es, it, pt, de, da, nl and fi 2 Larger vocabularies achieve only slight improvements.</note>

			<note place="foot" n="3"> https://github.com/facebookresearch/ fairseq-py</note>

			<note place="foot" n="5"> LID itself may also commit errors, we used https://fasttext.cc/docs/en/ language-identification.html</note>

			<note place="foot" n="6"> http://paracrawl.eu/download.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation via pseudo in-domain data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extracting parallel sentences from comparable corpora with STACC variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andoni</forename><surname>Azpeitia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva Martínez</forename><surname>Thierry Etchegoyhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BUCC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Synthetic and artificial noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1711" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">H2@bucc18: Parallel sentence extraction from comparable coprora using multlingual sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houda</forename><surname>Bouamor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BUCC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Balaraman Ravindran, Vikas Raykar, and Amrita Saha</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS DL wshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Multilingual deep learning</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chunga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Choa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1603.06147" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical analysis of nmt-derived interlingual embeddings and their use in parallel sentence identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>España-Bonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adám Csaba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Sigmal Processing</title>
		<imprint>
			<biblScope unit="page" from="1340" to="1348" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Settheoretic alignment of comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Etchegoyhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andoni</forename><surname>Azpeitia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bucc 2017 shared task: a first attempt toward a deep learning framework for identifying parallel sentences in comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Grégoire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Langlais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BUCC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Achieving human parity on automatic chinese to english translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Aue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Chowdhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.05567" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1611.04558" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Um-paligner: Neural network-based parallel sentence identification model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongman</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BUCC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from parallel and nonparallel corpora for cross-language classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditua</forename><surname>Mogadala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achim</forename><surname>Rettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="692" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving machine translation performance by exploiting non-parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Dragos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="504" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning distributed representations for multilingual text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Vector Space Modeling for NLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the use of comparable corpora to improve SMT performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdul</forename><surname>Sadaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Rauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The web as a parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="349" to="380" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data selection with cluster-based language difference models and cynical selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucía</forename><surname>Santamaría</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="137" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning joint multilingual sentence representations with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL workshop on Representation Learning for NLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reliable measures for aligning Japanese-English news articles and sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic data selection for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Marlies Van Der Wees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1400" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1610.05011" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment classification with bilingual document representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Overview of the third bucc shared task: Spottign parallel sentences in comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Sharoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BUCC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="60" to="67" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
