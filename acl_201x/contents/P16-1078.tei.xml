<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tree-to-Sequence Attentional Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>3-7-1 Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>3-7-1 Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>3-7-1 Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tree-to-Sequence Attentional Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="823" to="833"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the de-coder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT&apos;15 English-to-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine Translation (MT) has traditionally been one of the most complex language processing problems, but recent advances of Neural Machine Translation (NMT) make it possible to perform translation using a simple end-to-end architecture. In the Encoder-Decoder model ( <ref type="bibr" target="#b3">Cho et al., 2014b;</ref><ref type="bibr" target="#b27">Sutskever et al., 2014</ref>), a Recurrent Neural Net- work (RNN) called the encoder reads the whole sequence of source words to produce a fixed- length vector, and then another RNN called the decoder generates the target words from the vec- tor. The Encoder-Decoder model has been ex- tended with an attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b15">Luong et al., 2015a)</ref>, which allows the model to jointly learn the soft alignment between the source language and the target language. NMT models have achieved state-of-the-art results in English-to-French and English-to-German trans- <ref type="figure">Figure 1</ref>: Alignment between an English phrase and a Japanese word. lation tasks ( <ref type="bibr" target="#b16">Luong et al., 2015b;</ref><ref type="bibr" target="#b15">Luong et al., 2015a</ref>). However, it is yet to be seen whether NMT is competitive with traditional Statistical Machine Translation (SMT) approaches in trans- lation tasks for structurally distant language pairs such as English-to-Japanese. <ref type="figure">Figure 1</ref> shows a pair of parallel sentences in English and Japanese. English and Japanese are linguistically distant in many respects; they have different syntactic constructions, and words and phrases are defined in different lexical units. In this example, the Japanese word "" is aligned with the English words "green" and "tea", and the English word sequence "a cup of" is aligned with a special symbol "null", which is not explic- itly translated into any Japanese words. One way to solve this mismatch problem is to consider the phrase structure of the English sentence and align the phrase "a cup of green tea" with "". In SMT, it is known that incorporating syntactic con- stituents of the source language into the models improves word alignment <ref type="bibr" target="#b29">(Yamada and Knight, 2001</ref>) and translation accuracy ( <ref type="bibr" target="#b14">Liu et al., 2006</ref>; <ref type="bibr" target="#b20">Neubig and Duh, 2014</ref>). However, the existing NMT models do not allow us to perform this kind of alignment.</p><p>In this paper, we propose a novel attentional NMT model to take advantage of syntactic infor-mation. Following the phrase structure of a source sentence, we encode the sentence recursively in a bottom-up fashion to produce a vector representa- tion of the sentence and decode it while aligning the input phrases and words with the output. Our experimental results on the WAT'15 English-to- Japanese translation task show that our proposed model achieves state-of-the-art translation accu- racy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder-Decoder Model</head><p>NMT is an end-to-end approach to data-driven machine translation <ref type="bibr" target="#b12">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b27">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. In other words, the NMT models directly estimate the conditional probability p(y|x) given a large collection of source and target sentence pairs (x, y). An NMT model consists of an en- coder process and a decoder process, and hence they are often called Encoder-Decoder models. In the Encoder-Decoder models, a sentence is treated as a sequence of words. In the encoder pro- cess, the encoder embeds each of the source words</p><formula xml:id="formula_0">x = (x 1 , x 2 , · · · , x n ) into a d-dimensional vector space.</formula><p>The decoder then outputs a word sequence y = (y 1 , y 2 , · · · , y m ) in the target language given the information on the source sentence provided by the encoder. Here, n and m are the lengths of the source and target sentences, respectively. RNNs allow one to effectively embed sequential data into the vector space.</p><p>In the RNN encoder, the i-th hidden unit h i ∈ R d×1 is calculated given the i-th input x i and the previous hidden unit h i−1 ∈ R d×1 ,</p><formula xml:id="formula_1">h i = f enc (x i , h i−1 ),<label>(1)</label></formula><p>where f enc is a non-linear function, and the initial hidden unit h 0 is usually set to zeros. The encod- ing function f enc is recursively applied until the n- th hidden unit h n is obtained. The RNN Encoder- Decoder models assume that h n represents a vec- tor of the meaning of the input sequence up to the n-th word. After encoding the whole input sentence into the vector space, we decode it in a similar way. The initial decoder unit s 1 is initialized with the input sentence vector (s 1 = h n ). Given the pre- vious target word and the j-th hidden unit of the decoder, the conditional probability that the j-th target word is generated is calculated as follows:</p><formula xml:id="formula_2">p(y j |y &lt;j , x) = g(s j ),<label>(2)</label></formula><p>where g is a non-linear function. The j-th hidden unit of the decoder is calculated by using another non-linear function f dec as follows:</p><formula xml:id="formula_3">s j = f dec (y j−1 , s j−1 ).<label>(3)</label></formula><p>We employ Long Short-Term Memory (LSTM) units <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b6">Gers et al., 2000</ref>) in place of vanilla RNN units. The t- th LSTM unit consists of several gates and two different types of states: a hidden unit h t ∈ R d×1 and a memory cell c t ∈ R d×1 ,</p><formula xml:id="formula_4">i t = σ(W (i) x t + U (i) h t−1 + b (i) ), f t = σ(W (f ) x t + U (f ) h t−1 + b (f ) ), o t = σ(W (o) x t + U (o) h t−1 + b (o) ), ˜ c t = tanh(W (˜ c) x t + U (˜ c) h t−1 + b (˜ c) ), c t = i t ⊙ ˜ c t + f t ⊙ c t−1 , h t = o t ⊙ tanh(c t ),<label>(4)</label></formula><p>where each of i t , f t , o t and˜cand˜ and˜c t ∈ R d×1 denotes an input gate, a forget gate, an output gate, and a state for updating the memory cell, respectively.</p><formula xml:id="formula_5">W (·) ∈ R d×d and U (·) ∈ R d×d are weight matri- ces, b (·) ∈ R d×1</formula><p>is a bias vector, and x t ∈ R d×1 is the word embedding of the t-th input word. σ(·) is the logistic function, and the operator ⊙ denotes element-wise multiplication between vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attentional Encoder-Decoder Model</head><p>The NMT models with an attention mecha- nism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b15">Luong et al., 2015a)</ref> have been proposed to softly align each decoder state with the encoder states. The attention mech- anism allows the NMT models to explicitly quan- tify how much each encoder state contributes to the word prediction at each time step.</p><p>In the attentional NMT model in <ref type="bibr" target="#b15">Luong et al. (2015a)</ref>, at the j-th step of the decoder process, the attention score α j (i) between the i-th source hidden unit h i and the j-th target hidden unit s j is calculated as follows:</p><formula xml:id="formula_6">α j (i) = exp(h i · s j ) ∑ n k=1 exp(h k · s j ) ,<label>(5)</label></formula><p>where h i · s j is the inner product of h i and s j , which is used to directly calculate the similarity score between h i and s j . The j-th context vector </p><formula xml:id="formula_7">d j = n ∑ i=1 α j (i)h i .<label>(6)</label></formula><p>To incorporate the attention mechanism into the decoding process, the context vector is used for the the j-th word prediction by putting an additional hidden layer˜slayer˜ layer˜s j :</p><formula xml:id="formula_8">˜ s j = tanh(W d [s j ; d j ] + b d ),<label>(7)</label></formula><p>where</p><formula xml:id="formula_9">[s j ; d j ] ∈ R 2d×1</formula><p>is the concatenation of s j and d j , and W d ∈ R d×2d and b d ∈ R d×1 are a weight matrix and a bias vector, respectively. The model predicts the j-th word by using the softmax function:</p><formula xml:id="formula_10">p(y j |y &lt;j , x) = softmax(W s ˜ s j + b s ),<label>(8)</label></formula><p>where W s ∈ R |V |×d and b s ∈ R |V |×1 are a weight matrix and a bias vector, respectively. |V | stands for the size of the vocabulary of the target lan- guage. <ref type="figure" target="#fig_0">Figure 2</ref> shows an example of the NMT model with the attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Objective Function of NMT Models</head><p>The objective function to train the NMT models is the sum of the log-likelihoods of the translation pairs in the training data:</p><formula xml:id="formula_11">J(θ) = 1 |D| ∑ (x,y)∈D log p(y|x),<label>(9)</label></formula><p>where D denotes a set of parallel sentence pairs. The model parameters θ are learned through Stochastic Gradient Descent (SGD).</p><p>3 Attentional Tree-to-Sequence Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tree-based Encoder + Sequential Encoder</head><p>The exsiting NMT models treat a sentence as a sequence of words and neglect the structure of a sentence inherent in language. We propose a novel tree-based encoder in order to explicitly take the syntactic structure into consideration in the NMT model. We focus on the phrase structure of a sentence and construct a sentence vector from phrase vectors in a bottom-up fashion. The sen- tence vector in the tree-based encoder is there- fore composed of the structural information rather than the sequential data. <ref type="figure" target="#fig_1">Figure 3</ref> shows our pro- posed model, which we call a tree-to-sequence at- tentional NMT model.</p><p>In Head-driven Phrase Structure Grammar (HPSG) ( <ref type="bibr" target="#b26">Sag et al., 2003</ref>), a sentence is composed of multiple phrase units and represented as a bi- nary tree as shown in <ref type="figure">Figure 1</ref>. Following the structure of the sentence, we construct a tree-based encoder on top of the standard sequential encoder. The k-th parent hidden unit h (phr) k for the k-th phrase is calculated using the left and right child hidden units h l k and h r k as follows:</p><formula xml:id="formula_12">h (phr) k = f tree (h l k , h r k ),<label>(10)</label></formula><p>where f tree is a non-linear function.</p><p>We construct a tree-based encoder with LSTM units, where each node in the binary tree is repre- sented with an LSTM unit. When initializing the leaf units of the tree-based encoder, we employ the sequential LSTM units described in Section 2.1. Each non-leaf node is also represented with an LSTM unit, and we employ Tree-LSTM ( <ref type="bibr" target="#b28">Tai et al., 2015)</ref> to calculate the LSTM unit of the par- ent node which has two child LSTM units. The hidden unit h (phr) k ∈ R d×1 and the memory cell c (phr) k ∈ R d×1 for the k-th parent node are calcu-lated as follows:</p><formula xml:id="formula_13">i k = σ(U (i) l h l k + U (i) r h r k + b (i) ), f l k = σ(U (f l ) l h l k + U (f l) r h r k + b (f l ) ), f r k = σ(U (fr) l h l k + U (fr) r h r k + b (fr) ), o k = σ(U (o) l h l k + U (o) r h r k + b (o) ), ˜ c k = tanh(U (˜ c) l h l k + U (˜ c) r h r k + b (˜ c) ), c (phr) k = i k ⊙ ˜ c k + f l k ⊙ c l k + f r k ⊙ c r k , h (phr) k = o k ⊙ tanh(c (phr) k ),<label>(11)</label></formula><p>where</p><formula xml:id="formula_14">i k , f l k , f r k , o j , ˜ c j ∈ R d×1</formula><p>are an input gate, the forget gates for left and right child units, an output gate, and a state for updating the mem- ory cell, respectively. c l k and c r k are the mem- ory cells for the left and right child units, respec- tively. U (·) ∈ R d×d denotes a weight matrix, and b (·) ∈ R d×1 represents a bias vector.</p><p>Our proposed tree-based encoder is a natural extension of the conventional sequential encoder, since Tree-LSTM is a generalization of chain- structured LSTM <ref type="bibr" target="#b28">(Tai et al., 2015)</ref>. Our encoder differs from the original Tree-LSTM in the cal- culation of the LSTM units for the leaf nodes. The motivation is to construct the phrase nodes in a context-sensitive way, which, for example, al- lows the model to compute different representa- tions for multiple occurrences of the same word in a sentence because the sequential LSTMs are cal- culated in the context of the previous units. This ability contrasts with the original Tree-LSTM, in which the leaves are composed only of the word embeddings without any contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initial Decoder Setting</head><p>We now have two different sentence vectors: one is from the sequence encoder and the other from the tree-based encoder. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, we provide another Tree-LSTM unit which has the fi- nal sequential encoder unit (h n ) and the tree-based encoder unit (h (phr) root ) as two child units and set it as the initial decoder s 1 as follows:</p><formula xml:id="formula_15">s 1 = g tree (h n , h (phr) root ),<label>(12)</label></formula><p>where g tree is the same function as f tree with an- other set of Tree-LSTM parameters. This initial- ization allows the decoder to capture information from both the sequential data and phrase struc- tures. <ref type="bibr" target="#b31">Zoph and Knight (2016)</ref> proposed a simi- lar method using a Tree-LSTM for initializing the decoder, with which they translate multiple source languages to one target language. When the syn- tactic parser fails to output a parse tree for a sen- tence, we encode the sentence with the sequential encoder by setting h (phr) root = 0. Our proposed tree- based encoder therefore works with any sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Mechanism in Our Model</head><p>We adopt the attention mechanism into our tree- to-sequence model in a novel way. Our model gives attention not only to sequential hidden units but also to phrase hidden units. This attention mechanism tells us which words or phrases in the source sentence are important when the model de- codes a target word. The j-th context vector d j is composed of the sequential and phrase vectors weighted by the attention score α j (i):</p><formula xml:id="formula_16">d j = n ∑ i=1 α j (i)h i + 2n−1 ∑ i=n+1 α j (i)h (phr) i .<label>(13)</label></formula><p>Note that a binary tree has n − 1 phrase nodes if the tree has n leaves. We set a final decoder˜sdecoder˜ decoder˜s j in the same way as Equation <ref type="formula" target="#formula_8">(7)</ref>. In addition, we adopt the input-feeding method ( <ref type="bibr" target="#b15">Luong et al., 2015a</ref>) in our model, which is a method for feeding˜sfeeding˜ feeding˜s j−1 , the previous unit to predict the word y j−1 , into the current target hidden unit s j ,</p><formula xml:id="formula_17">s j = f dec (y j−1 , [s j−1 ; ˜ s j−1 ]),<label>(14)</label></formula><p>where [s j−1 ; ˜ s j−1 ] is the concatenation of s j−1 and˜sand˜ and˜s j−1 . The input-feeding approach contributes to the enrichment in the calculation of the decoder, because˜sbecause˜ because˜s j−1 is an informative unit which can be used to predict the output word as well as to be compacted with attentional context vectors. <ref type="bibr" target="#b15">Luong et al. (2015a)</ref> showed that the input-feeding approach improves BLEU scores. We also ob- served the same improvement in our preliminary experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sampling-Based Approximation to the NMT Models</head><p>The biggest computational bottleneck of train- ing the NMT models is in the calculation of the softmax layer described in Equation <ref type="formula" target="#formula_10">(8)</ref>, because its computational cost increases linearly with the size of the vocabulary. The speedup technique with GPUs has proven useful for sequence-based NMT models ( <ref type="bibr" target="#b27">Sutskever et al., 2014;</ref><ref type="bibr" target="#b15">Luong et al., 2015a</ref>) but it is not easily applicable when deal- ing with tree-structured data. In order to reduce the training cost of the NMT models at the soft- max layer, we employ BlackOut (Ji et al., 2016), a sampling-based approximation method. BlackOut has been shown to be effective in RNN Language Models (RNNLMs) and allows a model to run rea- sonably fast even with a million word vocabulary with CPUs.</p><p>At each word prediction step in the training, BlackOut estimates the conditional probability in Equation <ref type="formula" target="#formula_2">(2)</ref>  <ref type="bibr" target="#b10">Ji et al. (2016)</ref>. Note that BlackOut can be used as the original softmax once the training is finished.</p><note type="other">for the target word and K neg- ative samples using a weighted softmax func- tion. The negative samples are drawn from the unigram distribution raised to the power β ∈ [0, 1] (Mikolov et al., 2013). The unigram dis- tribution is estimated using the training data and β is a hyperparameter. BlackOut is closely re- lated to Noise Contrastive Estimation (NCE) (Gut- mann and Hyvärinen, 2012) and achieves better perplexity than the original softmax and NCE in RNNLMs. The advantages of Blackout over the other methods are discussed in</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Data</head><p>We applied the proposed model to the English-to- Japanese translation dataset of the ASPEC corpus given in WAT'15. 1 Following Zhu (2015), we ex- tracted the first 1.5 million translation pairs from the training data. To obtain the phrase structures of the source sentences, i.e., English, we used the probabilistic HPSG parser Enju ( <ref type="bibr" target="#b18">Miyao and Tsujii, 2008)</ref>. We used Enju only to obtain a binary phrase structure for each sentence and did not use any HPSG specific information. For the target language, i.e., Japanese, we used KyTea (Neubig et al., 2011), a Japanese segmentation tool, and performed the pre-processing steps recommended in WAT'15. <ref type="bibr">2</ref> We then filtered out the translation pairs whose sentence lengths are longer than 50 and whose source sentences are not parsed suc- cessfully. <ref type="table" target="#tab_0">Table 1</ref> shows the details of the datasets used in our experiments. We carried out two ex- periments on a small training dataset to investigate <ref type="table" target="#tab_0">Sentences Parsed successfully  Train  1,346,946  1,346,946  Development  1,790  1,789  Test  1,812</ref> 1,811  the effectiveness of our proposed model and on a large training dataset to compare our proposed methods with the other systems. The vocabulary consists of words observed in the training data more than or equal to N times. We set N = 2 for the small training dataset and N = 5 for the large training dataset. The out-of- vocabulary words are mapped to the special token "unk". We added another special symbol "eos" for both languages and inserted it at the end of all the sentences. <ref type="table" target="#tab_1">Table 2</ref> shows the details of each train- ing dataset and its corresponding vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>The biases, softmax weights, and BlackOut weights are initialized with zeros. The hyperpa- rameter β of BlackOut is set to 0.4 as recom- mended by <ref type="bibr" target="#b10">Ji et al. (2016)</ref>. Small Training Dataset We conduct experi- ments with our proposed model and the sequential attentional NMT model with the input-feeding ap- proach. Each model has 256-dimensional hidden units and word embeddings. The number of nega- tive samples K of BlackOut is set to 500 or 2000.</p><p>Large Training Dataset Our proposed model has 512-dimensional word embeddings and d- dimensional hidden units (d ∈ {512, 768, 1024}). K is set to 2500.</p><p>Our code 3 is implemented in C++ using the Eigen library, 4 a template library for linear alge- bra, and we run all of the experiments on multi- core CPUs. <ref type="bibr">5</ref> It takes about a week to train a model on the large training dataset with d = 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Decoding process</head><p>We use beam search to decode a target sentence for an input sentence x and calculate the sum of the log-likelihoods of the target sentence y = (y 1 , · · · , y m ) as the beam score:</p><formula xml:id="formula_18">score(x, y) = m ∑ j=1 log p(y j |y &lt;j , x).<label>(15)</label></formula><p>Decoding in the NMT models is a generative pro- cess and depends on the target language model given a source sentence. The score becomes smaller as the target sentence becomes longer, and thus the simple beam search does not work well when decoding a long sentence ( <ref type="bibr" target="#b2">Cho et al., 2014a;</ref><ref type="bibr" target="#b25">Pouget-Abadie et al., 2014</ref>). In our preliminary experiments, the beam search with the length nor- malization in <ref type="bibr" target="#b2">Cho et al. (2014a)</ref> was not effective in English-to-Japanese translation. The method in Pouget- <ref type="bibr" target="#b25">Abadie et al. (2014)</ref> needs to estimate the conditional probability p(x|y) using another NMT model and thus is not suitable for our work.</p><p>In this paper, we use statistics on sentence lengths in beam search. Assuming that the length of a target sentence correlates with the length of a source sentence, we redefine the score of each candidate as follows:</p><formula xml:id="formula_19">score(x, y) = L x,y + m ∑ j=1 log p(y j |y &lt;j , x),(16) L x,y = log p(len(y)|len(x)),<label>(17)</label></formula><p>where L x,y is the penalty for the conditional prob- ability of the target sentence length len(y) given the source sentence length len(x). It allows the model to decode a sentence by considering the length of the target sentence. In our exper- iments, we computed the conditional probability p(len(y)|len(x)) in advance following the statis- tics collected in the first one million pairs of the training dataset. We allow the decoder to generate up to 100 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation</head><p>We evaluated the models by two automatic eval- uation metrics, RIBES ( <ref type="bibr" target="#b9">Isozaki et al., 2010</ref>) and BLEU ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>) following WAT'15. We used the KyTea-based evaluation script for the translation results. <ref type="bibr">6</ref> The RIBES score is a metric based on rank correlation coefficients with word precision, and the BLEU score is based on n-gram word precision and a Brevity Penalty (BP) for out- puts shorter than the references. RIBES is known to have stronger correlation with human judge- ments than BLEU in translation between English and Japanese as discussed in <ref type="bibr" target="#b9">Isozaki et al. (2010)</ref>. <ref type="table" target="#tab_3">Table 3</ref> shows the perplexity, BLEU, RIBES, and the training time on the development data with the Attentional NMT (ANMT) models trained on the small dataset. We conducted the experiments with our proposed method using BlackOut and soft- max. We decoded a translation by our proposed beam search with a beam size of 20. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the results of our proposed model with BlackOut improve as the number of negative samples K increases. Although the result of softmax is better than those of BlackOut <ref type="bibr">(K = 500, 2000</ref>), the training time of softmax per epoch is about three times longer than that of BlackOut even with the small dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Small Training Dataset</head><p>As to the results of the ANMT model, reversing the word order in the input sentence decreases the scores in English-to-Japanese translation, which contrasts with the results of other language pairs reported in previous work <ref type="bibr" target="#b27">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b15">Luong et al., 2015a)</ref>. By taking syntactic infor- mation into consideration, our proposed model improves the scores, compared to the sequential attention-based approach.</p><p>We found that better perplexity does not always lead to better translation scores with BlackOut as shown in <ref type="table" target="#tab_3">Table 3</ref>. One of the possible reasons is that BlackOut distorts the target word distribution    by the modified unigram-based negative sampling where frequent words can be treated as the nega- tive samples multiple times at each training step.</p><note type="other">Perplexity RIBES BLEU Time/epoch (min.) Proposed</note><note type="other">ANMT (Luong et al., 2015a) 2000 23.1 71.5 19.4 60 + reverse input 2000 26.1 69.5 17.5 60</note><p>Effects of the proposed beam search <ref type="table" target="#tab_4">Table 4</ref> shows the results on the development data of pro- posed method with BlackOut (K = 2000) by the simple beam search and our proposed beam search. The beam size is set to 6 or 20 in the sim- ple beam search, and to 20 in our proposed search. We can see that our proposed search outperforms the simple beam search in both scores. Unlike RIBES, the BLEU score is sensitive to the beam size and becomes lower as the beam size increases. We found that the BP had a relatively large im- pact on the BLEU score in the simple beam search as the beam size increased. Our search method works better than the simple beam search by keep- ing long sentences in the candidates with a large beam size.</p><p>Effects of the sequential LSTM units We also investigated the effects of the sequential LSTMs at the leaf nodes in our proposed tree-based en- coder. <ref type="table" target="#tab_6">Table 5</ref> shows the result on the develop- ment data of our proposed encoder and that of an attentional tree-based encoder without sequential LSTMs with BlackOut (K = 2000). <ref type="bibr">7</ref> The results show that our proposed encoder considerably out- 7 For this evaluation, we used the 1,789 sentences that were successfully parsed by Enju because the encoder with- out sequential LSTMs always requires a parse tree.  performs the encoder without sequential LSTMs, suggesting that the sequential LSTMs at the leaf nodes contribute to the context-aware construction of the phrase representations in the tree. <ref type="table" target="#tab_8">Table 6</ref> shows the experimental results of RIBES and BLEU scores achieved by the trained models on the large dataset. We decoded the target sen- tences by our proposed beam search with the beam size of 20. <ref type="bibr">8</ref> The results of the other systems are the ones reported in <ref type="bibr" target="#b19">Nakazawa et al. (2015)</ref>. All of our proposed models show similar per- formance regardless of the value of d. Our ensem- ble model is composed of the three models with d = 512, 768, and 1024, and it shows the best RIBES score among all systems. <ref type="bibr">9</ref> As for the time required for training, our im- plementation needs about one day to perform one epoch on the large training dataset with d = 512. It would take about 11 days without using the BlackOut sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RIBES BLEU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Large Training Dataset</head><p>Comparison with the NMT models The model of Zhu <ref type="formula" target="#formula_1">(2015)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Analysis</head><p>We illustrate the translations of test data by our model with d = 512 and several attentional rela- tions when decoding a sentence. In <ref type="figure" target="#fig_5">Figures 4</ref> and 5, an English sentence represented as a binary tree is translated into Japanese, and several attentional relations between English words or phrases and In <ref type="figure" target="#fig_5">Figure 4</ref>, the Japanese word "" means "liquid crystal", and it has a high attention score (α = 0.41) with the English phrase "liquid crystal for active matrix". This is because the j-th tar- get hidden unit s j has the contextual information about the previous words y &lt;j including " " ("for active matrix" in English). The Japanese word "" is softly aligned with the phrase "the cells" with the highest attention score (α = 0.35). In Japanese, there is no defi- nite article like "the" in English, and it is usually aligned with null described as Section 1.</p><p>In <ref type="figure">Figure 5</ref>, in the case of the Japanese word "" ("showed" in English), the attention score with the English phrase "showed excellent perfor- mance" (α = 0.25) is higher than that with the English word "showed" (α = 0.01). The Japanese word "" ("of" in English) is softly aligned with the phrase "of Si dot MOS capacitor" with the highest attention score (α = 0.30). It is because our attention mechanism takes each previous con- text of the Japanese phrases " " ("ex- cellent performance" in English) and " " ("Si dot MOS capacitor" in English) into account and softly aligned the target words with the whole phrase when translating the English verb "showed" and the preposition "of". Our proposed model can thus flexibly learn the at- tentional relations between English and Japanese.</p><p>We observed that our model translated the word "active" into "", a synonym of the reference word "". We also found similar exam- ples in other sentences, where our model outputs <ref type="figure">Figure 5</ref>: Translation example of a long sentence and the attentional relations by our proposed model. synonyms of the reference words, e.g. "" and " " ("female" in English) and "NASA" and " " ("National Aeronautics and Space Ad- ministration" in English). These translations are penalized in terms of BLEU scores, but they do not necessarily mean that the translations were wrong. This point may be supported by the fact that the NMT models were highly evaluated in WAT'15 by crowd sourcing <ref type="bibr" target="#b19">(Nakazawa et al., 2015</ref>). <ref type="bibr" target="#b12">Kalchbrenner and Blunsom (2013)</ref> were the first to propose an end-to-end NMT model using Con- volutional Neural Networks (CNNs) as the source encoder and using RNNs as the target decoder. The Encoder-Decoder model can be seen as an ex- tension of their model, and it replaces the CNNs with RNNs using GRUs ( <ref type="bibr" target="#b3">Cho et al., 2014b</ref>) or LSTMs ( <ref type="bibr" target="#b27">Sutskever et al., 2014)</ref>. <ref type="bibr" target="#b27">Sutskever et al. (2014)</ref> have shown that mak- ing the input sequences reversed is effective in a French-to-English translation task, and the tech- nique has also proven effective in translation tasks between other European language pairs ( <ref type="bibr" target="#b15">Luong et al., 2015a</ref>). All of the NMT models mentioned above are based on sequential encoders. To incor- porate structural information into the NMT mod- els, <ref type="bibr" target="#b2">Cho et al. (2014a)</ref> proposed to jointly learn structures inherent in source-side languages but did not report improvement of translation perfor- mance. These studies motivated us to investigate the role of syntactic structures explicitly given by existing syntactic parsers in the NMT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) has promoted NMT onto the next stage. It enables the NMT models to translate while align- ing the target with the source. <ref type="bibr" target="#b15">Luong et al. (2015a)</ref> refined the attention model so that it can dynami- cally focus on local windows rather than the entire sentence. They also proposed a more effective at- tentional path in the calculation of ANMT models. Subsequently, several ANMT models have been proposed <ref type="bibr" target="#b1">(Cheng et al., 2016;</ref><ref type="bibr" target="#b4">Cohn et al., 2016)</ref>; however, each model is based on the existing se- quential attentional models and does not focus on a syntactic structure of languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a novel syntactic ap- proach that extends attentional NMT models. We focus on the phrase structure of the input sen- tence and build a tree-based encoder following the parsed tree. Our proposed tree-based encoder is a natural extension of the sequential encoder model, where the leaf units of the tree-LSTM in the encoder can work together with the origi- nal sequential LSTM encoder. Moreover, the at- tention mechanism allows the tree-based encoder to align not only the input words but also input phrases with the output words. Experimental re- sults on the WAT'15 English-to-Japanese transla- tion dataset demonstrate that our proposed model achieves the best RIBES score and outperforms the sequential attentional NMT model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attentional Encoder-Decoder model.</figDesc><graphic url="image-2.png" coords="3,110.27,62.81,141.73,99.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Proposed model: Tree-to-sequence attentional NMT model.</figDesc><graphic url="image-3.png" coords="3,345.54,62.95,141.73,124.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Following Józefowicz et al. (2015), we initialize the forget gate biases of LSTM and Tree-LSTM with 1.0. The remaining model parameters in the NMT models in our ex- periments are uniformly initialized in [−0.1, 0.1]. The model parameters are optimized by plain SGD with the mini-batch size of 128. The initial learn- ing rate of SGD is 1.0. We halve the learning rate when the development loss becomes worse. Gra- dient norms are clipped to 3.0 to avoid exploding gradient problems (Pascanu et al., 2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>K</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>dimensional word embeddings.</head><label></label><figDesc>The model of Lee et al. (2015) is also an ANMT model with a bi- directional Gated Recurrent Unit (GRU) encoder, and uses 1000-dimensional hidden units and 200- dimensional word embeddings. Both models are sequential ANMT models. Our single proposed model with d = 512 outperforms the best result of Zhu (2015)'s end-to-end NMT model with ensem- ble and unknown replacement by +1.19 RIBES and by +0.17 BLEU scores. Our ensemble model shows better performance, in both RIBES and BLEU scores, than that of Zhu (2015)'s best sys- tem which is a hybrid of the ANMT and SMT models by +1.54 RIBES and by +0.74 BLEU scores and Lee et al. (2015)'s ANMT system with special character-based decoding by +1.30 RIBES and +1.20 BLEU scores. Comparison with the SMT models PB, HPB and T2S are the baseline SMT systems in WAT'15: a phrase-based model, a hierarchical phrase-based model, and a tree-to-string model, respectively (Nakazawa et al., 2015). The best model in WAT'15 is Neubig et al. (2015)'s tree- to-string SMT model enhanced with reranking by ANMT using a bi-directional LSTM encoder. Our proposed end-to-end NMT model compares favor- ably with Neubig et al. (2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Translation example of a short sentence and the attentional relations by our proposed model.</figDesc><graphic url="image-4.png" coords="8,307.28,63.80,221.10,102.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Dataset in ASPEC corpus.</head><label>1</label><figDesc></figDesc><table>Train (small) Train (large) 
sentence pairs 
100,000 
1,346,946 
|V | in English 
25,478 
87,796 
|V | in Japanese 
23,532 
65,680 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Training dataset and the vocabulary sizes.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Evaluation results on the development data using the small training data. The training time per 
epoch is also shown, and K is the number of negative samples in BlackOut. 

Beam size RIBES BLEU (BP) 

Simple BS 
6 
72.3 
20.0 (90.1) 
20 
72.3 
19.5 (85.1) 
Proposed BS 
20 
72.6 
20.5 (91.7) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Effects of the Beam Search (BS) on the 
development data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Effects of the sequential LSTMs in our 
proposed tree-based encoder on the development 
data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 : Evaluation results on the test data.</head><label>6</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://orchid.kuee.kyoto-u.ac.jp/WAT/ WAT2015/index.html 2 http://orchid.kuee.kyoto-u.ac.jp/WAT/ WAT2015/baseline/dataPreparationJE.html</note>

			<note place="foot" n="3"> https://github.com/tempra28/tree2seq 4 http://eigen.tuxfamily.org/index.php 5 16 threads on Intel(R) Xeon(R) CPU E5-2667 v3 @ 3.20GHz</note>

			<note place="foot" n="6"> http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation/automatic_evaluation_systems/ automaticEvaluationJA.html</note>

			<note place="foot" n="8"> We found two sentences which ends without eos with d = 512, and then we decoded it again with the beam size of 1000 following Zhu (2015). 9 Our ensemble model yields a METEOR (Denkowski and Lavie, 2014) score of 53.6 with language option &quot;-l other&quot;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their con-structive comments and suggestions. This work was supported by CREST, JST, and JSPS KAK-ENHI Grant Number 15J12597.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: EncoderDecoder Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</title>
		<meeting>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating Structural Alignment Biases into an Attentional Neural Translation Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to Forget: Continual Prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">A</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="307" to="361" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic Evaluation of Translation Quality for Distant Language Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="944" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Shihao Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadathur</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
		<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Empirical Exploration of Recurrent Network Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">NAVER Machine Translation System for WAT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoung-Gyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Seok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ki</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Asian Translation (WAT2015)</title>
		<meeting>the 2nd Workshop on Asian Translation (WAT2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="69" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Treeto-string alignment template for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Addressing the Rare Word Problem in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature Forest Models for Probabilistic HPSG Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Overview of the 2nd Workshop on Asian Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideya</forename><surname>Mino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Asian Translation (WAT2015)</title>
		<meeting>the 2nd Workshop on Asian Translation (WAT2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the elements of an accurate tree-to-string machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="143" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pointwise Prediction for Robust, Adaptable Japanese Morphological Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Asian Translation (WAT2015)</title>
		<meeting>the 2nd Workshop on Asian Translation (WAT2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Understanding the exploding gradient problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overcoming the curse of sentence length for neural machine translation using automatic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="78" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Syntactic Theory: A Formal Introduction. Center for the Study of Language and Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Wasow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A syntaxbased statistical translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluating Neural Machine Translation in English-Japanese Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Asian Translation (WAT2015)</title>
		<meeting>the 2nd Workshop on Asian Translation (WAT2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-Source Neural Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
