<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Batch IS NOT Heavy: Learning Word Representations From All Samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joemon</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing Science</orgName>
								<orgName type="institution">University of Glasgow</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Batch IS NOT Heavy: Learning Word Representations From All Samples</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1853" to="1862"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations. However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution. Besides, SGD suffers from dramatic fluctuation due to the one-sample learning scheme. In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples. Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples. We evaluate AllVec on several benchmark tasks. Experiments show that AllVec outperforms sampling-based SGD methods with comparable efficiency , especially for small training corpora .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representing words using dense and real-valued vectors, aka word embeddings, has become the cornerstone for many natural language processing (NLP) tasks, such as document classification <ref type="bibr" target="#b37">(Sebastiani, 2002</ref>), parsing ( <ref type="bibr" target="#b16">Huang et al., 2012)</ref>, dis- course relation recognition ( <ref type="bibr" target="#b20">Lei et al., 2017)</ref> and named entity recognition ( <ref type="bibr" target="#b41">Turian et al., 2010)</ref>. Word embeddings can be learned by optimizing that words occurring in similar contexts have sim- ilar embeddings, i.e. the well-known distribu- tional hypothesis <ref type="bibr" target="#b14">(Harris, 1954)</ref>. A representa- tive method is skip-gram (SG) ( <ref type="bibr">Mikolov et al., 2013a,b)</ref>, which realizes the hypothesis using a * The first two authors contributed equally to this paper and share the first-authorship.</p><p>(a) (b) <ref type="figure">Figure 1</ref>: Impact of different settings of negative sampling on skip-gram for the word analogy task on Text8. Clearly, the accuracy depends largely on (a) the sampling size of negative words, and (b) the sampling distribution (β = 0 means the uniform distribution and β = 1 means the word frequency distribution).</p><p>shallow neural network model. The other family of methods is count-based, such as GloVe <ref type="bibr" target="#b31">(Pennington et al., 2014</ref>) and <ref type="bibr">LexVec (Salle et al., 2016a,b)</ref>, which exploit low-rank models such as matrix factorization (MF) to learn embeddings by reconstructing the word co-occurrence statistics.</p><p>By far, most state-of-the-art embedding meth- ods rely on SGD and negative sampling for opti- mization. However, the performance of SGD is highly sensitive to the sampling distribution and the number of negative samples <ref type="bibr" target="#b9">(Chen et al., 2018;</ref><ref type="bibr" target="#b42">Yuan et al., 2016)</ref>, as shown in <ref type="figure">Figure 1</ref>. Es- sentially, sampling is biased, making it difficult to converge to the same loss with all examples, regardless of how many update steps have been taken. Moreover, SGD exhibits dramatic fluc- tuation and suffers from overshooting on local minimums <ref type="bibr" target="#b34">(Ruder, 2016)</ref>. These drawbacks of SGD can be attributed to its one-sample learning scheme, which updates parameters based on one training sample in each step.</p><p>To address the above-mentioned limitations of SGD, a natural solution is to perform exact (full) batch learning. In contrast to SGD, batch learning does not involve any sampling procedure and com- putes the gradient over all training samples. As such, it can easily converge to a better optimum in a more stable way. Nevertheless, a well-known difficulty in applying full batch learning lies in the expensive computational cost for large-scale data. Taking the word embedding learning as an exam- ple, if the vocabulary size is |V |, then evaluating the loss function and computing the full gradient takes O(|V | 2 k) time, where k is the embedding size. This high complexity is unaffordable in prac- tice, since |V | 2 can easily reach billion level or even higher.</p><p>In this paper, we introduce AllVec, an exact and efficient word embedding method based on full batch learning. To address the efficiency challenge in learning from all training samples, we devise a regression-based loss function for word embed- ding, which allows fast optimization with memo- rization strategies. Specifically, the acceleration is achieved by reformulating the expensive loss over all negative samples using a partition and a decou- ple operation. By decoupling and caching the bot- tleneck terms, we succeed to use all samples for each parameter update in a manageable time com- plexity which is mainly determined by the positive samples. The main contributions of this work are summarized as follows:</p><p>• We present a fine-grained weighted least square loss for learning word embeddings. Unlike GloVe, it explicitly accounts for all negative samples and reweights them with a frequency-aware strategy.</p><p>• We propose an efficient and exact optimiza- tion algorithm based on full batch gradient optimization. It has a comparable time com- plexity with SGD, but being more effective and stable due to the consideration of all sam- ples in each parameter update.</p><p>• We perform extensive experiments on several benchmark datasets and tasks to demonstrate the effectiveness, efficiency, and convergence property of our AllVec method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Skip-gram with Negative Sampling</head><p>Mikolov et al. (2013a,b) proposed the skip-gram model to learn word embeddings. SG formulates the problem as a predictive task, aiming at predict- ing the proper context c for a target word w within a local window. To speed up the training process, it applies the negative sampling ( <ref type="bibr" target="#b28">Mikolov et al., 2013b</ref>) to approximate the full softmax. That is, each positive (w, c) pair is trained with n ran- domly sampled negative pairs (w, w i ). The sam- pled loss function of SG is defined as</p><formula xml:id="formula_0">L SG wc = log σ(Uw˜UUw˜ Uw˜U T c )+ n i=1 E w i ∼Pn(w) log σ(−Uw˜UUw˜ Uw˜U T w i )</formula><p>where U w and˜Uand˜ and˜U c denote the k-dimensional em- bedding vectors for word w and context c. P n (w) is the distribution from which negative context w i is sampled. Plenty of research has been done based on SG, such as the use of prior knowledge from another source ( <ref type="bibr" target="#b19">Kumar and Araki, 2016;</ref><ref type="bibr" target="#b24">Liu et al., 2015a;</ref><ref type="bibr" target="#b5">Bollegala et al., 2016)</ref>, incorporating word type in- formation <ref type="bibr" target="#b8">(Cao and Lu, 2017;</ref><ref type="bibr" target="#b30">Niu et al., 2017)</ref>, character level n-gram models ( ) and jointly learning with topic models like LDA ( <ref type="bibr" target="#b38">Shi et al., 2017;</ref><ref type="bibr">Liu et al., 2015b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Importance of the Sampling Distribution</head><p>Mikolov et al. (2013b) showed that the unigram distribution raised to the 3/4th power as P n (w) significantly outperformed both the unigram and the uniform distribution. This suggests that the sampling distribution (of negative words) has a great impact on the embedding quality. Further- more, <ref type="bibr" target="#b9">Chen et al. (2018)</ref> and <ref type="bibr" target="#b13">Guo et al. (2018)</ref> recently found that replacing the original sam- pler with adaptive samplers could result in bet- ter performance. The adaptive samplers are used to find more informative negative examples dur- ing the training process. Compared with the orig- inal word-frequency based sampler, adaptive sam- plers adapt to both the target word and the current state of the model. They also showed that the fine- grained samplers not only speeded up the conver- gence but also significantly improved the embed- ding quality. Similar observations were also found in other fields like collaborative filtering ( <ref type="bibr" target="#b42">Yuan et al., 2016)</ref>. While being effective, it is proven that negative sampling is a biased approximation and does not converges to the same loss as the full softmax -regardless of how many update steps have been taken <ref type="bibr" target="#b2">(Bengio and Senécal, 2008;</ref><ref type="bibr" target="#b3">Blanc and Rendle, 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Count-based Embedding Methods</head><p>Another line of research is the count-based em- bedding, such as GloVe ( <ref type="bibr" target="#b31">Pennington et al., 2014</ref>). GloVe performs a biased MF on the word-context co-occurrence statistics, which is a common ap-proach in the field of collaborative filtering <ref type="bibr" target="#b18">(Koren, 2008</ref>). However, GloVe only formulates the loss on positive entries of the co-occurrence ma- trix, meaning that negative signals about word- context co-occurrence are discarded. A remedy solution is <ref type="bibr">LexVec (Salle et al., 2016a</ref>,b) which integrates negative sampling into MF. Some other methods ( <ref type="bibr" target="#b23">Li et al., 2015;</ref><ref type="bibr" target="#b39">Stratos et al., 2015;</ref><ref type="bibr" target="#b0">Ailem et al., 2017</ref>) also use MF to approximate the word-context co-occurrence statistics. Al- though predictive models and count-based models seem different at first glance, <ref type="bibr" target="#b21">Levy and Goldberg (2014)</ref> proved that SG with negative sampling is implicitly factorizing a shifted pointwise mutual information (PMI) matrix, which means that the two families of embedding models resemble each other to a certain degree.</p><p>Our proposed method departs from all above methods by using the full batch gradient optimizer to learn from all (positive and negative) samples. We propose a fast learning algorithm to show that such batch learning is not "heavy" even with tens of billions of training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AllVec Loss</head><p>In this work, we adopt the regression loss that is commonly used in count-based models <ref type="bibr" target="#b31">(Pennington et al., 2014;</ref><ref type="bibr" target="#b39">Stratos et al., 2015;</ref><ref type="bibr" target="#b0">Ailem et al., 2017</ref>) to perform matrix factorization on word co- occurrence statistics. As highlighted, to retain the modeling fidelity, AllVec eschews using any sam- pling but optimizes the loss on all positive and negative word-context pairs. Given a word w and a symmetric window of win contexts, the set of positive contexts can be obtained by sliding through the corpus. Let c de- note a specific context, M wc be the number of co- occurred (w, c) pairs in the corpus within the win- dow. M wc = 0 means that the pair (w, c) has never been observed, i.e. the negative signal. r wc is the association coefficient between w and c, which is calculated from M wc . Specifically, we use r + wc to denote the ground truth value for positive (w, c) pairs and a constant value r − (e.g., 0 or -1) for neg- ative ones since there is no interaction between w and c in negative pairs. Finally, with all positive and negative pairs considered, a regular loss func- tion can be given as Eq. <ref type="formula">(1)</ref>, where V is the vocab- ulary and S is the set of positive pairs. α + wc and α − wc represent the weight for positive and negative</p><formula xml:id="formula_1">(w, c) pairs, respectively. L = (w,c)∈S α + wc (r + wc − Uw˜UUw˜ Uw˜U T c ) 2 L P + (w,c)∈(V×V )\S α − wc (r − − Uw˜UUw˜ Uw˜U T c ) 2 L N (1)</formula><p>When it comes to r + wc , there are several choices. For example, GloVe applies the log of M wc with bias terms for w and c. However, research from <ref type="bibr" target="#b21">Levy and Goldberg (2014)</ref> showed that the SG model with negative sampling implicitly factorizes a shifted PMI matrix. The PMI value for a (w, c) pair can be defined as</p><formula xml:id="formula_2">P M I wc = log P (w, c) P (w)P (c) = log M wc M * * M w * M * c (2)</formula><p>where '*' denotes the summation of all corre- sponding indexes (e.g., M w * = c∈V M wc ). In- spired by this connection, we set r + wc as the posi- tive point-wise mutual information (PPMI) which has been commonly used in the NLP literature ( <ref type="bibr" target="#b39">Stratos et al., 2015;</ref><ref type="bibr" target="#b21">Levy and Goldberg, 2014)</ref>. Sepcifically, PPMI is the positive version of PMI by setting the negative values to zero. Finally, r + wc is defined as r + wc = P P M I wc = max(P M I wc , 0) (3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Weighting Strategies</head><p>Regarding α + wc , we follow the design in GloVe, where it is defined as</p><formula xml:id="formula_3">α + wc = (M wc /xmax) ρ M wc &lt; xmax 1 M wc ≥ xmax (4)</formula><p>As for the weight for negative instances α − wc , con- sidering that there is no interaction between w and negative c, we set α − wc as α − c (or α − w ), which means that the weight is determined by the word itself rather than the word-context interaction. Note that either α − wc = α − c or α − wc = α − w does not in- fluence the complexity of AllVec learning algo- rithm described in the next section. The design of α − c is inspired by the frequency-based oversam- pling scheme in skip-gram and missing data re- weighting in recommendation ( <ref type="bibr" target="#b15">He et al., 2016</ref>). The intuition is that a word with high frequency is more likely to be a true negative context word if there is no observed word-context interactions. Hence, to effectively differentiate the positive and negative examples, we assign a higher weight for the negative examples that have a higher word fre-quency, and a smaller weight for infrequent words. Formally, α − wc is defined as</p><formula xml:id="formula_4">α − wc = α − c = α 0 M δ * c c∈V M δ * c<label>(5)</label></formula><p>where α 0 can be seen as a global weight to control the overall importance of negative samples. α 0 = 0 means that no negative information is utilized in the training. The exponent δ is used for smoothing the weights. Specially, δ = 0 means a uniform weight for all negative examples and δ = 1 means that no smoothing is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fast Batch Gradient Optimization</head><p>Once specifying the loss function, the main chal- lenge is how to perform an efficient optimization for Eq.(1). In the following, we develop a fast batch gradient optimization algorithm that is based on a partition reformulation for the loss and a de- couple operation for the inner product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Loss Partition</head><p>As can be seen, the major computational cost in Eq.</p><p>(1) lies in the term L N , because the size of (V ×V ) \ S is very huge, which typically contains over billions of negative examples. To this end, we show our first key design that separates the loss of negative samples into the difference between the loss on all samples and that on positive samples 1 . The loss partition serves as the prerequisite for the efficient computation of full batch gradients.</p><formula xml:id="formula_5">LN= w∈V c∈V α − c (r − −Uw˜UUw˜ Uw˜U T c ) 2 − (w,c)∈S α − c (r − − Uw˜UUw˜ Uw˜U T c ) 2<label>(6)</label></formula><p>By replacing L N in Eq. <ref type="formula">(1)</ref> with Eq.(6), we can ob- tain a new loss function with a more clear struc- ture. We further simplify the loss function by merging the terms on positive examples. Finally, we achieve a reformulated loss</p><formula xml:id="formula_6">L = w∈V c∈V α − c (r − −Uw˜UUw˜ Uw˜U T c ) 2 L A + (w,c)∈S (α + wc − α − c )(∆ − Uw˜UUw˜ Uw˜U T c ) 2 L P +C<label>(7)</label></formula><p>where</p><formula xml:id="formula_7">∆ = (α + wc r + wc − α − c r − )/(α + wc − α − c )</formula><p>. It can be seen that the new loss function consists of two components: the loss L A on the whole V × V training examples and L P on positive examples. The major computation now lies in L A which has a time complexity of O(k|V | 2 ). In the following, we show how to reduce the huge volume of com- putation by a simple mathematical decouple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decouple</head><p>To clearly show the decouple operation, we rewrite L A as L A by omitting the constant term α − c (r − ) 2 . Note that u wd and˜uand˜ and˜u cd denote the d-th element in U w and˜Uand˜ and˜U c , respectively. </p><p>Now we show our second key design that is based on a decouple manipulation for the inner product operation. Interestingly, we observe that the sum- mation operator and elements in U w and˜Uand˜ and˜U c can be rearranged by the commutative property ( <ref type="bibr" target="#b10">Dai et al., 2007</ref>), as shown below.</p><formula xml:id="formula_9">LA = k d=0 k d =0 w∈V u wd u wd c∈V α − c ˜ u cd˜ucd˜ cd˜u cd − 2r − k d=0 w∈V u wd c∈V α − c ˜ u cd<label>(9)</label></formula><p>An important feature in Eq. <ref type="formula" target="#formula_9">(9)</ref> is that the original inner product terms are disappeared, while in the new equation c∈V α − c ˜ u cd˜ucd˜ cd˜u cd and c∈V α − c ˜ u cd are "constant" values relative to u wd u wd and u wd respectively. This means that they can be pre-calculated before training in each iteration. Specifically, we define p w dd , p c dd , q w d and q c d as the pre-calculated terms</p><formula xml:id="formula_10">p w dd = w∈V u wd u wd q w d = w∈V u wd p c dd = c∈V α − c ˜ u cd˜ucd˜ cd˜u cd q c d = c∈V α − c ˜ u cd<label>(10)</label></formula><p>Then the computation of˜Lof˜ of˜L A can be simplified to k</p><formula xml:id="formula_11">d=0 k d =0 p w dd p c dd − 2r − q w d q c d .</formula><p>It can be seen that the time complexity to com- pute all p w dd is O(|V |k 2 ), and similarly, O(|V |k 2 ) for p c dd and O(|V |k) for q w d and q c d . With all terms pre-calculated before each iteration, the time com- plexity of computing˜Lcomputing˜ computing˜L A is just O(k 2 ). As a result, the total time complexity of computing L A is de- creased to O(2|V |k 2 + 2|V |k + k 2 ) ≈ O(2|V |k 2 ), which is much smaller than the original O(k|V | 2 ). Moreover, it's worth noting that our efficient com- putation for˜Lfor˜ for˜L A is strictly equal to its original value, which means AllVec does not introduce any approximation in evaluating the loss function.</p><p>Finally, we can derive the batch gradients for u wd and˜uand˜ and˜u cd as</p><formula xml:id="formula_12">∂L ∂u wd = k d =0 u wd p c dd − c∈I + w Λ · ˜ u cd − r − q c d ∂L ∂ ˜ u cd = k d =0˜u =0˜ =0˜u cd p w dd α − c − w∈I + c Λ · u wd − r − α − c q w d<label>(11)</label></formula><p>where I + w denotes the set of positive contexts for w, I + c denotes the set of positive words for c and </p><formula xml:id="formula_13">Λ = (α + wc −α − c )(∆−U w ˜ U T c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Time Complexity Analysis</head><p>In the following, we show that AllVec can achieve the same time complexity with negative sampling based SGD methods.</p><p>Given the sample size n, the total time com- plexity for SG is O((n + 1)|S|k), where n + 1 denotes n negative samples and 1 positive exam- ple. Regarding the complexity of AllVec, we can see that the overall complexity of Algorithm 1 is O(4|S|k + 4|V |k 2 ).</p><p>For the ease of discussion, we denote c as the average number of positive contexts for a word in the training corpus, i.e. |S| = c|V | (c ≥ 1000 in most cases). We then obtain the ratio</p><formula xml:id="formula_14">4|S|k + 4|V |k 2 (n + 1)|S|k = 4 n + 1 (1 + k c )<label>(12)</label></formula><p>where k is typically set from 100 to 300 <ref type="bibr" target="#b27">(Mikolov et al., 2013a;</ref><ref type="bibr" target="#b31">Pennington et al., 2014</ref>), resulting in k ≤ c. Hence, we can give the lower and upper bound for the ratio:</p><formula xml:id="formula_15">4 n+1 &lt; 4|S|k+4|V |k 2 (n+1)|S|k = 4 n+1 (1+ k c ) ≤ 8 n+1<label>(13)</label></formula><p>The above analysis suggests that the complexity of AllVec is same as that of SGD with negative sample size between 3 and 7. In fact, considering that c is much larger than k in most datasets, the major cost of AllVec comes from the part 4|S|k (see Section 5.4 for details), which is linear with respect to the number of positive samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments on three popular evalua- tion tasks, namely word analogy ( <ref type="bibr" target="#b27">Mikolov et al., 2013a</ref>), word similarity <ref type="bibr" target="#b11">(Faruqui and Dyer, 2014</ref>) and QVEC ( <ref type="bibr" target="#b40">Tsvetkov et al., 2015)</ref>.</p><p>Word analogy task. The task aims to answer questions like, "a is to b as c is to ?". We adopt the Google testbed 2 which contains 19, 544 such questions in two categories: semantic and syntac- tic. The semantic questions are usually analogies about people or locations, like "king is to man as queen is to ?", while the syntactic questions fo- cus on forms or tenses, e.g., "swimming is to swim as running to ?".</p><p>Word similarity tasks. We perform evalua- tion on six datasets, including MEN ( <ref type="bibr" target="#b7">Bruni et al., 2012</ref>), MC <ref type="bibr" target="#b29">(Miller and Charles, 1991)</ref>, RW (Lu- ong et al., 2013), RG <ref type="bibr" target="#b33">(Rubenstein and Goodenough, 1965)</ref>, WS-353 Similarity (WSim) and Relatedness (WRel) ( <ref type="bibr" target="#b12">Finkelstein et al., 2001</ref>). We compute the spearman rank correlation be- tween the similarity scores calculated based on the trained embeddings and human labeled scores.</p><p>QVEC. QVEC is an intrinsic evaluation met- ric of word embeddings based on the alignment to features extracted from manually crafted lexi- cal resources. QVEC has shown strong correlation with the performance of embeddings in several se- mantic tasks ( <ref type="bibr" target="#b40">Tsvetkov et al., 2015)</ref>.</p><p>We compare AllVec with the following word embedding methods.</p><p>• SG: This is the original skip-gram model with SGD and negative sampling (Mikolov et al., 2013a,b).</p><p>• SGA: This is the skip-gram model with an adaptive sampler ( <ref type="bibr" target="#b9">Chen et al., 2018</ref>). For all baselines, we use the original implementa- tion released by the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Experimental Setup</head><p>We evaluate the performance of AllVec on four real-world corpora, namely Text8 3 , NewsIR 4 , Wiki-sub and Wiki-all. Wiki-sub is a subset of 2017 Wikipedia dump 5 . All corpora have been pre-processed by a standard pipeline (i.e. remov- ing non-textual elements, lowercasing and tok- enization). <ref type="table" target="#tab_1">Table 1</ref> summarizes the statistics of these corpora. To obtain M wc for positive (w, c) pairs, we fol- low GloVe where word pairs that are x words apart contribute 1/x to M wc . The window size is set as win = 8. Regarding α + wc , we set xmax = 100 and ρ = 0.75. For a fair comparison, the embed- ding size k is set as 200 for all models and cor- pora. AllVec can be easily trained by AdaGrad (Zeiler, 2012) like GloVe or Newton-like ( <ref type="bibr" target="#b1">Bayer et al., 2017;</ref><ref type="bibr" target="#b6">Bradley et al., 2011</ref>) second order methods. For models based on negative sampling (i.e. SG, SGA and LexVec), the sample size is set as n = 25 for Text8, n = 10 for NewsIR and n = 5 for Wiki-sub and Wiki-all. The setting is also suggested by <ref type="bibr" target="#b28">Mikolov et al. (2013b)</ref>. Other detailed hyper-parameters are reported in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Accuracy Comparison</head><p>We present results on the word analogy task in <ref type="table">Table 2</ref>. As shown, AllVec achieves the high- est total accuracy (Tot.) in all corpora, particu-larly in smaller corpora (Text8 and NewsIR). The reason is that in smaller corpora the number of positive (w, c) pairs is very limited, thus making use of negative examples will bring more benefits. Similar reason also explains the poor accuracy of GloVe in Text8, because GloVe does not consider negative samples. Even in the very large corpus (Wiki-all), ignoring negative samples still results in sub-optimal performance.</p><p>Our results also show that SGA achieves better performance than SG, which demonstrates the im- portance of a good sampling strategy. However, regardless what sampler (except the full softmax sampling) is utilized and how many updates are taken, sampling is still a biased approach. AllVec achieves the best performance because it is trained on the whole batch data for each parameter update rather than a fraction of sampled data.</p><p>Another interesting observation is AllVec per- forms better in semantic tasks in general. The rea- son is that our model utilizes global co-occurrence statistics, which capture more semantic signals than syntactic signals. While both AllVec and GloVe use global contexts, AllVec performs much better than GloVe in syntactic tasks. We argue that the main reason is because AllVec can dis- till useful signals from negative examples, while GloVe simply ignores all negative information. By contrast, local-window based methods, such as SG and SGA, are more effective to capture local sentence features, resulting in good perfor- mance on syntactic analogies. However, <ref type="bibr" target="#b32">Rekabsaz et al. (2017)</ref> argues that these local-window based methods may suffer from the topic shifting issue. <ref type="table" target="#tab_3">Table 3 and Table 4</ref> provide results in the word similarity and QVEC tasks. We can see that Al- lVec achieves the best performance in most tasks, which admits the advantage of batch learning with all samples. Interestingly, although GloVe per- forms well in semantic analogy tasks, it shows extremely worse results in word similarity and QVEC. The reason shall be the same as that it per- forms poorly in syntactic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Impact of α − c</head><p>In this subsection, we investigate the impact of the proposed weighting scheme for negative (context) words. We show the performance change of word analogy tasks on NewsIR in <ref type="figure">Figure 2</ref> by tuning α 0 and δ. Results in other corpora show similar trends thus are omitted due to space limitation. <ref type="table">Table 2</ref>: Results ("Tot." denotes total accuracy) on the word analogy task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus</head><p>Text8 The parameter columns (para.) for each model are given from left to right as follows. SG: subsampling of frequent words, window size and the number of negative samples; SGA: λ ( <ref type="bibr" target="#b9">Chen et al., 2018</ref>) that controls the distribution of the rank, the other parameters are the same with SG; GloVe: xmax, window size and symmetric window; LexVec: subsampling of frequent words and the number of negative samples; AllVec: the negative weight α0 and δ. Boldface denotes the highest total accuracy. <ref type="figure">Figure 2(a)</ref> shows the impact of the overall weight α 0 by setting δ as 0.75 (inspired by the set- ting of skip-gram). Clearly, we observe that all results (including semantic, syntactic and total ac- curacy) have been greatly improved when α 0 in- creases from 0 to a larger value. As mentioned before, α 0 = 0 means that no negative informa- tion is considered. This observation verifies that negative samples are very important for learning good embeddings. It also helps to explain why GloVe performs poorly on syntactic tasks. In addi- tion, we find that in all corpora the optimal results are usually obtained when α 0 falls in the range of 50 to 400. For example, in the NewIR corpus as shown, AllVec achieves the best performance when α 0 = 100. <ref type="figure">Figure 2(b)</ref> shows the impact of δ with α 0 = 100. As mentioned before, δ = 0 de- notes a uniform value for all negative words and δ = 1 denotes that no smoothing is applied to word frequency. We can see that the total accuracy is only around 55% when δ = 0. By increasing its value, the performance is gradually improved, achieving the highest score when δ is around 0.8. Further increase of δ will degrade the total accu- racy. This analysis demonstrates the effectiveness of the proposed negative weighting scheme. hibits a more stable convergence due to its full batch learning. In contrast, GloVe has a more dramatic fluctuation because of the one-sample learning scheme. <ref type="figure">Figure 3(b)</ref> shows the relation- ship between the embedding size k and runtime on NewsIR. Although the analysis in Section 4.3 demonstrates that the time complexity of AllVec is O(4|S|k + 4|V |k 2 ), the actual runtime shows a near linear relationship with k. This is because 4|V |k 2 /4|S|k = k/c, where c generally ranges from 1000 ∼ 6000 and k is set from 200 to 300 in practice. The above ratio explains the fact that 4|S|k dominates the complexity, which is linear  with k and |S|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Convergence Rate and Runtime</head><p>We also compare the overall runtime of AllVec and SG on NewsIR and show the results in <ref type="table" target="#tab_5">Table  5</ref>. As can be seen, the runtime of AllVec falls in the range of SG-3 and SG-7 in a single iteration, which confirms the theoretical analysis in Section 4.3. In contrast with SG, AllVec needs more itera- tions to converge. The reason is that each parame- ter in SG is updated many times during each iter- ation, although only one training example is used in each update. Despite this, the total run time of AllVec is still in a feasible range. Assuming the convergence is measured by the number of param- eter updates, our AllVec yields a much faster con- vergence rate than the one-sample SG method.</p><p>In practice, the runtime of our model in each it- eration can be further reduced by increasing the number of parallel workers. Although baseline methods like SG and GloVe can also be paral- lelized, the stochastic gradient steps in these meth- ods unnecessarily influence each other as there is no exact way to separate these updates for differ- ent workers. In other words, the parallelization of SGD is not well suited to a large number of work- ers. In contrast, the parameter updates in AllVec are completely independent of each other, there- fore AllVec does not have the update collision is- sue. This means we can achieve the embarrassing parallelization by simply separating the updates by words; that is, letting different workers update the model parameters for disjoint sets of words. As such, AllVec can provide a near linear scaling without any approximation since there is no poten- tial conflicts between updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented AllVec, an efficient batch learning based word embedding model that is capable to leverage all positive and negative training examples without any sampling and ap- proximation. In contrast with models based on SGD and negative sampling, AllVec shows more stable convergence and better embedding quality by the all-sample optimization. Besides, both the- oretical analysis and experiments demonstrate that AllVec achieves the same time complexity with the classic SGD models. In future, we will extend our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model. Moreover, we will integrate prior knowledge, such as the words that are synonyms and antonyms, into the word em- bedding process. Lastly, we are interested in ex- ploring the recent adversarial learning techniques to enhance the robustness of word embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>u</head><label></label><figDesc>wd˜uwd˜ wd˜u cd k d =0 u wd˜uwd˜ wd˜u cd − 2r − w∈V c∈V α − c k d=0 u wd˜uwd˜ wd˜u cd</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 (Figure 2 :Figure 3 :</head><label>323</label><figDesc>Figure 3(a) compares the convergence between AllVec and GloVe on NewsIR. Clearly, AllVec ex</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Corpora statistics. Corpus Tokens Vocab Size</head><label>1</label><figDesc></figDesc><table>Text8 
17M 
71K 
100M 
NewsIR 
78M 
83K 
500M 
Wiki-sub 
0.8B 
190K 
4.0G 
Wiki-all 
2.3B 
200K 13.4G 

• GloVe: This method applies biased MF on 
the positive samples of word co-occurrence 
matrix (Pennington et al., 2014). 
• LexVec: This method applies MF on the 
PPMI matrix. The optimization is done with 
negative sampling and mini-batch gradient 
descent (Salle et al., 2016b). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Results on the word similarity task.</head><label>3</label><figDesc></figDesc><table>Corpus 
Text8 
NewsIR 

MEN MC RW 
RG WSim WRel MEN MC RW 
RG WSim WRel 

SG 
.6868 .6776 .3336 .6904 .7082 .6539 .7293 .7328 .3705 .7184 .7176 .6147 
SGA 
.6885 .6667 .3399 .7035 .7291 .6708 .7409 .7513 .3797 .7508 .7442 .6398 
GloVe .4999 .3349 .2614 .3367 .5168 .5115 .5839 .5637 .2487 .6284 .6029 .5329 
LexVec .6660 .6267 .2935 .6076 .7005 .6862 .7301 .8403 .3614 .8341 .7404 .6545 
AllVec .6966 .6975 .3424 .6588 .7484 .7002 .7407 .7642 .4610 .7753 .7453 .6322 

Wiki-sub 
Wiki-all 

SG 
.7532 .7943 .4250 .7555 .7627 .6563 .7564 .8083 .4311 .7678 .7662 .6485 
SGA 
.7465 .7983 .4296 .7623 .7715 .6560 .7577 .7940 .4379 .7683 .7110 .6488 
GloVe .6898 .6963 .3184 .7041 .6669 .5629 .7370 .7767 .3197 .7499 .7359 .6336 
LexVec .7318 .7591 .4225 .7628 .7292 .6219 .7256 .8219 .4383 .7797 .7548 .6091 
AllVec .7155 .8305 .4667 .7945 .7675 .6459 .7396 .7840 .4966 .7800 .7492 .6518 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Results on QVEC.</head><label>4</label><figDesc></figDesc><table>Qvec 
Text8 NewsIR Wiki-sub Wiki-all 

SG 
.3999 .4182 
.4280 
.4306 
SGA 
.4062 .4159 
.4419 
.4464 
GloVe .3662 .3948 
.4174 
.4206 
LexVec .4211 .4172 
.4332 
.4396 
AllVec .4211 .4319 
.4351 
.4489 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 : Comparison of runtime.</head><label>5</label><figDesc></figDesc><table>Model 
SI 
Iter 
Tot. 

SG-3 
259s 
15 
65m 
SG-7 
521s 
15 
131m 
SG-10 
715s 
15 
179m 
AllVec 
388s 
50 
322m 

SG-n represents n negative samples for skip-gram, 
SI represents the runtime for a single iteration, and 
Tot. denotes the total runtime. All models are of 
embedding size 200 and trained with 8 threads. 

</table></figure>

			<note place="foot" n="1"> The idea here is similar to that used in (He et al., 2016; Li et al., 2016) for a different problem.</note>

			<note place="foot" n="2"> https://code.google.com/archive/p/word2vec/</note>

			<note place="foot" n="3"> http://mattmahoney.net/dc/text8.zip 4 http://research.signalmedia.co/newsir16/signaldataset.html 5 https://dumps.wikimedia.org/enwiki/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization meets word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Ailem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1081" to="1084" />
		</imprint>
	</monogr>
	<note>Aghiles Salah, and Mohamed Nadif</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A generic coordinate descent framework for learning from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Immanuel</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhargav</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling to accelerate training of a neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="page" from="713" to="722" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adaptive sampled softmax with kernel based sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Blanc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00527</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint word representation learning using a corpus and a semantic lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Alsuhaibani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2690" to="2696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Parallel coordinate descent for l1-regularized loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Joseph K Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1105.5379</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namkhanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving word embeddings with convolutional feature learning and subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3144" to="3151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving negative sampling for word representation using self-embedded features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Co-clustering based classification for outof-domain documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="210" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Community evaluation and exchange of word vectors at wordvectors. org</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximating word ranking and negative sampling for word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fajie</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast matrix factorization for online recommendation with implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatseng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Fasttext. zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Incorporating relational knowledge into word representations using subspace regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="506" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swim: A simple word interaction model for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meichun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4026" to="4032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A relaxed ranking-based factor model for recommender system from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1683" to="1689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word embedding revisited: A new representation learning and explicit matrix factorization perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3650" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning semantic word embeddings based on ordinal knowledge constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1501" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Maosong Sun. 2015b. Topical word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and cognitive processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved word representation learning with sememes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2049" to="2058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Word embedding causes topic shifting; exploit global context! In SIGIR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navid</forename><surname>Rekabsaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1105" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Enhancing the lexvec distributed word representation model using positional contexts and external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Salle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Idiart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01283</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Salle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Idiart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00819</idno>
		<title level="m">Matrix factorization using window sampling and negative sampling for improved word representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Jointly learning word embeddings and latent topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoaib</forename><surname>Jameel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Schockaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwun Ping</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Model-based word embeddings from decompositions of count matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1282" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evaluation of word vector representations by subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2049" to="2054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lambdafm: learning optimal ranking with factorization machines using lambda surrogates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="227" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
