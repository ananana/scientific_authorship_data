<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Co-Matching Model for Multi-choice Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information System</orgName>
								<orgName type="institution">Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information System</orgName>
								<orgName type="institution">Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Co-Matching Model for Multi-choice Reading Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="746" to="751"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multi-choice reading comprehension is a challenging task, which involves the matching between a passage and a question-answer pair. This paper proposes a new co-matching approach to this problem , which jointly models whether a passage can match both a question and a candidate answer. Experimental results on the RACE dataset demonstrate that our approach achieves state-of-the-art performance .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Enabling machines to understand natural language text is arguably the ultimate goal of natural lan- guage processing, and the task of machine read- ing comprehension is an intermediate step towards this ultimate goal ( <ref type="bibr" target="#b8">Richardson et al., 2013;</ref><ref type="bibr" target="#b2">Hermann et al., 2015;</ref><ref type="bibr" target="#b3">Hill et al., 2015;</ref><ref type="bibr" target="#b7">Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b6">Nguyen et al., 2016)</ref>. Recently, <ref type="bibr" target="#b5">Lai et al. (2017)</ref> released a new multi-choice machine comprehension dataset called RACE that was ex- tracted from middle and high school English ex- aminations in China. <ref type="figure" target="#fig_1">Figure 1</ref> shows an exam- ple passage and two related questions from RACE.</p><p>The key difference between RACE and previ- ously released machine comprehension datasets (e.g., the CNN/Daily Mail dataset ( <ref type="bibr" target="#b2">Hermann et al., 2015</ref>) and SQuAD ( <ref type="bibr" target="#b7">Rajpurkar et al., 2016)</ref>) is that the answers in RACE often cannot be directly ex- tracted from the given passages, as illustrated by the two example questions (Q1 &amp; Q2) in <ref type="figure" target="#fig_1">Figure 1</ref>. Thus, answering these questions is more challeng- ing and requires more inferences.</p><p>Previous approaches to machine comprehen- sion are usually based on pairwise sequence matching, where either the passage is matched against the sequence that concatenates both the question and a candidate answer ( <ref type="bibr" target="#b17">Yin et al., 2016)</ref>, or the passage is matched against the question alone followed by a second step of selecting an an- swer using the matching result of the first step ( <ref type="bibr" target="#b5">Lai et al., 2017;</ref><ref type="bibr" target="#b18">Zhou et al., 2018)</ref>. However, these approaches may not be suitable for multi-choice reading comprehension since questions and an- swers are often equally important. Matching the passage only against the question may not be meaningful and may lead to loss of information from the original passage, as we can see from the first example question in <ref type="figure" target="#fig_1">Figure 1</ref>. On the other hand, concatenating the question and the answer into a single sequence for matching may not work, either, due to the loss of interaction information between a question and an answer. As illustrated by Q2 in <ref type="figure" target="#fig_1">Figure 1</ref>, the model may need to recog- nize what "he" and "it" in candidate answer (c) refer to in the question, in order to select (c) as the correct answer. This observation of the RACE dataset shows that we face a new challenge of matching sequence triplets (i.e., passage, question and answer) instead of pairwise matching.</p><p>In this paper, we propose a new model to match a question-answer pair to a given passage. Our co- matching approach explicitly treats the question and the candidate answer as two sequences and jointly matches them to the given passage. Specif- ically, for each position in the passage, we com- pute two attention-weighted vectors, where one is from the question and the other from the candi- date answer. Then, two matching representations are constructed: the first one matches the passage with the question while the second one matches the passage with the candidate answer. These two newly constructed matching representations together form a co-matching state. Intuitively, it encodes the locational information of the question and the candidate answer matched to a specific context of the passage. Finally, we apply a hierar-Passage: My father wasn't a king, he was a taxi driver, but I am a prince-Prince Renato II, of the country Pontinha , an island fort on Funchal harbour. In 1903, the king of Portugal sold the land to a wealthy British family, the Blandys, who make Madeira wine. Fourteen years ago the family decided to sell it for just EUR25,000, but nobody wanted to buy it either. I met Blandy at a party and he asked if I'd like to buy the island. Of course I said yes, but I had no money-I was just an art teacher. I tried to find some business partners, who all thought I was crazy. So I sold some of my possessions, put my savings together and bought it. Of course, my family and my friends-all thought I was mad ... If l want to have a national flag, it could be blue today, red tomorrow. ... My family sometimes drops by, and other people come every day because the country is free for tourists to visit ...  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>For the task of multi-choice reading comprehen- sion, the machine is given a passage, a question and a set of candidate answers. The goal is to se- lect the correct answer from the candidates. Let us use P ∈ R d×P , Q ∈ R d×Q and A ∈ R d×A to represent the passage, the question and a candidate answer, respectively, where each word in each se- quence is represented by an embedding vector. d is the dimensionality of the embeddings, and P , Q, and A are the lengths of these sequences. Overall our model works as follows. For each candidate answer, our model constructs a vector that represents the matching of P with both Q and A. The vectors of all candidate answers are then used for answer selection. Because we simultane- ously match P with Q and A, we call this a co- matching model. In Section 2.1 we introduce the word-level co-matching mechanism. Then in Sec- tion 2.2 we introduce a hierarchical aggregation process. Finally in Section 2.3 we present the ob- jective function. An overview of our co-matching model is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Co-matching</head><p>The co-matching part of our model aims to match the passage with the question and the candidate answer at the word-level. Inspired by some previ- ous work ( <ref type="bibr" target="#b13">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b12">Trischler et al., 2016)</ref>, we first use bi-directional LSTMs (Hochre- iter and Schmidhuber, 1997) to pre-process the se- quences as follows:</p><formula xml:id="formula_0">H p = Bi-LSTM(P), H q = Bi-LSTM(Q), H a = Bi-LSTM(A),<label>(1)</label></formula><p>where H p ∈ R l×P , H q ∈ R l×Q and H a ∈ R l×A are the sequences of hidden states generated by the bi-directional LSTMs. We then make use of the at- tention mechanism to match each state in the pas- sage to an aggregated representation of the ques- tion and the candidate answer. The attention vec- tors are computed as follows:</p><formula xml:id="formula_1">G q = SoftMax (W g H q + b g ⊗ e Q ) T H p , G a = SoftMax (W g H a + b g ⊗ e Q ) T H p , H q = H q G q , H a = H a G a ,<label>(2)</label></formula><p>where W g ∈ R l×l and b g ∈ R l are the parame- ters to learn. e Q ∈ R Q is a vector of all 1s and it is used to repeat the bias vector into the matrix. G q ∈ R Q×P and G a ∈ R A×P are the attention  </p><formula xml:id="formula_2">M q = ReLU W m H q H p H q ⊗ H p + b m , M a = ReLU W m H a H p H a ⊗ H p + b m , C = M q M a ,<label>(3)</label></formula><p>where W g ∈ R l×2l and b g ∈ R l are the parame- ters to learn. · · is the column-wise concatenation of two matrices, and · · and · ⊗ · are the element- wise subtraction and multiplication between two matrices, which are used to build better match- ing representations <ref type="bibr" target="#b10">(Tai et al., 2015;</ref>. M q ∈ R l×P represents the match- ing between the hidden states of the passage and the corresponding attention-weighted representa- tions of the question. Similarly, we match the passage with the candidate answer and represent the matching results using M a ∈ R l×P . Finally C ∈ R 2l×P is the concatenation of M q ∈ R l×P and M a ∈ R l×P and represents how each pas- sage state can be matched with the question and the candidate answer. We refer to c ∈ R 2l , which is a single column of C, as a co-matching state that concurrently matches a passage state with both the question and the candidate answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Aggregation</head><p>In order to capture the sentence structure of the passage, we further modify the model presented earlier and build a hierarchical LSTM ( <ref type="bibr" target="#b11">Tang et al., 2015</ref>) on top of the co-matching states. Specifi- cally, we first split the passage into sentences and we use P 1 , P 2 , . . . , P N to represent these sen- tences, where N is the number of sentences in the passage. For each triplet {P n , Q, A}, n ∈ [1, N ], we can get the co-matching states C n through Eqn. <ref type="figure" target="#fig_1">(1-3)</ref>. Then we build a bi-directional LSTM followed by max pooling on top of the co- matching states of each sentence as follows:</p><formula xml:id="formula_3">h s n = MaxPooling (Bi-LSTM (C n )) ,<label>(4)</label></formula><p>where the function MaxPooling(·) is the row-wise max pooling operation. h s n ∈ R l , n ∈ [1, N ] is the sentence-level aggregation of the co-matching states. All these representations will be further integrated by another Bi-LSTM to get the final triplet matching representation.  where H s ∈ R l×N is the concatenation of all the sentence-level representations and it is the input of a higher level LSTM. h t ∈ R l is the final output of the matching between the sequences of the pas- sage, the question and the candidate answer.</p><formula xml:id="formula_4">H s = [h s 1 ; h s 2 ; . . . ; h s N ], h t = MaxPooling (Bi-LSTM (H s )) , (5) RACE-M RACE-H RACE</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Objective function</head><p>For each candidate answer A i , we can build its matching representation h t i ∈ R l with the ques- tion and the passage through Eqn. (5). Our loss function is computed as follows:</p><formula xml:id="formula_5">L(A i |P, Q) = − log exp(w T h t i ) 4 j=1 exp(w T h t j ) ,<label>(6)</label></formula><p>where w ∈ R l is a parameter to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>To evaluate the effectiveness of our hierarchical co-matching model, we use the RACE dataset <ref type="bibr" target="#b5">(Lai et al., 2017)</ref>, which consists of two subsets: RACE-M comes from middle school examinations while RACE-H comes from high school examina- tions. RACE is the combination of the two. We compare our model with a number of base- line models. We also compare with two variants of our model for an ablation study.</p><p>Comparison with Baselines We compare our model with the following baselines:</p><p>• Sliding Window based method ( <ref type="bibr" target="#b8">Richardson et al., 2013</ref>) computes the matching score based on the sum of the tf-idf values of the matched words between the question-answer pair and each sub- passage with a fixed a window size.</p><p>• Stanford Attentive Reader (AR) <ref type="bibr" target="#b0">(Chen et al., 2016</ref>) first builds a question-related passage representation through attention mechanism and then compares it with each candidate answer rep- resentation to get the answer probabilities.</p><p>• GA ( <ref type="bibr" target="#b1">Dhingra et al., 2017</ref>) uses gated atten- tion mechanism with multiple hops to extract the question-related information of the passage and compares it with candidate answers.</p><p>• <ref type="bibr">ElimiNet (Soham et al., 2017</ref>) tries to first eliminate the most irrelevant choices and then se- lect the best answer.</p><p>• HAF (Zhou et al., 2018) considers not only the matching between the three sequences, namely, passage, question and candidate answer, but also the matching between the candidate an- swers.</p><p>• MUSIC ( <ref type="bibr" target="#b16">Xu et al., 2017</ref>) integrates different sequence matching strategies into the model and also adds a unit of multi-step reasoning for select- ing the answer.</p><p>Besides, we also report the following two re- sults as reference points: Turkers is the perfor- mance of Amazon Turkers on a randomly sampled subset of the RACE test set. Ceiling is the percent- age of the unambiguous questions with a correct answer in a subset of the test set.</p><p>The performance of our model together with the baselines are shown in <ref type="table" target="#tab_3">Table 2</ref>. We can see that our proposed complete model, Hier-Co- Matching, achieved the best performance among all the public results. Still, there is a huge gap be- tween the best machine reading performance and the human performance, showing the great poten- tial for further research.</p><p>Ablation Study Moreover, we conduct an abla- tion study of our model architecture. In this study, we are mainly interested in the contribution of each component introduced in this work to our fi- nal results. We studied two key factors: (1) the co- matching module and (2) the hierarchical aggre- gation approach. We observed a 4 percentage per- formance decrease by replacing the co-matching module with a single matching state (i.e., only M a in Eqn (3)) by directly concatenating the question with each candidate answer ( <ref type="bibr" target="#b17">Yin et al., 2016</ref>). We also observe about 2 percentage decrease when we treat the passage as a plain sequence, and run a two-layer LSTM (to ensure the numbers of param- eters are comparable) over the whole passage in- stead of the hierarchical LSTM.</p><p>Question Type Analysis We also conducted an analysis on what types of questions our model can handle better. We find that our model ob- tains similar performance on the "wh" questions such as "why," "what," "when" and "where" ques- tions, on which the performance is usually around 50%. We also check statement-justification ques- tions with the keyword "true" (e.g., "Which of the following statements is true"), negation questions with the keyword "not" (e.g., "which of the fol- lowing is not true"), and summarization questions with the keyword "title" (e.g., "what is the best title for the passage?"), and their performance is 51%, 52% and 48%, respectively. We can see that the performance of our model on different types of questions in the RACE dataset is quite simi- lar. However, our model is only based on word- level matching and may not have the ability of rea- soning. In order to answer questions that require summarization, inference or reasoning, we still need to further explore the dataset and improve the model. Finally, we further compared our model to the baseline, which concatenates the question with each candidate answer, and our model can achieve better performance on different types of questions. For example, on the subset of the ques- tions with pronouns, our model can achieve bet- ter accuracy of 49.8% than 47.9%. Similarly, on statement-justification questions with the keyword "true", our model could achieve better accuracy of 51% than 47%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we proposed a co-matching model for multi-choice reading comprehension. The model consists of a co-matching component and a hierarchical aggregation component. We showed that our model could achieve state-of-the-art per- formance on the RACE dataset. In the future, we will adapt the idea of co-matching and hierarchical aggregation to the standard open-domain QA set- ting for answer candidate reranking ( . We will also further study how to explic- itly model inference and reasoning on the RACE dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Q1:</head><label></label><figDesc>Which statement of the following is true? Q2: How did the author get the island? a. The author made his living by driving. a. It was a present from Blandy. b. The author's wife supported to buy the island. b. The king sold it to him. c. Blue and red are the main colors of his national flag. c. He bought it from Blandy. d. People can travel around the island free of charge. d. He inherited from his father.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of the model that builds a matching representation for a triplet {P, Q, A} (i.e., passage, question and candidate answer).</figDesc><graphic url="image-1.png" coords="3,81.41,62.81,432.00,232.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>An example passage and two related multi-choice questions. The ground-truth answers are in 
bold. 

chical LSTM (Tang et al., 2015) over the sequence 
of co-matching states at different positions of the 
passage. Information is aggregated from word-
level to sentence-level and then from sentence-
level to document-level. In this way, our model 
can better deal with the questions that require evi-
dence scattered in different sentences in the pas-
sage. Our model improves the state-of-the-art 
model by 3 percentage on the RACE dataset. Our 
code will be released under https://github. 
com/shuohangwang/comatch. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experiment Results.  *  means it's signifi-
cant to the models ablating either the hierarchical 
aggregation or co-matching state. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgement</head><p>This work was partially supported by DSO grant DSOCL15223.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A thorough examination of the CNN/Daily Mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Association for Computational Linguistics</title>
		<meeting>the Conference on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Association for Computational Linguistics</title>
		<meeting>the Conference on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Advances in Neural Information Processing Systems</title>
		<meeting>the Conference on Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RACE: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m">MS MARCO: A human generated machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MCTest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Eliminet: A model for eliminating options for reading comprehension with multiple choice questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikh</forename><surname>Soham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Ananya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nema</forename><surname>Preksha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Openreview</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Association for Computational Linguistics</title>
		<meeting>the Conference on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A parallel-hierarchical model for machine comprehension on sparse data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Association for Computational Linguistics</title>
		<meeting>the Conference on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning natural language inference with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on the North American Chapter</title>
		<meeting>the Conference on the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05116</idno>
		<title level="m">Evidence aggregation for answer re-ranking in open-domain question answering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards human-level machine reading comprehension: Reasoning and inference with multiple strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04964</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Attention-based convolutional neural network for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.04341</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical attention flow for multiplechoice reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Furu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
