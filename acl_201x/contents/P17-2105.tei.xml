<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Character-Aware Neural Morphological Disambiguation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alymzhan</forename><surname>Toleu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gulmira</forename><surname>Tolegen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aibek</forename><surname>Makazhanov</surname></persName>
						</author>
						<title level="a" type="main">Character-Aware Neural Morphological Disambiguation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="666" to="671"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2105</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We develop a language-independent, deep learning-based approach to the task of morphological disambiguation. Guided by the intuition that the correct analysis should be &quot;most similar&quot; to the context, we propose dense representations for morphological analyses and surface context and a simple yet effective way of combining the two to perform disambiguation. Our approach improves on the language-dependent state of the art for two agglu-tinative languages (Turkish and Kazakh) and can be potentially applied to other morphologically complex languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morphological disambiguation (MD) is a long standing problem in processing morphologically complex languages (MCL). POS tagging is a somewhat related problem, however in MD, in addition to POS tags, one typically has to pre- dict lemmata (roots hereinafter) that surface forms stem from and morphemes <ref type="bibr">1</ref>   <ref type="bibr" target="#b4">(Hakkani-Tür et al., 2002</ref>). Thus, if one counts analyses as tags, MD can be cast as a tagging prob- lem with an extremely large tagset. This fact dis- courages direct application of the state of the art approaches designed for small fixed tagsets.</p><p>To develop a language independent dense repre- sentation of the analyses, we segment 2 an analysis <ref type="bibr">1</ref> We use the term morpheme for its universal recognition within the community. A more appropriate term might be grammeme, i.e. a value of grammatical category.</p><p>2 Such a segmentation is denoted by the squared brackets numbered in the respective order (cf. Turkish example).</p><p>into (i) the root, (ii) its POS and (iii) the morpheme chain (MC). We then proceed to jointly learn the embeddigns for the root and the POS segments and to combine them and the MC segment representa- tion into a single dense representation. MC seg- ments are represented as binary vectors that, for a given analysis, encode presence or absence of each morpheme found in the train set. This en- sures language independence and contrasts previ- ous work (at least on Turkish and Kazakh), where only certain morphemes are chosen as features de- pending on their position ( <ref type="bibr" target="#b0">Assylbekov et al., 2016;</ref><ref type="bibr" target="#b4">Hakkani-Tür et al., 2002</ref>) or presence <ref type="bibr" target="#b9">(Makhambetov et al., 2015</ref>) in an analysis, or the authors' intuition ( <ref type="bibr" target="#b17">Yildiz et al., 2016;</ref><ref type="bibr" target="#b14">Tolegen et al., 2016;</ref><ref type="bibr" target="#b12">Sak et al., 2007)</ref>.</p><p>Apart from the sparseness of analyses distribu- tion MCL notoriously raise free word order and long dependency issues. Thus, decoding analysis sequences using only the leftmost context may not be enough. To address this we leverage the right- most context as well. We model the left-and right- most surface context in two ways: using (i) BiL- STM ( <ref type="bibr" target="#b3">Greff et al., 2015</ref>) with a character-based sub-layer ( <ref type="bibr" target="#b6">Ling et al., 2015)</ref> and (ii) with a feed forward network on word embeddings. We then entertain the idea that given a word with multiple analyses and its surface context, the correct analy- sis might be "closer" to the context. Following our intuition, we have tried computing the distance be- tween the analysis and the context representations, and a simple dot product (as in unnormalized co- sine similarity) has yielded the best performance.</p><p>We evaluate our approach on Turkish and Kazakh data sets, using several baselines (includ- ing the state of the art methods for both languages) and a variety of settings and metrics. In terms of general accuracy our approach has achieved a nearly 1% improvement over the state of the art for Turkish and a marginal improvement for Kazakh.</p><p>Our contribution amounts to the following: (i) a general MD framework for MCL that can be ana- lyzed in &lt;root, POS, MC&gt; triplets; (ii) improve- ment on language-dependent state of the art for Turkish and Kazakh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>In this section we describe our approach to encod- ing morphological analyses and the context into the embeddings and combining them to perform morphological disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Morphological Representation</head><p>We treat a morphological analysis as a combina- tion of three main constituents: the root, its POS and the morpheme chain. These constituents are represented as d r , d p , and d m -dimensional vectors respectively. The former two vectors correspond to dense word embeddings <ref type="bibr" target="#b2">(Collobert et al., 2011)</ref>, and the latter is a binary vector which encodes the presence of a certain morpheme in the chain. The size of the binary vector, d m , is equal to the size of the morpheme dictionary obtained from the data.</p><p>Given a sentence and the j-th surface word form with N analyses, we represent the k-th analysis as:</p><formula xml:id="formula_0">A k j = tanh(W r r k + W p p k + W m m k )<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">A j ∈ R d h ×|N | , d h is the dimension of each analysis embedding, r k ∈ R dr×1 , p k ∈ R dp×1</formula><p>, m k ∈ {0, 1} dp×1 are constituent vectors of the k-th analysis, and</p><formula xml:id="formula_2">W r ∈ R d h ×dr , W p ∈ R d h ×dp ,W m ∈ R d h ×dm are the model parame- ters.</formula><p>The bias term was left out for clarity. This representation is shown on <ref type="figure">Figure 1</ref> (bottom).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recurrent Neural Disambiguation</head><p>The model architecture is shown on <ref type="figure">Figure 1</ref>. It consists of two main blocks that learn the surface context (top) and the morphological analyses rep- resentations (bottom). When it comes to modeling context via word embeddings for morphologically complex lan- guages, it is impractical to actually store vectors for all words, since majority of words in such languages has a large number of surface realiza- tions. Our solution to this problem is to con- struct a surface word representation from charac- ters that not only reduces data sparseness, but also help in dealing with the out-of-vocabulary (OOV) words ( <ref type="bibr" target="#b6">Ling et al., 2015)</ref>. We represent each char- acter of each word as a vector x i ∈ R dc and the Hidden layer:</p><formula xml:id="formula_3">A j k = tanh(W r r k + W p p k +W m m k ) LSTM hidden layer C a t Cat sat on the mat . . . . . . . . . root pos MC Characters Context Input layer: [r k , p k , m k ] Morphological representation . . . LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM . . . . . . . . . S j = tanh(D c h c (j)+ D a L j +b j ) Similarity h w = [h f ,h b ] h c (j)</formula><p>Figure 1: Model architecture entire embedding matrix as E c ∈ R dc×|C| , where C is the character vocabulary extracted from the training set including alphanumeric characters and other possible symbols. Given an input surface word w i with its character embeddings x 1 , ..., x n , the hidden state h t at the time step t can be com- puted via the following Vanilla LSTM calcula- tions:</p><formula xml:id="formula_4">i t = σ(W i x t + U i h t−1 + b i ) (2) f t = σ(W f x t + U f h t−1 + b f ) (3) o t = σ(W o x t + U o h t−1 + b o ) (4) z t = tanh(W z x t + U z h t−1 + b z ) (5) c t = f t c t−1 + i t z t (6) h t = o t tanh(c t )<label>(7)</label></formula><p>where σ(·) and tanh(·) are the non-linear func- tions. i t , f t , o t are referred to three gates: input, forget, output that control the information flow of inputs. Parameters of the LSTM are W * , U * , b * , where * can be any of {i, f, o, g}. The peephole connections were left out for clarity. We use both forward and backward LSTM to learn word representations obtained by concatena- tion of the last states in both direction h f and h b , e.g. h w = h f , h b . Character-based word em- beddings obtained in this manner do not yet con-tain the context information on a sentence level. Thus, we adopt another LSTM to learn context- sensitive information for each word in both direc- tions. We denote the concatenation of the em- beddings learned from the forward and backward LSTM states as h c (j) ∈ R 2hs×1 (where hs is the output size for the j-th word), and represent sur- face context as:</p><formula xml:id="formula_5">S j = tanh(D c h c (j) + b j )<label>(8)</label></formula><p>where</p><formula xml:id="formula_6">S j ∈ R d h ×1 is a hidden layer output, h c (j) ∈ R 2hs×1</formula><p>is a context vector of the j-th word, and b j ∈ R d h ×1 is a bias term.</p><p>For the final prediction, we score each analysis by computing the inner product between its repre- sentation and the context's representations:</p><formula xml:id="formula_7">P k j = S j · A k j (9)</formula><p>where S j and A k j are computed as equations <ref type="formula" target="#formula_5">(8)</ref> and <ref type="formula" target="#formula_0">(1)</ref> respectively. We normalize the obtained scores using softmax and choose the analysis with the maximum score as the correct one. In what follows we refer to this model as BiLSTM.</p><p>Finally, in a separate setting, in addition to the surface context in the hidden layer we also incor- porate the immediate (left and right) morphologi- cal context in the form of the average of the anal- yses representations:</p><formula xml:id="formula_8">S * j = tanh(D c h c (j) + D a L j + b j )<label>(10)</label></formula><p>where </p><formula xml:id="formula_9">L j ∈ R 2d h ×1 is concatenation</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>In all models, the top layer of the networks has a softmax that computes the normalized scores over morphological candidates given the input word. The networks are trained to minimize the cross entropy of the predicted and true morphological analyses. Back-propagation is employed to com- pute the gradient of the corresponding object func- tion with respect to the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Sets</head><p>We conduct our experiments on Kazakh (Assyl- bekov et al., 2016) and Turkish ( <ref type="bibr" target="#b18">Yuret and Türe, 2006</ref>) data sets <ref type="bibr">3</ref> . <ref type="table">Table 1</ref> shows the corpora statis- tics. Kazakh data set is almost 50 times smaller than that of Turkish, with four times the OOV rate and almost twice as many analyses per word on average. Given such a drastic difference in the resources it would be interesting to see how our models perform on otherwise similar languages (both Turkic). Lastly, while the corpora provide train and test splits, there are no tuning sets, so we withdraw small portions from the training sets for tuning hyper-parameters 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We compare our models to three other approaches. For Kazakh we use an HMM based tagger and its version extended with the rule-based constraint grammar ( <ref type="bibr" target="#b0">Assylbekov et al., 2016)</ref>, which is con- sidered the state of the art for the language. We <ref type="bibr">3</ref> For Turkish, we used a test set that was manually re- annotated by <ref type="bibr" target="#b17">Yildiz et al. (2016)</ref>. <ref type="bibr">4</ref> The following hyper-parameters are used in all the ex- periments: character embedding size dc = 35, character and context LSTM states are 50, root and POS embedding sizes are all set to 50, hidden layer size d h = 200, learning rate is set to 0.01. The window size of DNN is set to 5. For regularization we use dropout ( <ref type="bibr" target="#b13">Srivastava et al., 2014</ref>) with probability 0.5 on the hidden layers. We further constrain the norm of gradient to be below 2 by using gradient clipping.   <ref type="bibr" target="#b1">(Collins, 2002</ref>) based tagger. We use our implementation of this baseline for Kazakh and the model devel- oped by <ref type="bibr" target="#b12">Sak et al. (2007)</ref> for Turkish. Lastly, we use a neural network model proposed by <ref type="bibr" target="#b17">Yildiz et al. (2016)</ref>, which is considered state of the art for Turkish. For this baseline too we use our own implementation (for both languages) and refer to it as MANN 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Setup</head><p>As described in the previous section, each of our models has two settings: the one that does not in- corporate surrounding morphological context and the one that does (the starred one). In addition to that we use pre-trained embeddings, by training word2vec ( <ref type="bibr" target="#b11">Mikolov et al., 2013</ref>) skip-gram model on Wikipedia texts. This setting is denoted by a double dagger ( ‡). We perform a single run evaluation in terms of token-and sentence-based accuracy. We consider four types of tokens: (i) all tokens; (ii) ambiguous tokens (the ones with at least two analyses); (iii) OOV tokens; (iv) ambiguous OOV tokens. Thus, we use a total of five metrics. In terms of strictness we deem correct only the predictions that match the golden truth completely, i.e. in root, POS and MC (up to a single morpheme tag). <ref type="bibr">5</ref> Note that all of the baselines are language dependent to a certain degree, with MANN being the least dependent and HMMCG the most. The latter baseline employs hand- engineered constraint grammar rules to perform initial disam- biguation, followed by application of the HMM tagger, which cherry-picks the most informative grammatical features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results and Discussion</head><p>The results are given in <ref type="table" target="#tab_3">Table 2</ref>. Unless stated oth- erwise we refer to the general (all tokens) accuracy when comparing model performances.</p><p>For Kazakh, DNN conditioned on the leftmost analysis yields 86.33% accuracy. DNN* that in addition uses the rightmost analysis embeddings, improves almost 1% over that result (87.25%). On the other hand BiLSTM, whose context repre- sentation uses surface forms only, performs even better (87.49%). When this model incorporates immediate morphological context, it (BiLSTM*) performs at 90.92% and beats the HMMCG base- line. However, the latter being a very strong lan- guage dependent baseline still outperforms our model in ambiguous OOV and sentence accuracy. When we evaluate our model under equal condi- tions (BiLSTM* ‡+CG) it beats HMMCG on all of the metrics. We separate this comparison from the rest because of a language-dependent set up.</p><p>In contrast, for Turkish DNN models outper- form BiLSTM on seen tokens and yield an al- most equal 92.2% accuracy regardless of using the rightmost morphological context. This perfor- mance is also higher than that of all baselines, in- cluding the state of the art MANN. However BiL- STM* is still better than DNN* in OOV token ac- curacy, both overall and ambiguous.</p><p>As it can be seen, pre-training boosts the per- formance of DNN* and BiLSTM* across all met- rics. For Kazakh pre-training results in .14% improvement in general token accuracy for BiL- STM*, which amounts to .67% improvement over the state of the art. For Turkish this results in an almost 1% net improvement in overall token accu- racy over MANN, the state of the art <ref type="bibr">6</ref> .</p><p>A cross-linguistic comparison reveals that al- though Kazakh data set is much smaller than that of Turkish and has more analyses per word on average and higher OOV rate, on certain met- rics the models perform on par or even better for Kazakh <ref type="bibr">7</ref> . To investigate this further we have made data sets comparable in size by randomly choos- ing 20.6K+ and 3.4K from Turkish training and test sets. On this data BiLSTM* ‡ yields 91.18, 82.0% general and ambiguous token accuracy and respective scores for OOV are 87.0, 74.6%. This result follows the pattern, where for Turkish only the general accuracy is higher than that of Kazakh. It turns out that Turkish data contains many unam- biguous tokens: 49% and 48% for full and small data sets (train + test average), against 36% for Kazakh. This suggests that the higher general ac- curacy on Turkish data can be explained by the higher rate of the unambiguous tokens. Also Turk- ish has a more complex derivational morphology, which "lengthens" the analyses, e.g. an average number of morphemes per analysis is higher for Turkish (5.25) than for Kazakh (4.6). This adds sparseness to the morpheme chains and certainly further complicates disambiguation, especially in an OOV scenario.</p><p>We also observe that BiLSTM* ‡ works best on all metrics for Kazakh, but for Turkish it beats DNN* ‡ only on the OOV part. Due to BiLSTM* ‡ being computationally prohibitive we ran it with significantly less number of epochs than DNN, and it also being a character-based model, we speculate that it was able to learn character aware context embeddings hence better at OOV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>A morphology-aware NN (MANN) for MD was proposed by <ref type="bibr" target="#b17">Yildiz et al. (2016)</ref>, and has been reported to achieve ambiguous token accuracies of 84.12, 88.35 and 93.78% for Turkish, Finish and Hungarian respectively. This approach dif- fers from ours in a number of ways. (i) Our <ref type="bibr">6</ref> For the un-pretrained model original work reports 84.12% accuracy on ambiguous tokens ( <ref type="bibr" target="#b17">Yildiz et al., 2016)</ref>, which is lower than 84.14% that un-pretrained DNN achieves on this metric. <ref type="bibr">7</ref> For instance, BiLSTM* ‡ applied to Kazakh performs better than any other model for Turkish in terms of sentence, ambiguous and OOV token accuracy. Moreover all of the models (including the baselines) perform better on Kazakh in terms of ambiguous OOV accuracy. analysis representation treats morpheme tags in a language-independent manner considering every tag found in the training set, whereas in MANN certain tags are chosen with a specific language in mind. (ii) MANN is a feed-forward NN that, unlike our approach, does not account for the sur- face context. (iii) As we understood, at the de- coding step MANN makes use of the golden truth, whereas our models have no need for that.</p><p>Although several statistical models have been proposed for Kazakh MD, such as HMM-( <ref type="bibr" target="#b7">Makazhanov et al., 2014;</ref><ref type="bibr" target="#b9">Makhambetov et al., 2015;</ref><ref type="bibr" target="#b0">Assylbekov et al., 2016)</ref>, voted perceptron-( <ref type="bibr" target="#b14">Tolegen et al., 2016)</ref> and transformation-based ( <ref type="bibr" target="#b5">Kessikbayeva and Cicekli, 2016</ref>) taggers, to our knowledge ours is the first deep learning-based approach to the problem that is also purely language independent.</p><p>It is becoming increasingly popular to use richer architectures to learn better embeddings from characters/words <ref type="bibr" target="#b16">(Yessenbayev and Makazhanov, 2016;</ref><ref type="bibr" target="#b6">Ling et al., 2015;</ref><ref type="bibr" target="#b15">Wieting et al., 2016)</ref>. <ref type="bibr" target="#b6">Ling et al. (2015)</ref> used a BiLSTM to learn word vectors, showing strong performance on language model- ing and POS tagging. <ref type="bibr" target="#b10">Melamud et al. (2016)</ref> proposed context2vec, a BiLSTM based model to learn context embedding of target words and achieved state-of-the-art results on sentence com- pletion and word sense disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a general MD framework for MCL that can be analyzed in &lt;root, POS, MC&gt; triplets. We have showed that the surface context can be useful to MD, especially if combined with morphological context. Our next step would be to assess our claims on a larger number of typologi- cally distant languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Results: here, tok. acc. and tok. amb. acc. denote the accuracy over all and ambiguous tokens respectively. Same goes for OOV acc. and OOV amb. acc.. Sentence accuracy is denoted as sen. acc.. refer to these baselines as HMM and HMMCG. Another baseline is a voted perceptron</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been conducted under the tar-geted program O.0743 (0115PK02473) of the Committee of Science of the Ministry of Educa-tion and Science of the Republic of Kazakhstan, and the research grant 129-2017/022-2017 of the Nazarbayev University.</p><p>The authors would like to thank Xiaoqing Zheng for tremendously helpful discussions, as well as Eray Yildiz and Zhenisbek Assylbekov for the data sets used in this study and prompt replies to all questions regarding those.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A free/open-source hybrid morphological disambiguation tool for Kazakh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenisbek</forename><surname>Assylbekov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Washington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assulan</forename><surname>Nurkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Sundetova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidana</forename><surname>Karibayeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balzhan</forename><surname>Abduali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Amirova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TurCLing 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the ACL-02 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA, EMNLP &apos;02</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural Language Processing (Almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">LSTM: A Search Space Odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno>CoRR abs/1503.04069</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical Morphological Disambiguation for Agglutinative Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dilek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gökhan</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and the Humanities</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="381" to="410" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Rule Based Morphological Analyzer and a Morphological Disambiguator for Kazakh Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gulshat</forename><surname>Kessikbayeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilyas</forename><surname>Cicekli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistics and Literature Studies</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="96" to="104" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lus</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Lus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On certain aspects of Kazakh part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aibek</forename><surname>Makazhanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhandos</forename><surname>Yessenbayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Sabyrgaliyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuar</forename><surname>Sharafudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olzhas</forename><surname>Makhambetov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Application of Information and Communication Technologies</title>
		<imprint>
			<publisher>AICT</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">IEEE 8th International Conference on</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data-Driven Morphological Analysis and Disambiguation for Kazakh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olzhas</forename><surname>Makhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aibek</forename><surname>Makazhanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Sabyrgaliyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhandos</forename><surname>Yessenbayev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing-16th International Conference, CICLing</title>
		<meeting><address><addrLine>Cairo, Egypt</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1420" />
			<biblScope unit="page" from="151" to="163" />
		</imprint>
	</monogr>
	<note>Proceedings Part I</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">context2vec: Learning Generic Context Embedding with Bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Morphological Disambiguation of Turkish Text with Perceptron Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Has¸imhas¸im</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tunga</forename><surname>Güngör</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Saraçlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CICLing</title>
		<meeting>CICLing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4394</biblScope>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Named Entity Recognition for Kazakh Using Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gulmira</forename><surname>Tolegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alymzhan</forename><surname>Toleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xiaoqing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4-th International Conference on Computer Processing of Turkic Languages TurkLang</title>
		<meeting>the 4-th International Conference on Computer Processing of Turkic Languages TurkLang</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="122" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Charagram: Embedding Words and Sentences via Character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1504" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Character-based Feature Extraction with LSTM Networks for POS-tagging Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhandos</forename><surname>Yessenbayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aibek</forename><surname>Makazhanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Application of Information and Communication Technologies (AICT)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="62" to="66" />
		</imprint>
	</monogr>
	<note>IEEE 10th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Morphology-Aware Network for Morphological Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eray</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Tirkaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bahadir Sahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Tolga Eren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><forename type="middle">Ozan</forename><surname>Sonmez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2863" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Morphological Disambiguation Rules for Turkish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Türe</surname></persName>
		</author>
		<idno>HLT-NAACL &apos;06</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="328" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning for Chinese word segmentation and POS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
