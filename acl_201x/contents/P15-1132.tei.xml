<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Tag Embeddings and Tag-specific Composition Functions in Recursive Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn , yang.liu@samsung.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Samsung R&amp;D Institute Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
							<email>xuan.zhu@samsung.com , zxy-dcs@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Samsung R&amp;D Institute Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="department" key="dep2">National Lab. for Information Science and Technology</orgName>
								<orgName type="department" key="dep3">Dept. of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Tag Embeddings and Tag-specific Composition Functions in Recursive Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1365" to="1374"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recursive neural network is one of the most successful deep learning models for natural language processing due to the compositional nature of text. The model recursively composes the vector of a parent phrase from those of child words or phrases, with a key component named composition function. Although a variety of composition functions have been proposed, the syntactic information has not been fully encoded in the composition process. We propose two models, Tag Guided RNN (TG-RNN for short) which chooses a composition function according to the part-of-speech tag of a phrase, and Tag Embedded RNN/RNTN (TE-RNN/RNTN for short) which learns tag embeddings and then combines tag and word embeddings together. In the fine-grained sentiment classification, experiment results show the proposed models obtain remarkable improvement: TG-RNN/TE-RNN obtain remarkable improvement over baselines, TE-RNTN obtains the second best result among all the top performing models, and all the proposed models have much less parameters/complexity than their counterparts .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Among a variety of deep learning models for nat- ural language processing, Recursive Neural Net- work (RNN) may be one of the most popular mod- els. Thanks to the compositional nature of natu- ral text, recursive neural network utilizes the re- cursive structure of the input such as a phrase or sentence, and has shown to be very effective for many natural language processing tasks including semantic relationship classification <ref type="bibr" target="#b20">(Socher et al., 2012)</ref>, syntactic parsing ( <ref type="bibr" target="#b21">Socher et al., 2013a</ref>), sentiment analysis ( <ref type="bibr" target="#b22">Socher et al., 2013b)</ref>, and ma- chine translation ( <ref type="bibr" target="#b9">Li et al., 2013)</ref>.</p><p>The key component of RNN and its variants is the composition function: how to compose the vector representation for a longer text from the vector of its child words or phrases. For instance, as shown in <ref type="figure" target="#fig_3">Figure 2</ref>, the vector of 'is very inter- esting' can be composed from the vector of the left node 'is' and that of the right node 'very interest- ing'. It's worth to mention again, the composition process is conducted with the syntactic structure of the text, making RNN more interpretable than other deep learning models.   is composed from the vectors of node 'very' and node 'interesting'. Similarly, the node 'is very in- teresting' is composed from the phrase node 'very interesting' and the word node 'is' .</p><p>There are various attempts to design the com- position function in RNN (or related models). In RNN <ref type="bibr" target="#b19">(Socher et al., 2011</ref>), a global matrix is used to linearly combine the elements of vectors. In RNTN ( <ref type="bibr" target="#b22">Socher et al., 2013b</ref>), a global tensor is used to compute the tensor products of dimen- sions to favor the association between different el-ements of the vectors. Sometimes it is challeng- ing to find a single function to model the compo- sition process. As an alternative, multiple com- position functions can be used. For instance, in MV-RNN <ref type="bibr" target="#b20">(Socher et al., 2012</ref>), different matrices is designed for different words though the model is suffered from too much parameters. In AdaMC RNN/RNTN ( <ref type="bibr" target="#b2">Dong et al., 2014</ref>), a fixed number of composition functions is linearly combined and the weight for each function is adaptively learned.</p><p>In spite of the success of RNN and its variants, the syntactic knowledge of the text is not yet fully employed in these models. Two ideas are moti- vated by the example shown in <ref type="figure" target="#fig_3">Figure 2</ref>: First, the composition function for the noun phrase 'the movie/NP' should be different from that for the adjective phrase 'very interesting/ADJP' since the two phrases are quite syntactically different. More specifically to sentiment analysis, a noun phrase is much less likely to express sentiment than an ad- jective phrase. There are two notable works men- tioned here: <ref type="bibr" target="#b21">(Socher et al., 2013a</ref>) presented to combine the parsing and composition processes, but the purpose is for parsing; <ref type="bibr">(Hermann and Blunsom, 2013</ref>) designed composition functions according to the combinatory rules and categories in CCG grammar, however, only marginal im- provement against Naive Bayes was reported. Our proposed model, tag guided RNN (TG-RNN), is designed to use the syntactic tag of the parent phrase to guide the composition process from the child nodes. As an example, we design a function for composing noun phrase (NP) and another one for adjective phrase (ADJP). This simple strategy obtains remarkable improvements against strong baselines. Second, when composing the adjective phrase 'very interesting/ADJP' from the left node 'very/RB' and the right node 'interesting/JJ', the right node is obviously more important than the left one. Furthermore, the right node 'interest- ing/JJ' apparently contributes more to sentiment expression. To address this issue, we propose <ref type="table">Tag embedded RNN/RNTN (TE-RNN/RNTN)</ref>, to learn an embedding vector for each word/phrase tag, and concatenate the tag vector with the word/phrase vector as input to the composition function. For instance, we have tag vectors for DT,NN,RB,JJ,ADJP,NP, etc. and the tag vectors are then used in composing the parent's vector. The proposed TE-RNTN obtain the second best re- sult among all the top performing models but with much less parameters and complexity. To the best of our knowledge, this is the first time that tag em- bedding is proposed.</p><p>To summarize, the contributions of our work are as follows:</p><p>â€¢ We propose tag-guided composition func- tions in recursive neural network, TG-RNN. Tag-guided RNN allocates a composition function for a phrase according to the part- of-speech tag of the phrase.</p><p>â€¢ We propose to learn embedding vectors for part-of-speech tags of words/phrases, and integrate the tag embeddings in RNN and RNTN respectively. The two models, TE- RNN and TE-RNTN, can leverage the syn- tactic information of child nodes when gen- erating the vector of parent nodes.</p><p>â€¢ The proposed models are efficient and effec- tive. The scale of the parameters is well con- trolled. Experimental results on the Stanford Sentiment Treebank corpus show the effec- tiveness of the models. TE-RNTN obtains the second best result among all publicly re- ported approaches, but with much less pa- rameters and complexity.</p><p>The rest of the paper is structured as follows: in Section 2, we survey related work. In Section 3, we introduce the traditional recursive neural net- work as background. We present our ideas in Sec- tion 4. The experiments are introduced in Section 5. We summarize the work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Different kinds of representations are used in sentiment analysis. Traditionally, the bag-of- words representations are used for sentiment anal- ysis ( <ref type="bibr" target="#b15">Pang and Lee, 2008)</ref>. To exploit the rela- tionship between words, word co-occurrence <ref type="bibr" target="#b23">(Turney et al., 2010</ref>) and syntactic contexts <ref type="bibr" target="#b13">(PadÃ³ and Lapata, 2007</ref>) are considered. In order to distinguish antonyms with similar contexts, neu- ral word vectors ( <ref type="bibr" target="#b1">Bengio et al., 2003</ref>) are pro- posed and can be learnt in an unsupervised man- ner. <ref type="bibr">Word2vec (Mikolov et al., 2013a</ref>) introduces a simpler network structure making computation more efficiently and makes billions of samples feasible for training.</p><p>Semantic composition deals with representing a longer text from its shorter components, which is extensively studied recently. In many previ- ous works, a phrase vector is usually obtained by average <ref type="bibr" target="#b7">(Landauer and Dumais, 1997</ref>), addition, element-wise multiplication <ref type="bibr" target="#b12">(Mitchell and Lapata, 2008</ref>) or tensor product <ref type="bibr" target="#b18">(Smolensky, 1990)</ref> of word vectors. In addition to using vector repre- sentations, matrices can also be used to represent phrases and the composition process can be done through matrix multiplication <ref type="bibr" target="#b16">(Rudolph and Giesbrecht, 2010;</ref><ref type="bibr" target="#b25">Yessenalina and Cardie, 2011</ref>).</p><p>Recursive neural models utilize the recursive structure (usually a parse tree) of a phrase or sen- tence for semantic composition. In Recursive Neural Network <ref type="bibr" target="#b19">(Socher et al., 2011</ref>), the tree with the least reconstruction error is built and the vectors for interior nodes is composed by a global matrix. Matrix-Vector Recursive Neural Network (MV-RNN) <ref type="bibr" target="#b20">(Socher et al., 2012</ref>) assigns matri- ces for every words so that it could capture the relationship between two children. In Recursive Neural Tensor Networks (RNTN) <ref type="bibr" target="#b22">(Socher et al., 2013b)</ref>, the composition process is performed on a parse tree in which every node is annotated with fine-grained sentiment labels, and a global tensor is used for composition. Adaptive Multi- Compositionality ( <ref type="bibr" target="#b2">Dong et al., 2014</ref>) uses multiple weighted composition matrices instead of sharing a single matrix.</p><p>The employment of syntactic information in RNN is still in its infant. In ( <ref type="bibr" target="#b21">Socher et al., 2013a</ref>), the part-of-speech tag of child nodes is considered in combining the processes of both composition and parsing. The main purpose is for better pars- ing by employing RNN, but it is not designed for sentiment analysis. In ( <ref type="bibr">Hermann and Blunsom, 2013)</ref>, the authors designed composition functions according to the combinatory rules and categories in CCG grammar. However, only marginal im- provement against Naive Bayes was reported. Un- like ( <ref type="bibr">Hermann and Blunsom, 2013)</ref>, our TG-RNN obtains remarkable improvements against strong baselines, and we are the first to propose tag em- bedded RNTN which obtains the second best re- sult among all reported approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background: Recursive Neural Models</head><p>In recursive neural models, the vector of a longer text (e.g., sentence) is composed from those of its shorter components (e.g., words or phrases). To compose a sentence vector through word/phrase vectors, a binary parse tree has to be built with a parser. The leaf nodes represent words and in- terior nodes represent phrases. Vectors of interior nodes are computed recursively by composition of child nodes' vectors. Specially, the root vector is regarded as the sentence representation. The com- position process is shown in <ref type="figure" target="#fig_2">Figure 1</ref>.</p><p>More formally, vector v i âˆˆ R d for node i is calculated via:</p><formula xml:id="formula_0">v i = f (g(v l i , v r i ))<label>(1)</label></formula><p>where v l i and v r i are child vectors, g is a compo- sition function, and f is a nonlinearity function, usually tanh. Different recursive neural models mainly differ in composition function. For exam- ple, the composition function for RNN is as below:</p><formula xml:id="formula_1">g(v l i , v r i ) = W [ v l i v r i ] + b (2)</formula><p>where W âˆˆ R dÃ—2d is a composition matrix and b is a bias vector. And the composition function for RNTN is as follows:</p><formula xml:id="formula_2">g(v l i , v r i ) = [ v l i v r i ] T [1:d] [ v l i v r i ] + W [ v l i v r i ] + b (3)</formula><p>where W and b are defined in the previous model and T <ref type="bibr">[1:d]</ref> âˆˆ R 2dÃ—2dÃ—d is the tensor that defines multiple bilinear forms. The vectors are used as feature inputs to a soft- max classifier. The posterior probability over class labels on a node vector v i is given by</p><formula xml:id="formula_3">y i = softmax(W s v i + b s ).<label>(4)</label></formula><p>The parameters in these models include the word table L, a composition matrix W in RNN, and W and T <ref type="bibr">[1:d]</ref> in RNTN, and the classification matrix W s for the softmax classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Incorporating Syntactic Knowledge into Recursive Neural Model</head><p>The central idea of the paper is inspired by the fact that words/phrases of different part-of-speech tags play different roles in semantic composition. As discussed in the introduction, a noun phrase (e.g., a movie/NP) may be composed different from a verb phrase (e.g., love movie/VP). Furthermore, when composing the phrase a movie/NP, the two child words, a/DT and movie/NN, may play dif- ferent roles in the composition process. Unfor- tunately, the previous RNN models neglect such syntactic information, though the models do em- ploy the parsing structure of a sentence. We have two approaches to improve the compo- sition process by leveraging tags on parent nodes and child nodes. One approach is to use different composition matrices for parent nodes with differ- ent tags so that the composition process could be guided by phrase type, for example, the matrix for 'NP' is different from that for 'VP' . The other ap- proach is to introduce 'tag embedding' for words and phrases, for example, to learn tag vectors for 'NP, VP, ADJP', etc., and then integrate the tag vectors with the word/phrase vectors during the composition process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tag Guided RNN (TG-RNN)</head><p>We propose Tag Guided RNN (TG-RNN) to re- spect the tag of a parent phrase during the com- position process. The model chooses a composi- tion function according to the part-of-speech tag of a phrase. For example, 'the movie' has tag NP, 'very interesting' has tag ADJP, the two phrases have different composition matrices.</p><p>More formally, we design composition func- tions g with a factor of the phrase tag of a parent node. The composition function becomes</p><formula xml:id="formula_4">g(t i , v l i , v r i ) = g t i (v l i , v r i ) = W t i [ v l i v r i ] + b t i (5)</formula><p>where t i is the phrase tag for node i, W t i and b t i are the parameters of function g t i , as defined in Equation 2. In other words, phrase nodes with various tags have their own composition functions such as g N P , g V P , and so on. There are to- tally k composition function in this model where k is the number of phrase tags. When composing child vectors, a function is chosen from the func- tion pool according to the tag of the parent node.</p><p>The process is depicted in <ref type="figure" target="#fig_4">Figure 3</ref>. We term this model <ref type="table">Tag guided RNN, TG-RNN for</ref>   But some tags have few occurrences in the cor- pus. It is hard and meaningless to train compo- sition functions for those infrequent tags. So we simply choose top k frequent tags and train k com- position functions. A common composition func- tion is shared across phrases with all infrequent tags. The value of k depends on the size of the training set and the occurrences of each tag. Spe- cially, when k = 0, the model is the same as the traditional RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tag Embedded RNN and RNTN (TE-RNN/RNTN)</head><p>In this section, we propose tag embedded RNN (TE-RNN) and tag embedded RNTN (TE-RNTN) to respect the part-of-speech tags of child nodes during composition. As mentioned above, tags of parent nodes have impact on composition. How- ever, some phrases with the same tag should be composed in different ways. For example, 'is in- teresting' and 'like swimming' have the same tag VP. But it is not reasonable to compose the two phrases using the previous model because the part- of-speech tags of their children are quite different. If we use different composition functions for chil- dren with different tags like TG-RNN, the number of tag pairs will amount to as many as kÃ—k, which makes the models infeasible due to too many pa- rameters.</p><p>In order to capture the compositional effects of the tags of child nodes, an embedding e t âˆˆ R de is created for every tag t, where d e is the dimension of tag vector. The tag vector and phrase vector are concatenated during composition as illustrated in <ref type="figure">Figure 4</ref>.</p><p>Formally, the phrase vector is composed by the function</p><formula xml:id="formula_5">g(v l i , e t l i , v r i , e t r i ) = W ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° v l i e t l i v r i e t r i ï£¹ ï£º ï£º ï£º ï£º ï£» + b<label>(6)</label></formula><p>where t l i and t r i are tags of the left and the right nodes respectively, e t l i and e t r i are tag vectors, and Figure 4: RNN with tag embedding. There is a tag embedding table, storing vectors for RB, JJ, and ADJP, etc. Then we compose the phrase vector 'very interesting' from the vectors for 'very' and 'interesting', and the tag vectors for RB and JJ.</p><formula xml:id="formula_6">W âˆˆ R dÃ—(</formula><p>Similarly, this idea can be applied to Recursive Neural Tensor Network ( <ref type="bibr" target="#b22">Socher et al., 2013b</ref>). In RNTN, the tag vector and the phrase vector can be interweaved together through a tensor. More specifically, the phrase vectors and tag vectors are multiplied by the composed tensor. The composi- tion function changes to the following:</p><formula xml:id="formula_7">g(v l i , e t l i , v r i , e t r i ) = ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° v l i e t l i v r i e t r i ï£¹ ï£º ï£º ï£º ï£º ï£» T [1:d] ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° v l i e t l i v r i e t r i ï£¹ ï£º ï£º ï£º ï£º ï£» + W ï£® ï£¯ ï£¯ ï£¯ ï£¯ ï£° v l i e t l i v r i e t r i ï£¹ ï£º ï£º ï£º ï£º ï£» + b<label>(7)</label></formula><p>where the variables are similar to those defined in equation 3 and equation 7. We term this model Tag embedded RNTN, TE-RNTN for short.</p><p>The phrase vectors and tag vectors are used as input to a softmax classifier, giving the posterior probability over labels via</p><formula xml:id="formula_8">y i = softmax(W s [ v i e t i ] + b s )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Training</head><p>Let y i be the target distribution for node i, Ë† y i be the predicted sentiment distribution. Our goal is to minimize the cross-entropy error between y i andË†y andË† andË†y i for all nodes. The loss function is defined as follows:</p><formula xml:id="formula_9">E(Î¸) = âˆ’ âˆ‘ i âˆ‘ j y j i logË†ylogË† logË†y i j + Î»||Î¸|| 2 (9)</formula><p>where j is the label index, Î» is a L 2 -regularization term, and Î¸ is the parameter set. Similar to RNN, the parameters for our mod- els include word vector table L, the composition matrix W , and the sentiment classification matrix W s . Besides, our models have some additional pa- rameters, as discussed below: TG-RNN: There are k composition matrices for top k frequent tags. They are defined as W t âˆˆ R kÃ—dÃ—2d . The original composition matrix W is for all infrequent tags. As a result, the parameter set of TG-RNN is Î¸ = (L, W, W t , W s ).</p><p>TE-RNN: The parameters include the tag em- bedding table E, which contains all the em- beddings for part-of-speech tags for words and phrases. And the size of matrix W âˆˆ R dÃ—(2d+2de) and the softmax classifier W s âˆˆ R N Ã—(de+d) . The parameter set of TE-RNN is Î¸ = (L, E, W, W s ).</p><p>TE-RNTN: This model has one more tensor T âˆˆ R (2d+2de)Ã—(2d+2de)Ã—d than TE-RNN. The pa- rameter set of TE-RNTN is Î¸ = (L, E, W, T, W s )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset and Experiment Setting</head><p>We evaluate our models on Stanford Sentiment Treebank which contains fully labeled parse trees. It is built upon 10,662 reviews and each sentence has sentiment labels on each node in the parse tree. The sentiment label set is {0,1,2,3,4}, where the numbers mean very negative, negative, neu- tral, positive, and very positive, respectively. We use standard split (train: 8,544 dev: 1,101, test: 2,210) on the corpus in our experiments. In addi- tion, we add the part-of-speech tag for each leaf node and phrase-type tag for each interior node using the latest version of Stanford Parser. Be- cause the newer parser generated trees different from those provided in the datasets, 74/11/11 re- views in train/dev/test datasets are ignored. Af- ter removing the broken reviews, our dataset con- tains 10566 reviews (train: 8,470, dev: 1,090, test: 2,199).</p><p>The word vectors were pre-trained on an unla- beled corpus (about 100,000 movie reviews) by <ref type="bibr">word2vec (Mikolov et al., 2013b</ref>) as initial val- ues and the other vectors is initialized by sampling from a uniform distribution U(âˆ’Ïµ, Ïµ) where Ïµ is 0.01 in our experiments. The dimension of word vectors is 25 for RNN models and 20 for RNTN models. Tanh is chosen as the nonlinearity func- tion. And after computing the output of node i with v i = f (g(v l i , v r i )), we set v i = v i ||v i || so that the resulting vector has a limited norm. Back- propagation algorithm <ref type="bibr" target="#b17">(Rumelhart et al., 1986</ref>) is used to compute gradients and we use mini- batch SGD with momentum as the optimization method, implemented with Theano ( <ref type="bibr" target="#b0">Bastien et al., 2012</ref>). We trained all our models using stochas- tic gradient descent with a batch size of 30 exam- ples, momentum of 0.9, L 2 -regularization weight of 0.0001 and a constant learning rate of 0.005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">System Comparison</head><p>We compare our models with several methods which are evaluated on the Sentiment Treebank corpus. The baseline results are reported in ( <ref type="bibr" target="#b2">Dong et al., 2014)</ref> and <ref type="bibr" target="#b6">(Kim, 2014)</ref>.</p><p>We make comparison to the following base- lines:</p><p>â€¢ SVM. A SVM model with bag-of-words rep- resentation ( <ref type="bibr" target="#b15">Pang and Lee, 2008</ref>).</p><p>â€¢ MNB/bi-MNB. Multinomial Naive Bayes and its bigram variant, adopted from ( <ref type="bibr" target="#b24">Wang and Manning, 2012</ref>).</p><p>â€¢ RNN. The first Recursive Neural Network model proposed by <ref type="bibr" target="#b19">(Socher et al., 2011</ref>).</p><p>â€¢ MV-RNN. Matrix Vector Recursive Neural Network (Socher et al., 2012) represents each word and phrase with a vector and a ma- trix. As reported, this model suffers from too many parameters.</p><p>â€¢  <ref type="table">Table 1</ref>: Classification accuray. Fine-grained stands for 5-class prediction and Pos./Neg. means binary prediction which ignores all neutral in- stances. All the accuracy is at the sentence level (root).</p><note type="other">45.7 85.4 AdaMC-RNN 45.8 87.1 AdaMC-RNTN 46.7 88.5 DRNN 49.8 87.7 TG-RNN (ours) 47.0 86.3 TE-RNN (ours) 48.0 86.8 TE-RNTN (ours) 48.9 87.7 CNN 48.0 88.1 DCNN 48.5 86.8 Para-Vec 48.7 87.8</note><p>for composition function which could model the meaning of longer phrases and capture negation rules.</p><p>â€¢ AdaMC. Adaptive Multi-Compositionality for RNN and RNTN ( <ref type="bibr" target="#b2">Dong et al., 2014</ref>) trains more than one composition functions and adaptively learns the weight for each function.</p><p>â€¢ DCNN/CNN. Dynamic Convolutional Neu- ral Network ( <ref type="bibr" target="#b5">Kalchbrenner et al., 2014</ref>) and a simple Convolutional Neural Network <ref type="bibr" target="#b6">(Kim, 2014)</ref>, though these models are of different genres to RNN, we include them here for fair comparison since they are among top per- forming approaches on this task.</p><p>â€¢ Para-Vec. A word2vec variant <ref type="bibr" target="#b8">(Le and Mikolov, 2014</ref>) that encodes paragraph in- formation into word embedding learning. A simple but very competitive model.</p><p>â€¢ DRNN. Deep Recursive Neural Network (Ir- soy and Cardie, 2014) stacks multiple recur- sive layers.</p><p>The comparative results are shown in Ta- ble 1. As illustrated, TG-RNN outperforms RNN, RNTN, MV-RNN, AdMC-RNN/RNTN. Compared with RNN, the fine-grained accuracy and binary accuracy of TG-RNN is improved by 3.8% and 3.9% respectively. When compared with AdaMC-RNN, the accuracy of our method rises by 1.2% on the fine-grained prediction. The results show that the syntactic knowledge does facilitate phrase vector composition in this task.</p><p>As for TE-RNN/RNTN, the fine-grained accu- racy of TE-RNN is boosted by 4.8% compared with RNN and the accuracy of TE-RNTN by 3.2% compared with RNTN. TE-RNTN also beat the AdaMC-RNTN by 2.2% on the fine-grained clas- sification task. TE-RNN is comparable to CNN and DCNN, another line of models for this task. TE-RNTN is better than CNN, DCNN, and Para- Vec, which are the top performing approaches on this task. TE-RNTN is worse than DRNN, but the complexity of DRNN is much higher than TE- RNTN, which will be discussed in the next sec- tion. Furthermore, TE-RNN is also better than TG-RNN. This implies that learning the tag em- beddings for child nodes is more effective than simply using the tags of parent phrases in com- position.</p><p>Note that the fine-grained accuracy is more convincible and reliable to compare different ap- proaches due to the two facts: First, for the bi- nary classification task, some approaches train an- other binary classifier for positive/negative clas- sification while other approaches, like ours, di- rectly use the fine-grained classifier for this pur- pose. Second, how the neutral instances are pro- cessed is quite tricky and the details are not re- ported in the literature. In our work, we sim- ply remove neural instances from the test data be- fore the evaluation. Let the 5-dimension vector y be the probabilities for each sentiment label in a test instance. The prediction will be positive if arg max i,iÌ¸ =2 y i is greater than 2, otherwise nega- tive, where i âˆˆ {0, 1, 2, 3, 4} means very negative, negative, neutral, positive, very positive, respec- tively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Complexity Analysis</head><p>To gain deeper understanding of the models pre- sented in <ref type="table">Table 1</ref>, we discuss here about the pa- rameter scale of the RNN/RNTN models since the prediction power of neural network models is highly correlated with the number of parameters.</p><p>The analysis is presented in <ref type="table" target="#tab_3">Table 2</ref> (the opti- mal values are adopted from the cited papers). The parameters for the word table have the same size n Ã— d across all recursive neural models, where n is the number of words and d is the dimension of word vector. Therefore, we ignore this part but fo- cus on the parameters of composition functions, termed model size. Our models, TG-RNN/TE- RNN, have much less parameters than RNTN and AdMC-RNN/RNTN, but have much better perfor- mance. Although TE-RNTN is worse than DRNN, however, the parameters of DRNN are almost 9 times of ours. This indicates that DRNN is much more complex, which requires much more data and time to train. As a matter of a fact, our TE- RNTN only takes 20 epochs for training which is 10 times less than DRNN. </p><formula xml:id="formula_10">Method model size # of parameters RNN 2d 2 1.8K RNTN 4d 3 108K AdaMC-RNN 2d 2 Ã— c 18.7K AdaMC-RNTN 4d 3 Ã— c 202K DRNN d Ã— h Ã— l +2h 2 Ã— l 451K TG-RNN (ours) 2d 2 Ã— (k + 1) 8.8K TE-RNN (ours) 2(d + d e ) Ã— d 1.7K TE-RNTN (ours) 4(d + d e ) 2 Ã— d 54K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Parameter Analysis</head><p>We have two key parameters to tune in our pro- posed models. For TG-RNN, the number of com- position functions k is an important parameter, which corresponds to the number of distinct POS tags of phrases. Let's start from the corpus analysis. As shown in <ref type="table">Table 3</ref>, the corpus contains 215,154 phrases but the distribution of phrase tags is extremely im- balanced. For example, the phrase tag 'NP' ap- pears 60,239 times while 'NAC' appears only 10 times. Hence, it is impossible to learn a composi-Phrase tag <ref type="table" target="#tab_3">Frequency Phrase tag Frequency  NP  60,239  ADVP  1,140  S  33,138  PRN  976  VP  26,956  FARG  792  PP  14,979  UCP  362  ADJP  7,912  SSINV  266  SBAR  5,308  others  1,102   Table 3</ref>: The distribution of phrase-type tags in the training data. The top 6 frequency tags cover more than 95% phrases.</p><p>tion function for the infrequent phrase tags. Each of the top k frequent phrase tags corre- sponds to a unique composition function, while all the other phrase tags share a same function. We compare different k for TG-RNN. The accuracy is shown in <ref type="figure" target="#fig_5">Figure 5</ref>. Our model obtains the best per- formance when k is 6, which is accordant with the statistics in <ref type="table">Table 3</ref>. For TE-RNN/RNTN, the key parameter to tune is the dimension of tag vectors. In the corpus, we have 70 types of tags for leaf nodes (words) and in- terior nodes (phrases). Infrequent tags whose fre- quency is less than 1,000 are ignored. There are 30 tags left and we learn an embedding for each of these frequent tags. We varies the dimension of the embedding d e from 0 to 30. <ref type="figure" target="#fig_6">Figure 6</ref> shows the accuracy for TE-RNN and TE-RNTN with different dimensions of d e . Our model obtains the best performance when d e is 8 for TE-RNN and 6 for TE-RNTN. The re- sults show that too small dimensions may not be sufficient to encode the syntactic information of tags and too large dimensions damage the perfor- mance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Tag Vectors Analysis</head><p>In order to prove tag vectors obtained from tag embedded models are meaningful, we inspect the similarity between vectors of tags. For each tag vector, we find the nearest neighbors based on Eu- clidean distance, summarized in <ref type="table" target="#tab_4">Table 4</ref>.  Adjectives and verbs are of significant impor- tance in sentiment analysis. Although 'JJ' and 'ADJP' are word and phrase tag respectively, they have similar tag vectors, because of playing the same role of Adjective in sentences. 'VP', 'VBD' and 'VBN' with similar representations all repre- sent verbs. What is more interesting is that the nearest neighbor of dot is colon, probably because both of them are punctuation marks. Note that tag classification is none of our training objectives and surprisingly the vectors of similiar tags are clus- tered together, which can provides additional in- formation during sentence composition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tag</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present two ways to leverage syn- tactic knowledge in Recursive Neural Networks.</p><p>The first way is to use different composition func- tions for phrases with different tags so that the composition processing is guided by phrase types (TG-RNN). The second way is to learn tag em- beddings and combine tag and word embeddings during composition (TE-RNN/RNTN). The pro- posed models are not only effective (w.r.t com- peting performance) but also efficient (w.r.t well- controlled parameter scale). Experiment results show that our models are among the top perform- ing approaches up to date, but with much less pa- rameters and complexity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The example process of vector composition in RNN. The vector of node 'very interesting' is composed from the vectors of node 'very' and node 'interesting'. Similarly, the node 'is very interesting' is composed from the phrase node 'very interesting' and the word node 'is' .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The parse tree for sentence 'The movie is very interesting' built by Stanford Parser.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The vector of phrase 'very interesting' is composed with highlighted g ADJP and 'is very interesting' with g V P .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The accuracy for TG-RNN with different k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The accuracy for TE-RNN and TERNTN with different dimensions of d e .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>short.</figDesc><table>so#max 

so#max 

so#max 

so#max 

so#max 

... 

very / RB 
interesting / JJ 

is / VBZ 
very interesting / ADJP 

is very interesting / VP 

g NP g ADJP 
g VP 

g NP g ADJP 
g VP 

... 

... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The model size. d is the dimension 
of word/phrase vectors (the optimal value is 30 
for RNN &amp; RNTN, 25 for AdaMC-RNN, 15 for 
AdaMC-RNTN, 300 for DRNN). For AdaMC, c 
is the number of composition functions (15 is the 
optimal setting). For DRNN, l and h is the number 
of layers and the width for each layer (the optimal 
values l = 4, h = 174). For our methods, k is the 
number of unshared composition matrices and d e 
the dimension of tag embedding, for the optimal 
setting refer to Section 5.4. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Top 1 or 2 nearest neighboring tags with 
definition in brackets. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partly supported by the Na-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">FrÃ©dÃ©ric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RÃ©jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adaptive multi-compositionality for recursive neural models with applications to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<editor>AAAI. AAAI</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The role of syntax in vector space models of compositional semantics</title>
	</analytic>
	<monogr>
		<title level="m">Association for Computer Linguistics</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="894" to="904" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
		<respStmt>
			<orgName>Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A solution to plato&apos;s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">211</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recursive autoencoders for ITG-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="567" to="577" />
		</imprint>
		<respStmt>
			<orgName>Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>PadÃ³</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compositional matrix-space models of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenie</forename><surname>Giesbrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="907" to="916" />
		</imprint>
		<respStmt>
			<orgName>Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tensor product variable binding and the representation of symbolic structures in connectionist systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="216" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
		<respStmt>
			<orgName>Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Compositional matrix-space models for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
