<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Translation Assistance by Translation of L1 Fragments in an L2 Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Van Gompel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Centre for Language Studies Radboud University Nijmegen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antal</forename><surname>Van Den Bosch</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Centre for Language Studies Radboud University Nijmegen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Translation Assistance by Translation of L1 Fragments in an L2 Context</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="871" to="880"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we present new research in translation assistance. We describe a system capable of translating native language (L1) fragments to foreign language (L2) fragments in an L2 context. Practical applications of this research can be framed in the context of second language learning. The type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known. These code switches are subsequently translated to L2 given the L2 context. We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense dis-ambiguation baselines. A classification-based approach is presented that is indeed found to improve significantly over these baselines by making use of a contex-tual window spanning a small number of neighbouring words.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Whereas machine translation generally concerns the translation of whole sentences or texts from one language to the other, this study focusses on the translation of native language (henceforth L1) words and phrases, i.e. smaller fragments, in a foreign language (L2) context. Despite the ma- jor efforts and improvements, automatic transla- tion does not yet rival human-level quality. Vex- ing issues are morphology, word-order change and long-distance dependencies. Although there is a morpho-syntactic component in this research, our scope is more constrained; its focus is on the faith- ful preservation of meaning from L1 to L2, akin to the role of the translation model in Statistical Ma- chine Translation (SMT).</p><p>The cross-lingual context in our research ques- tion may at first seem artificial, but its design ex- plicitly aims at applications related to computer- aided language learning ( <ref type="bibr" target="#b13">Laghos and Panayiotis, 2005;</ref><ref type="bibr" target="#b14">Levy, 1997</ref>) and computer-aided transla- tion ( <ref type="bibr" target="#b2">Barrachina et al., 2009</ref>). Currently, lan- guage learners need to refer to a bilingual dictio- nary when in doubt about a translation of a word or phrase. Yet, this problem arises in a context, not in isolation; the learner may have already trans- lated successfully a part of the text into L2 leading up to the problematic word or phrase. Dictionar- ies are not the best source to look up context; they may contain example usages, but remain biased to- wards single words or short expressions.</p><p>The proposed application allows code switch- ing and produces context-sensitive suggestions as writing progresses. In this research we test the feasibility of the foundation of this idea.The fol- lowing examples serve to illustrate the idea and demonstrate what output the proposed translation assistance system would ideally produce. The parts in bold correspond to respectively the in- serted fragment and the system translation.</p><p>• Input (L1=English,L2=Spanish): "Hoy va- mos a the swimming pool." Desired output: "Hoy vamos a la piscina."</p><p>• Input (L1-English, L2=German): "Das wet- ter ist wirklich abominable." Desired output: "Das wetter ist wirklich ekelhaft."</p><p>• Input (L1=French,L2=English): "I rentrè a la maison because I am tired." Desired output: "I return home because I am tired."</p><p>• Input (L1=Dutch, L2=English): "Workers are facing a massive aanval op their employ-ment and social rights." Desired output: "Workers are facing a mas- sive attack on their employment and social rights."</p><p>The main research question in this research is how to disambiguate an L1 word or phrase to its L2 translation based on an L2 context, and whether such cross-lingual contextual approaches provide added value compared to baseline models that are not context informed or compared to stan- dard language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data preparation</head><p>Preparing the data to build training and test data for our intended translation assistance system is not trivial, as the type of interactive translation as- sistant we aim to develop does not exist yet. We need to generate training and test data that real- istically emulates the task. We start with a par- allel corpus that is tokenised for both L1 and L2. No further linguistic processing such as part-of- speech tagging or lemmatisation takes place in our experiments; adding this remains open for future research.</p><p>The parallel corpus is randomly sampled into two large and equally-sized parts. One is the basis for the training set, and the other is the basis for the test set. The reason for such a large test split shall become apparent soon.</p><p>From each of the splits (S), a phrase-translation table is constructed automatically in an unsuper- vised fashion. This is done using the scripts provided by the Statistical Machine Translation system Moses ( <ref type="bibr" target="#b10">Koehn et al., 2007)</ref>. It invokes GIZA++ <ref type="bibr" target="#b16">(Och and Ney, 2000</ref>) to establish sta- tistical word alignments based on the IBM Mod- els and subsequently extracts phrases using the grow-diag-final algorithm <ref type="bibr" target="#b17">(Och and Ney, 2003)</ref>. The result, independent for each set, will be a phrase-translation table (T ) that maps phrases in L1 to L2. For each phrase-pair (f s , f t ) this phrase-translation table holds the computed trans- lation probabilities P (f s |f t ) and P (f t |f s ).</p><p>Given these phrase-translation tables, we can now extract both training data and test data using the algorithm in <ref type="figure" target="#fig_0">Figure 1</ref>. In our discourse, the source language (s) corresponds to L1, the fall- back language used for by the end-user for insert- ing fragments, whilst the target language (t) is L2.</p><p>Step 4 is effectively a filter: two thresholds can be configured to discard weak alignments, 1. using phrase-translation table T and par- allel corpus split S 2. for each aligned sentence pair (sentence s ∈ S s , sentence t ∈ S t ) in the parallel corpus split (S s ,S t ):</p><formula xml:id="formula_0">3. for each fragment (f s ∈ sentence s , f t ∈ sentence t ) where (f s , f t ) ∈ T : 4. if P (f s |f t ) · P (f t |f s ) ≥ λ 1 and P (f s |f t ) · P (f t |f s ) ≥ λ 2 · P (f s |f strongest t ) · P (f strongest t |f s ):</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Output a pair (sentence t , sentence t ) where sentence t is a copy of t but with fragment f t substituted by f s , i.e. the introduction of an L1 word or phrase in an L2 sentence. i.e. those with low probabilities, from the phrase- translation table so that only strong couplings make it into the generated set. The parameter λ 1 adds a constraint based on the product of the two conditional probabilities (P (f t |f s )·P (f s |f t )), and sets a threshold that has to be surpassed. A second parameter λ 2 further limits the con- sidered phrase pairs (f s , f t ) to have the prod- uct of their conditional probabilities not not devi- ate more than a fraction λ 2 from the joint prob- ability for the strongest possible pairing for f s , the source fragment. f strongest t in <ref type="figure" target="#fig_0">Figure 1</ref> corresponds to the best scoring translation for a given source fragment f s . This metric thus effec- tively prunes weaker alternative translations in the phrase-translation table from being considered if there is a much stronger candidate. Nevertheless, it has to be noted that even with λ 1 and λ 2 , the test set will include a certain amount of errors. This is due to the nature of the unsupervised method with which the phrase-translation table is constructed. For our purposes however, the test set suffices to test our hypothesis.</p><p>In our experiments, we choose fixed values for these parameters, by manual inspection and judge- ment of the output. The λ 1 parameter was set to 0.01 and λ 2 to 0.8. Whilst other thresholds may possibly produce cleaner sets, this is hard to eval- uate as finding optimal values causes a prohibitive increase in complexity of the search space, and again this is not necessary to test our hypothesis.</p><p>The output of the algorithm in <ref type="figure" target="#fig_0">Fig- ure 1</ref> is a modified set of sentence pairs (sentence t , sentence t ), in which the same sentence pair may be used multiple times with different L1 substitutions for different fragments. The final test set is created by randomly sampling the desired number of test instances.</p><p>Note that the training set and test set are con- structed on their own respective and indepen- dently generated phrase-translation tables. This ensures complete independence of training and test data. Generating test data using the same phrase-translation table as the training data would introduce a bias. The fact that a phrase-translation table needs to be constructed for the test data is also the reason that the parallel corpus split from which the test data is derived has to be large enough, ensuring better quality.</p><p>We concede that our current way of testing is a mere approximation of the real-world scenario. An ideal test corpus would consist of L2 sentences with L1 fallback as crafted by L2 language learn- ers with an L1 background. However, such cor- pora do not exist as yet. Nevertheless, we hope to show that our automated way of test set genera- tion is sufficient to test the feasibility of our core hypothesis that L1 fragments can be translated to L2 using L2 context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System</head><p>We develop a classifier-based system composed of so-called "classifier experts". Numerous classi- fiers are trained and each is an expert in translating a single word or phrase. In other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained. The clas- sifier maps the L1 word or phrase in its L2 context to its L2 translation. Words or phrases that always map to a single translation are stored in a sim- ple mapping table, as a classifier would have no added value in such cases. The classifiers use the IB1 algorithm ( <ref type="bibr" target="#b0">Aha et al., 1991)</ref> as implemented in TiMBL ( <ref type="bibr" target="#b4">Daelemans et al., 2009</ref>). 1 IB1 im- plements k-nearest neighbour classification. The choice for this algorithm is motivated by the fact that it handles multiple classes with ease, but first and foremost because it has been successfully em- ployed for word sense disambiguation in other studies ( <ref type="bibr" target="#b9">Hoste et al., 2002;</ref><ref type="bibr" target="#b5">Decadt et al., 2004</ref>), in particular in cross-lingual word sense disam- biguation, a task closely resembling our current task <ref type="bibr" target="#b20">(van Gompel and van den Bosch, 2013)</ref>. It has also been used in machine translation stud- ies in which local source context is used to clas- sify source phrases into target phrases, rather than looking them up in a phrase table <ref type="bibr" target="#b19">(Stroppa et al., 2007;</ref><ref type="bibr" target="#b8">Haque et al., 2011</ref>). The idea of local phrase selection with a discriminative machine learning classifier using additional local (source-language) context was introduced in parallel to <ref type="bibr" target="#b19">Stroppa et al. (2007)</ref> by <ref type="bibr" target="#b3">Carpuat and Wu (2007)</ref> and <ref type="bibr">Giménez and Márquez (2007)</ref>; cf. <ref type="bibr" target="#b8">Haque et al. (2011)</ref> for an overview of more recent methods.</p><p>The feature vector for the classifiers represents a local context of neighbouring words, and op- tionally also global context keywords in a binary- valued bag-of-words configuration. The local con- text consists of an X number of L2 words to the left of the L1 fragment, and Y words to the right.</p><p>When presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our sim- ple mapping table. If so, we are done quickly and need not rely on context information. If not, we check for the presence of a classifier expert for the offered L1 fragment; only then we can proceed by extracting the desired number of L2 local context words to the immediate left and right of this frag- ment and adding those to the feature vector. The classifier will return a probability distribution of the most likely translations given the context and we can replace the L1 fragment with the highest scoring L2 translation and present it back to the user.</p><p>In addition to local context features, we also ex- perimented with global context features. These are a set of L2 contextual keywords for each L1 word/phrase and its L2 translation occurring in the same sentence, not necessarily in the immediate neighbourhood of the L1 word/phrase. The key- words are selected to be indicative for a specific translation. We used the method of extraction by <ref type="bibr" target="#b15">Ng and Lee (1996)</ref> and encoded all keywords in a binary bag of words model. The experiments however showed that inclusion of such keywords did not make any noticeable impact on any of the results, so we restrict ourselves to mentioning this negative result.</p><p>Our full system, including the scripts for data preparation, training, and evaluation, is implemented in Python and freely available as open-source from http://github.com/ proycon/colibrita/ . Version tag v0.2.1 is representative for the version used in this re- search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Language Model</head><p>We also implement a statistical language model as an optional component of our classifier-based sys- tem and also as a baseline to compare our system to. The language model is a trigram-based back- off language model with Kneser-Ney smooth- ing, computed using SRILM <ref type="bibr" target="#b18">(Stolcke, 2002</ref>) and trained on the same training data as the translation model. No additional external data was brought in, to keep the comparison fair.</p><p>For any given hypothesis H, results from the L1 to L2 classifier are combined with results from the L2 language model. We do so by normalising the class probability from the classifier (score T (H)), which is our translation model, and the language model (score lm (H)), in such a way that the high- est classifier score for the alternatives under con- sideration is always 1.0, and the highest language model score of the sentence is always 1.0. Take score T (H) and score lm (H) to be log probabili- ties, the search for the best (most probable) trans- lation hypothesisˆHhypothesisˆ hypothesisˆH can then be expressed as:</p><formula xml:id="formula_1">ˆ H = arg max H (score T (H) + score lm (H)) (1)</formula><p>If desired, the search can be parametrised with variables λ 3 and λ 4 , representing the weights we want to attach to the classifier-based translation model and the language model, respectively. In the current study we simply left both weights set to one, thereby assigning equal importance to trans- lation model and language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Several automated metrics exist for the evaluation of L2 system output against the L2 reference out- put in the test set. We first measure absolute accu- racy by simply counting all output fragments that exactly match the reference fragments, as a frac- tion of the total amount of fragments. This mea- sure may be too strict, so we add a more flexible word accuracy measure which takes into account partial matches at the word level. If output o is a subset of reference r then a score of |o| |r| is as- signed for that sentence pair. If instead, r is a sub- set of o, then a score of |r| |o| will be assigned. A perfect match will result in a score of 1 whereas a complete lack of overlap will be scored 0. The word accuracy for the entire set is then computed by taking the sum of the word accuracies per sen- tence pair, divided by the total number of sentence pairs.</p><p>We also compute a recall metric that measures the number of fragments that the system provided a translation for as a fraction of the total number of fragments in the input, regardless of whether the fragment is translated correctly or not. The system may skip fragments for which it can find no solution at all.</p><p>In addition to these, the system's output can be compared against the L2 reference translation(s) using established Machine Translation evaluation metrics. We report on BLEU, NIST, METEOR, and word error rate metrics WER and PER. These scores should generally be much better than the typical MT system performances as only local changes are made to otherwise "perfect" L2 sen- tences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baselines</head><p>A context-insensitive yet informed baseline was constructed to assess the impact of L2 context in- formation in translating L1 fragments. The base- line selects the most probable L1 fragment per L2 fragment according to the phrase-translation ta- ble. This baseline, henceforth referred to as the 'most likely fragment' baseline (MLF) is analo- gous to the 'most frequent sense'-baseline com- mon in evaluating WSD systems.</p><p>A second baseline was constructed by weigh- ing the probabilities from the translation table di- rectly with the L2 language model described ear- lier. It adds a LM component to the MLF base- line. This LM baseline allows the comparison of classification through L1 fragments in an L2 con- text, with a more traditional L2 context modelling (i.e. target language modelling) which is also cus-tomary in MT decoders. Computing this base- line is done in the same fashion as previously il- lustrated in Equation 1, where score T then repre- sents the normalised p(t|s) score from the phrase- translation table rather than the class probability from the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments &amp; Results</head><p>The data for our experiments were drawn from the Europarl parallel corpus ( <ref type="bibr" target="#b12">Koehn, 2005</ref>) from which we extracted two sets of 200, 000 sentence pairs each for several language pairs. These were used to form the training and test sets. The final test sets are a randomly sampled 5, 000 sentence pairs from the 200, 000-sentence test split for each language pair.</p><p>All input data for the experiments in this section are publicly available 2 .</p><p>Let us first zoom in to convey a sense of scale on a specific language pair. The actual Europarl training set we generate for English (L1) to Span- ish (L2), i.e. English fallback in a Spanish con- text, consists of 5, 608, 015 sentence pairs. This number is much larger than the 200, 000 we men- tioned before because single sentence pairs may be reused multiple times with different marked frag- ments. From this training set of sentence pairs over 100, 000 classifier experts are derived. The eleven largest classifiers are shown in <ref type="table">Table 1</ref>  For the classifier-based system, we tested var- ious different feature vector configurations. The first experiment, of which the results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>, sets a fixed and symmetric local context size across all classifiers, and tests three context widths. Here we observe that a context width of one yields the best results. The BLEU scores, not included in the figure but shown in <ref type="table" target="#tab_2">Table 2</ref>, show a similar trend. This trend holds for all the MT metrics. <ref type="table" target="#tab_2">Table 2</ref> shows the results for English to Span- ish in more detail and adds a comparison with the two baseline systems. The various lXrY config- urations use the same feature vector setup for all classifier experts. Here X indicates the left context size and Y the right context size. The auto con- figuration does not uniformly apply the same fea- ture vector setup to all classifier experts but instead seeks to find the optimal setup per classifier expert. This shall be further discussed in Section 6.1.</p><p>As expected, the LM baseline substantially out- performs the context-insensitive MLF baseline. Second, our classifier approach attains a sub- stantially higher accuracy than the LM baseline. Third, we observe that adding the language model to our classifier leads to another significant gain  Statistical significance on the BLEU scores was tested using pairwise bootstrap sampling <ref type="bibr" target="#b11">(Koehn, 2004</ref>). All significance tests were performed with 5, 000 iterations. We compared the out- comes of several key configurations. We first tested l1r1 against both baselines; both differ- ences are significant at p &lt; 0.01 for both. The same significance level was found when compar- ing l1r1+LM against l1r1, auto+LM against auto, as well as the LM baseline against the MLF baseline. Automatic feature selection auto was found to perform statistically better than l1r1, but only at p &lt; 0.05. Conclusions with regard to context width may have to be tempered somewhat, as the performance of the l1r1 configuration was found to not be significantly better than that of the l2r2 configuration. However, l1r1 performs significantly better than l3r3 at p &lt; 0.01, and l2r2 performs significantly better than l3r3 at p &lt; 0.01.</p><p>In <ref type="table">Table 3</ref> we present some illustrative exam- ples from the English→Spanish Europarl data. We show the difference between the most-likely- fragment baseline and our system.</p><p>Likewise, <ref type="table">Table 4</ref> exemplifies small fragments from the l1r1 configuration compared to the same configuration enriched with a language model. We observe in this data that the language model often has the added power to choose a cor- rect translation that is not the first prediction of the classifier, but one of the weaker alternatives that nevertheless fits better. Though the classifier generally works best in the l1r1 configuration, i.e. with context size one, the trigram-based lan- guage model allows further left-context informa- tion to be incorporated that influences the weights of the classifier output, successfully forcing the system to select alternatives. This combination of a classifier with context size one and trigram- based language model proves to be most effective and reaches the best results so far. We have not conducted experiments with language models of other orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Context optimisation</head><p>It has been argued that classifier experts in a word sense disambiguation ensemble should be individ- ually optimised <ref type="bibr" target="#b5">(Decadt et al., 2004;</ref><ref type="bibr" target="#b20">van Gompel and van den Bosch, 2013)</ref>. The latter study on cross-lingual WSD finds a positive impact when conducting feature selection per classifier. This in- tuitively makes sense; a context of one may seem to be better than any other when uniformly applied to all classifier experts, but it may well be that cer- tain classifiers benefit from different feature selec- tions. We therefore proceed with this line of inves- tigation as well.</p><p>Automatic configuration selection was done by performing leave-one-out testing (for small num- ber of instances) or 10-fold-cross validation (for larger number of instances, n ≥ 20) on the train- ing data per classifier expert. Various configura- tions were tested. Per classifier expert, the best scoring configuration was selected, referred to as the auto configuration in <ref type="table" target="#tab_2">Table 2</ref>. The auto configuration improves results over the uniformly Input: Mientras no haya prueba en contrario , la financiación de partidos políticos European sólo se justifica , incluso después del tratado de Niza , desde el momento en que concurra a la expresión del sufragio universal , que es lá unica definición aceptable de un partido político . MLF baseline: Mientras no haya prueba en contrario , la financiación de partidos políticos Europea sólo se justifica , incluso después del tratado de Niza , desde el momento en que concurra a la expresión del sufragio universal , que es lálá unica definición aceptable de un partido político . l1r1: Mientras no haya prueba en contrario , la financiación de partidos políticos europeos sólo se justifica , incluso después del tratado de Niza , desde el momento en que concurra a la expresión del sufragio universal , que es lá unica definición aceptable de un partido político .</p><p>Input: Esta Directiva es nuestra oportunidad to marcar una verdadera diferencia , reduciendo la trágica pérdida de vidas en nuestras carreteras . MLF baseline: Esta Directiva es nuestra oportunidad a marcar una verdadera diferencia , reduciendo la trágica pérdida de vidas en nuestras carreteras . l1r1: Esta Directiva es nuestra oportunidad para marcar una verdadera diferencia , reduciendo la trágica pérdida de vidas en nuestras carreteras .</p><p>Input: Es la last vez que me dirijo a esta Cámara . MLF baseline: Es la pasado vez que me dirijo a esta Cámara . l1r1: Es lá ultima vez que me dirijo a esta Cámara .</p><p>Input: Pero el enfoque actual de la Comisión no puede conducir a una buena política ya que es tributario del fun- cionamiento del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguen siendo desfavorables para los developing countries . MLF baseline: Pero el enfoque actual de la Comisión no puede conducir a una buena política ya que es tributario del funcionamiento del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguen siendo desfavorables para los los países en desarrollo . l1r1: Pero el enfoque actual de la Comisión no puede conducir a una buena política ya que es tributario del funcionamiento del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguen siendo desfavor- ables para los países en desarrollo . <ref type="table">Table 3</ref>: Some illustrative examples of MLF-baseline output versus system output, in which system output matches the correct human reference output. The actual fragments concerned are highlighted in bold. The first example shows our system correcting for number agreement, the second a correction in selecting the right preposition, and the third shows that the English word last can be translated in different ways, only one of which is correct in this context. The last example shows a phrasal translation, in which the determiner was duplicated in the baseline applied feature selection. However, if we enable the language model as we do in the auto+LM configuration we do not notice an improvement over l1r1+LM, surprisingly. We suspect the lack of impact here can be explained by the trigram- based Language Model having less added value when the (left) context size of the classifier is two or three; they are now less complementary. <ref type="table">Table 5</ref> lists what context sizes have been cho- sen in the automatic feature selection. A context size of one prevails in the vast majority of cases, which is not surprising considering the good re- sults we have already seen with this configuration.</p><p>In this study we did not yet conduct optimisa- tion of the classifier parameters. We used the IB1 algorithm with k = 1 and the default values of the TiMBL implementation. In earlier work van Gompel and van den Bosch (2013), we reported a decrease in performance due to overfitting when 66.5% l1r1 19.9% l2r2 7.7% l3r3 3.5% l4r4 2.4% l5r5 <ref type="table">Table 5</ref>: Frequency of automatically selected con- figurations on English to Spanish Europarl dataset this is done, so we do not expect it to make a pos- itive impact. The second reason for omitting this is more practical in nature; to do this in combina- tion with feature selection would add substantial search complexity, making experiments far more time consuming, even prohibitively so.</p><p>The bottom lines in <ref type="table" target="#tab_2">Table 2</ref> represent results when all right-context is omitted, emulating a real- time prediction when no right context is available yet. This has a substantial negative impact on re-Input: Sin ese tipo de protección la gente no aprovechará la oportunidad to vivir , viajar y trabajar donde les parezca en la Unión Europea . l1r1: Sin ese tipo de protección la gente no aprovechará la oportunidad para vivir , viajar y trabajar donde les parezca en la Unión Europea . l1r1+LM: Sin ese tipo de protección la gente no aprovechará la oportunidad de vivir , viajar y trabajar donde les parezca en la Unión Europea .</p><p>Input: La Comisión también está acometiendo medidas en eí ambito social y educational con vistas a mejorar la situación de los niños . l1r1: La Comisión también está acometiendo medidas en eí ambito social y educativas con vistas a mejorar la situación de los niños . l1r1+LM: La Comisión también está acometiendo medidas en eí ambito social y educativo con vistas a mejorar la situación de los niños . <ref type="table">Table 4</ref>: Some examples of l1r1 versus the same configuration enriched with a language model. sults. We experimented with several asymmetric configurations and found that taking two words to the left and one to the right yields even better re- sults than symmetric configurations for this data set. This result is in line with the positive effect of adding the LM to the l1r1.</p><p>In order to draw accurate conclusions, experi- ments on a single data set and language pair are not sufficient. We therefore conducted a number of ex- periments with other language pairs, and present the abridged results in <ref type="table">Table 6</ref>.</p><p>There are some noticeable discrepancies for some experiments in <ref type="table">Table 6</ref> when compared to our earlier results in <ref type="table" target="#tab_2">Table 2</ref>. We see that the lan- guage model baseline for English→French shows the same substantial improvement over the base- line as our English→Spanish results. The same holds for the Chinese→English experiment. How- ever, for English→Dutch and English→Chinese we find that the LM baseline actually performs slightly worse than baseline. Nevertheless, in all these cases, the positive effect of including a Lan- guage Model to our classifier-based system again shows. Also, we note that in all cases our system performs better than the two baselines.</p><p>Another discrepancy is found in the BLEU scores of the English→Chinese experiments, where we measure an unexpected drop in BLEU score under baseline. However, all other scores do show the expected improvement. The error rate metrics show improvement as well. We therefore attach low importance to this deviation in BLEU here.</p><p>In all of the aforementioned experiments, the system produced a single solution for each of the fragments, the one it deemed best, or no solution at all if it could not find any. Alternative evaluation metrics could allow the system to output multiple alternatives. Omission of a solution by definition causes a decrease in recall. In all of our experi- ments recall is high (well above 90%), mostly be- cause train and test data lie in the same domain and have been generated in the same fashion, lower re- call is expected with more real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and conclusion</head><p>In this study we have shown the feasibility of a classifier-based translation assistance system in which L1 fragments are translated in an L2 con- text, in which the classifier experts are built indi- vidually per word or phrase. We have shown that such a translation assistance system scores both above a context-insensitive baseline, as well as an L2 language model baseline.</p><p>Furthermore, we found that combining this cross-language context-sensitive technique with an L2 language model boosts results further.</p><p>The presence of a one-word right-hand side context proves crucial for good results, which has implications for practical translation assistance ap- plication that translate as soon as the user finishes an L1 fragment. Revisiting the translation when right context becomes available would be advis- able.</p><p>We tested various configurations and conclude that small context sizes work better than larger ones. Automated configuration selection had pos- itive results, yet the system with context size one and an L2 language model component often pro- duces the best results. In static configurations, the failure of a wider context window to be more suc-  <ref type="table">Table 6</ref>: Results on different datasets and language pairs. The iwslt12ted set is the dataset used in the <ref type="bibr">IWSLT 2012</ref><ref type="bibr">Evaluation Campaign (Federico et al., 2012</ref>), and is formed by a collection of transcriptions of TED talks. Here we used of just over 70, 000 sentences for training. Recall for each of the four datasets is 0.9498 (en-nl), 0.9494 (en-fr), 0.9386 (en-zh), and 0.9366 (zh-en) cesful may be attributed to the increased sparsity that comes from such an expansion. The idea of a comprehensive translation assis- tance system may extend beyond the translation of L1 fragments in an L2 context. There are more NLP components that might play a role if such a system were to find practical application. Word completion or predictive editing (in combination with error correction) would for instance seem an indispensable part of such a system, and can be implemented alongside the technique proposed in this study. A point of more practically-oriented future research is to see how feasible such combi- nations are and what techniques can be used.</p><p>An application of our idea outside the area of translation assistance is post-correction of the out- put of some MT systems that, as a last-resort heuristic, copy source words or phrases into their output, producing precisely the kind of input our system is trained on. Our classification-based ap- proach may be able to resolve some of these cases operating as an add-on to a regular MT system - or as a independent post-correction system.</p><p>Our system allows L1 fragments to be of arbi- trary length. If a fragment was not seen during training stage, and is therefore not covered by a classifier expert, then the system will be unable to translate it. Nevertheless, if a longer L1 frag- ment can be decomposed into subfragments that are known, then some recombination of the trans- lations of said sub-fragments may be a good trans- lation for the whole. We are currently exploring this line of investigation, in which the gap with MT narrows further.</p><p>Finally, an important line of future research is the creation of a more representative test set. Lacking an interactive system that actually does what we emulate, we hypothesise that good ap- proximations would be to use gap exercises, or cloze tests, that test specific aspects difficulties in language learning. Similarly, we may use L2 learner corpora with annotations of code- switching points or errors. Here we then assume that places where L2 errors occur may be indica- tive of places where L2 learners are in some trou- ble, and might want to fall back to generating L1. By then manually translating gaps or such prob- lematic fragments into L1 we hope to establish a more realistic test set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Algorithm for extracting training and test data on the basis of a phrase-translation table (T ) and subset/split from a parallel corpus (S). The indentation indicates the nesting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy for different local context sizes, Europarl English to Spanish</figDesc><graphic url="image-1.png" coords="5,307.28,62.81,226.77,170.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Configuration Accuracy Word Accuracy BLEU METEOR NIST</head><label></label><figDesc></figDesc><table>WER 
PER 
MLF baseline 0.6164 
0.6662 
0.972 
0.9705 
17.0784 1.4465 1.4209 
LM baseline 
0.7158 
0.7434 
0.9785 0.9739 
17.1573 1.1735 1.1574 
l1r1 
0.7588 
0.7824 
0.9801 0.9747 
17.1550 1.1625 1.1444 
l2r2 
0.7574 
0.7801 
0.9800 0.9746 
17.1550 1.1750 1.1569 
l3r3 
0.7514 
0.7742 
0.9796 0.9744 
17.1445 1.1946 1.1780 
l1r1+LM 
0.7810 
0.7973 
0.9816 0.9754 
17.1685 1.0946 1.077 
auto 
0.7626 
0.7850 
0.9803 0.9748 
17.1544 1.1594 1.1424 
auto+LM 
0.7796 
0.7966 
0.9815 0.9754 
17.1664 1.1021 1.0845 
l1r0 
0.6924 
0.7223 
0.9757 0.9723 
17.1087 1.3415 1.3249 
l2r0 
0.6960 
0.7245 
0.9759 0.9724 
17.1091 1.3364 1.3193 
l2r1 
0.7624 
0.7849 
0.9803 0.9748 
17.1558 1.1554 1.1378 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Europarl results for English to Spanish (i.e English fallback in Spanish context). Recall = 
0.9422 

(configuration l1r1+LM in the results in Ta-
ble 2). It appears that the classifier approach and 
the L2 language model are able to complement 
each other. 

</table></figure>

			<note place="foot" n="1"> http://ilk.uvt.nl/timbl</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Instance-based learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine</title>
		<imprint>
			<biblScope unit="volume">879</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Learning</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="37" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical approaches to computer-assisted translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barrachina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Casacuberta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khadivi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lagarda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tomás</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Vilar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation using word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">TiMBL: Tilburg memory based learner, version 6.2, reference guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zavrel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Der Sloot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bosch</surname></persName>
		</author>
		<idno>ILK 09-01</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>ILK Research Group, Tilburg University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">GAMBL, genetic algorithm optimization of memory-based WSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Decadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3)</title>
		<editor>R. Mihalcea and P. Edmonds</editor>
		<meeting>the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3)<address><addrLine>New Brunswick, NJ. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="108" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Overview of the IWSLT 2012 evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT)</title>
		<meeting>the seventh International Workshop on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="12" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Context-aware discriminative phrase selection for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Giménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
		<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="159" to="166" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Integrating source-language context into phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar Naskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="285" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>September</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parameter optimization for machine learning of word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="311" to="325" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<editor>Dekang Lin and Dekai Wu</editor>
		<meeting>EMNLP 2004<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Machine Translation Summit X ([MT]&apos;05)</title>
		<meeting>the Machine Translation Summit X ([MT]&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Computer assisted/aided language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laghos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Panayiotis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="331" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Computer-assisted language learning: Context and conceptualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Clarendon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H. Beng</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Giza++: Training of statistical translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>RWTH Aachen, University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<idno>ICSLP2002 -INTER- SPEECH 2002</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Spoken Language Processing</title>
		<editor>John H. L. Hansen and Bryan L. Pellom</editor>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09-16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploiting source similarity for SMT using contextinformed features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Stroppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Theoretical Issues in Machine Translation (TMI 2007)</title>
		<editor>A. Way and B. Gawronska</editor>
		<meeting>the 11th International Conference on Theoretical Issues in Machine Translation (TMI 2007)<address><addrLine>Skövde, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">WSD2: Parameter optimisation for memory-based crosslingual word-sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Gompel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
