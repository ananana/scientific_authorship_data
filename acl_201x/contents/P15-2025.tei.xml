<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation Based Translation Evaluation Metrics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Research Council</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Research Council</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Representation Based Translation Evaluation Metrics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="150" to="155"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Precisely evaluating the quality of a translation against human references is a challenging task due to the flexible word ordering of a sentence and the existence of a large number of synonyms for words. This paper proposes to evaluate translations with distributed representations of words and sentences. We study several metrics based on word and sentence representations and their combination. Experiments on the WMT metric task shows that the metric based on the combined representations achieves the best performance, outperforming the state-of-the-art translation metrics by a large margin. In particular , training the distributed representations only needs a reasonable amount of mono-lingual, unlabeled data that is not necessary drawn from the test domain.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic machine translation (MT) evaluation metrics measure the quality of the translations against human references. They allow rapid com- parisons between different systems and enable the tuning of parameter values during system train- ing. Many machine translation metrics have been proposed in recent years, such as BLEU ( <ref type="bibr" target="#b9">Papineni et al., 2002</ref>), NIST <ref type="bibr" target="#b4">(Doddington, 2002</ref>), TER ( <ref type="bibr" target="#b11">Snover et al., 2006</ref>), Meteor (Banerjee and <ref type="bibr" target="#b0">Lavie, 2005</ref>) and its extensions, and the MEANT family (Lo and Wu, 2011), amongst others.</p><p>Precisely evaluating translation, however, is not easy. This is mainly caused by the flexible word ordering and the existence of the large number of synonyms for words. One straightforward so- lution to improve the evaluation quality is to in- crease the number of various references. Never- theless, it is expensive to create multiple refer- ences. In order to catch synonym matches be- tween the translations and references, synonym dictionaries or paraphrasing tables have been used. For example, Meteor (Banerjee and <ref type="bibr" target="#b0">Lavie, 2005</ref>) uses WordNet <ref type="bibr" target="#b8">(Miller, 1995)</ref>; TER-Plus ( <ref type="bibr" target="#b12">Snover et al., 2009</ref>) and Meteor Universal <ref type="bibr" target="#b3">(Denkowski and Lavie, 2014</ref>) deploy paraphrasing tables. These dictionaries have helped to improve the accuracy of the evaluation; however, not all languages have synonym dictionaries or paraphrasing tables, espe- cially for those low resource languages.</p><p>This paper leverages recent developments on distributed representations to address the above mentioned two challenges. A distributed represen- tation maps each word or sentence to a continu- ous, low dimensional space, where words or sen- tences having similar syntactic and semantic prop- erties are close to one another ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b13">Socher et al., 2011;</ref><ref type="bibr" target="#b7">Mikolov et al., 2013)</ref>. For example, the words vacation and holiday are close to each other in the vector space, but both are far from the word business in that space.</p><p>We propose to evaluate the translations with dif- ferent word and sentence representations. Specif- ically, we investigate the use of three widely de- ployed representations: one-hot representations, distributed word representations learned from a neural network model, and distributed sentence representations computed with recursive auto- encoder. In particular, to leverage the different ad- vantages and focuses, in terms of benefiting eval- uation, of various representations, we concatenate the three representations to form one vector rep- resentation for each sentence. Our experiments on the WMT metric task show that the metric based on the concatenated representation outper- forms several state-of-the-art machine translation metrics, by a large margin on both segment and system-level. Furthermore, our results also indi- cate that the representation based metrics are ro- bust to a variety of training conditions, such as the data volume and domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>150</head><p>A representation, in the context of NLP, is a math- ematical object associated with each word, sen- tence, or document. This object is typically a vec- tor where each element's value describes, to some degree, the semantic or syntactic properties of the associated word, sentence, or document. Using word or phrase representations as extra features has been proven to be an effective and simple way to improve the predictive performance of an NLP system ( <ref type="bibr" target="#b15">Turian et al., 2010;</ref><ref type="bibr" target="#b2">Cherry and Guo, 2015)</ref>. Our evaluation metrics are based on three widely used representations, as discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">One-hot Representations</head><p>Conventionally, a word is represented by a one-hot vector. In a one-hot representation, a vocabulary is first defined, and then each word in the vocabu- lary is assigned a symbolic ID. In this scenario, for each word, the feature vector has the same length as the size of the vocabulary, and only one dimen- sion that corresponds to the word is on, such as a vector with one element set to 1 and all others set to 0. This feature representation has been tra- ditionally used for many NLP systems. On the other hand, recent years have witnessed that sim- ply plugging in distributed word vectors as real- valued features is an effective way to improve a NLP system ( <ref type="bibr" target="#b15">Turian et al., 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distributed Word Representations</head><p>Distributed word representations, also called word embeddings, map each word deterministically to a real-valued, dense vector ( <ref type="bibr" target="#b1">Bengio et al., 2003)</ref>. A widely used approach for generating useful word vectors is developed by <ref type="bibr" target="#b7">(Mikolov et al., 2013)</ref>. This method scales very well to very large training corpora. Their skip-gram model, which we adopt here, learns word vectors that are good at predict- ing the words in a context window surrounding it. A very promising perspective of such distributed representation is that words that have similar con- texts, and therefore similar syntactic and semantic properties, will tend to be near one another in the low-dimensional vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentence Vector Representations</head><p>Word level representation often cannot properly capture more complex linguistic phenomena in a sentence or multi-word phrase. Therefore, we adopt an effective and efficient method for multi- word phrase distributed representation, namely the greedy unsupervised recursive auto-encoder strat- egy (RAE) <ref type="bibr" target="#b13">(Socher et al., 2011</ref>). This method works under an unsupervised setting. In particular, it does not rely on a parsing tree structure in order to generate sentence level vectors. This character- istic makes it very desirable for applying it to the outputs of machine translation systems. This is be- cause the outputs of translation systems are often not syntactically correct sentences; parsing them is possible to introduce unexpected noise.</p><p>For a given sentence, the greedy unsupervised RAE greedily searches a pair of words that re- sults in minimal reconstruction error by an auto- encoder. The corresponding hidden vector of the auto-encoder (denoted as the two children's par- ent vector), which has the same size as that of the two child vectors, is then used to replace the two children vectors. This process repeats and treats the new parent vector like any other word vectors. In such a recursive manner, the parent vector gen- erated from the word pool with only two vectors left will be used as the vector representation for the whole sentence. Interested readers are referred to <ref type="bibr" target="#b13">(Socher et al., 2011</ref>) for detailed discussions of the strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combined Representations</head><p>Each of the above mentioned representations has a different strength in terms of encoding syntactic and semantic contextual information for a given sentence. Specifically, the one-hot representation is able to reflect the particular words that occur in the sentence. The word embeddings can rec- ognize synonyms of words appearing in the sen- tence, through the co-occurrence information en- coded in the vector's representation. Finally, the RAE vector can encode the composed semantic information of the given sentence. These obser- vations suggest that it is beneficial to take various types of representations into account.</p><p>The most straightforward way to integrate mul- tiple vectors is using concatenation. In our studies here, we first compute the sentence-level one-hot, word embedding, and RAE representations. Next, we concatenate the three sentence-level represen- tations to form one vector for each sentence.</p><p>Consider we have the sentence representations for the translations (t) and references (r), the translation quality is measured with a similarity score computed with Cosine function and a length penalty. Suppose the size of the vector is N , we calculate the quality as follows.</p><p>Score(t, r) = Cos α (t, r) × P len (1)</p><formula xml:id="formula_0">Cos(t, r) = i=N i=1 v i (t) · v i (r) i=N i=1 v 2 i (t) i=N i=1 v 2 i (r)<label>(2)</label></formula><formula xml:id="formula_1">P len = exp(1 − l r /l t ) if (l t &lt; l r ) exp(1 − l t /l r ) if (l t ≥ l r )<label>(3)</label></formula><p>where α is a free parameter, v i (.) is the value of the vector element, P len is the length penalty, and l r , l t are length of the translation and reference, respectively.</p><p>In the scenarios of there exist multiple refer- ences, we compute the score with each reference, then choose the highest one. Also, we treat the document-level score as the weighted average of sentence-level scores, with the weights being the reference lengths, as follows.</p><formula xml:id="formula_2">Score d = D i=1 len(r i )Score i D i=1 len(r i )<label>(4)</label></formula><p>where Score i denotes the score of sentence i, and D is the size of the document in sentences. With these score equations, we then can formulate our five presentations based metrics as follows.</p><p>For the one-hot representation metric, once we have the representations of the words and n-grams, we sum all the vectors to obtain the representation of the sentence. For efficiency, we only keep the entries which are not both zero in the reference and translation vectors. After we generate the two vectors for both translation and reference, we then compute the score using Equation 1.</p><p>For the word embedding based metric, we first learn the word vector representation using the code provided by (Mikolov et al., 2013) <ref type="bibr">1</ref> . Next, following (Zou et al., 2013), we average the word embeddings of all words in the sentence to obtain the representation of the sentence.</p><p>As discussed in Section 2.4, the three sentence- level one-hot, word embedding and RAE repre- sentations have different strength when they are 1 https://code.google.com/p/word2vec/ used to compare two sentences. In our metric here, each of the three vectors is first scaled with a par- ticular weight (learned on dev data) and then the vectors are concatenated. With these concatena- tion vectors, we then calculate the similarity score using Equation 1.</p><p>For comparison, we also combine the strength of the three representations using weighted aver- age of the three metrics computed. Weights are tuned using development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted experiments on the WMT met- ric task data. Development sets include WMT 2011 all-to-English, and English-to-all submis- sions. Test sets contain <ref type="bibr">WMT 2012</ref><ref type="bibr">, and WMT 2013</ref><ref type="bibr">all-to-English, plus 2012</ref> English- to-all submissions. The languages "all" include French, Spanish, German and Czech. For training the word embedding and recursive auto-encoder model, we used WMT 2013 training data 2 .</p><p>We compared our metrics with smoothed BLEU (mteval-v13a), TER 3 , Meteor v1.0 4 , and Meteor Universal (i.e. v1.5) <ref type="bibr">5</ref> . We used the default set- tings for all these four metrics.</p><p>When considering the representation based met- rics, we tuned all the parameters to maximize the system-level γ score for all representation based metrics on the dev sets. We tuned the weights for combining the three vectors automatically, us- ing the downhill simplex method as described in <ref type="bibr" target="#b10">(Press et al., 2002</ref>). The weights are 1 for the RAE vector, about 0.1 for the word embedding vector, and around 0.01 for the one-hot vector, re- spectively. We tuned other parameters manually. Specifically, we set n equal to 2 for the one-hot n-gram representation, the vector size of the re- cursive auto-encoder to 10, and the vector size of word embeddings to 80.</p><p>Following WMT 2013's metric task <ref type="bibr" target="#b6">(Macháček and Bojar, 2013)</ref>, to measure the correlation with human judgment, we use Kendall's rank correla- tion coefficient τ for the segment level, and Pear- son's correlation coefficient (γ in the below tables and figures) for the system-level respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Into-Eng</head><p>Out-of-Eng metric seg τ sys γ seg τ sys γ BLEU 0. <ref type="bibr">220</ref>   <ref type="figure" target="#fig_1">rep. 0.259 0.874 0.191 0.</ref>832 Wghted avg. 0.247 0.863 0.185 0.798 <ref type="table">Table 1</ref>: Correlations with human judgment on WMT data for Into-English and Out-of-English task. Results are averaged on all test sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">General Performance</head><p>We first report the main experimental results con- ducted on the Into-English and Out-of-English tasks. Results in Tables 1 suggest that metrics based on three single representations all obtained comparable or better performance than BLEU, TER and Meteor. In particular, the metric based on recursive auto-encoder outperformed the other testing metrics on system-level. When combin- ing the strengths of the three representations, our experimental results show that the metric based on the combined representation outperformed all state-of-the-art metrics by a large margin on both segment-and system-level.</p><p>Regarding the evaluation speed of the represen- tation metrics, it took around 1 minute to score about 2000 sentences with the above settings on a machine with a 2.33GHz Intel CPU. It is worth noting that if we increase the vector size of the RAE model and word embeddings, longer execu- tion time is expected for the scoring processes.  WMT data for Into-English task for combined rep- resentation based metric when increasing the size of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effect of the Training Data Size</head><p>In our second experiment, we measure the per- formance on the Into-English task and increase the training data from 20K sentences to 11 mil- lion sentences. The sentences are randomly se- lected from the whole training data, which in- clude the English side of WMT 2013 French-to- English parallel data ("Europarl v7", "News Com- mentary" and "UN Corpus"). The results are re- ported in <ref type="figure" target="#fig_1">Figure 1</ref>. From this figure, one can con- clude that the performance improves with the in- creasing of the training data, however, when more than 1.28M sentences are used, the performance stabilizes. This result indicates that training a sta- ble and good model for our metric does not need a huge amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sensitivity to Data Across Domains</head><p>The last experiment aimed at the following ques- tion: should the test domain be consistent with the training domain? In this experiment, we sampled three training sets from different domain data sets in equal number (136K) of sentences: Europarl (EP), News Commentary (NC), and United Na- tion proceedings (UN), while the test domain re- mains the same, i.e., the news domain. The met- ric trained on NC domain data achieved slightly higher segment-level τ score (0.181 vs 0.178 for EP, 0.176 for UN) and system-level Pearson's cor- relation score γ (0.821 vs 0.820 for EP, 0.817 for UN). Nevertheless, the results are consistent across domains. This is explainable: although the same test sentence may have different representa- tions w.r.t. the training domain, the distance be- tween the translation and its reference may stay consistent. Practically, the training and test data not necessary being in the same domain is a very attractive characteristic for the translation metrics. It means that we do not have to train the word em- beddings and RAE model for each testing domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Cope with Word Ordering and Synonym</head><p>In order to better understand why metrics based on combined representations can achieve better cor- relation with human judgment than other metrics, we select, in <ref type="table">Table 2</ref>, some interesting examples for further analysis.</p><p>Consider, for instance, the first reference (de- noted as "1 R" in <ref type="table">Table 2</ref>) and their translations. If we replace the word vacation in the reference with words business and holiday, respectively, then we id sentence BLEU rep. 1 R i had a wonderful vacation in italy - - 1 H1 i had a wonderful business in italy 0.489 0.555 1 H2 i had a wonderful holiday in italy 0.489 0.865 1 H3 in italy i had a wonderful vacation 0.707 0.804 1 H4 vacation in i had a wonderful italy 0.508 0.305 2 R but the decision was not his to make - - 2 H1 but it is not up to him to decide 0.063 0.652 2 H2 but the decision not him to take 0.241 0.620 2 H3 but the decision was not the to make 0.595 0.612 3 R they were set to go on trial in jan - - 3 H1 they should appear in court in jan 0.109 0.498 3 H2 the trial was scheduled in jan 0.109 0.454 3 H3 the procedures were prepared in jan 0.109 0.445 <ref type="table">Table 2</ref>: Examples evaluated with smoothed BLEU and combined representation based metric. Examples 2-3 are picked up from the real test sets; human judgment ranks H1 better than H2, and H2 better than H3 for each of these example sentences. The combined representation based metric better matches human judgment than BLEU does.</p><p>have hypothesis 1 and hypothesis 2, denoted as "1 H1" and "1 H2", respectively, in <ref type="table">Table 2</ref> . In this scenario, the metric BLEU assigns the same score of 0.489 for these two translations. In contrast, the representation based metric associates hypothesis 2 with a much higher score than that of hypothesis 1, namely 0.865 and 0.555, respectively. In other words, the score for hypothesis 2 is close to one, suggesting that the RAE based metric considers this translation is almost identical to the reference. The reason here is that the vector representations for the two words are very near to one another in the vector space. Consequently, the representation based metric treats the holiday as a synonym of vacation, which matches human's judgment per- fectly.</p><p>Let us continue with this example. Suppose, in hypothesis 3, we reorder the phrase in italy. The representation based metric still considers this to be a good translation with respect to the reference, thus associating a very close score as that of the reference, namely 0.804. The reason for represen- tation metric's correct judgment is that H3 and the reference, in the vector space, embed very similar semantic knowledge, although they have different word orderings. Now let us take this example a bit further. We randomly mess up the words in the reference, resulting in hypothesis 4 (denoted as "1 H4" as shown in <ref type="table">Table 2</ref>). In such scenario, the representation metric score drops sharply because the syntactic and semantic information embedded in the vector space is very different from the refer- ence. Interestingly, the BLEU metric still consider this translation is not a very bad translation.</p><p>We made up the first example sentence for il- lustrative purpose, however, the examples 2-3 are picked up from the real test sets. According to the human judgment, hypothesis 1 (H1) is better than hypothesis 2 (H2); hypothesis 2 is better than hypothesis 3 (H3) for each of these example sen- tences. These results indicate that the combined representation based metric better matches the hu- man judgment than BLEU does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We studied a series of translation evaluation metrics based on three widely used representa- tions. Experiments on the WMT metric task in- dicate that the representation metrics obtain bet- ter correlations with human judgment on both system-level and segment-level, compared to pop- ular translation evaluation metrics such as BLEU, Meteor, Meteor Universal, and TER. Also, the representation-based metrics use only monolin- gual, unlabeled data for training; such data are easy to obtain. Furthermore, the proposed metrics are robust to various training conditions, such as the data size and domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Correlations with human judgment on WMT data for Into-English task for combined representation based metric when increasing the size of the training data.</figDesc></figure>

			<note place="foot" n="3"> Representations Based Metrics Our translation evaluation metrics are built on the four representations as discussed in Section 2.</note>

			<note place="foot" n="2"> http://www.statmt.org/wmt13/translation-task.html 3 http://www.cs.umd.edu/ snover/tercom/ 4 http://www.cs.cmu.edu/ alavie/METEOR/ 5 Meteor universal package does not include paraphrasing table for other target language except English, so we did not run Out-of-English experiments for this metric.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Colin Cherry and Roland Kuhn for useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of word representations for twitter named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA, June</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Authomatic evaluation of machine translation quality using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference</title>
		<meeting>the Human Language Technology Conference<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">128132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meant: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Results of the WMT13 metrics shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matouš</forename><surname>Macháček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Proceedings of a meeting held</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comunications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL)<address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Numerical Recipes in C++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flannery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ter-plus: Paraphrase, semantic, and alignment enhancements to translation edit rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">G</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Translation</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="117" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher Potts</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA,</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
	<note>October. Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
