<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Deep Relevance Model for Zero-Shot Document Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2300</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Cyber Science and Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Deep Relevance Model for Zero-Shot Document Filtering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2300" to="2310"/>
							<date type="published">July 15-20, 2018. 2018. 2300</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In the era of big data, focused analysis for diverse topics with a short response time becomes an urgent demand. As a fundamental task, information filtering therefore becomes a critical necessity. In this paper , we propose a novel deep relevance model for zero-shot document filtering, named DAZER. DAZER estimates the relevance between a document and a category by taking a small set of seed words relevant to the category. With pre-trained word embeddings from a large external corpus, DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process. The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner. Experiments on two document collections of two different tasks (i.e., topic catego-rization and sentiment analysis) demonstrate that DAZER significantly outper-forms the existing alternative solutions, including the state-of-the-art deep relevance ranking models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Filtering irrelevant information and organizing rel- evant information into meaningful topical cate- gories is indispensable and ubiquitous. For ex- ample, a data analyst tracking an emerging event would like to retrieve the documents relevant to a specific topic (category) from a large document collection in a short response time. In the era of big data, the potentially possible categories covered by documents would be limitless. It is unrealistic to manually identify a lot of posi- tive examples for each possible category. How- ever, new information needs indeed emerge ev- erywhere in many real-world scenarios. Recent studies on dataless text classification show promis- ing results on reducing labeling effort ( <ref type="bibr" target="#b22">Liu et al., 2004;</ref><ref type="bibr" target="#b4">Druck et al., 2008;</ref><ref type="bibr" target="#b1">Chang et al., 2008;</ref><ref type="bibr" target="#b35">Song and Roth, 2014;</ref><ref type="bibr" target="#b17">Hingmire et al., 2013;</ref><ref type="bibr" target="#b16">Hingmire and Chakraborti, 2014;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b21">Li et al., 2016)</ref>. Without any labeled document, a data- less classifier performs text classification by us- ing a small set of relevant words for each category (called "seed words"). However, existing dataless classifiers do not consider document filtering. We need to provide the seed words for each category covered by the document collection, which is of- ten infeasible in the real world.</p><p>To this end, we are particularly interested in the task of zero-shot document filtering. Here, zero- shot means that the instances of the targeted cat- egories are unseen during the training phase. To facilitate zero-shot filtering, we take a small set of seed words to represent a category of inter- est. This is extremely useful when the informa- tion need (i.e., the categories of interest) is dy- namic and the text collection is large and tempo- rally updated (e.g., the possible categories are hard to know). Specifically, we propose a novel deep relevance model for zero-shot document filtering, named DAZER. In DAZER, we use the word em- beddings learnt from an external large text cor- pus to represent each word. A category can then be well represented also in the embedding space (called category embedding) through some com- position with the word embeddings of the pro- vided seed words. Given a small number of seed words provided for a category as input, DAZER is devised to produce a score indicating the rele- vance between a document and the category. It is intuitive to connect zero-shot document filtering with the task of ad-hoc retrieval. Indeed, by treat- ing the seed words of each category as a query, the zero-shot document filtering is equivalent to ranking documents based on their relevance to the query. The relevance ranking is a core task in in- formation retrieval, and has been studied for many years. Although they share the same formulation, these two tasks diverge fundamentally. For ad-hoc retrieval, a user constructs a query with a specific information need. The relevant documents are as- sumed to contain these query words. This is con- firmed by the existing works that exact keyword match is still the most important signal of rele- vance in ad-hoc retrieval <ref type="bibr" target="#b6">(Fang and Zhai, 2006;</ref><ref type="bibr" target="#b37">Wu et al., 2007;</ref><ref type="bibr" target="#b5">Eickhoff et al., 2015;</ref><ref type="bibr">Guo et al., 2016a,b)</ref>.</p><p>For document filtering, the seed words for a cat- egory are expected to convey the conceptual mean- ing of the latter. It is impossible to list all the words to fully cover the relevant documents of a category. Therefore, it is essential to capture the conceptual relevance for zero-shot document fil- tering. The classical retrieval models simply es- timate the relevance based on the query keyword matching, which is far from capturing the concep- tual relevance. The existing deep relevance mod- els for ad-hoc retrieval utilize the statistics of the hard/soft-match signals in terms of cosine simi- larity between two word embeddings ( <ref type="bibr" target="#b13">Guo et al., 2016a;</ref><ref type="bibr" target="#b38">Xiong et al., 2017)</ref>. However, the scalar in- formation like cosine similarity between two em- bedding vectors is too coarse or limited to reflect the conceptual relevance. On the contrary, we be- lieve that the embedding features could provide rich knowledge towards the conceptual relevance. A key challenge is to endow DAZER a strong generalization ability to also successfully extract the relevance signals for unseen categories. To achieve this purpose, we extract the relevance sig- nals based on the hidden feature interactions be- tween the category and each word in the embed- ding space. Specifically, two element-wise opera- tions are utilized in DAZER: element-wise sub- traction and element-wise product. Since these two kinds of interactions represent the relative in- formation encoded in hidden embedding space, we expect that the relevance signal extraction pro- cess could generalize well to unseen categories. Firstly, DAZER utilizes a gated convolutional op- eration with k-max pooling to extract the rele- vance signals. Then, DAZER abstracts higher- level relevance features through a multi-layer per- ceptron, which can be considered as a relevance aggregation procedure. At last, DAZER calcu- lates an overall score indicating the relevance be- tween a document and the category. Without fur- ther constraints, it is possible for DAZER to en- code the bias towards the category-specific fea- tures seen during the training (i.e., model over- fitting). Therefore, we further introduce an ad- versarial learning over the output of the relevance aggregation procedure. The purpose is to ensure that the higher-level relevance features contain no category-dependent information, leading to a bet- ter zero-shot filtering performance.</p><p>To the best of our knowledge, DAZER is the first deep model to conduct zero-shot document filtering. We conduct extensive experiments on two real-world document collections from two different domains (i.e., 20-Newsgroup for topic categorization, and Movie Review for sentiment analysis). Our experimetnal results suggest that DAZER achieves promising filtering performance and performs significantly better than the exist- ing alternative solutions, including state-of-the-art deep relevance ranking methods. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the network structure of the proposed DAZER model. It consists of two main components: relevance signal extraction and rel- evance aggregation. In the following, we present each component in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Zero-Shot Document Filtering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Relevance Signal Extraction</head><p>Given a document d = (w 1 , w 2 , ..., w |d| ) and a set of seed words S c = {s c,i } for category c, we first map each word w into its dense word embedding representation e w ∈ R le where l e denotes the di- mension number. The embedding representation is pre-trained by using a representation learning method from an external large text corpus. Since our aim is to capture the conceptual relevance, we simply take the averaged embedding of the seed words to represent a category in the embedding space: c c = 1/|S c | s∈Sc e s . Interaction-based Representation. It is widely recognized that word embeddings are useful be- cause both syntactic and semantic information of words are well encoded ( <ref type="bibr" target="#b23">Mikolov et al., 2013;</ref><ref type="bibr" target="#b29">Pennington et al., 2014</ref>). The element-wise hid- den feature difference is a kind of relative infor-   <ref type="table" target="#tab_0">Table 1</ref> lists several interesting patterns observed for the embedding offsets be- tween a category and a word in 20-Newsgroup dataset (ref. Section 3.2 for more details). We can see that the embedding offsets are somehow con- sistent with a particular relation between the two category-word pairs.</p><p>An effective way to measure the relatedness for two words is the inner product or cosine similarity between two corresponding word em- beddings. This can be considered as a partic- ular linear combination of corresponding feature products for the two embeddings:</p><formula xml:id="formula_0">rel(e 1 , e 2 ) = i g(e 1 , e 2 , i)e 1,i · e 2,i = g(e 1 , e 2 ) T (e 1 e 2 )</formula><p>where g(e 1 , e 2 , i) refers to the weight cal- culated for i-th dimension, and g(e 1 , e 2 ) = [g(e 1 , e 2 , 1); ...; g(e 1 , e 2 , l e )], is the element- wise product operation. The element-wise product between two embeddings is also a kind of relative information. The sign of a product of two em- beddings in a specific dimension indicates whether the two embeddings share the same polarity in this dimension. And the resultant value manifests to what extent that this agreement/disagreement reaches. It is intuitive that the element-wise <ref type="table">Table 2</ref>: Examples by using element-wise prod- uct.</p><formula xml:id="formula_1">Examples sign(c mideast e muslim ) ≈ sign(c med e doctor ) sign(cspace e orbit ) ≈ sign(c hockey eespn) sign(c electronics ecircuit) ≈ sign(cpc e controller ) sign(ccrypt e algorithm ) ≈ sign(cspace e burning )</formula><p>product offers some kinds of semantic relations. We conduct the element-wise product for each category-word pair in 20-Newsgroup dataset. Ta- ble 2 lists some interesting patterns we observe. The sign(x) function returns 1 when x ≥ 0, oth- erwise return −1. Shown in the table, the sign pattern of the element-wise product encodes the relevance information between a category and its related words.</p><p>Inspired by these observations, we use these two kinds of element-wise interactions to comple- ment the representation of a word in a document. Specifically, for each word w in document d, we derive its interaction-based representation e c w to- wards category c as follows:</p><formula xml:id="formula_2">e dif f c,w = c c − e w (1) e prod c,w = c c e w (2) e c w = [e w ⊕ e dif f c,w ⊕ e prod c,w ] (3)</formula><p>where ⊕ is the vector concatenation operation. Note that these two kinds of feature interactions are mainly overlooked by the existing literature. The embedding offsets are used in deriving word semantic hierarchies in ( <ref type="bibr" target="#b8">Fu et al., 2014</ref>). How- ever, there is no existing work incorporating these two kinds of feature interactions for relevance es- timation. Here, we expect that these two kinds of feature interactions can magnify the relevance in- formation regarding the category.</p><p>Convolution with k-max Pooling. We utilize m convolution filters to extract the relevance sig- nals for each word based on its local window of size l in the document. Specifically, after cal- culating the interaction-based representation d = (e c 1 , e c 2 , ..., e c |d| ) for document d and category c, we apply the convolution operation as follows:</p><formula xml:id="formula_3">r i = W 1 e c i−l:i+l + b 1 (4)</formula><p>where r i ∈ R m is the hidden features regard- ing the relevance signal extracted for i-th word, W 1 ∈ R m×3le(2l+1) and b 1 ∈ R m are the weight matrix and the corresponding bias vector respec- tively, e c i−l:i+l refers to the concatenation from e c i−l to e c i+l . Both l zero vectors are padded to the begining and the end of the document. With a local window of size l, the convolution operation can extract more accurate relevance information by taking the consecutive words (e.g., phrases) into account. We then apply k-max pooling strat- egy to obtain the k most active features for each filter. Let r j k−max denote the k largest values for filter j, we form the overall relevance signals r d extracted by all m filters through the concatena- tion:</p><formula xml:id="formula_4">r c,d = [r 1 k−max ⊕ r 2 k−max ... ⊕ r m k−max ].</formula><p>Category-specific Gating Mechanism. Given a specific word w, the interaction-based representa- tion e c w for each category c could be very differ- ent. Therefore, for a specific local context, the extracted relevance signal from a particular con- volution filter could be also distinct for different categories. It is then reasonable to assume that the relevance signals for a specific category are cap- tured by a subset of filters. We propose to identify which filters are relevant to a category through a category-specific gating mechanism. Given cate- gory c, category-specific gates a c ∈ R m are calcu- lated as follows:</p><formula xml:id="formula_5">a c = σ(W 2 e c + b 2 )<label>(5)</label></formula><p>where W 2 ∈ R m×3le and b 2 ∈ R m are the weight matrix and bias vectors respectively, σ(·) is the sigmoid function. With category-specific gating mechanism, Equation 4 can be rewritten as fol- lows:</p><formula xml:id="formula_6">r i = a c (W 1 e c i−l:i+l + b 1 )</formula><p>Here, a c works as on-off switches for m filters. While a c,j → 1 indicates that j-th filter should be turned on to capture the relevance singals under category c to its fullness, a c,j → 0 indicates that the filter is turned off due to its irrelevance. This collaboration of the convolution operation and gating mechanism is similar to the Gated Lin- ear Units (GLU) recently proposed in ( <ref type="bibr" target="#b3">Dauphin et al., 2017)</ref>. Given an input X, GLU calculates the output as follow: h(X) = (XW + b) σ(XV + c) where the first term in the right side refers to the convolution operation and the second term in the right side refers to the gating mech- anism. In GLU, both the convolution operation and the gates share the same input X. In contrast, in this work, we aim to identify which filters cap- ture the relevance signals in a category-dependent manner. The experimental results validate that this category-dependent setting brings significant ben- efit for zero-shot filtering performance (ref. Sec- tion 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relevance Aggregation</head><p>The raw relevance signals r c,d are somehow category-dependent, since the relevant filters are category-dependent. The hidden features regard- ing the relevance are distilled through a fully- connected hidden layer with nonlinearity:</p><formula xml:id="formula_7">h c,d = g a (W 3 r c,d + b 3 )<label>(6)</label></formula><p>where W 3 ∈ R la×3km and b 3 ∈ R la are the weight matrix and bias vector respectively, g a (·) is the tanh function. This procedure can be con- sidered as a relevance aggregation process. Then, the overall relevance score is then estimated as fol- low:</p><formula xml:id="formula_8">f (c|d) = tanh(w T h c,d + b)<label>(7)</label></formula><p>where w ∈ R la and b are the parameters and bias respective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Training</head><p>Adversarial Learning The hidden features h c,d are expected to be category-independent. How- ever, there is no guarantee that the category- specific information is not mixed with the rele- vance information extracted in h c,d . Here, we in- troduce an adversarial learning mechanism to en- sure that no category-specific information can be memorized during the training. Otherwise, the proposed DAZER may not generalize well to un- seen categories. Specifically, we introduce an cat- egory classifier over h c,d to calculate the probabil- ity that h c,d belongs to each category seen during the training:</p><formula xml:id="formula_9">p cat (·|h c,d ) = sof tmax(W 4 h c,d + b 4 )</formula><p>where W 4 ∈ R C×la and b 4 ∈ R C are the weight matrix and bias vector for the clas- sifier, C is the number of categories covered by the training set. We aim to optimize param- eters φ = {W 4 , b 4 } to successfully classify h c,d to its true category. Let θ denote the pa- rameters regarding the calculation of h c,d , i.e.,</p><formula xml:id="formula_10">θ = {W 1 , W 2 , W 3 , b 1 , b 2 , b 3 }, φ</formula><p>is optimized to minimize the negative log-likelihood:</p><formula xml:id="formula_11">L cat (θ, φ) = 1 |T| (d,y)∈T −p cat (y|h y,d )<label>(8)</label></formula><p>where T denotes the training set {(d, y)} such that document d is relevant to category y. On the other hand, we expect that h c,d carries no cate- gory specific information, such that the classifier can not perform the category classification pre- cisely. Hence, we add the Gradient Reversal Layer (GRL) ( <ref type="bibr" target="#b9">Ganin and Lempitsky, 2015;</ref><ref type="bibr" target="#b10">Ganin et al., 2016</ref>) between h c,d and the category classifier. We can consider GRL as a pseudo-function R λ (x):</p><formula xml:id="formula_12">R λ (x) = x; ∂R λ ∂x = −λI<label>(9)</label></formula><p>It means that θ is optimized to make h c,d indis- tinguishable by the classifier. In Equation 9, pa- rameter λ controls the importance of the adversar- ial learning. DAZER is devised to return a rel- evance score, we utilize the pairwise margin loss for model training:</p><formula xml:id="formula_13">L hinge (θ, δ) = 1 |T| (d,y)∈T max(0, ∆ − f (y|d) + f (y|d − y ))<label>(10)</label></formula><p>where document d − y is the negative sample for cat- egory y, ∆ is the margin and set to be 1 in this work, and δ = {w, b}. Overall, the proposed DAZER is an end-to-end neural network model. The parameters Θ = {θ, φ, δ} are optimized via back propagation and stochastic gradient descent. Specifically, we utilize Adam ( <ref type="bibr" target="#b20">Kingma and Ba, 2014</ref>) algorithm for parameter update over mini- batches. The final objective loss used in the train- ing is as follow:</p><formula xml:id="formula_14">L(Θ) =L hinge (θ, δ) + L cat (θ, φ) + λ Θ Θ 2<label>(11)</label></formula><p>where λ Θ controls the importance of the regular- izaton term. <ref type="table">Seed Words  very negative  bad, horrible, negative, disgusting  negative  bad, confused, unexpected, useless, negative  neutral  normal, moderate, neutral, objective, impersonal  positive</ref> good, positive, outstanding, satisfied, pleased very positive positive, impressive, unbelievable, awesome </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head><p>In this section, we conduct experiments on two real-world document collections to evaluate the ef- fectiveness of the proposed DAZER 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Existing Alternative Methods</head><p>Here, we compare the proposed DAZER against the following alternative solutions.</p><p>BM25 Model: BM25 is a widely known retrieval model based on keyword matching <ref type="bibr" target="#b33">(Robertson and Walker, 1994)</ref>. The default parameter setting is used in the experiments.</p><p>DSSM: DSSM utilizes a multi-layer perceptron to extract hidden representations for both the docu- ment and the query ( <ref type="bibr" target="#b19">Huang et al., 2013</ref>). Then, co- sine similarity is calculated as the relevance score based on the representation vectors. Since we use pre-trained word embeddings from a large text corpus, we choose to replace the letter-tri-grams representation with the word embedding represen- tation instead. We use the recommended network setting by its authors.</p><p>DRMM: DRMM calculates the relevance based on the histogram information of the semantic relat- edness scores between each word in the document and each query word ( <ref type="bibr" target="#b13">Guo et al., 2016a</ref>). The rec- ommended network setting (i.e., LCH×IDF) and parameter setting are used.</p><p>K-NRM: K-NRM is a kernel based neural model for relevance ranking based on word-level hard/soft matching signals ( <ref type="bibr" target="#b38">Xiong et al., 2017)</ref>. We use the recommended setting as in their paper.</p><p>DeepRank: DeepRank is a neural relevance ranking model based on the query-centric con- text ( <ref type="bibr" target="#b28">Pang et al., 2017</ref>). The recommended setting is used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed-based Support Vector Machines (SSVM):</head><p>We build a seed-driven training set by labeling a training document with a category if the document <ref type="bibr">1</ref> The implementation is available at https://github.com/WHUIR/ DAZER contains any seed word of that category. Then, we adopt a one-class SVM implemented by sklearn 2 for document filtering <ref type="bibr">3</ref> . The optimal performance is reported by tuning the hyper-parameter. For each sentiment label, we randomly split the reviews into a training set (80%) and a test set (20%). Since our work targets at zero-shot document filtering for unseen categories, the word embed- dings pre-trained by Glove over a large text corpus with total 840 billion tokens 6 are used across all the methods and the two datasets. The dimension of the word embeddings is l e = 300. No further word embedding fine-tuning is applied. For both datasets, the stop words are removed firstly. Then, all the words are converted into their lowercased forms. We further remove the words whose word embeddings are not supported by Glove.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets and Experimental</head><p>Evaluation Protocol. With the specified unseen categories, we take all the training documents of the other categories to train a model. Then, all documents in the test set are used for evaluation. For each unseen category, the task is to rank the documents of that category higher than the oth- ers. Here, we choose to report mean average pre- cision (MAP) for performance evaluation. MAP is a widely used metric to evaluate the ranking qual- ity. The higher the relevant documents are ranked, the larger the MAP value is, which means a bet- ter filtering performance. For all neural networks based models, the training documents from one randomly sampled training category work as the validation set for early stop. We report the aver- aged results over 5 runs for all the methods (ex- cluding SSVM and BM25). The statistical signifi- cance is conducted by applying the student t-test.</p><p>Seed Word Selection. For 20NG dataset, we directly use the seed words 7 manually compiled in <ref type="bibr" target="#b35">(Song and Roth, 2014</ref>). These seed words are selected from the category descriptions and widely used in the works of dataless text classi- fication ( <ref type="bibr" target="#b35">Song and Roth, 2014;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b21">Li et al., 2016</ref>). For Movie Review, following the seed word selection process (i.e., assisted by stan- dard LDA) proposed in ( <ref type="bibr" target="#b2">Chen et al., 2015)</ref>, we manually select the seed words for each sentiment label. <ref type="table" target="#tab_1">Table 3</ref> lists the seed words selected for each sentiment label for Movie Review dataset. There are on average 5.2 and 4.6 seed words for each cat- egory over 20NG and Movie Review respectively. It is worthwhile to highlight that no category infor- mation is exploited within the seed word selection process.</p><p>Parameter Setting. For DAZER, the number of convolution filters is m = 50 and k = 3 is used for k-max pooling. The dimension size for rele- vance aggregation is l a = 75. The local window size l is set to be 2. The learning rate is 0.00001. The models are trained with a batch size of 16 and λ Θ = 0.0001, λ = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance Comparison</head><p>For 20NG dataset, we randomly create 9 docu- ment filtering tasks which cover 10 out of 20 cate- gories. For Movie Review, we take each sentiment label as an unseen category for evaluation. Ta- ble 4 lists the performance of 7 methods in terms of MAP for these filtering tasks. Here, we make the following observations. First, the proposed DAZER significantly achieves much better filtering performance on all 14 tasks across the two datasets. The averaged MAP of DAZER over these 14 filtering tasks is 0.671. Note that only 5.2 and 4.6 seed words are used on average for each task. The second best performer is K-NRM, which achieves the second  <ref type="table">Table 4</ref>: Performance of the 7 methods for zero-shot document filtering in terms of MAP. The best and second best results are highlighted in boldface and underlined respectively, on each task. † indicates that the difference to the best result is statistically significant at 0.05 level. Avg: averaged MAP over all tasks.</p><p>best on 7 tasks. Overall, the averaged performance gain for DAZER over K-NRM is about 30.8%. Second, We observe that DSSM performs sign- ficantly better for sentiment analysis than for topic categorization. As discussed in Section 4, DSSM is designed to perform semantic matching. Com- pared with topic categorization, sentiment analy- sis is more like a semantic matching task. SSVM delivers the worst performance on both datasets. This illustrates that the quality of the labeled doc- uments is essential for supervised learning tech- niques. Apparently, recruiting training documents with the provided seed words in a simple fashion is error-prone. We also note that BM25 achieves inconsistent performance over the two kinds of tasks. It performs especially worse for sentiment analysis. This is reasonable because there are more diverse ways to express a specific sentiment. It is hard to cover a reasonable proportion of doc- uments with limited number of sentimental seed words. In comparison, the proposed DAZER ob- tains a consistent performance for both topic cate- gorization and sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis of DAZER</head><p>Component Setting. Here, we further discuss the impact of different component settings of DAZER on both 20NG and Movie Review datasets. Ta- ble 5 and 6 report the impacts of each component setting via an ablation test on the two datasets re- spectively. We can see that each component brings significantly positive benefit for document filter- ing. First, we can see that either element-wise sub- traction or product contributes signifcantly to the performance improvement. Specifically, from Ta- ble 6, we can see that both the element-wise sub- traction and element-wise product play equally on Movie Review dataset. On the other hand, it is observed that DAZER experiences significantly a much larger performance degradation on 20NG dataset. For example, a MAP of only 0.154 is achieved when e prod c,w is excluded from DAZER for the filtering task space. A much severer case is for the filtering task baseball-hockey. By ex- cluding e prod c,w , the MAP performance of DAZER is reduced from 0.782 to 0.045. That is, the element-wise product is more critical for extract- ing relevance signals for topical categorization. We also observe that these two hidden feature in- teractions together play a more important role for DAZER. For example, without both e dif f c,w and e prod c,w , DAZER only achieves a MAP of 0.126 for filtering task space. The large performance deteri- oration is also observed for other filtering tasks on 20NG dataset.</p><p>Either adversarial learning or category-specific gate mechanism enhances the filtering perfor- mance of DAZER, which validates the effective- ness of the two components for enhancing con-   <ref type="table">Table 6</ref>: Impact of different settings for DAZER on Movie Review. The best results are highlighted in boldface. -e dif f c,w : no element-wise subtraction; -e prod c,w : no element-wise product; -Gate: no category- specific gate mechanism; -Adv: no adversarial learning. ceptual relevance extraction. Also, without using adversarial learning, DAZER still achieves much better filtering performance than the existing base- line methods compared in Section 3.3. This obser- vation is also held on 20NG dataset. This further validates that the two kinds of hidden feature in- teractions indeed encode rich knowledge towards the conceptual relevance.</p><p>Impact of Seed Words. It has been recognized that the less seed words incur worse document classification performance in the existing data- less document classification techniques <ref type="bibr" target="#b35">(Song and Roth, 2014;</ref><ref type="bibr" target="#b2">Chen et al., 2015;</ref><ref type="bibr" target="#b21">Li et al., 2016)</ref>. Following these works, we also use the words ap- pearing in the category name of 20NG dataset as the corresponding seed words 8 . There are on aver- age 2.75 seed words for a category of 20NG. Ta- ble 7 reports the MAP performace of each method on 20NG dataset. The experimental results show that all methods investigated in Section 3.3 ex- perience signficant performance degradation for most filtering tasks. We plan to incorporate the pseudo-relevance feedback into DAZER to tackle the scarcity of the seed words. One possible so- lution is to enrich the architecture of DAZER to allow few-shot document filtering. That is, the fil- tering decisions of high-confidence are utilized to derive more seed words for better filtering perfor- mance. <ref type="bibr">8</ref> The seed words based on the category name are available at https://github.com/WHUIR/STM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Document filtering is the task to separate rele- vant documents from the irrelevant ones for a spe- cific topic <ref type="bibr" target="#b32">(Robertson and Soboroff, 2002;</ref><ref type="bibr" target="#b25">Nanas et al., 2010;</ref><ref type="bibr" target="#b12">Gao et al., , 2015</ref><ref type="bibr" target="#b30">Proskurnia et al., 2017)</ref>. Both ranking and classifica- tion based solutions have been developed <ref type="bibr" target="#b15">(Harman, 1994;</ref><ref type="bibr" target="#b32">Robertson and Soboroff, 2002;</ref><ref type="bibr" target="#b34">Soboroff and Robertson, 2003)</ref>. In earlier days, a fil- tering system is mainly devised to facilitate the document retrieval for the long-term information needs ( <ref type="bibr" target="#b24">Mostafa et al., 1997</ref>). The term-based pattern mining techniques are widely developed to perform document filtering. A network-based topic profile is built to exploit the term correla- tion patterns for document filtering ( <ref type="bibr" target="#b25">Nanas et al., 2010)</ref>. Frequent term patterns in terms of fine- grained hidden topics are proposed in ( <ref type="bibr" target="#b12">Gao et al., , 2015</ref>) for doucment filtering. Very recently, frequent term patterns are also utilized to perform event-based microblog filtering ( <ref type="bibr" target="#b30">Proskurnia et al., 2017)</ref>. However, these approaches are all based on supervised-learning, which requires a signifi- cant amount of positive documents for each topic. In the era of big data, the information space and new information needs are continuously growing. Retrieval of the relevance information in a short response time becomes a fundamental need. Re- cently, many works have been proposed to con- duct document filtering in an entity-centric man- ner ( <ref type="bibr" target="#b7">Frank et al., 2012;</ref><ref type="bibr" target="#b0">Balog and Ramampiaro, 2013;</ref><ref type="bibr" target="#b39">Zhou and Chang, 2013;</ref><ref type="bibr" target="#b31">Reinanda et al., 2016)</ref>. The task is to identify the documents rele- vant to a specific entity that is well defined in an  <ref type="table">Table 7</ref>: Performance of the 7 methods for zero-shot document filtering in terms of MAP. The words ap- pearing in the category name are used as the seed words. The best and second best results are highlighted in boldface and underlined respectively, on each task.</p><p>external knowledge base. Specifically, <ref type="bibr" target="#b0">Balog and Ramampiaro (2013)</ref> examine the choice of classi- fication against ranking approaches. They found that ranking approach is more suitable for the fil- tering task. Following this conclusion, we formu- late the zero-shot document filtering as a relevance ranking task. Many information needs may not be well represented by a specific entity. Hence, these entity-centric solutions are restricted to knowledge base related tasks.</p><p>Many ad-hoc retrieval models can be used to perform zero-shot document filtering. In- deed, traditional term-based document filtering approaches utilize many term-weighting schemes developed for ad-hoc retrieval. Traditional ad- hoc retrieval models mainly estimate the relevance based on keyword matching. BM25 <ref type="bibr" target="#b33">(Robertson and Walker, 1994)</ref> can be considered as the op- timal practice in this line of literature. The re- cent advances in word embedding offer effective learning of word semantic relations from a large external corpus. Several neural relevance ranking models are proposed to preform ad-hoc retrieval based on word embeddings. Both K-NRM ( <ref type="bibr" target="#b38">Xiong et al., 2017</ref>) and DRMM ( <ref type="bibr" target="#b13">Guo et al., 2016a</ref>) es- timate the relevance based on the macro-statistics of the hard/soft-match signals in terms of cosine similarity between two word embeddings. Deep- Rank ( <ref type="bibr" target="#b28">Pang et al., 2017</ref>) first measures the rel- evance signals from the query-centric context of each query keyword matching point through con- volutional operations. Then, RNN based networks are adopted to aggregate these relevance signals. These works achieve significantly better retrieval performance than the keyword matching based so- lutions and represent the new state-of-the-art. The relevance between a query and a document can also be considered as a matching task between two pieces of text. There are many deep matching models, e.g., <ref type="bibr">DSSM (Huang et al., 2013)</ref>, ARC- II ( <ref type="bibr" target="#b18">Hu et al., 2014</ref>), <ref type="bibr">MatchPyramid (Pang et al., 2016)</ref>, Match-SRNN ( ). These models are mainly developed for some specific semantic matching tasks, e.g., paraphrase identi- fication. Therefore, information like grammati- cal structure or sequence of words are often taken into consideration, which is not applicable to seed word based zero-shot document filtering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel deep relevance model for zero-shot document filtering, named DAZER. To enable DAZER to capture con- ceptual relevance and generalize well to unseen categories, two kinds of feature interactions, a gated convolutional network and an category- independent adversarial learning are devised. The experimental results over two different tasks val- idate the superiority of the proposed model. In the future, we plan to enrich the architecture of DAZER to allow few-shot document filtering by incorporating several labeled examples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of DAZER Examples c atheism − e atheist ≈ c baseball − e hitter cautos − etoyata ≈ c motorcycles − e yamaha c baseball − e stadium ≈ c med − e hosptial c religion.misc − e f aith ≈ c med − epatient</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>No.61502344), Natural Science Foundation of Hubei Province (No.2017CFB502), Natural Scientific Research Program of Wuhan Univer- sity (No.2042017kf0225). Chenliang Li is the corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Examples by using embedding offset. 

mation that captures the offset bettwen a word 
and a category in the embedding space. These 
embedding offsets contain more intricate relation-
ships for a word pair. A well known example 
is: e king − e queen ≈ e man − e woman (Mikolov 
et al., 2013). Similar observations are made when 
we calculate the embedding offset between words 
and categories. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 : Seed words selected for Movie Review.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 : Impact of different settings for DAZER on 20NG. The best results are highlighted in boldface.</head><label>5</label><figDesc></figDesc><table>-e dif f 
c,w : no element-wise subtraction; -e prod 
c,w : no element-wise product; -Gate: no category-specific gate 
mechanism; -Adv: no adversarial learning. 

Setting 
very negative 
negative 
neutral 
positive 
very positive 

DAZER 
0.290 
0.807 
0.798 
0.862 
0.479 
-e dif f 

c,w 

0.246 
0.773 
0.776 
0.847 
0.453 
-e prod 

c,w 

0.258 
0.779 
0.785 
0.847 
0.430 
-Gate 
0.278 
0.755 
0.785 
0.848 
0.429 
-Adv 
0.261 
0.779 
0.776 
0.827 
0.444 

</table></figure>

			<note place="foot" n="2"> http://scikit-learn.org 3 Signed distance to the separating hyperplan is used for ranking documents. 4 http://qwone.com/ ˜ jason/20Newsgroups/ 5 The Movie Review dataset is available at http://www.cs. cornell.edu/people/pabo/movie-review-data/ 6 https://nlp.stanford.edu/projects/glove/</note>

			<note place="foot" n="7"> The seed words are available at https://github.com/WHUIR/ STM</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cumulative citation recommendation: classification vs. ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heri</forename><surname>Ramampiaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="941" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Importance of semantic representation: Dataless classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="830" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dataless text classification with descriptive LDA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning from labeled features using generalized expectation criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Druck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An eye-tracking study of query reformulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Dungs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vu</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic term matching in axiomatic approaches to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building an entity-centric stream filtering test collection for TREC 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kleiman-Weiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies via word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Patternbased topic models for information filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="921" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Patternbased topics for document modelling in information filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1629" to="1642" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic matching by non-linear word transportation for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Overview of the third text retrieval conference (TREC-3). In TREC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donna</forename><surname>Harman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Topic labeled text classification: A weakly supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Hingmire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutanu</forename><surname>Chakraborti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Document classification by topic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Hingmire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Chougule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><forename type="middle">K</forename><surname>Palshikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutanu</forename><surname>Chakraborti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="877" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective document labeling with very few seed words: A topic model approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyang</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Text classification by labeling words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multilevel approach to intelligent information filtering: Model, system, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javed</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snehasis</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><forename type="middle">J</forename><surname>Palakal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="368" to="399" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A network-based model for highdimensional information filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Nanas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Vavalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">N</forename><surname>De Roeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="202" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Text matching as image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengxian</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2793" to="2799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeprank: A new deep architecture for relevance ranking in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient document filtering using vector space topic expansion and pattern-mining: The case of event detection in microposts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Proskurnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Mavlyutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Aberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Cudré-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Document filtering for long-tail entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ridho</forename><surname>Reinanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="771" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The TREC 2002 filtering track report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Building a filtering test collection for TREC 2002</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="243" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On dataless hierarchical text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1579" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Match-srnn: Modeling the recursive matching structure with spatial RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2922" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A retrospective study of a hybrid document-context based retrieval model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">W P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1308" to="1331" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Entity-centric document filtering: boosting feature mapping through meta-features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin Chen-Chuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
