<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
							<email>zengdj@csust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Changsha University of Science &amp; Technology</orgName>
								<address>
									<postCode>410114</postCode>
									<settlement>Changsha</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="volume">506</biblScope>
							<biblScope unit="page" from="506" to="514"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. We divided the sentences into three types according to triplet overlap degree, including Normal, EntityPairOverlap and SingleEn-tiyOverlap. Existing methods mainly focus on Normal class and fail to extract re-lational triplets precisely. In this paper, we propose an end-to-end model based on sequence-to-sequence learning with copy mechanism, which can jointly extract rela-tional facts from sentences of any of these classes. We adopt two different strategies in decoding process: employing only one united decoder or applying multiple separated decoders. We test our models in two public datasets and our model outperform the baseline method significantly.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, to build large structural knowledge bases (KB), great efforts have been made on extract- ing relational facts from natural language texts. A relational fact is often represented as a triplet which consists of two entities (an entity pair) and a semantic relation between them, such as &lt; Chicago, country, U nitedStates &gt;.</p><p>So far, most previous methods mainly focused on the task of relation extraction or classification which identifies the semantic relations between t- wo pre-assigned entities. Although great progress- es have been made <ref type="bibr" target="#b7">(Hendrickx et al., 2010;</ref><ref type="bibr" target="#b19">Zeng et al., 2014;</ref><ref type="bibr">Xu et al., 2015a,b)</ref>, they all assume that the entities are identified beforehand and ne- glect the extraction of entities. To extract both of entities and relations, early works( <ref type="bibr" target="#b18">Zelenko et al., 2003;</ref><ref type="bibr" target="#b0">Chan and Roth, 2011</ref>  <ref type="figure">Figure 1</ref>: Examples of Normal, EntityPairOver- lap (EPO) and SingleEntityOverlap (SEO) class- es. The overlapped entities are marked in yel- low. S1 belongs to Normal class because none of its triplets have overlapped entities; S2 belongs to EntityPairOverlap class since the entity pair &lt; Sudan, Khartoum &gt; of it's two triplets are overlapped; And S3 belongs to SingleEntityOver- lap class because the entity Aarhus of it's two triplets are overlapped and these two triplets have no overlapped entity pair.</p><p>manner, where they first conduct entity recogni- tion and then predict relations between extract- ed entities. However, the pipeline framework ig- nores the relevance of entity identification and re- lation prediction ( <ref type="bibr" target="#b11">Li and Ji, 2014</ref>). Recent work- s attempted to extract entities and relations joint- ly. <ref type="bibr" target="#b17">Yu and Lam (2010)</ref>; <ref type="bibr" target="#b11">Li and Ji (2014)</ref>; <ref type="bibr" target="#b13">Miwa and Sasaki (2014)</ref> designed several elaborate fea- tures to construct the bridge between these two subtasks. Similar to other natural language pro- cessing (NLP) tasks, they need complicated fea- ture engineering and heavily rely on pre-existing NLP tools for feature extraction. Recently, with the success of deep learning on many NLP tasks, it is also applied on relation- al facts extraction. <ref type="bibr" target="#b19">Zeng et al. (2014)</ref>; <ref type="bibr">Xu et al. (2015a,b)</ref> employed CNN or RNN on relation classification. <ref type="bibr" target="#b12">Miwa and Bansal (2016)</ref>; <ref type="bibr" target="#b5">Gupta et al. (2016)</ref>; <ref type="bibr" target="#b20">Zhang et al. (2017)</ref> treated relation extraction task as an end-to-end (end2end) table- filling problem. <ref type="bibr" target="#b21">Zheng et al. (2017)</ref> proposed a novel tagging schema and employed a Recurrent Neural Networks (RNN) based sequence labeling model to jointly extract entities and relations.</p><p>Nevertheless, the relational facts in sentences are often complicated. Different relational triplet- s may have overlaps in a sentence. Such phe- nomenon makes aforementioned methods, what- ever deep learning based models and traditional feature engineering based joint models, always fail to extract relational triplets precisely. Generally, according to our observation, we divide the sen- tences into three types according to triplet over- lap degree, including Normal, EntityPairOverlap (EPO) and SingleEntityOverlap (SEO). As shown in <ref type="figure">Figure 1</ref>, a sentence belongs to Normal class if none of its triplets have overlapped entities. A sen- tence belongs to EntityPairOverlap class if some of its triplets have overlapped entity pair. And a sentence belongs to SingleEntityOverlap class if some of its triplets have an overlapped entity and these triplets don't have overlapped entity pair. In our knowledge, most previous methods focused on Normal type and seldom consider other type- s. Even the joint models based on neural network ( <ref type="bibr" target="#b21">Zheng et al., 2017)</ref>, it only assigns a single tag to a word, which means one word can only partici- pate in at most one triplet. As a result, the triplet overlap issue is not actually addressed.</p><p>To address the aforementioned challenge, we aim to design a model that could extract triplets, including entities and relations, from sentences of Normal, EntityPairOverlap and SingleEntityOver- lap classes. To handle the problem of triplet over- lap, one entity must be allowed to freely partici- pate in multiple triplets. Different from previous neural methods, we propose an end2end model based on sequence-to-sequence (Seq2Seq) learn- ing with copy mechanism, which can jointly ex- tract relational facts from sentences of any of these classes. Specially, the main component of this model includes two parts: encoder and decoder. The encoder converts a natural language sentence (the source sentence) into a fixed length semantic vector. Then, the decoder reads in this vector and generates triplets directly. To generate a triplet, firstly, the decoder generates the relation. Second- ly, by adopting the copy mechanism, the decoder copies the first entity (head entity) from the source sentence. Lastly, the decoder copies the second entity (tail entity) from the source sentence. In this way, multiple triplets can be extracted (In de- tail, we adopt two different strategies in decod- ing process: employing only one unified decoder (OneDecoder) to generate all triplets or applying multiple separated decoders (MultiDecoder) and each of them generating one triplet). In our mod- el, one entity is allowed to be copied several times when it needs to participate in different triplets. Therefore, our model could handle the triplet over- lap issue and deal with both of EntityPairOverlap and SingleEntityOverlap sentence types. More- over, since extracting entities and relations in a single end2end neural network, our model could extract entities and relations jointly.</p><p>The main contributions of our work are as fol- lows:</p><p>• We propose an end2end neural model based on sequence-to-sequence learning with copy mechanism to extract relational facts from sentences, where the entities and relations could be jointly extracted.</p><p>• Our model could consider the relational triplet overlap problem through copy mecha- nism. In our knowledge, the relational triplet overlap problem has never been addressed before.</p><p>• We conduct experiments on two public datasets. Experimental results show that we outperforms the state-of-the-arts with 39.8% and 31.1% improvements respectively.  <ref type="figure">Figure 2</ref>: The overall structure of OneDecoder model. A bi-directional RNN is used to encode the source sentence and then a decoder is used to generate triples directly. The relation is predicted and the entity is copied from source sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>By giving a sentence without any annotated en- tities, researchers proposed several methods to ex- tract both entities and relations. Pipeline based methods, like <ref type="bibr" target="#b18">Zelenko et al. (2003)</ref> and <ref type="bibr" target="#b0">Chan and Roth (2011)</ref>, neglected the relevance of entity ex- traction and relation prediction. To resolve this problem, several joint models have been proposed. Early works ( <ref type="bibr" target="#b17">Yu and Lam, 2010;</ref><ref type="bibr" target="#b11">Li and Ji, 2014;</ref><ref type="bibr" target="#b13">Miwa and Sasaki, 2014</ref>) need complicated pro- cess of feature engineering and heavily depends on NLP tools for feature extraction. Recent models, like Miwa and Bansal (2016); <ref type="bibr" target="#b5">Gupta et al. (2016)</ref>; <ref type="bibr" target="#b20">Zhang et al. (2017)</ref>; <ref type="bibr" target="#b21">Zheng et al. (2017)</ref>, jointly ex- tract the entities and relations based on neural net- works. These models are based on tagging frame- work, which assigns a relational tag to a word or a word pair. Despite their success, none of these models can fully handle the triplet overlap prob- lem mentioned in the first section. The reason is in their hypothesis, that is, a word (or a word pair) can only be assigned with just one relational tag.</p><p>This work is based on sequence-to-sequence learning with copy mechanism, which have been adopted for some NLP tasks. <ref type="bibr" target="#b2">Dong and Lapata (2016)</ref> presented a method based on an attention- enhanced and encoder-decoder model, which en- codes input utterances and generates their logical forms. <ref type="bibr" target="#b4">Gu et al. (2016)</ref>; <ref type="bibr" target="#b6">He et al. (2017)</ref> applied copy mechanism to sentence generation. They copy a segment from the source sequence to the target sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Model</head><p>In this section, we introduce a differentiable neu- ral model based on Seq2Seq learning with copy mechanism, which is able to extract multiple rela- tional facts in an end2end fashion.</p><p>Our neural model encodes a variable-length sentence into a fixed-length vector representation first and then decodes this vector into the corre- sponding relational facts (triplets). When decod- ing, we can either decode all triplets with one u- nified decoder or decode every triplet with a sep- arated decoder. We denote them as OneDecoder model and MultiDecoder model separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">OneDecoder Model</head><p>The overall structure of OneDecoder model is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Encoder</head><p>To encode a sentence s = [w 1 , .., w n ], where w t represent the t-th word and n is the source sen- tence length, we first turn it into a matrix X = [x 1 , · · · , x n ], where x t is the embedding of t-th word.</p><p>The canonical RNN encoder reads this matrix X sequentially and generates output o E t and hid- den state h E t in time step t(1 ≤ t ≤ n) by</p><formula xml:id="formula_0">o E t , h E t = f (x t , h E t−1 )<label>(1)</label></formula><p>where f (· ) represents the encoder function. Following ( <ref type="bibr" target="#b4">Gu et al., 2016)</ref>, our encoder uses a bi-directional RNN ( <ref type="bibr" target="#b1">Chung et al., 2014</ref>) to en- code the input sentence. The forward and back-</p><formula xml:id="formula_1">ward RNN obtain output sequence { − → o E 1 , · · · , − → o E n } and { ← − o E n , · · · , ← − o E 1 }, respectively. We then concate- nate − → o E t and ← −−− − o E n−t+1 to represent the t-th word. We use O E = [o E 1 , ..., o E n ], where o E t = [ − → o E t ; ← −−− − o E n−t+1 ]</formula><p>, to represent the concatenate result. Similarly, the concatenation of forward and backward RNN hid- den states are used as the representation of sen- tence, that is s = [</p><formula xml:id="formula_2">− → h E n ; ← − h E n ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Decoder</head><p>The decoder is used to generate triplets direct- ly. Firstly, the decoder generates a relation for the triplet. Secondly, the decoder copies an enti- ty from the source sentence as the first entity of the triplet. Lastly, the decoder copies the second entity from the source sentence. Repeat this pro- cess, the decoder could generate multiple triplets. Once all valid triplets are generated, the decoder will generate NA triplets, which means "stopping" and is similar to the "eos" symbol in neural sen- tence generation. Note that, a NA triplet is com- posed of an NA-relation and an NA-entity pair. As shown in <ref type="figure" target="#fig_0">Figure 3</ref> (a), in time step t (1 ≤ t), we calculate the decoder output o D t and hidden state h D t as follows:</p><formula xml:id="formula_3">o D t , h D t = g(u t , h D t−1 )<label>(2)</label></formula><p>where g(· ) is the decoder function and h D t−1 is the hidden state of time step t − 1. We initialize h D 0 with the representation of source sentence s. u t is the decoder input in time step t and we calculate it as:</p><formula xml:id="formula_4">u t = [v t ; c t ]· W u<label>(3)</label></formula><p>where c t is the attention vector and v t is the em- bedding of copied entity or predicted relation in time step t − 1. W u is a weight matrix. Attention Vector. The attention vector c t is cal- culated as follows:</p><formula xml:id="formula_5">c t = n i=1 α i × o E i (4) α = sof tmax(β)<label>(5)</label></formula><formula xml:id="formula_6">β i = selu([h D t−1 ; o E i ]· w c )<label>(6)</label></formula><p>where o E i is the output of encoder in time step i, α = [α 1 , ..., α n ] and β = [β 1 , ..., β n ] are vectors, w c is a weight vector. selu(· ) is activation func- tion ( <ref type="bibr" target="#b10">Klambauer et al., 2017)</ref>.</p><p>After we get decoder output o D t in time step t (1 ≤ t), if t%3 = 1 (that is t = 1, 4, 7, ...), we use o D t to predict a relation, which means we are decoding a new triplet. Otherwise, if t%3 = 2 (that is t = 2, 5, 8, ...), we use o D t to copy the first entity from the source sentence, and if t%3 = 0 (that is t = 3, 6, 9, ...), we copy the second entity.</p><p>Predict Relation. Suppose there are m valid relations in total. We use a fully connected layer to calculate the confidence vector q r = [q r 1 , ..., q r m ] of all valid relations:</p><formula xml:id="formula_7">q r = selu(o D t · W r + b r )<label>(7)</label></formula><p>where W r is the weight matrix and b r is the bias. When predict the relation, it is possible to predic- t the NA-relation when the model try to generate NA-triplet. To take this into consideration, we cal- culate the confidence value of NA-relation as:</p><formula xml:id="formula_8">q N A = selu(o D t · W N A + b N A )<label>(8)</label></formula><p>where W N A is the weight matrix and b N A is the bias. We then concatenate q r and q N A to form the confidence vector of all relations (including the NA-relation) and apply softmax to obtain the probability distribution p r = [p r 1 , ..., p r m+1 ] as:</p><formula xml:id="formula_9">p r = sof tmax([q r ; q N A ])<label>(9)</label></formula><p>We select the relation with the highest probability as the predict relation and use it's embedding as the next time step input v t+1 .  Copy the First Entity. To copy the first en- tity, we calculate the confidence vector q e = [q e 1 , ..., q e n ] of all words in source sentence as:</p><formula xml:id="formula_10">q e i = selu([o D t ; o E i ]· w e )<label>(10)</label></formula><p>where w e is the weight vector. Similar with the relation prediction, we concatenate q e and q N A to form the confidence vector and apply soft- max to obtain the probability distribution p e = [p e 1 , ..., p e n+1 ]:</p><formula xml:id="formula_11">p e = sof tmax([q e ; q N A ])<label>(11)</label></formula><p>Similarly, We select the word with the highest probability as the predict the word and use it's em- bedding as the next time step input v t+1 . Copy the Second Entity. Copy the second en- tity is almost the same as copy the first entity. The only difference is when copying the second enti- ty, we cannot copy the first entity again. This is because in a valid triplet, two entities must be dif- ferent. Suppose the first copied entity is the k-th word in the source sentence, we introduce a mask vector M with n (n is the length of source sen- tence) elements, where:</p><formula xml:id="formula_12">M i = 1, i = k 0, i = k<label>(12)</label></formula><p>then we calculate the probability distribution p e as:</p><formula xml:id="formula_13">p e = sof tmax([M ⊗ q e ; q N A ])<label>(13)</label></formula><p>where ⊗ is element-wise multiplication. Just like copy the first entity, We select the word with the highest probability as the predict word and use it's embedding as the next time step input v t+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MultiDecoder Model</head><p>MultiDecoder model is an extension of the pro- posed OneDecoder model. The main difference is when decoding triplets, MultiDecoder model de- code triplets with several separated decoders.  </p><formula xml:id="formula_14">o D i t , h D i t = g D i (u t , h D i t−1 ), t%3 = 2, 0 g D i (u t , ˆ h D i t−1 ), t%3 = 1<label>(14)</label></formula><p>g D i (· ) is the decoder function of decoder i. u t is the decoder input in time step t and we calculated it as Eq 3. h D i t−1 is the hidden state of i-th decoder in time step t − 1. ˆ h</p><formula xml:id="formula_15">D i</formula><p>t−1 is the initial hidden state of i-th decoder, which is calculated as follows: </p><formula xml:id="formula_16">ˆ h D i t−1 = s, i = 1 1 2 (s + h D i−1 t−1 ), i &gt; 1<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Both OneDecoder and MultiDecoder models are trained with the negative log-likelihood loss func- tion. Given a batch of data with B sentences S = {s 1 , ..., s B } with the target results Y = {y 1 , ..., y B }, where</p><formula xml:id="formula_17">y i = [y 1 i , ..., y T i ]</formula><p>is the target result of s i , the loss function is defined as follows:</p><formula xml:id="formula_18">L = 1 B × T B i=1 T t=1 −log(p(y t i |y &lt;t i , s i , θ)) (16)</formula><p>T is the maximum time step of decoder. p(x|y) is the conditional probability of x given y. θ denotes parameters of the entire model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>To evaluate the performance of our methods, we conduct experiments on two widely used datasets. The first is New York Times (NYT) dataset, which is produced by distant supervision method ( <ref type="bibr" target="#b14">Riedel et al., 2010)</ref>. This dataset consists of 1.18M sentences sampled from 294k 1987-2007 New York Times news articles. There are 24 valid relations in total. In this paper, we treat this dataset as supervised data as the same as <ref type="bibr" target="#b21">Zheng et al. (2017)</ref>. We filter the sentences with more than 100 words and the sentences containing no posi- tive triplets, and 66195 sentences are left. We ran- domly select 5000 sentences from it as the test set, 5000 sentences as the validation set and the rest 56195 sentences are used as train set.</p><p>The second is WebNLG dataset ( <ref type="bibr" target="#b3">Gardent et al., 2017)</ref>. It is originally created for Natural Lan- guage Generation (NLG) task. This dataset con- tains 246 valid relations. In this dataset, a instance including a group of triplets and several standard sentences (written by human). Every standard sen- tence contains all triplets of this instance. We on- ly use the first standard sentence in our experi- ments and we filter out the instances if all enti- ties of triplets are not found in this standard sen- tence. The origin WebNLG dataset contains train set and development set. In our experiments, we treat the origin development set as test set and ran- domly split the origin train set into validation set and train set. After filtering and splitting, the train set contains 5019 instances, the test set contains 703 instances and the validation set contains 500 instances.</p><p>The number of sentences of every class in NYT and WebNLG dataset are shown in <ref type="table">Table 1</ref>. It's worthy noting that a sentence can belongs to both EntityPairOverlap class and SingleEntityOverlap class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>In our experiments, for both dataset, we use LSTM <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>) as the model cell; The cell unit number is set to 1000; The em- bedding dimension is set to 100; The batch size is 100 and the learning rate is 0.001; The maximum time steps T is 15, which means we predict at most 5 triplets for each sentence (therefore, there are 5 decoders in MultiDecoder model). These hyper- parameters are tuned on the validation set. We use Adam ( <ref type="bibr" target="#b9">Kingma and Ba, 2015)</ref> to optimize param- eters and we stop the training when we find the best result in the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline and Evaluation Metrics</head><p>We compare our models with NovelTagging mod- el ( <ref type="bibr" target="#b21">Zheng et al., 2017)</ref>, which conduct the best per- formance on relational facts extraction. We direct- ly run the code released by <ref type="bibr" target="#b21">Zheng et al. (2017)</ref> to acquire the results.</p><p>Following <ref type="bibr" target="#b21">Zheng et al. (2017)</ref>, we use the stan- dard micro Precision, Recall and F1 score to eval- uate the results. Triplets are regarded as correc- t when it's relation and entities are both correc- t. When copying the entity, we only copy the last word of it. A triplet is regarded as NA-triplet when and only when it's relation is NA-relation and it has an NA-entity pair. The predicted NA-triplets will be excluded.   As we can see, in NYT dataset, our MultiDe- coder model achieves the best F1 score, which is 0.587. There is 39.8% improvement compared with the NovelTagging model, which is 0.420. Be- sides, our OneDecoder model also outperforms the NovelTagging model. In the WebNLG dataset, MultiDecoder model achieves the highest F1 score (0.371). MultiDecoder and OneDecoder models outperform the NovelTagging model with 31.1% and 7.8% improvements, respectively. These ob- servations verify the effectiveness of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We can also observe that, in both NYT and WebNLG dataset, the NovelTagging model achieves the highest precision value and lowest re- call value. By contrast, our models are much more balanced. We think that the reason is in the struc- ture of the proposed models. The NovelTagging method finds triplets through tagging the word- s. However, they assume that only one tag could be assigned to just one word. As a result, one word can participate at most one triplet. There- fore, the NovelTagging model can only recall a small number of triplets, which harms the recal- l performance. Different from the NovelTagging model, our models apply copy mechanism to find entities for a triplet, and a word can be copied many times when this word needs to participate in multiple different triplets. Not surprisingly, our models recall more triplets and achieve higher re- call value. Further experiments verified this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Detailed Results on Different Sentence Types</head><p>To verify the ability of our models in handling the overlapping problem, we conduct further experi- ments on NYT dataset. <ref type="figure" target="#fig_2">Figure 4</ref> shows the results of NovelTagging, OneDecoder and MultiDecoder model in Normal, EntityPairOverlap and SingleEntityOverlap class- es. As we can see, our proposed models perform much better than NovelTagging model in Entity- PairOverlap class and SingleEntityOverlap class- es. Specifically, our models achieve much high- er performance on all metrics. Another observa- tion is that NovelTagging model achieves the best performance in Normal class. This is because the NovelTagging model is designed more suitable for Normal class. However, our proposed models are more suitable for the triplet overlap issues. Fur- thermore, it is still difficult for our models to judge how many triplets are needed for the input sen- tence. As a result, there is a loss in our models for Normal class. Nevertheless, the overall perfor-  <ref type="table">Table 3</ref>: F1 values of entity generation.</p><p>mance of the proposed models still outperforms NoverTagging. Moreover, we notice that the w- hole extracted performance of EntityPairOverlap and SingleEntityOverlap class is lower than that in Normal class. It proves that extracting relation- al facts from EntityPairOverlap and SingleEntity- Overlap classes are much more challenging than from Normal class.</p><p>We also compare the model's ability of extract- ing relations from sentences that contains differ- ent number of triplets. We divide the sentences in NYT test set into 5 subclasses. Each class con- tains sentences that has 1,2,3,4 or &gt;= 5 triplets. The results are shown in <ref type="figure">Figure 5</ref>. When extract- ing relation from sentences that contains 1 triplet- s, NovelTagging model achieve the best perfor- mance. However, when the number of triplets in- creases, the performance of NovelTagging mod- el decreases significantly. We can also observe the huge decrease of recall value of NovelTagging model. These experimental results demonstrate the ability of our model in handling multiple re- lation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">OneDecoder vs. MultiDecoder</head><p>As shown in the previous experiments (Table 2, <ref type="figure" target="#fig_2">Figure 4</ref> and <ref type="figure">Figure 5</ref>), our MultiDecoder model performs better then OneDecoder model and Nov-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>NYT WebNLG OneDecoder 0.874 0.759 MultiDecoder 0.870 0.751 <ref type="table">Table 4</ref>: F1 values of relation generation.</p><p>elTagging model. To find out why MultiDecoder model performs better than OneDecoder model, we analyzed their ability of entity generation and relation generation. The experiment results are shown in <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref>. We can observe that on both NYT and WebNLG datasets, these t- wo models have comparable abilities on relation generation. However, MultiDecoder performs bet- ter than OneDecoder model when generating en- tities. We think that it is because MultiDecoder model utilizes different decoder to generate dif- ferent triplets so that the entity generation results could be more diverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Future Work</head><p>In this paper, we proposed an end2end neural model based on Seq2Seq learning framework with copy mechanism for relational facts extraction. Our model can jointly extract relation and entity from sentences, especially when triplets in the sen- tences are overlapped. Moreover, we analyze the different overlap types and adopt two strategies for this issue, including one unified decoder and mul- tiple separated decoders. We conduct experiments on two public datasets to evaluate the effectiveness of our models. The experiment results show that our models outperform the baseline method signif-icantly and our models can extract relational facts from all three classes. This challenging task is far from being solved. Our future work will concentrate on how to im- prove the performance further. Another future work is test our model in other NLP tasks like event extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The inputs and outputs of the decoder(s) of OneDecoder model and MultiDecoder model. (a) is the decoder of OneDecoder model. As we can see, only one decoder (the green rectangle with shadows) is used and this encoder is initialized with the sentence representation s. (b) is the decoders of MultiDecoder model. There are two decoders (the green rectangle and blue rectangle with shadows). The first decoder is initialized with s; Other decoder(s) are initialized with s and previous decoder's state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig- ure 3 (</head><label>3</label><figDesc>b) shows the inputs and outputs of decoders of MultiDecoder model. There are two decoders (the green and blue rectangle with shadows</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of NovelTagging, OneDecoder, and MultiDecoder model in Normal, EntityPairOverlap and SingleEntityOverlap classes in NYT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 shows the Precision,</head><label>2</label><figDesc></figDesc><table>Recall and F1 value 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of different models in NYT dataset and WebNLG dataset. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Creating training corpora for nlg micro-planners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schtze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Shizhu He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: a Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML PKDD</title>
		<meeting>ECML PKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic relation classification via convolutional neural networks with simple negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="536" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end neural relation extraction with global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1730" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
