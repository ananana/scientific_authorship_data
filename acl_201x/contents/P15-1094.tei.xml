<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nghia</forename><forename type="middle">The</forename><surname>Pham</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Mind/Brain Sciences</orgName>
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="971" to="981"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce C-PHRASE, a distributional semantic model that learns word representations by optimizing context prediction for phrases at all levels in a syntactic tree, from single words to full sentences. C-PHRASE outperforms the state-of-the-art C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a composi-tional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional semantic models, that induce vector-based meaning representations from pat- terns of co-occurrence of words in corpora, have proven very successful at modeling many lexical relations, such as synonymy, co-hyponomy and analogy <ref type="bibr" target="#b36">(Mikolov et al., 2013c;</ref><ref type="bibr" target="#b47">Turney and Pantel, 2010</ref>). The recent evaluation of <ref type="bibr" target="#b8">Baroni et al. (2014b)</ref> suggests that the C-BOW model intro- duced by <ref type="bibr" target="#b34">Mikolov et al. (2013a)</ref> is, consistently, the best across many tasks. <ref type="bibr">1</ref> Interestingly, C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts. This is reminiscent of how the parame- ters of some compositional distributional seman- <ref type="bibr">1</ref> We refer here not only to the results reported in <ref type="bibr" target="#b8">Baroni et al. (2014b)</ref>, but also to the more exten- sive evaluation that Baroni and colleagues present in the companion website (http://clic.cimec.unitn. it/composes/semantic-vectors.html). The ex- periments there suggest that only the Glove vectors of <ref type="bibr" target="#b41">Pennington et al. (2014)</ref> are competitive with C-BOW, and only when trained on a corpus several orders of magnitude larger than the one used for C-BOW. tic models are estimated by optimizing the pre- diction of the contexts in which phrases occur in corpora ( <ref type="bibr" target="#b5">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b19">Guevara, 2010;</ref><ref type="bibr" target="#b15">Dinu et al., 2013)</ref>. However, these compo- sitional approaches assume that word vectors have already been constructed, and contextual evidence is only used to induce optimal combination rules to derive representations of phrases and sentences.</p><p>In this paper, we follow through on this observa- tion to propose the new C-PHRASE model. Sim- ilarly to C-BOW, C-PHRASE learns word repre- sentations by optimizing their joint context pre- diction. However, unlike in flat, window-based C-BOW, C-PHRASE groups words according to their syntactic structure, and it simultaneously op- timizes context-predictions at different levels of the syntactic hierarchy. For example, given train- ing sentence "A sad dog is howling in the park", C-PHRASE will optimize context prediction for dog, sad dog, a sad dog, a sad dog is howling, etc., but not, for example, for howling in, as these two words do not form a syntactic constituent by themselves.</p><p>C-PHRASE word representations outperform C-BOW on several word-level benchmarks. In ad- dition, because they are estimated in a composi- tional way, C-PHRASE word vectors, when com- bined through simple addition, produce sentence representations that are better than those obtained when adding other kinds of vectors, and competi- tive against ad-hoc compositional methods on var- ious sentence meaning benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The C-PHRASE model</head><p>We start with a brief overview of the models pro- posed by <ref type="bibr" target="#b34">Mikolov et al. (2013a)</ref>, as C-PHRASE builds on them. The Skip-gram model derives the vector of a target word by setting its weights to predict the words surrounding it in the corpus.</p><p>More specifically, the objective function is:</p><formula xml:id="formula_0">1 T T t=1 −c≤j≤c,j =0</formula><p>log p(w t+j |w t )</p><p>where the word sequence w 1 , w 2 , ..., w T is the training corpus and c is the size of the window around the target word w t , consisting of the con- text words w t+j that must be predicted by the in- duced vector representation for the target. While Skip-gram learns each word represen- tation separately, the C-BOW model takes their combination into account. More precisely, it tries to predict a context word from the combination of the previous and following words, where the com- bination method is vector addition. The objective function is:</p><formula xml:id="formula_2">1 T T t=1 log p(w t |w t−c ..w t−1 , w t+1 ..w t+c ) (2)</formula><p>While other distributional models consider se- quences of words jointly as context when estimat- ing the parameters for a single word ( <ref type="bibr" target="#b0">Agirre et al., 2009;</ref><ref type="bibr" target="#b33">Melamud et al., 2014</ref>), C-BOW is unique in that it estimates the weights of a sequence of words jointly, based on their shared context. In this respect, C-BOW extends the distributional hy- pothesis <ref type="bibr" target="#b20">(Harris, 1954</ref>) that words with similar context distributions should have similar meanings to longer sequences. However, the word combi- nations of C-BOW are not natural linguistic con- stituents, but arbitrary n-grams (e.g., sequences of 5 words with a gap in the middle). Moreover, the model does not attempt to capture the hierarchical nature of syntactic phrasing, such that big brown dog is a meaningful phrase, but so are its children brown dog and dog.</p><p>C-PHRASE aims at capturing the same intu- ition that word combinations with similar con- text distributions will have similar meaning, but it applies it to syntactically motivated, potentially nested phrases. More precisely, we estimate word vectors such that they and their summed combi- nations are able to predict the contexts of words, phrases and sentences. The model is formalized as follows. We start from a parsed text corpus T, composed of constituents C[w l , · · · , w r ], where w l , · · · , w r are the words spanned by the con- stituent, located in positions l to r in the corpus. We minimize an objective function analogous to equations (1) and (2), but instead of just using in- dividual words or bags of words to predict context, we use summed vector representations of well- formed constituents at all levels in the syntactic tree to predict the context of these constituents. There are similarities with both CBOW and Skip- gram. At the leaf nodes, C-PHRASE acts like Skip-gram, whereas at higher node in the parse tree, it behaves like CBOW model. Concretely, we try to predict the words located within a window c C from every constituent in the parse tree. <ref type="bibr">2</ref> In or- der to do so, we learn vector representations for words v w by maximizing the sum of the log prob- abilities of the words in the context window of the well-formed constituents with stochastic gradient descent:</p><formula xml:id="formula_3">C[w l ,··· ,wr]∈T 1≤j≤c C log p(w l−j |C[w l , · · · , w r ]) + log p(w r+j |C[w l , · · · , w r ])<label>(3)</label></formula><p>with p theoretically defined as:</p><formula xml:id="formula_4">p(w O |C[w l , · · · , w r ]) = exp v w O r i=l vw i r−l+1 W w=1 exp v w r i=l vw i r−l+1</formula><p>where W is the size of the vocabulary, v and v denote output (context) and input vectors, respec- tively, and we take the input vectors to represent the words. In practice, since the normalization constant for the above probability is expensive to compute, we follow <ref type="bibr" target="#b35">Mikolov et al. (2013b)</ref> and use negative sampling.</p><p>We let the context window size c C vary as a function of the height of the constituent in the syntactic tree. The height h(C) of a constituent is given by the maximum number of intermedi- ate nodes separating it from any of the words it dominates (such that h = 0 for words, h = 1 for two-word phrases, etc.). Then, for a constituent of height h(C), C-PHRASE considers c C = c 1 + h(C)c 2 context words to its left and right (the non- negative integers c 1 and c 2 are hyperparameters of the model; with c 2 = 0, context becomes constant across heights). The intuition for enlarging the window proportionally to height is that, for shorter phrases, narrower contexts are likely to be most in- formative (e.g., a modifying adjective for a noun), whereas for longer phrases and sentences it might be better to focus on broader "topical" information spread across larger windows (paragraphs contain- ing sentences about weather might also contain the words rain and sun, but without any tendency for these words to be perfectly adjacent to the target sentences). <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the prediction objective for a two-word phrase and its children. Since all constituents (except the topmost) form parts of larger constituents, their representations will be learned both from the objective of predicting their own contexts, and from error propagation from the same objective applied higher in the tree. As a side effect, words, being lower in the syntactic tree, will have their vectors updated more often, and thus might have a greater impact on the learned pa- rameters. This is another reason for varying win- dow size with height, so that the latter effect will be counter-balanced by higher constituents having larger context windows to predict.</p><p>For lexical tasks, we directly use the vectors induced by C-PHRASE as word representations. For sentential tasks, we simply add the vectors of the words in a sentence to obtain its representation, exploiting the fact that C-PHRASE was trained to predict phrase contexts from the additive combi- nation of their elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint optimization of word and phrase vectors</head><p>The C-PHRASE hierarchical learning objective can capture, in parallel, generalizations about the contexts of words and phrases at different levels of complexity. This results, as we will see, in bet- ter word vectors, presumably because C-PHRASE is trained to predict how the contexts of a word change based on its phrasal collocates (cup will have very different contexts in world cup vs. cof- fee cup ). At the same time, because the vectors are optimized based on their occurrence in phrases of different syntactic complexity, they produce good sentence representations when they are combined. To the best of our knowledge, C-PHRASE is the first model that is jointly optimized for lexical and compositional tasks. C-BOW uses shallow com- position information to learn word vectors. Con- versely, some compositional models -e.g., <ref type="bibr" target="#b24">Kalchbrenner et al. (2014)</ref>, <ref type="bibr" target="#b45">Socher et al. (2013)</ref>-in- duce word representations, that are only optimized for a compositional task and are not tested at the lexical level. Somewhat relatedly to what we do, <ref type="bibr" target="#b22">Hill et al. (2014)</ref> evaluated representations learned in a sentence translation task on word- level benchmarks. Some a priori justification for treating word and sentence learning as joint prob- lems comes from human language acquisition, as it is obvious that children learn word and phrase meanings in parallel and interactively, not sequen- tially <ref type="bibr" target="#b46">(Tomasello, 2003)</ref>.</p><p>Knowledge-leanness and simplicity For train- ing, C-PHRASE requires a large, syntactically- parsed corpus (more precisely, it only requires the constituent structure assigned by the parser, as it is blind to syntactic labels). Both large unan- notated corpora and efficient pre-trained parsers are available for many languages, making the C- PHRASE knowledge demands feasible for practi- cal purposes. There is no need to parse the sen- tences we want to build representations for at test time, since the component word vectors are sim- ply added. The only parameters of the model are the word vectors; specifically, no extra parameters are needed for composition (composition models such as the one presented in <ref type="bibr" target="#b44">Socher et al. (2012)</ref> require an extra parameter matrix for each word in the vocabulary, and even leaner models such as the one of Guevara (2010) must estimate a param- eter matrix for each composition rule in the gram- mar). This makes C-PHRASE as simple as addi- tive and multiplicative composition <ref type="bibr" target="#b37">(Mitchell and Lapata, 2010)</ref>, 3 but C-PHRASE is both more ef- fective in compositional tasks (see evaluation be- low), and it has the further advantage that it learns its own word vectors, thus reducing the number of arbitrary choices to be made in modeling.</p><p>Supervision Unlike many recent composition models <ref type="bibr" target="#b23">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b24">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b44">Socher et al., 2012;</ref><ref type="bibr" target="#b45">Socher et al., 2013</ref>, among others), the context-prediction objective of C-PHRASE does not require anno- tated data, and it is meant to provide general- purpose representations that can serve in differ- ent tasks. C-PHRASE vectors can also be used as initialization parameters for fully supervised, task-specific systems. Alternatively, the current unsupervised objective could be combined with task-specific supervised objectives to fine-tune C- PHRASE to specific purposes.</p><p>Sensitivity to syntactic structure During train- ing, C-PHRASE is sensitive to syntactic structure. To cite an extreme example, boy flowers will be joined in a context-predicting phrase in "these are considered [boy flowers]", but not in "he gave [the boy] [flowers]". A more common case is that of determiners, that will only occur in phrases that also contain the following word, but not nec- essarily the preceding one. Sentence composi- tion at test time, on the other hand, is additive, and thus syntax-insensitive. Still, the vectors be- ing combined will reflect syntactic generalizations learned in training. Even if C-PHRASE produces the same representation for red+car and car+red, this representation combines a red vector that, dur- ing training, has often occurred in the modifier position of adjective-noun phrases, whereas car will have often occurred in the corresponding head position. So, presumably, the red+car=car+red vector will encode the adjective-noun asymmetry induced in learning. While the model won't be able to distinguish the rare cases in which car red is genuinely used as a phrase, in realistic scenar- ios this won't be a problem, because only red car will be encountered. In this respect, the successes and failures of C-PHRASE can tell us to what ex- tent word order information is truly distinctive in practice, and to what extent it can instead be re- constructed from the typical role that words play in sentences.</p><p>Comparison with traditional syntax-sensitive word representations Syntax has often been exploited in distributional semantics for a richer characterization of context. By relying on a syn- tactic parse of the input corpus, a distributional model can take more informative contexts such as subject-of-eat vs. object-of-eat into account ( <ref type="bibr" target="#b4">Baroni and Lenci, 2010;</ref><ref type="bibr" target="#b13">Curran and Moens, 2002;</ref><ref type="bibr" target="#b18">Grefenstette, 1994;</ref><ref type="bibr" target="#b16">Erk and Padó, 2008;</ref><ref type="bibr" target="#b30">Levy and Goldberg, 2014a;</ref><ref type="bibr" target="#b38">Padó and Lapata, 2007;</ref><ref type="bibr" target="#b42">Rothenhäusler and Schütze, 2009)</ref>. In this approach, syntactic information serves to select and/or enrich the contexts that are used to build representations of target units. On the other hand, we use syntax to determine the target units that we build representations for (in the sense that we jointly learn representations of their constituents). The focus is thus on unrelated aspects of model in- duction, and we could indeed use syntax-mediated contexts together with our phrasing strategy. Cur- rently, given eat (red apples), we treat eat as window-based context of red apples, but we could also take the context to be object-of-eat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data sets</head><p>Semantic relatedness of words In this classic lexical task, the models are required to quantify the degree of semantic similarity or relatedness of pairs of words in terms of cosines between the cor- responding vectors. These scores are then com- pared to human gold standards. Performance is as- sessed by computing the correlation between sys- tem and human scores (Spearman correlation in all tasks except rg, where it is customary to re- port Pearson). We used, first of all, the MEN (men) data set of <ref type="bibr" target="#b11">Bruni et al. (2014)</ref>, that is split into 1K pairs for training/development, and 1K pairs for testing. We used the training set to tune the hyperparameters of our model, and report per- formance on the test set. The C-BOW model of <ref type="bibr" target="#b8">Baroni et al. (2014b)</ref> achieved state-of-the art performance on MEN test. We also evaluate on the widely used WordSim353 set introduced by <ref type="bibr" target="#b17">Finkelstein et al. (2002)</ref>, which consists of 353 word pairs. The WordSim353 data were split by <ref type="bibr" target="#b0">Agirre et al. (2009)</ref> into similarity (wss) and re- latedness (wsr) subsets, focusing on strictly taxo- nomic (television/radio) vs. broader topical cases (Maradona/football), respectively. State-of-the- art performance on both sets is reported by <ref type="bibr" target="#b8">Baroni et al. (2014b)</ref>, with the C-BOW model. We fur- ther consider the classic data set of <ref type="bibr" target="#b43">Rubenstein and Goodenough (1965)</ref> (rg), consisting of 65 noun pairs. We report the state-of-the-art from Hassan and Mihalcea (2011), which exploited Wikipedia's linking structure.</p><p>Concept categorization Systems are asked to group a set of nominal concepts into broader cate- gories (e.g. arthritis and anthrax into illness; ba- nana and grape into fruit). As in previous work, we treat this as an unsupervised clustering task. We feed the similarity matrix produced by a model for all concepts in a test set to the CLUTO toolkit <ref type="bibr" target="#b25">(Karypis, 2003)</ref>, that clusters them into n groups, where n is the number of categories. We use standard CLUTO parameters from the literature, and quantify performance by cluster purity with respect to the gold categories. The Almuhareb- Poesio benchmark <ref type="bibr" target="#b3">(Almuhareb, 2006</ref>) (ap) con- sists of 402 concepts belonging to 21 categories. A distributional model based on carefully chosen syntactic relations achieved top ap performance <ref type="bibr" target="#b42">(Rothenhäusler and Schütze, 2009)</ref>. The ESSLLI 2008 data set ( <ref type="bibr" target="#b6">Baroni et al., 2008</ref>) (esslli) consists of 6 categories and 42 concepts. State of the art was achieved by <ref type="bibr" target="#b26">Katrenko and Adriaans (2008)</ref> by using full-Web queries and manually crafted pat- terns.</p><p>Semantic analogy The last lexical task we pick is analogy (an), introduced in <ref type="bibr" target="#b36">Mikolov et al. (2013c)</ref>. We focus on their semantic challenge, containing about 9K questions. In each question, the system is given a pair exemplifying a relation (man/king) and a test word (woman); it is then asked to find the word (queen) that instantiates the same relation with the test word as that of the ex- ample pair. <ref type="bibr" target="#b36">Mikolov et al. (2013c)</ref> subtract the vector of the first word in a pair from the sec- ond, add the vector of the test word and look for the nearest neighbor of the resulting vector (e.g., find the word whose vector is closest to king - man + woman). We follow the method introduced by <ref type="bibr" target="#b31">Levy and Goldberg (2014b)</ref>, which returns the word x maximizing cos(king,x)×cos(woman,x) cos <ref type="bibr">(man,x)</ref> . This method yields better results for all models. Per- formance is measured by accuracy in retrieving the correct answer (in our search space of 180K words). The current state of the art on the seman- tic part and on the whole data set was reached by <ref type="bibr" target="#b41">Pennington et al. (2014)</ref>, who trained their word representations on a huge corpus consisting of 42B words.</p><p>Sentential semantic relatedness Similarly to word relatedness, composed sentence representa- tions can be evaluated against benchmarks where humans provided relatedness/similarity scores for sentence pairs (sentences with high scores, such as "A person in a black jacket is doing tricks on a mo- torbike"/"A man in a black jacket is doing tricks on a motorbike" from the SICK data-set, tend to be near-paraphrases). Following previous work on these data sets, Pearson correlation is our figure of merit, and we report it between human scores and sentence vector cosine similarities computed by the models. SICK ( <ref type="bibr" target="#b32">Marelli et al., 2014</ref>) (sick-r) was created specifically for the purpose of evalu- ating compositional models, focusing on linguistic phenomena such as lexical variation and word or- der. Here we report performance of the systems on the test part of the data set, which contains 5K sentence pairs. The top performance (from the SICK SemEval shared task) was reached by <ref type="bibr" target="#b49">Zhao et al. (2014)</ref> using a heterogeneous set of features that include WordNet and extra training corpora. <ref type="bibr" target="#b1">Agirre et al. (2012)</ref> and <ref type="bibr" target="#b2">Agirre et al. (2013)</ref> cre- ated two collections of sentential similarities con- sisting of subsets coming from different sources. From these, we pick the Microsoft Research video description dataset (msrvid), where near para- phrases are descriptions of the same short video, and the OnWN 2012 (onwn1) and OnWN 2013 (onwn2) data sets (each of these sets contains 750 pairs). The latter are quite different from other sentence relatedness benchmarks, since they com- pare definitions for the same or different words taken from WordNet and OntoNotes: these glosses often are syntactic fragments ("cause something to pass or lead somewhere"), rather than full sen- tences. We report top performance on these tasks from the respective shared challenges, as sum- marized by <ref type="bibr" target="#b1">Agirre et al. (2012)</ref> and <ref type="bibr" target="#b2">Agirre et al. (2013)</ref>. Again, the top systems use feature-rich, supervised methods relying on distributional sim- ilarity as well as other sources, such as WordNet and named entity recognizers.</p><p>Sentential entailment Detecting the presence of entailment between sentences or longer pas- sages is one of the most useful features that the computational analysis of text could provide <ref type="bibr" target="#b14">(Dagan et al., 2009)</ref>. We test our model on the SICK entailment task (sick-e) ( <ref type="bibr" target="#b32">Marelli et al., 2014</ref>). All SICK sentence pairs are labeled as ENTAIL- ING ("Two teams are competing in a football match"/"Two groups of people are playing foot- ball"), CONTRADICTING ("The brown horse is near a red barrel at the rodeo"/"The brown horse is far from a red barrel at the rodeo") or NEU- TRAL ("A man in a black jacket is doing tricks on a motorbike"/"A person is riding the bicycle on one wheel"). For each model, we train a simple SVM classifier based on 2 features: cosine simi- larity between the two sentence vectors, as given by the models, and whether the sentence pair con- tains a negation word (the latter has been shown to be a very informative feature for SICK entail- ment). The current state-of-the-art is reached by , using a much richer set of features, that include WordNet, the denota- tion graph of <ref type="bibr" target="#b48">Young et al. (2014)</ref> and extra training data from other resources.</p><p>Sentiment analysis Finally, as sentiment analy- sis has emerged as a popular area of application for compositional models, we test our methods on the Stanford Sentiment Treebank ( <ref type="bibr" target="#b45">Socher et al., 2013</ref>) (sst), consisting of 11,855 sentences from movie reviews, using the coarse annotation into 2 sentiment degrees (negative/positive). We fol- low the official split into train <ref type="bibr">(8,</ref><ref type="bibr">544)</ref>, develop- ment (1,101) and test <ref type="bibr">(2,</ref><ref type="bibr">210)</ref> parts. We train an SVM classifier on the training set, using the sen- tence vectors composed by a model as features, and report accuracy on the test set. State of the art is obtained by <ref type="bibr" target="#b29">Le and Mikolov (2014)</ref> with the Paragraph Vector approach we describe below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model implementation</head><p>The source corpus we use to build the lex- ical vectors is created by concatenating three sources: ukWaC, 4 a mid-2009 dump of the En- glish Wikipedia <ref type="bibr">5</ref> and the British National Corpus 6 (about 2.8B words in total). We build vectors for the 180K words occurring at least 100 times in the corpus. Since our training procedure requires parsed trees, we parse the corpus using the Stan- ford parser <ref type="bibr" target="#b27">(Klein and Manning, 2003)</ref>.</p><p>C-PHRASE has two hyperparameters (see Sec- tion 2 above), namely basic window size (c 1 ) and height-dependent window enlargement factor (c 2 ). Moreover, following <ref type="bibr" target="#b35">Mikolov et al. (2013b)</ref>, dur- ing training we sub-sample less informative, very frequent words: this option is controlled by a pa- rameter t, resulting in aggressive subsampling of words with relative frequency above it. We tune on MEN-train, obtaining c 1 = 5, c 2 = 2 and t = 10 −5 . As already mentioned, sentence vec- tors are built by summing the vectors of the words in them.</p><p>In lexical tasks, we compare our model to the best C-BOW model from <ref type="bibr" target="#b8">Baroni et al. (2014b)</ref>, <ref type="bibr">7</ref> and to a Skip-gram model built using the same hy- perparameters as C-PHRASE (that also led to the best MEN-train results for Skip-gram).</p><p>In sentential tasks, we compare our model against adding the best C-BOW vectors pre- trained by Baroni and colleagues, 8 and adding our Skip-gram vectors. We compare the additive ap- proaches to two sophisticated composition mod- els. The first is the Practical Lexical Function (PLF) model of <ref type="bibr" target="#b40">Paperno et al. (2014)</ref>. This is a linguistically motivated model in the tradition of the "functional composition" approaches of <ref type="bibr" target="#b12">Coecke et al. (2010)</ref> and <ref type="bibr" target="#b7">Baroni et al. (2014a)</ref>, and the only model in this line of research that has been shown to empirically scale up to real-life sen- tence challenges. In short, in the PLF model all words are represented by vectors. Words acting as argument-taking functions (such as verbs or ad- jectives) are also associated to one matrix for each argument they take (e.g., each transitive verb has a subject and an object matrix). Vector represen- tations of arguments are recursively multiplied by function matrices, following the syntactic tree up to the top node. The final sentence representa- tion is obtained by summing all the resulting vec- tors. The PLF approach requires syntactic parsing both in training and in testing and, more cumber- somely, to train a separate matrix for each argu- ment slot of each function word (the training ob- jective is again a context-predicting one). Here, we report PLF results on msrvid and onwn2 from <ref type="bibr" target="#b40">Paperno et al. (2014)</ref>, noting that they also used two simple but precious cues (word overlap and sentence length) we do not adopt here. We used their pre-trained vectors and matrices also for the SICK challenges, while the number of new ma-trices to estimate made it too time-consuming to implement this model in the onwn1 and sst tasks.</p><p>Finally, we test the Paragraph Vector (PV) ap- proach recently proposed by <ref type="bibr" target="#b29">Le and Mikolov (2014)</ref>. Under PV, sentence representations are learned by predicting the words that occur in them. This unsupervised method has been shown by the authors to outperform much more sophisti- cated, supervised neural-network-based composi- tion models on the sst task. We use our own imple- mentation for this approach. Unlike in the original experiments, we found the PV-DBOW variant of PV to consistently outperform PV-DM, and so we report results obtained with the former.</p><p>Note that PV-DBOW aims mainly at providing representations for sentences, not words. When we do not need to induce vectors for sentences in the training corpus, i.e., only train to learn single words' representations and the softmax weights, PV-DBOW essentially reduces to Skip- gram. Therefore, we produce the PV-DBOW vec- tors for the sentences in the evaluation data sets using the softmax weights learned by Skip-gram. However, it is not clear that, if we were to train PV- DBOW jointly for words and sentences, we would get word vectors as good as those that Skip-gram induces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>The results on the lexical tasks reported in <ref type="table">Table 1</ref> prove that C-PHRASE is providing excellent word representations, (nearly) as good or better than the C-BOW vectors of Baroni and colleagues in all cases, except for ap. Whenever C-PHRASE is not close to the state of the art results, the latter relied on richer knowledge sources and/or much larger corpora (ap, esslli, an).</p><p>Turning to the sentential tasks <ref type="table" target="#tab_1">(Table 2)</ref>, we first remark that using high-quality word vectors (such as C-BOW) and summing them leads to good re- sults in all tasks, competitive with those obtained with more sophisticated composition models. This confirms the observation made by Blacoe and La- pata (2012) that simple-minded composition mod- els are not necessarily worse than advanced ap- proaches. Still, C-PHRASE is consistently better than C-BOW in all tasks, except sst, where the two models reach the same performance level.</p><p>C-PHRASE is outperforming PV on all tasks except sick-e, where the two models have the same performance, and onwn2, where PV is slightly better. C-PHRASE is outperforming PLF by a large margin on the SICK sets, whereas the two models are equal on msrvid, and PLF better on onwn2. Recall, however, that on the latter two benchmarks PLF used extra word overlap and sen- tence length features, so the comparison is not en- tirely fair.</p><p>The fact that state-of-the-art performance is well above our models is not surprising, since the SOA systems are invariably based on a wealth of knowledge sources, and highly optimized for a task. To put some of our results in a broader perspective, C-PHRASE's sick-r performance is 1% better than the median result of systems that participated in the SICK SemEval challenge, and comparable to that of <ref type="bibr" target="#b9">Beltagy et al. (2014)</ref>, who entered the competition with a system combining distributional semantics with a supervised proba- bilistic soft logic system. For sick-e (the entail- ment task), C-PHRASE's performance is less than one point below the median of the SemEval sys- tems, and slightly above that of the Stanford sub- mission, that used a recursive neural network with a tensor layer.</p><p>Finally, the performance of all our models, in- cluding PV, on sst is remarkably lower than the state-of-the-art performance of PV as reported by <ref type="bibr" target="#b29">Le and Mikolov (2014)</ref>. We believe that the crucial difference is that these authors estimated PV vectors specifically on the sentiment treebank training data, thus building ad-hoc vectors encod- ing the semantics of movie reviews. We leave it to further research to ascertain whether we could better fine-tune our models to sst by including the sentiment treebank training phrases in our source corpus.</p><p>Comparing vector lengths of C-BOW and C- PHRASE We gather some insight into how the C-PHRASE objective might adjust word represen- tations for composition with respect to C-BOW by looking at how the length of word vectors changes across the two models. <ref type="bibr">9</ref> While this is a very coarse measure, if a word vector is much longer/shorter (relative to the length of other word vectors of the same model) for C-PHRASE vs. C-BOW, it means that, when sentences are composed by addition, the effect of the word on the resulting sentence representation will be stronger/weaker. men wss wsr rg ap esslli an <ref type="table" target="#tab_1">Skip-gram  78  77  66 80 65  82  63  C-BOW  80  78  68 83 71  77  68  C-PHRASE 79  79  70 83 65  84  69  SOA  80  80  70 86 79  91  82   Table 1</ref>  The relative-length-difference test returns the following words as the ones that are most severely de-emphasized by C-PHRASE compared to C- BOW: be, that, an, not, they, he, who, when, well, have. Clearly, C-PHRASE is weighting down grammatical terms that tend to be context- agnostic, and will be accompanied, in phrases, by more context-informative content words. Indeed, the list of terms that are instead emphasized by C-PHRASE include such content-rich, monose- mous words as gnawing, smackdown, demograph- ics. This is confirmed by a POS-level analysis that indicates that the categories that are, on av- erage, most de-emphasized by C-PHRASE are: determiners, modals, pronouns, prepositions and (more surprisingly) proper nouns. The ones that are, in relative terms, more emphasized are: -ing verb forms, plural and singular nouns, adjectives and their superlatives. While this reliance on con- tent words to the detriment of grammatical terms is not always good for sentential tasks ("not al- ways good" means something very different from "always good"!), the convincing comparative per- formance of C-PHRASE in such tasks suggests that the semantic effect of grammatical terms is in any case beyond the scope of current corpus- based models, and often not crucial to attain com- petitive results on typical benchmarks (think, e.g., of how little modals, one of the categories that C- PHRASE downplays the most, will matter when detecting paraphrases that are based on picture de- scriptions).</p><p>We also applied the length difference test to words in specific categories, finding similar pat- terns. For example, looking at -ly adverbs only, those that are de-emphasized the most by C- PHRASE are recently, eventually, originally, no- tably and currently -all adverbs denoting tempo- ral factors or speaker attitude. On the other hand, the ones that C-PHRASE lengthens the most, rel- ative to C-BOW, are clinically, divinely, ecolog- ically, noisily and theatrically: all adverbs with more specific, content-word-like meanings, that are better captured by distributional methods, and are likely to have a bigger impact on tasks such as paraphrasing or entailment.</p><p>Effects of joint optimization at word and phrase levels As we have argued before, C- PHRASE is able to obtain good word representa- tions presumably because it learns to predict how the context of a word changes in the presence of different collocates. To gain further insight into this claim, we looked at the nearest neighbours of some example terms, like neural, network and neural network (the latter, composed by addition) both in C-PHRASE and C-BOW. The results for this particular example can be appreciated in <ref type="table">Ta- ble 3</ref>.</p><p>Interestingly, while for C-BOW we observe some confusion between the meaning of the indi- vidual words and the phrase, C-PHRASE seems to provide more orthogonal representations for the lexical items. For example, neural in C-C-BOW <ref type="table">C-PHRASE  neural  network  neural network  neural  network  neural network  neuronal  networks  network  neuronal  networks  network  neurons  superjanet4  neural  cortical  internetwork  neural  hopfield  backhaul  networks  connectionist  wans  perceptron  cortical  fiber-optic  hopfield  neurophysiological  network.  networks  connectionist  point-to-multipoint packet-switched  sensorimotor  multicasting  hebbian  feed-forward  nsfnet  small-world  sensorimotor  nsfnet  neurons  feedforward  multi-service  local-area  neocortex  networking  neocortex  neuron  circuit-switched  superjanet4  electrophysiological  tymnet  connectionist  backpropagation</ref> wide-area neuronal neurobiological x.25 neuronal <ref type="table">Table 3</ref>: Nearest neighbours of neural, network and neural network both for C-BOW and C-PHRASE BOW contains neighbours that fit well with neu- ral network, like hopfield, connectionist and feed- forward. Conversely, neural network has neigh- bours that correspond to network like local-area and packet-switched. In contrast, C-PHRASE neighbours for neural are mostly related to the brain sense of the word, e.g., cortical, neurophys- iological, etc. (with the only exception of connec- tionist). The first neighbour of neural network, ex- cluding its own component words, quite sensibly, is perceptron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced C-PHRASE, a distributional se- mantic model that is trained on the task of pre- dicting the contexts surrounding phrases at all lev- els of a hierarchical sentence parse, from single words to full sentences. Consequently, word vec- tors are induced by taking into account not only their contexts, but also how co-occurrence with other words within a syntactic constituent is af- fecting these contexts. C-PHRASE vectors outperform state-of-the-art C-BOW vectors in a wide range of lexical tasks. Moreover, because of the way they are induced, when C-PHRASE vectors are summed, they pro- duce sentence representations that are as good or better than those obtained with sophisticated com- position methods.</p><p>C-PHRASE is a very parsimonious approach: The only major resource required, compared to a completely knowledge-free, unsupervised method, is an automated parse of the training cor- pus (but no syntactic labels are required, nor pars- ing at test time). C-PHRASE has only 3 hyperpa- rameters and no composition-specific parameter to tune and store.</p><p>Having established a strong empirical baseline with this parsimonious approach, in future re- search we want to investigate the impact of possi- ble extensions on both lexical and sentential tasks. When combining the vectors, either for induction or composition, we will try replacing plain addi- tion with other operations, starting with something as simple as learning scalar weights for different words in a phrase <ref type="bibr" target="#b37">(Mitchell and Lapata, 2010)</ref>. We also intend to explore more systematic ways to incorporate supervised signals into learning, to fine-tune C-PHRASE vectors to specific tasks.</p><p>On the testing side, we are fascinated by the good performance of additive models, that (at test time, at least) do not take word order nor syntactic structure into account. We plan to perform a sys- tematic analysis of both existing benchmarks and natural corpus data, both to assess the actual im- pact that such factors have on the aspects of mean- ing we are interested in (take two sentences in an entailment relation: how often does shuffling the words in them make it impossible to detect entail- ment?), and to construct new benchmarks that are more challenging for additive methods.</p><p>The C-PHRASE vectors described in this paper are made publicly available at: http://clic. cimec.unitn.it/composes/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: C-PHRASE context prediction objective for the phrase small cat and its children. The phrase vector is obtained by summing the word vectors. The predicted window is wider for the higher constituent (the phrase).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Sentential task performance. See Section 3.1 for figures of merit (all in percentage form) and 
state-of-the-art references. The PLF results on msrvid and onwn2 are taken from Paperno et al. 2014. 

</table></figure>

			<note place="foot" n="2"> Although here we only use single words as context, the latter can be extended to encompass any sensible linguistic item, e.g., frequent n-grams or, as discussed below, syntactically-mediated expressions</note>

			<note place="foot" n="3"> We do not report results for component-wise multiplicative in our evaluation because it performed much worse than addition in all the tasks.</note>

			<note place="foot" n="4"> http://wacky.sslmit.unibo.it 5 http://en.wikipedia.org 6 http://www.natcorp.ox.ac.uk</note>

			<note place="foot" n="7"> For fairness, we report their results when all tasks were evaluated with the same set of parameters, tuned on rg: this is row 8 of their Table 2. 8 http://clic.cimec.unitn.it/composes/ semantic-vectors.html</note>

			<note place="foot" n="9"> We performed the same analysis for C-PHRASE and Skip-gram, finding similar general trends to the ones we report for C-PHRASE and C-BOW.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Gemma Boleda and the anonymous re-viewers for useful comments. We acknowledge ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and WordNet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL<address><addrLine>Boulder, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note>Marius Pas¸caPas¸ca, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2012 Task 6: a 979 pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of *SEM</title>
		<meeting>*SEM<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalezagirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<title level="m">SEM 2013 shared task: Semantic Textual Similarity</title>
		<meeting><address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
	<note>Proceedings of *SEM</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Attributes in Lexical Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdulrahman</forename><surname>Almuhareb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Essex</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Phd thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributional Memory: A general framework for corpus-based semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="721" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Evert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
		<title level="m">Bridging the Gap between Semantic Theory and Computational Simulations: Proceedings of the ESSLLI Workshop on Distributional Lexical Semantic. FOLLI</title>
		<meeting><address><addrLine>Hamburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="5" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">UTexas: Natural language semantics using distributional semantics and probabilistic logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="796" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="546" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic Analysis</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="345" to="384" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improvements in automatic thesaurus extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition</title>
		<meeting>the ACL Workshop on Unsupervised Lexical Acquisition<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recognizing textual entailment: rationale, evaluation and approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="459" to="476" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">General estimation and evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>ACL Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A structured vector space model for word meaning in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="897" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Explorations in Automatic Thesaurus Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Grefenstette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Kluwer</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A regression model of adjective-noun compositionality in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiliano</forename><surname>Guevara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP GEMS Workshop</title>
		<meeting>the EMNLP GEMS Workshop<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1456" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic relatedness using salient semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="884" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Not all neural embeddings are born equal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coline</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/learningsemantics2014/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS Learning Semantics Workshop</title>
		<meeting>the NIPS Learning Semantics Workshop<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Published online</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for discourse compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>ACL Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">CLUTO: A clustering toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno>02-017</idno>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of Minnesota Department of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Qualia structures and their impact on the concrete noun categorization task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Katrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Adriaans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ESSLLI Workshop on Distributional Lexical Semantics</title>
		<meeting>the ESSLLI Workshop on Distributional Lexical Semantics<address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Illinois-lh: A denotational and distributional approach to semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="329" to="334" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics and Dublin City University</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL<address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Probabilistic modeling of joint-context in distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL<address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS<address><addrLine>Lake Tahoe, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dependency-based construction of semantic space models</title>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="161" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A practical and linguistically-motivated approach to compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="90" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised classification with dependency based word spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rothenhäusler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL GEMS Workshop</title>
		<meeting>the EACL GEMS Workshop<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Constructing a Language: A Usage-Based Theory of Language Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tomasello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Harvard University Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
			<biblScope unit="page" from="271" to="277" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics and Dublin City University</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
