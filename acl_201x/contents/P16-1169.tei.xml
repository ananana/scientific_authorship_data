<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Concept Taxonomies from Multi-modal Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Concept Taxonomies from Multi-modal Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1791" to="1801"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study the problem of automatically building hypernym taxonomies from tex-tual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leverag-ing text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human knowledge is naturally organized as se- mantic hierarchies. For example, in WordNet <ref type="bibr" target="#b22">(Miller, 1995)</ref>, specific concepts are categorized and assigned to more general ones, leading to a semantic hierarchical structure (a.k.a taxonomy). A variety of NLP tasks, such as question answer- ing ( <ref type="bibr" target="#b15">Harabagiu et al., 2003)</ref>, document cluster- ing ( <ref type="bibr" target="#b16">Hotho et al., 2002</ref>) and text generation <ref type="bibr" target="#b2">(Biran and McKeown, 2013)</ref> can benefit from the con- ceptual relationship present in these hierarchies.</p><p>Traditional methods of manually constructing taxonomies by experts (e.g. WordNet) and interest communities (e.g. Wikipedia) are either knowl- edge or time intensive, and the results have lim- ited coverage. Therefore, automatic induction of taxonomies is drawing increasing attention in both NLP and computer vision. On one hand, a num- ber of methods have been developed to build hi- erarchies based on lexical patterns in text <ref type="bibr" target="#b33">(Yang and Callan, 2009;</ref><ref type="bibr" target="#b27">Snow et al., 2006</ref>; <ref type="bibr" target="#b19">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b23">Navigli et al., 2011;</ref><ref type="bibr" target="#b10">Fu et al., 2014;</ref><ref type="bibr" target="#b0">Bansal et al., 2014;</ref><ref type="bibr" target="#b30">Tuan et al., 2015</ref>). These works generally ignore the rich visual data which encode important perceptual semantics ( <ref type="bibr" target="#b4">Bruni et al., 2014</ref>) and have proven to be complemen- tary to linguistic information and helpful for many tasks <ref type="bibr" target="#b24">(Silberer and Lapata, 2014;</ref><ref type="bibr" target="#b17">Kiela and Bottou, 2014;</ref>).</p><p>On the other hand, researchers have built visual hi- erarchies by utilizing only visual features ( <ref type="bibr" target="#b13">Griffin and Perona, 2008;</ref><ref type="bibr" target="#b32">Yan et al., 2015;</ref><ref type="bibr" target="#b26">Sivic et al., 2008)</ref>. The resulting hierarchies are limited in in- terpretability and usability for knowledge transfer. Hence, we propose to combine both visual and textual knowledge to automatically build tax- onomies. We induce is-a taxonomies by su- pervised learning from existing entity ontologies where each concept category (entity) is associated with images, either from existing dataset (e.g. Im- ageNet ( <ref type="bibr" target="#b8">Deng et al., 2009)</ref>) or retrieved from the web using search engines, as illustrated in <ref type="figure">Fig 1.</ref> Such a scenario is realistic and can be extended to a variety of tasks; for example, in knowledge base construction ), text and image collections are readily available but label relations among categories are to be uncovered. In large- scale object recognition, automatically learning relations between labels can be quite useful <ref type="bibr" target="#b9">(Deng et al., 2014;</ref><ref type="bibr" target="#b35">Zhao et al., 2011</ref>).</p><p>Both textual and visual information provide im- portant cues for taxonomy induction. <ref type="figure">Fig 1 il</ref>- lustrates this via an example. The parent cate- gory seafish and its two child categories shark and ray are closely related as: <ref type="formula">(1)</ref> there is a hypernym-hyponym (is-a) relation between the words "seafish" and "shark"/"ray" through text de- scriptions like "...seafish, such as shark and ray...", "...shark and ray are a group of seafish..."; (2) images of the close neighbors, e.g., shark and ray are usually visually similar and images of the child, e.g. shark/ray are similar to a sub- set of images of seafish. To effectively capture these patterns, in contrast to previous works that rely on various hand-crafted features <ref type="bibr" target="#b0">Bansal et al., 2014</ref>), we extract features by leveraging the distributed representations that em- bed images <ref type="bibr" target="#b25">(Simonyan and Zisserman, 2014</ref>) and words (  as compact vectors, based on which the semantic closeness is directly measured in vector space. Further, we develop a probabilistic framework that integrates the rich multi-modal features to induce "is-a" relations be- tween categories, encouraging local semantic con- sistency that each category should be visually and textually close to its parent and siblings. In summary, this paper has the following con- tributions: (1) We propose a novel probabilistic Bayesian model (Section 3) for taxonomy induc- tion by jointly leveraging textual and visual data. The model is discriminatively trained and can be directly applied to build a taxonomy from scratch for a collection of semantic labels. (2) We de- sign novel features (Section 4) based on general- purpose distributed representations of text and im- ages to capture both textual and visual relations between labels. (3) We evaluate our model and features on the ImageNet hierarchies with two dif- ferent taxonomy induction tasks (Section 5). We achieve superior performance on both tasks and improve the F 1 score by 2x in the taxonomy con- struction task, compared to previous approaches. Extensive comparisons demonstrate the effective- ness of integrating visual features with language features for taxonomy induction. We also provide qualitative analysis on our features, the learned model, and the taxonomies induced to provide fur- ther insights (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many approaches have been recently developed that build hierarchies purely by identifying either lexical patterns or statistical features in text cor- pora ( <ref type="bibr" target="#b33">Yang and Callan, 2009;</ref><ref type="bibr" target="#b27">Snow et al., 2006;</ref><ref type="bibr" target="#b19">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b23">Navigli et al., 2011;</ref><ref type="bibr" target="#b36">Zhu et al., 2013;</ref><ref type="bibr" target="#b10">Fu et al., 2014;</ref><ref type="bibr" target="#b0">Bansal et al., 2014;</ref><ref type="bibr" target="#b29">Tuan et al., 2014;</ref><ref type="bibr" target="#b30">Tuan et al., 2015;</ref><ref type="bibr" target="#b18">Kiela et al., 2015</ref>). The approaches in <ref type="bibr" target="#b33">Yang and Callan (2009)</ref> and <ref type="bibr" target="#b27">Snow et al. (2006)</ref> assume a starting incomplete hierarchy and try to extend it by in- serting new terms. <ref type="bibr" target="#b19">Kozareva and Hovy (2010)</ref> and <ref type="bibr" target="#b23">Navigli et al. (2011)</ref> first find leaf nodes and then use lexical patterns to find intermediate terms and all the attested hypernymy links between them. In ( <ref type="bibr" target="#b29">Tuan et al., 2014</ref>), syntactic contextual similarity is exploited to construct the taxonomy, while <ref type="bibr" target="#b30">Tuan et al. (2015)</ref> go one step further to consider trusti- ness and collective synonym/contrastive evidence. Different from them, our model is discriminatively trained with multi-modal data. The works of <ref type="bibr" target="#b10">Fu et al. (2014)</ref> and <ref type="bibr" target="#b0">Bansal et al. (2014)</ref> use similar language-based features as ours. Specifically, in ( <ref type="bibr" target="#b10">Fu et al., 2014</ref>), linguistic regularities between pretrained word vectors (  are modeled as projection mappings. The trained projection matrix is then used to induce pairwise hypernym-hyponym relations between words. Our features are partially motivated by <ref type="bibr" target="#b10">Fu et al. (2014)</ref>, but we jointly leverage both textual and visual in- formation. In <ref type="bibr" target="#b18">Kiela et al. (2015)</ref>, both textual and visual evidences are exploited to detect pairwise lexical entailments. Our work is significantly dif- ferent as our model is optimized over the whole taxonomy space rather than considering only word pairs separately. In ( <ref type="bibr" target="#b0">Bansal et al., 2014</ref>), a struc- tural learning model is developed to induce a glob- ally optimal hierarchy. Compared with this work, we exploit much richer features from both text and images, and leverage distributed representations instead of hand-crafted features.</p><p>Several approaches ( <ref type="bibr" target="#b13">Griffin and Perona, 2008;</ref><ref type="bibr" target="#b1">Bart et al., 2008;</ref><ref type="bibr" target="#b20">Marszałek and Schmid, 2008)</ref> have also been proposed to construct visual hier- archies from image collections. In ( <ref type="bibr" target="#b1">Bart et al., 2008)</ref>, a nonparametric Bayesian model is devel- oped to group images based on low-level features.</p><p>In <ref type="bibr" target="#b13">(Griffin and Perona, 2008)</ref> and <ref type="bibr" target="#b20">(Marszałek and Schmid, 2008)</ref>, a visual taxonomy is built to ac- celerate image categorization. In , only binary object-object relations are ex- tracted using co-detection matrices. Our work dif- fers from all of these as we integrate textual with visual information to construct taxonomies.</p><p>Also of note are several works that integrate text and images as evidence for knowledge base autocompletion <ref type="bibr" target="#b3">(Bordes et al., 2011</ref>) and zero- shot recognition ( <ref type="bibr" target="#b12">Gan et al., 2015;</ref><ref type="bibr" target="#b28">Socher et al., 2013)</ref>. Our work is different be- cause our task is to accurately construct multi- level hyponym-hypernym hierarchies from a set of (seen or unseen) categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Taxonomy Induction Model</head><p>Our model is motivated by the key observation that in a semantically meaningful taxonomy, a cate- gory tends to be closely related to its children as well as its siblings. For instance, there exists a hypernym-hyponym relation between the name of category shark and that of its parent seafish. Be- sides, images of shark tend to be visually simi- lar to those of ray, both of which are seafishes. Our model is thus designed to encourage such lo- cal semantic consistency; and by jointly consider- ing all categories in the inference, a globally opti- mal structure is achieved. A key advantage of the model is that we incorporate both visual and tex- tual features induced from distributed representa- tions of images and text (Section 4). These fea- tures capture the rich underlying semantics and facilitate taxonomy induction. We further distin- guish the relative importance of visual and tex- tual features that could vary in different layers of a taxonomy. Intuitively, visual features would be increasingly indicative in the deeper layers, as sub-categories under the same category of specific objects tend to be visually similar. In contrast, textual features would be more important when inducing hierarchical relations between the cate- gories of general concepts (i.e. in the near-root layers) where visual characteristics are not neces- sarily similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Problem</head><p>Assume a set of N categories x = {x 1 , x 2 , . . . , x N }, where each category x n consists of a text term t n as its name, as well as a set of images i n = {i 1 , i 2 , . . . }. Our goal is to construct a taxonomy tree T over these categories 1 , such that categories of specific object types (e.g. shark) are grouped and assigned to those of general concepts (e.g. seafish). As the categories in x may be from multiple disjoint taxonomy trees, we add a pseudo category x 0 as the hyper-root so that the optimal taxonomy is en- sured to be a single tree. Let z n ∈ {1, . . . , N } be the index of the parent of category x n , i.e. x zn is the hypernymic category of x n . Thus the problem of inducing a taxonomy structure is equivalent to inferring the conditional distribution p(z|x) over the set of (latent) indices z = {z 1 , . . . , z n }, based on the images and text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>We formulate the distribution p(z|x) through a model which leverages rich multi-modal features. Specifically, let c n be the set of child nodes of cat- egory x n in a taxonomy encoded by z. Our model is defined as</p><formula xml:id="formula_0">pw(z, π|x, α) ∝ p(π|α) N n=1 x n ∈cn πngw(xn, x n , cn\x n ) (1)</formula><p>where g w (x n , x n , c n \x n ), defined as</p><formula xml:id="formula_1">gw(xn, x n , cn\x n ) = exp{w d(x n ) f n,n ,cn\x n },</formula><p>measures the semantic consistency between cate- gory x n , its parent x n as well as its siblings in- dexed by c n \x n . The function g w (·) is loglin- ear with respect to f n,n ,cn\x n , which is the fea- ture vector defined over the set of relevant cate- gories (x n , x n , c n \x n ), with c n \x n being the set of child categories excluding x n (Section 4). The simple exponential formulation can effectively en- courage close relations among nearby categories in the induced taxonomy. The function has com- bination weights w = {w 1 , . . . , w L }, where L is the maximum depth of the taxonomy, to capture the importance of different features, and the func- tion d(x n ) to return the depth of x n in the current taxonomy. Each layer l (1 ≤ l ≤ L) of the tax- onomy has a specific w l thereby allowing varying weights of the same features in different layers. The parameters are learned in a supervised man- ner. In eq 1, we also introduce a weight π n for each node x n , in order to capture the varying popular- ity of different categories (in terms of being a par- ent category). For example, some categories like plant can have a large number of sub-categories, while others such as stone have less. We model π as a multinomial distribution with Dirichlet prior α = (α 1 , . . . , α N ) to encode any prior knowledge of the category popularity 2 ; and the conjugacy al- lows us to marginalize out π analytically to get</p><formula xml:id="formula_2">pw(z|x, α) ∝ p(π|α) N n=1 x n ∈cn πngw(xn, x n , cn\x n )dπ ∝ n Γ(qn + αn) x n ∈cn gw(xn, x n , cn\x n )<label>(2)</label></formula><p>where q n is the number of children of category x n . Next, we describe our approach to infer the ex- pectation for each z n , and based on that select a particular taxonomy structure for the category nodes x. As z is constrained to be a tree (i.e. cycle without loops), we include with eq 2, an indicator factor 1(z) that takes 1 if z corresponds a tree and 0 otherwise. We modify the inference algorithm appropriately to incorporate this constraint. Inference. Exact inference is computationally in- tractable due to the normalization constant of eq 2. We therefore use Gibbs Sampling, a procedure for approximate inference. Here we present the sam- pling formula for each z n directly, and defer the details to the supplementary material. The sam- pling procedure is highly efficient because the nor- malization term and the factors that are irrelevant to z n are cancelled out. The formula is</p><formula xml:id="formula_3">p(zn =m|z\zn, ·) ∝ 1(zn = m, z\zn) · q −n m + αm · x n ∈cm∪{xn} gw(xm, x n , cm ∪ {xn}) x n ∈cm\xn gw(xm, x n , cm\xn) ,<label>(3)</label></formula><p>where q m is the number of children of category m; the superscript −n denotes the number exclud- ing x n . Examining the validity of the taxonomy structure (i.e. the tree indicator) in each sampling step can be computationally prohibitive. To han- dle this, we restrict the candidate value of z n in eq 3, ensuring that the new z n is always a tree. Specifically, given a tree T , we define a structure operation as the procedure of detaching one node x n in T from its parent and appending it to another node x m which is not a descendant of x n .</p><p>Proposition 1.</p><p>(1) Applying a structure operation on a tree T will result in a structure that is still a tree. (2) Any tree structure over the node set x that has the same root node with tree T can be achieved by applying structure operation on T a finite number of times.</p><p>2 α could be estimated using training data.</p><p>The proof is straightforward and we omit it due to space limitations. We also add a pseudo node x 0 as the fixed root of the taxonomy. Hence by initializing a tree-structured state rooted at x 0 and restricting each updating step as a structure opera- tion, our sampling procedure is able to explore the whole valid tree space. Output taxonomy selection. To apply the model to discover the underlying taxonomy from a given set of categories, we first obtain the marginals of z by averaging over the samples generated through eq 3, then output the optimal taxonomy z * by find- ing the maximum spanning tree (MST) using the Chu-Liu-Edmonds algorithm ( <ref type="bibr" target="#b7">Chu and Liu, 1965;</ref><ref type="bibr" target="#b0">Bansal et al., 2014</ref>). Training. We need to learn the model parame- ters w l of each layer l, which capture the rela- tive importance of different features. The model is trained using the EM algorithm. Let (x n ) be the depth (layer) of category x n ; and˜zand˜ and˜z (siblings˜c siblings˜ siblings˜c n ) denote the gold structure in training data. Our training algorithm updates w through maximum likelihood estimation, wherein the gradient of w l is (see the supplementary materials for details):</p><formula xml:id="formula_4">δw l = n:(xn)=l {f (x˜znx˜zn , xn, ˜ cn\xn)−Ep[f (xz n , xn, cn\xn)]} ,</formula><p>which is the net difference between gold feature vectors and expected feature vectors as per the model. The expectation is approximated by col- lecting samples using the sampler described above and averaging them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Features</head><p>In this section, we describe the feature vector f used in our model, and defer more details in the supplementary material. Compared to previous taxonomy induction works which rely purely on linguistic information, we exploit both perceptual and textual features to capture the rich spectrum of semantics encoded in images and text. Moreover, we leverage the distributed representations of im- ages and words to construct compact and effec- tive features. Specifically, each image i is repre- sented as an embedding vector v i ∈ R a extracted by deep convolutional neural networks. Such im- age representation has been successfully applied in various vision tasks. On the other hand, the category name t is represented by its word em- bedding v t ∈ R b , a low-dimensional dense vec- tor induced by the Skip-gram model ) which is widely used in diverse NLP ap- plications too. Then we design f (x n , x n , c n \x n ) based on the above image and text representations. The feature vector f is used to measure the local semantic consistency between category x n and its parent category x n as well as its siblings c n \x n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Features</head><p>Sibling similarity. As mentioned above, close neighbors in a taxonomy tend to be visually simi- lar, indicating that the embedding of images of sib- ling categories should be close to each other in the vector space R a . For a category x n and its image set i n , we fit a Gaussian distribution N (v in , Σ n ) to the image vectors, where v in ∈ R a is the mean vector and Σ n ∈ R a×a is the covariance matrix. For a sibling category x m of x n , we define the vi- sual similarity between x n and x m as</p><formula xml:id="formula_5">vissim(xn, xm)=[N (v im ; v in , Σn)+N (v in ; v im , Σm)]/2</formula><p>which is the average probability of the mean im- age vector of one category under the Gaussian dis- tribution of the other. This takes into account not only the distance between the mean images, but also the closeness of the images of each category. Accordingly, we compute the visual similarity be- tween x n and the set c n \x n by averaging:</p><formula xml:id="formula_6">vissim(x n , cn\x n ) = xm∈cn\x n vissim(x n , xm) |cn| − 1 .</formula><p>We then bin the values of vissim(x n , c n \x n ) and represent it as an one-hot vector, which consti- tutes f as a component named as siblings image- image relation feature (denoted as S-V1 3 ). Parent prediction. Similar to feature S-V1, we also create the similarity feature between the im- age vectors of the parent and child, to measure their visual similarity. However, the parent node is usually a more general concept than the child, and it usually consists of images that are not necessar- ily similar to its child. Intuitively, by narrowing the set of images to those that are most similar to its child improves the feature. Therefore, different from S-V1, when estimating the Gaussian distri- bution of the parent node, we only use the top K images with highest probabilities under the Gaus- sian distribution of the child node. We empirically show in section 5.3 that choosing an appropriate K consistently boosts the performance. We name this feature as parent-child image-image relation feature (denoted as PC-V1).</p><p>Further, inspired by the linguistic regularities of word embedding, i.e. the hypernym-hyponym re- lationship between words can be approximated by a linear projection operator between word vectors ( <ref type="bibr" target="#b10">Fu et al., 2014</ref>), we design a similar strategy to ( <ref type="bibr" target="#b10">Fu et al., 2014</ref>) between im- ages and words so that the parent can be "pre- dicted" given the image embedding of its child category and the projection matrix. Specifically, let (x n , x n ) be a parent-child pair in the training data, we learn a projection matrix Φ which min- imizes the distance between Φv i n (i.e. the pro- jected mean image vector v i n of the child) and v tn (i.e. the word embedding of the parent):</p><formula xml:id="formula_7">Φ * = argmin Φ 1 N n Φv i n − vt n 2 2 + λΦ1,</formula><p>where N is the number of parent-child pairs in the training data. Once the projection matrix has been learned, the similarity between a child node x n and its parent x n is computed as Φv i n − v tn , and we also create an one-hot vector by binning the feature value. We call this feature as parent- child image-word relation feature (PC-V2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Features</head><p>We briefly introduce the text features employed. More details about the text feature extraction could be found in the supplementary material. Word embedding features.d PC-V1, We in- duce features using word vectors to measure both sibling-sibling and parent-child closeness in text domain ( <ref type="bibr" target="#b10">Fu et al., 2014</ref>). One exception is that, as each category has only one word, the sibling sim- ilarity is computed as the cosine distance between two word vectors (instead of mean vectors). This will produce another two parts of features, parent- child word-word relation feature (PC-T1) and sib- lings word-word relation feature (S-T1). Word surface features. In addition to the embedding-based features, we further leverage lexical features based on the surface forms of child/parent category names. Specifically, we employ the Capitalization, Ends with, Contains, Suffix match, LCS and Length different features, which are commonly used in previous works in taxonomy induction ( <ref type="bibr" target="#b33">Yang and Callan, 2009;</ref><ref type="bibr" target="#b0">Bansal et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We first disclose our implementation details in section 5.1 and the supplementary material for bet-ter reproducibility. We then compare our model with previous state-of-the-art methods ( <ref type="bibr" target="#b10">Fu et al., 2014;</ref><ref type="bibr" target="#b0">Bansal et al., 2014</ref>) with two taxonomy in- duction tasks. Finally, we provide analysis on the weights and taxonomies induced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Details</head><p>Dataset. We conduct our experiments on the Im- ageNet2011 dataset <ref type="bibr" target="#b8">(Deng et al., 2009)</ref>, which provides a large collection of category items (synsets), with associated images and a label hi- erarchy (sampled from WordNet) over them. The original ImageNet taxonomy is preprocessed, re- sulting in a tree structure with 28231 nodes. Word embedding training. We train word em- bedding for synsets by replacing each word/phrase in a synset with a unique token and then us- ing Google's word2vec tool ( . We combine three public available cor- pora together, including the latest Wikipedia dump <ref type="bibr" target="#b31">(Wikipedia, 2014)</ref>, the One Billion Word Lan- guage Modeling Benchmark ( <ref type="bibr" target="#b5">Chelba et al., 2013)</ref> and the UMBC webbase corpus ( <ref type="bibr" target="#b14">Han et al., 2013)</ref>, resulting in a corpus with total 6 billion tokens. The dimension of the embedding is set to 200.</p><p>Image processing. we employ the ILSVRC12 pre-trained convolutional neural networks <ref type="bibr" target="#b25">(Simonyan and Zisserman, 2014</ref>) to embed each im- age into the vector space. Then, for each category x n with images, we estimate a multivariate Gaus- sian parameterized by N xn = (µ xn , Σ xn ), and constrain Σ xn to be diagonal to prevent overfitting. For categories with very few images, we only es- timate a mean vector µ xn . For nodes that do not have images, we ignore the visual feature. Training configuration. The feature vector is a concatenation of 6 parts, as detailed in section 4. All pairwise distances are precomputed and stored in memory to accelerate Gibbs sampling. The ini- tial learning rate for gradient descent in the M step is set to 0.1, and is decreased by a fraction of 10 every 100 EM iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Experimental Settings</head><p>We evaluate our model on three subtrees sampled from the ImageNet taxonomy. To collect the sub- trees, we start from a given root (e.g. consumer goods) and traverse the full taxonomy using BFS, and collect all descendant nodes within a depth h (number of nodes in the longest path). We vary h  <ref type="table">Table 1</ref>: Statistics of our evaluation set. The bot- tom 4 rows give the number of nodes within each height h ∈ {4, 5, 6, 7}. The scale of the threes range from small to large, and there is no overlap- ping among them.</p><p>to get a series of subtrees with increasing heights h ∈ {4, 5, 6, 7} and various scales (maximally 1326 nodes) in different domains. The statistics of the evaluation sets are provided in <ref type="table">Table 1</ref>.</p><p>To avoid ambiguity, all nodes used in ILSVRC 2012 are removed as the CNN feature extractor is trained on them.</p><p>We design two different tasks to evaluate our model. (1) In the hierarchy completion task, we randomly remove some nodes from a tree and use the remaining hierarchy for training. In the test phase, we infer the parent of each removed node and compare it with groundtruth. This task is de- signed to figure out whether our model can suc- cessfully induce hierarchical relations after learn- ing from within-domain parent-child pairs. (2) Different from the previous one, the hierarchy construction task is designed to test the gener- alization ability of our model, i.e. whether our model can learn statistical patterns from one hi- erarchy and transfer the knowledge to build a tax- onomy for another collection of out-of-domain la- bels. Specifically, we select two trees as the train- ing set to learn w. In the test phase, the model is required to build the full taxonomy from scratch for the third tree.</p><p>We use Ancestor F 1 as our evaluation metric ( <ref type="bibr" target="#b19">Kozareva and Hovy, 2010;</ref><ref type="bibr" target="#b23">Navigli et al., 2011;</ref><ref type="bibr" target="#b0">Bansal et al., 2014</ref>). Specifically, we measure F 1 = 2P R/(P + R) values of predicted "is-a" re- lations where the precision (P) and recall (R) are:</p><formula xml:id="formula_8">P = |isa predicted ∩ isa gold | |isa predicted | , R = |isa predicted ∩ isa gold | |isa gold | .</formula><p>We compare our method to two previously state-of-the-art models by <ref type="bibr" target="#b10">Fu et al. (2014)</ref> and <ref type="bibr" target="#b0">Bansal et al. (2014)</ref>, which are closest to ours.    <ref type="formula" target="#formula_2">(2014)</ref> on two tasks. The ancestor-F 1 scores are reported.</p><note type="other">= 4 h = 5 h = 6 h = 7 Hierarchy Completion Fu2014 0.</note><note type="other">(L) 0.58 0.41 0.36 0.30 Ours (LB) 0.68 0.55 0.45 0.40 Ours (LV) 0.66 0.52 0.42 0.34 Ours (LVB -E) 0.68 0.55 0.44 0.39 Ours (LVB) 0.70 0.57 0.49 0.43</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results</head><p>Hierarchy completion. In the hierarchy comple- tion task, we split each tree into 70% nodes for training and 30% for test, and experiment with different h. We compare the following three sys- tems: (1) Fu2014 4 (Fu et al., 2014); (2) Ours (L):</p><p>Our model with only language features enabled (i.e. surface features, parent-child word-word re- lation feature and siblings word-word relation fea- ture); (3) Ours (LV): Our model with both lan- guage features and visual features 5 . The aver- age performance on three trees are reported at Ta- ble 2. We observe that the performance gradu- ally drops when h increases, as more nodes are inserted when the tree grows higher, leading to a more complex and difficult taxonomy to be ac- curately constructed. Overall, our model outper- forms Fu2014 in terms of the F 1 score, even with- out visual features. In the most difficult case with h = 7, our model still holds an F 1 score of 0.42 (2× of Fu2014), demonstrating the superiority of our model. Hierarchy construction. The hierarchy construc- tion task is much more difficult than hierarchy completion task because we need to build a taxon- omy from scratch given only a hyper-root. For this task, we use a leave-one-out strategy, i.e. we train our model on every two trees and test on the third, and report the average performance in <ref type="table" target="#tab_2">Table 2</ref>. We compare the following methods: (1) Fu2014, (2) Ours (L), and (3) Ours (LV), as described above; (4) Bansal2014: The model by <ref type="bibr" target="#b0">Bansal et al. (2014)</ref> retrained using our dataset; (5) Ours (LB): By ex- cluding visual features, but including other lan- guage features from <ref type="bibr" target="#b0">Bansal et al. (2014)</ref>; (6) Ours (LVB): Our full model further enhanced with all semantic features from <ref type="bibr" target="#b0">Bansal et al. (2014)</ref>; <ref type="formula">(7)</ref> Ours (LVB -E): By excluding word embedding- based language features from Ours (LVB). As shown, on the hierarchy construction task, our model with only language features still outper- forms Fu2014 with a large gap (0.30 compared to 0.18 when h = 7), which uses similar embedding- based features. The potential reasons are two-fold. First, we take into account not only parent-child relations but also siblings. Second, their method is designed to induce only pairwise relations. To build the full taxonomy, they first identify all pos- sible pairwise relations using a simple threshold- ing strategy and then eliminate conflicted relations to obtain a legitimate tree hierarchy. In contrast, our model is optimized over the full space of all legitimate taxonomies by taking the structure op- eration in account during Gibbs sampling.</p><p>When comparing to Bansal2014, our model with only word embedding-based features under- performs theirs. However, when introducing vi- sual features, our performance is comparable (p- value = 0.058).Furthermore, if we discard visual features but add semantic features from <ref type="bibr" target="#b0">Bansal et al. (2014)</ref>, we achieve a slight improvement of 0.02 over Bansal2014 (p-value = 0.016), which is largely attributed to the incorporation of word embedding-based features that encode high-level linguistic regularity. Finally, if we enhance our full model with all semantic features from <ref type="bibr" target="#b0">Bansal et al. (2014)</ref>, our model outperforms theirs by a gap of 0.04 (p-value &lt; 0.01), which justifies our intuition that perceptual semantics underneath vi- sual contents are quite helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Analysis</head><p>In this section, we conduct qualitative studies to investigate how and when the visual information helps the taxonomy induction task. Contributions of visual features. To evaluate the contribution of each part of the visual fea- tures to the final performance, we train our model jointly with textual features and different combi- nations of visual features, and report the ancestor- F 1 scores. As shown in <ref type="table" target="#tab_4">Table 3</ref>. When incorporat- ing the feature S-V1, the performance is substan- tially boosted by a large gap at all heights, show-S-V1 PC-V1 PC-V2 h = 4 h = 5 h = 6 h = 7 0.58 0  ing that visual similarity between sibling nodes is a strong evidence for taxonomy induction. It is intuitively plausible, as it is highly likely that two specific categories share a common (and more general) parent category if similar visual contents are observed between them. Further, adding the PC-V1 feature gains us a better improvement than adding PC-V2, but both minor than S-V1. Compared to that of siblings, the visual similar- ity between parents and children does not strongly holds all the time. For example, images of Terres- trial animal are only partially similar to those of Feline, because the former one contains the later one as a subset. Our feature captures this type of "contain" relation between parents and children by considering only the top-K images from the par- ent category that have highest probabilities under the Gaussian distribution of the child category. To see this, we vary K while keep all other settings, and plot the F 1 scores in <ref type="figure" target="#fig_2">Fig 2.</ref> We observe a trend that when we gradually increase K, the per- formance goes up until reaching some maximal; It then slightly drops (or oscillates) even when more images are available, which confirms with our fea- ture design that only top images should be consid- ered in parent-child visual similarity.</p><p>Overall, the three visual features complement each other, and achieve the highest performance when combined. Visual representations. To investigate how the image representations affect the final performance, we compare the ancestor-F1 score when differ- ent pre-trained CNNs are used for visual fea- ture extraction. Specifically, we employ both the CNN-128 model (128 dimensional feature with 15.6% top-5 error on ILSVRC12) and the VGG- 16 model (4096 dimensional feature with 7.5% top-5 error) by <ref type="bibr" target="#b25">Simonyan and Zisserman (2014)</ref>, but only observe a slight improvement of 0.01 on the ancestor-F1 score for the later one. Relevance of textual and visual features v.s. depth of tree. Compared to <ref type="bibr" target="#b0">Bansal et al. (2014)</ref>,  a major difference of our model is that differ- ent layers of the taxonomy correspond to different weights w l , while in ( <ref type="bibr" target="#b0">Bansal et al., 2014</ref>) all layers share the same weights. Intuitively, introducing layer-wise w not only extends the model capacity, but also differentiates the importance of each fea- ture at different layers. For example, the images of two specific categories, such as shark and ray, are very likely to be visually similar. However, when the taxonomy goes from bottom to up (spe- cific to general), the visual similarity is gradually undermined -images of fish and terrestrial ani- mal are not necessarily similar any more. Hence, it is necessary to privatize the weights w for differ- ent layers to capture such variations, i.e. the visual features become more and more evident from shal- low to deep layers, while the textual counterparts, which capture more abstract concepts, relatively grow more indicative oppositely from specific to general.</p><p>To visualize the variations across layers, for each feature component, we fetch its correspond- ing block in w as V . Then, we average |V | and observe how its values change with the layer depth h. For example, for the parent-child word-word relation feature, we first fetch its corresponding weights V from w as a 20 × 6 matrix, where 20 is the feature dimension and 6 is the number of layers. We then average its absolute values 6 in column and get a vector v with length 6. After 2 normalization, the magnitude of each entry in v directly reflects the relative importance of the feature as an evidence for taxonomy induction. <ref type="figure" target="#fig_3">Fig 3(b)</ref> plots how their magnitudes change with h for every feature component averaged on three train/test splits. It is noticeable that for both word- word relations (S-T1, PC-T1), their corresponding weights slightly decrease as h increases. On the contrary, the image-image relation features (S-V1, PC-V1) grows relatively more prominent. The re- sults verify our conjecture that when the category hierarchy goes deeper into more specific classes, the visual similarity becomes relatively more in- dicative as an evidence for taxonomy induction.</p><p>Visualizing results. Finally, we visualize some excerpts of our predicted taxonomies, as compared to the groundtruth in <ref type="figure" target="#fig_4">Fig 4.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>"</head><label></label><figDesc>Figure 1: An overview of our system. (a) Input: a collection of label items, represented by text and images; (b) Output: we build a taxonomy from scratch by extracting features based on distributed representations of text and images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Ancestor-F 1 scores changes over K (number of images used in the PC-V1 feature) at different heights. The values in the x-axis are K/100; K = ∞ means all images are used.</figDesc><graphic url="image-10.png" coords="8,421.46,145.88,102.05,76.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Normalized weights of each feature v.s. the layer depth.</figDesc><graphic url="image-14.png" coords="8,310.37,318.69,108.30,86.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Excerpts of the prediction taxonomies, compared to the groundturth. Edges marked as red and green are false predictions and unpredicted groundtruth links, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparisons among different variants of 
our model, Fu et al. (2014) and Bansal et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The performance when different combi-
nations of visual features are enabled. 

</table></figure>

			<note place="foot" n="1"> We assume T to be a tree. Most existing taxonomies are modeled as trees (Bansal et al., 2014), since a tree helps simplify the construction and ensures that the learned taxonomy is interpretable. With minor modifications, our model also works on non-tree structures.</note>

			<note place="foot" n="3"> S: sibling, PC: parent-child, V: visual, T: textual.</note>

			<note place="foot" n="4"> We tried different parameter settings for the number of clusters C and the identification threshold δ, and reported the best performance we achieved. 5 In the comparisons to (Fu et al., 2014) and (Bansal et al., 2014), we simply set K = ∞, i.e. we use all available images of the parent category to estimate the PC-V1 feature.</note>

			<note place="foot" n="6"> We take the absolute value because we only care about the relevance of the feature as an evidence for taxonomy induction, but note that the weight can either encourage (positive) or discourage (negative) connections of two nodes. 6 Conclusion In this paper, we study the problem of automatically inducing semantically meaningful concept taxonomies from multi-modal data. We propose a probabilistic Bayesian model which leverages distributed representations for images and words. We compare our model and features to previous ones on two different tasks using the ImageNet hierarchies, and demonstrate superior performance of our model, and the effectiveness of exploiting visual contents for taxonomy induction. We further conduct qualitative studies and distinguish the relative importance of visual and textual features in constructing various parts of a taxonomy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank anonymous reviewers for their valuable feedback. We would also like to thank Mohit Bansal for helpful suggestions. We thank NVIDIA for GPU donations. The work is supported by NSF Big Data IIS1447676.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Structured learning for taxonomy induction with belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual taxonomies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Porteous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Classifying taxonomic relations between pairs of wikipedia articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>number EPFL-CONF-192344</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neil: Extracting visual knowledge from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Sinica</title>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartmut</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning semantic hierarchies via word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiji</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recognizing an action using its name: A knowledge-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring semantic inter-class relationships (SIR) for zero-shot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning and using taxonomies for fast visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Umbc ebiquitycore: Semantic textual similarity systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lushan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Atlanta, Georgia, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Open-domain textual question answering techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><forename type="middle">A</forename><surname>Maiorano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pasca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ontology-based text document clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hotho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Maedche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Staab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning image embeddings using convolutional neural networks for improved multi-modal semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting image generality for lexical entailment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A semi-supervised method to learn and construct taxonomies using the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constructing category hierarchies for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A graph-based algorithm for inducing lexical taxonomies from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Velardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Faralli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning grounded meaning representations with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of visual object class hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>William T Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic taxonomy induction from heterogenous evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Taxonomy construction using syntactic contextual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Jae</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng See</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Incorporating trustiness and collective synonym/contrastive evidence into taxonomy construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Jae</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng See</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://dumps.wikimedia.org/enwiki/20141208/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hd-cnn: Hierarchical deep convolutional neural networks for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A metric-based framework for automatic taxonomy induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic topic modeling for monitoring market competition from online text and image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-scale category structure aware image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Topic hierarchy construction for the organization of multi-source user generated contents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao-Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatseng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
