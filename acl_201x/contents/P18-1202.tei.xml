<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">‡ SAP Innovation Center</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinno</forename><forename type="middle">Jialin</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">‡ SAP Innovation Center</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2171" to="2181"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2171</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Fine-grained opinion analysis aims to extract aspect and opinion terms from each sentence for opinion summarization. Supervised learning methods have proven to be effective for this task. However, in many domains, the lack of labeled data hinders the learning of a precise extraction model. In this case, unsupervised domain adaptation methods are desired to transfer knowledge from the source domain to any un-labeled target domain. In this paper, we develop a novel recursive neural network that could reduce domain shift effectively in word level through syntactic relations. We treat these relations as invariant &quot;pivot information&quot; across domains to build structural correspondences and generate an auxiliary task to predict the relation between any two adjacent words in the dependency tree. In the end, we demonstrate state-of-the-art results on three benchmark datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of fine-grained opinion analysis in- volves extraction of opinion targets (or aspect terms) and opinion expressions (or opinion terms) from each review sentence. For example, in the sen- tence: "They offer good appetizers", the aspect and opinion terms are appetizers and good correspond- ingly. Many supervised deep models have been pro- posed for this problem ( <ref type="bibr" target="#b14">Liu et al., 2015;</ref><ref type="bibr" target="#b28">Yin et al., 2016;</ref><ref type="bibr" target="#b26">Wang et al., 2017)</ref>, and obtained promising results. However, these methods fail to adapt well across domains, because the aspect terms from two different domains are usually disjoint, e.g., lap- top v.s. restaurant, leading to large domain shift in the feature vector space. Though unsupervised methods ( <ref type="bibr" target="#b8">Hu and Liu, 2004;</ref><ref type="bibr" target="#b21">Qiu et al., 2011</ref>) can deal with data with few labels, their performance is unsatisfactory compared with supervised ones.</p><p>There have been a number of domain adaptation methods for coarse-grained sentiment classification problems across domains, where an overall senti- ment polarity of a sentence or document is being predicted. Nevertheless, very few approaches exist for cross-domain fine-grained opinion analysis due to the difficulties in fine-grained adaptation, which is more challenging than coarse-grained problems. <ref type="bibr" target="#b13">Li et al. (2012)</ref> proposed a bootstrap method based on the TrAdaBoost algorithm ( <ref type="bibr" target="#b4">Dai et al., 2007)</ref> to iteratively expand opinion and aspect lexicons in the target domain by exploiting source-domain labeled data and cross-domain common relations between aspect terms and opinion terms. However, their model requires a seed opinion lexicon in the target domain and pre-mined syntactic patterns as a bridge. <ref type="bibr" target="#b6">Ding et al. (2017)</ref> proposed to use rules to generate auxiliary supervision on top of a recurrent neural network to learn domain-invariant hidden representation for each word. The performance highly depends on the quality of the manually de- fined rules and the prior knowledge of a sentiment lexicon. In addition, the recurrent structure fails to capture the syntactic interactions among words in- trinsically for opinion extraction. The requirement for rules makes the above methods non-flexible.</p><p>In this paper, we propose a novel cross-domain Recursive Neural Network (RNN) 1 for aspect and opinion terms co-extraction across domains. Our motivations are twofold: 1) The dependency re- lations capture the interactions among different words. These relations are especially important for identifying aspect terms and opinion terms (Qiu et al., 2011; <ref type="bibr" target="#b25">Wang et al., 2016)</ref>, which are also domain-invariant within the same language. There- fore, they can be used as "pivot" information to bridge the gap between different domains. 2) In- spired by the idea of structural learning (Ando and <ref type="bibr">Zhang, 2005)</ref>, the success of target task depends on the ability of finding good predictive structures learned from other related tasks, e.g., structural cor- respondence learning (SCL) <ref type="bibr" target="#b1">(Blitzer et al., 2006</ref>) for coarse-grained cross-domain sentiment classifi- cation. Here, we aim to generate an auxiliary task on dependency relation classification. Different from previous approaches, our auxiliary task and the target extraction task are of heterogeneous label spaces. We aim to integrate this auxiliary task with distributed relation representation learning into a recursive neural network.</p><p>Specifically, we generate a dependency tree for each sentence from the dependency parser and con- struct a unified RNN that integrates an auxiliary task into the computation of each node. The aux- iliary task is to classify the dependency relation for each direct edge in the dependency tree by learning a relation feature vector. To reduce la- bel noise brought by inaccurate parsing trees, we further propose to incorporate an autoencoder into the auxiliary task to group the relations into dif- ferent clusters. Finally, to model the sequential context interaction, we develop a joint architec- ture that combines RNN with a sequential labeling model for aspect and opinion terms extraction. Ex- tensive experiments are conducted to demonstrate the advantage of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Existing works for single-domain aspect/opinion terms extraction include unsupervised methods based on association rule mining ( <ref type="bibr" target="#b8">Hu and Liu, 2004</ref>), syntactic rule propagation ( <ref type="bibr" target="#b21">Qiu et al., 2011)</ref> or topic modeling <ref type="bibr" target="#b24">(Titov and McDonald, 2008;</ref><ref type="bibr" target="#b15">Lu et al., 2009;</ref>, as well as su- pervised methods based on extensive feature engi- neering with graphical models <ref type="bibr" target="#b10">(Jin and Ho, 2009;</ref><ref type="bibr" target="#b12">Li et al., 2010</ref>) or deep learning ( <ref type="bibr" target="#b14">Liu et al., 2015;</ref><ref type="bibr" target="#b31">Zhang et al., 2015;</ref><ref type="bibr" target="#b26">Wang et al., 2017;</ref><ref type="bibr" target="#b28">Yin et al., 2016</ref>). Among exiting deep models, improved re- sults are obtained using dependency relations <ref type="bibr" target="#b28">(Yin et al., 2016;</ref><ref type="bibr" target="#b25">Wang et al., 2016)</ref>, which indicates the significance of syntactic word interactions for tar- get term extraction. In cross-domain setting, there are very few works for aspect/opinion terms extrac- tion including a pipelined approach ( <ref type="bibr" target="#b13">Li et al., 2012</ref>) and a recurrent neural network ( <ref type="bibr" target="#b6">Ding et al., 2017)</ref>. Both of the methods require manual construction of common and pivot syntactic patterns or rules, which are indicative of aspect or opinion words.</p><p>There have been a number of domain adaptation approaches proposed for coarse-grained sentiment classification. Among existing methods, one active line focuses on projecting original feature spaces of two domains into the same low-dimensional space to reduce domain shift using pivot features as a bridge <ref type="bibr">(Blitzer et al., 2007;</ref><ref type="bibr" target="#b18">Pan et al., 2010;</ref><ref type="bibr" target="#b2">Bollegala et al., 2015;</ref><ref type="bibr" target="#b29">Yu and Jiang, 2016</ref>). An- other line learns domain-invariant features via auto- encoders <ref type="bibr" target="#b7">(Glorot et al., 2011;</ref><ref type="bibr" target="#b3">Chen et al., 2012;</ref>. Our work is more related to the first line by utilizing pivot information to transfer knowledge across domains, but we integrate the idea into a unified deep structure that can fully uti- lize syntactic structure for domain adaptation in fine-grained sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition &amp; Motivation</head><p>Our task is to extract opinion and aspect terms within each review sentence. We denote a sen- tence by a sequence of tokens x = (w 1 , w 2 , ..., w n ). The output is a sequence of token-level labels y = (y 1 , y 2 , ..., y n ), with y i ∈ {BA, IA, BO, IO, N} that represents beginning of an aspect (BA), inside of an aspect (IA), beginning of an opinion (BO), inside of an opinion (IO) or none of the above (N). A subsequence of labels started with "BA" and fol- lowed by "IA" indicates a multi-word aspect term. In unsupervised domain adaptation, we are given a set of labeled review sentences from a source do- main</p><formula xml:id="formula_0">D S = {(x S i , y S i )} n S i=1</formula><p>, and a set of unlabeled sentences from a target domain</p><formula xml:id="formula_1">D T = {x T j } n T j=1 . Our goal is to predict token-level labels on D T .</formula><p>Existing works for cross-domain aspect and/or opinion terms extraction require hand-coded rules and a sentiment lexicon in order to transfer knowl- edge across domains. For example in <ref type="figure" target="#fig_0">Figure 1</ref>, given a review sentence "They offer good appe- tizers" in the source domain and "The laptop has a nice screen" in the target domain. If nice has been extracted as a common sentiment word, and "OPINION-amod-ASPECT" has been identified as a common syntactic pattern from the source do- main, screen could be deduced as an aspect term us- ing the identified syntactic pattern ( <ref type="bibr" target="#b13">Li et al., 2012</ref>). Similarly, <ref type="bibr" target="#b6">Ding et al. (2017)</ref> used a set of pre- defined rules based on syntactic relations and a sentiment lexicon to generate auxiliary labels to learn high-level feature representations through a recurrent neural network. On one hand, these previous attempts have ver- ified that syntactic information between words, which can be used as a bridge between domains, is crucial for domain adaptation. On the other hand, dependency-tree-based RNN ( <ref type="bibr" target="#b23">Socher et al., 2010</ref>) has proven to be effective to learn high-level fea- ture representation of each word by encoding syn- tactic relations between aspect terms and opinion terms ( <ref type="bibr" target="#b25">Wang et al., 2016</ref>). With the above findings, we propose a novel RNN named Recursive Neural Structural Correspondence Network (RNSCN) to learn high-level representation for each word across different domains. Our model is built upon depen- dency trees generated from a dependency parser. Different from previous approaches, we do not re- quire any hand-coded rules or pre-selected pivot features to construct correspondences, but rather focus on the automatically generated dependency relations as the pivots. The model associates each direct edge in the tree with a relation feature vector, which is used to predict the corresponding depen- dency relation as an auxiliary task.</p><p>Note that the relation vector is the key in the model: it associates with the two interacting words and is used to construct structural correspondences between two different domains. Hence, the aux- iliary task guides the learning of relation vectors, which in turn affects their correspondingly interac- tive words. Specifically in <ref type="figure" target="#fig_0">Figure 1</ref>, the relation vector for "amod" is computed from the features of its child and parent words, and also used to pro- duce the hidden representation of its parent. For this relation path in both sentences, the auxiliary task enforces close proximity for these two relation vectors. This pushes the hidden representations for their parent nodes appetizers and screen closer to each other, provided that good and nice have sim- ilar representations. In a word, the auxiliary task bridges the gap between two different domains by drawing the words with similar syntactic properties closer to each other.</p><p>However, the relation vectors may be sensitive to the accuracy of the dependency parser. It might harm the learning process when some noise ex- ists for certain relations, especially for informal texts. This problem of noisy labels has been ad- dressed using perceptual consistency ( <ref type="bibr" target="#b22">Reed et al., 2015)</ref>. Inspired by the taxonomy of dependency re- lations ( <ref type="bibr" target="#b5">de Marneffe and Manning, 2008)</ref>, relations with similar functionalities could be grouped to- gether, e.g., dobj, iobj and pobj all indicate objects. We propose to use an auto-encoder to automatically group these relations in an unsupervised manner. The reconstruction loss serves as the consistency objective that reduces label noise by aligning rela- tion features with their intrinsic relation group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proposed Methodology</head><p>Our model consists of two components. The first component is a Recursive Neural Structural Cor- respondence Network (RNSCN), and the second component is a sequence labeling classifier. In this paper, we focus on Gated Recurrent Unit (GRU) as an implementation for the sequence labeling classi- fier. We choose GRU because it is able to deal with long-term dependencies compared to a simple Re- current neural network and requires less parameters making it easier to train than LSTM. The resultant deep learning model is denoted by RNSCN-GRU. We also implement Conditional Random Field as the sequence labeling classifier, and denote the model by RNSCN-CRF accordingly.</p><p>The overall architecture of RNSCN-GRU with- out auto-encoder on relation denoising is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The left and right are two example sentences from the source and the target domain, respectively. In the first component, RNSCN, an auxiliary task to predict the dependency relation for each direct edge is integrated into a dependency- tree-based RNN. We generate a relation vector for each direct edge from its child node to parent node, and use it to predict the relation and produce the hidden representation for the parent node in the de- pendency tree. To address the issues of noisy rela- tion labels, we further incorporate an auto-encoder into RNSCN, as will be shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>While RNSCN mainly focuses on syntactic in- teractions among the words, the second component, GRU, aims to compute linear-context interactions. GRU takes the hidden representation of each word computed from RNSCN as inputs and further pro- duces final representation of each word by taking linear contexts into consideration. We describe each component in detail in the following sections.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recursive Neural Structural Correspondence Network</head><p>RNSCN is built on the dependency tree of each sen- tence, which is pre-generated from a dependency parser. Specifically, each node in the tree is asso- ciated with a word w n , an input word embedding x n ∈ R d and a transformed hidden representation h n ∈ R d . Each direct edge in the dependency tree associates with a relation feature vector r nm ∈ R d and a true relation label vector y R nm ∈ R K , where K is the total number of dependency relations, n and m denote the indices of the parent and child word of the dependency edge, respectively. Based on the dependency tree, the hidden representations are generated in a recursive manner from leaf nodes until reaching the root node. Consider the source- domain sentence shown in <ref type="figure" target="#fig_1">Figure 2</ref> as an illustra- tive example, we first compute hidden representa- tions for leaf nodes they and good:</p><formula xml:id="formula_2">h 1 =tanh(W x x 1 + b), h 3 =tanh(W x x 3 + b),</formula><p>where W x ∈ R d×d transforms word embeddings to hidden space. For non-leaf node appetizer, we first generate the relation vector r 43 for the depen- dency edge x 4 (appetizers) amod − −−− → x 3 (good) by</p><formula xml:id="formula_3">r 43 = tanh(W h h 3 + W x x 4 ),</formula><p>where W h ∈ R d×d transforms the hidden repre- sentation to the relation vector space. We then compute the hidden representation for appetizer:</p><formula xml:id="formula_4">h 4 = tanh(W amod r 43 + W x x 4 + b).</formula><p>Moreover, the relation vector r 43 is used for the auxiliary task on relation prediction:</p><formula xml:id="formula_5">ˆ y R 43 = softmax(W R r 43 + b R ),</formula><p>where W R ∈ R K×d is the relation classifica- tion matrix. The supervised relation classifier en- forces close proximity of similar {r nm }'s in the dis- tributed relation vector space. The relation features bridge the gap of word representations in different domains by incorporating them into the forward computations. In general, the hidden representation h n for a non-leaf node is produced through</p><formula xml:id="formula_6">h n = tanh( m∈Mn W Rnm r nm + W x x n + b),<label>(1)</label></formula><p>where r nm = tanh(W h ·h m +W x ·x n ), M n is the set of child nodes of w n , and W Rnm is the relation transformation matrix tied with each relation R nm . The predicted label vectorˆyvectorˆ vectorˆy R nm for r nm isˆy</p><formula xml:id="formula_7">isˆ isˆy R nm = softmax(W R · r nm + b R ).<label>(2)</label></formula><p>Here we adopt the the cross-entropy loss for re- lation classification between the predicted label vectorˆyvectorˆ vectorˆy R nm and the ground-truth y R nm to encode relation side information into feature learning:</p><formula xml:id="formula_8">R = K k=1 −y R nm[k] logˆylogˆ logˆy R nm[k] .<label>(3)</label></formula><p>Through the auxiliary task, similar relations en- force participating words close to each other so that words with similar syntactic functionalities are clustered across domains. On the other hand, the pre-trained word embeddings group semantically- similar words. By taking them as input to RNN, together with the auxiliary task, our model encodes both semantic and syntactic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reduce Label Noise with Auto-encoders</head><p>As discussed in Section 3, it might be hard to learn an accurate relation classifier when each class is a unique relation, because the dependency parser may generate incorrect relations as noisy labels. To address it, we propose to integrate an autoencoder into RNSCN. Suppose there is a set of latent groups of relations: G = {1, 2, ..., |G|}, where each rela- tion belongs to only one group. For each relation vector, r nm , an autoencoder is performed before feeding it into the auxiliary classifier (2). The goal is to encode the relation vector to a probability dis- tribution of assigning this relation to any group. As can be seen <ref type="figure" target="#fig_2">Figure 3</ref>, each relation vector r nm is first passed through the autoencoder as follows,</p><formula xml:id="formula_9">p(G nm = i|r nm ) = exp(r nm W enc g i ) j∈G exp(r nm W enc g j ) ,<label>(4)</label></formula><p>where G nm denotes the inherent relation group for r nm , g i ∈ R d represents the feature embedding for group i, and W enc ∈R d×d is the encoding matrix that computes bilinear interactions between relation vector r nm and relation group embedding g i . Thus, p(G nm = i|r nm ) represents the probability of r nm being mapped to group i. An accumulated relation group embedding is computed as:</p><formula xml:id="formula_10">g nm = |G| i=1 p(G nm = i|r nm )g i .<label>(5)</label></formula><p>For decoding, the decoder takes g nm as input and tries to reconstruct the relation feature input r nm . Moreover, g nm is also used as the higher-level fea- ture vector for r nm for predicting the relation label. Therefore, the objective for the auxiliary task in (3) becomes:</p><formula xml:id="formula_11">R = R 1 + α R 2 + β R 3 ,<label>(6)</label></formula><p>where  Here R 1 is the reconstruction loss with W dec being the decoding matrix, R 2 follows (3) withˆy withˆ withˆy R nm = softmax(W R g nm + b R ) and R 3 is the regularization term on the correlations among la- tent groups with I being the identity matrix and ¯ G being a normalized group embedding matrix that consists of normalized g i 's as column vectors. This regularization term enforces orthogonality between g i and g j for i = j. α and β are used to con- trol the trade-off among different losses. With the auto-encoder, the auxiliary task of relation classi- fication is conditioned on group assignment. The reconstruction loss further ensures the consistency between relation features and groupings, which is supposed to dominate classification loss when the observed labels are inaccurate. We denote RNSCN with auto-encoder by RNSCN + .</p><formula xml:id="formula_12">R 1 = r nm − W dec g nm 2 2 ,<label>(7)</label></formula><formula xml:id="formula_13">R 2 = K k=1 −y R nm[k] logˆylogˆ logˆy R nm[k] ,<label>(8)</label></formula><formula xml:id="formula_14">R 3 = I − ¯ G ¯ G 2 F .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint Models for Sequence Labeling</head><p>RNSCN or RNSCN + focuses on capturing and rep- resenting syntactic relations to build a bridge be- tween domains and learn more powerful represen- tations for tokens. However, it ignores the linear- chain correlations among tokens within a sentence, which is important for aspect and opinion terms ex- traction. Therefore, we propose a joint model, de- noted by RNSCN-GRU (RNSCN + -GRU), which integrates a GRU-based recurrent neural network on top of RNSCN (RNSCN + ), i.e., the input for GRU is the hidden representations h n learned by RNSCN or RNSCN + for the n-th token in the sen- tence. For simplicity in presentation, we denote the computation of GRU by using the notation f GRU . To be specific, by taking h n as input, the final fea- ture representation h n for each word is obtained through</p><formula xml:id="formula_15">h n = f GRU (h n−1 , h n ; Θ),<label>(10)</label></formula><p>where Θ is the collection of the GRU parameters. The final token-level prediction is made throughˆy</p><formula xml:id="formula_16">throughˆ throughˆy n = softmax(W l · h n + b l ),<label>(11)</label></formula><p>where W l ∈ R 5×d transforms a d -dimensional feature vector to class probabilities (note that we have 5 different classes as defined in Section 3). The second joint model, namely RNSCN-CRF, combines a linear-chain CRF with RNSCN to learn the discriminative mapping from high-level fea- tures to labels. The advantage of CRF is to learn sequential interactions between each pair of adja- cent words as well as labels and provide structural outputs. Formally, the joint model aims to output a sequence of labels with maximum conditional probability given its input. Denote by y a sequence of labels for a sentence and by H the embedding matrix for each sentence (each column denotes a hidden feature vector of a word in the sentence learned by RNSCN), the inference is computed as:</p><formula xml:id="formula_17">ˆ y= arg max y p(y|H) = arg max y 1 Z(H) c∈C expW c , g(H, y c )(12)</formula><p>where C indicates the set of different cliques (unary and pairwise cliques in the context of linear-chain). W c is tied for each different y c , which indicates the labels for clique c. The operator ·, ·· is the element-wise multiplication, and g(·) produces the concatenation of {h n }'s in a context window of each word. The above two models both consider the sequential interaction of the words within each sentence, but the formalization and training are totally different. We will report the results for both joint models in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training</head><p>Recall that in our cross-domain setting, the labels for terms extraction are only available in the source domain, but the auxiliary relation labels can be automatically produced for both domains via the dependency parser. Besides the source domain la- beled data</p><formula xml:id="formula_18">D S = {(x S i , y S i )} n S i=1</formula><p>, we denote by D R = {(r j , y R j )} n R j=1 the combined source and tar- get domain data with auxiliary relation labels. For training, the total loss consists of token-prediction loss S and relation-prediction loss R :</p><formula xml:id="formula_19">L = D S S (y S i , ˆ y S i ) + γ D R R (r j , y R j ),<label>(13)</label></formula><p>where γ is the trade-off parameter, S is the cross- entropy loss between the predicted extraction label in (11) and the ground-truth, and R is defined in (6) for RNSCN + or (3) for RNSCN. For RNSCN- CRF, the loss becomes the negative log probability of the true label given the corresponding input:</p><formula xml:id="formula_20">S (y S i , ˆ y S i ) = − log(y S i |h S i ).<label>(14)</label></formula><p>Dataset Description # <ref type="table" target="#tab_3">Sentences Training Testing  R  Restaurant  5,841  4,381  1,460  L  Laptop  3,845  2,884  961  D  Device  3,836  2,877  959   Table 1</ref>: Data statistics with number of sentences.</p><p>The parameters for token-level predictions and relation-level predictions are updated jointly such that the information from the auxiliary task could be propagated to the target task to obtain better performance. This idea is in accordance with struc- tural learning proposed by <ref type="bibr">Ando and Zhang (2005)</ref>, which shows that multiple related tasks are use- ful for finding the optimal hypothesis space. In our case, the set of multiple tasks includes the tar- get terms extraction task and the auxiliary relation prediction task, which are closely related. The pa- rameters are all shared across domains. The joint model is trained using back-propagation from the top layer of GRU or CRF to RNSCN until reaching to the input word embeddings in the bottom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data &amp; Experimental Setup</head><p>The data is taken from the benchmark customer re- views in three different domains, namely restaurant, laptop and digital devices. The restaurant domain contains a combination of restaurant reviews from SemEval 2014 task 4 subtask 1 ( <ref type="bibr" target="#b20">Pontiki et al., 2014)</ref> and SemEval 2015 task 12 subtask 1 ( <ref type="bibr" target="#b19">Pontiki et al., 2015)</ref>. The laptop domain consists of laptop re- views from SemEval 2014 task 4 subtask 1. For digital device, we take reviews from ( <ref type="bibr" target="#b8">Hu and Liu, 2004</ref>) containing sentences from 5 digital devices. The statistics for each domain are shown in <ref type="table">Table 1</ref>.</p><p>In our experiments, we randomly split the data in each domain into training set and testing set with the proportion being 3:1. To obtain more rigorous result, we make three random splits for each do- main and test the learned model on each split. The number of sentences for training and testing after each split is also shown in <ref type="table">Table 1</ref>. Each sentence is labeled with aspect terms and opinion terms.</p><p>For each cross-domain task, we conduct both inductive and transductive experiments. Specifi- cally, we train our model only on the training sets from both (labeled) source and (unlabeled) target domains. For testing, the inductive results are ob- tained using the test data from the target domain, and the transductive results are obtained using the (unlabeled) training data from the target domain.</p><p>The evaluation metric we used is F1 score. Fol- lowing the setting from existing work, only exact match could be counted as correct.</p><p>For experimental setup, we use Stanford Depen- dency Parser ( <ref type="bibr" target="#b11">Klein and Manning, 2003)</ref> to gen- erate dependency trees. There are in total 43 dif- ferent dependency relations, i.e. 43 classes for the auxiliary task. We set the number of latent rela- tion groups as 20. The input word features for RNSCN are pre-trained word embeddings using word2vec <ref type="bibr" target="#b17">(Mikolov et al., 2013)</ref> which is trained on 3M reviews from the Yelp dataset 2 and electron- ics dataset in Amazon reviews <ref type="bibr">3 (McAuley et al., 2015)</ref>. The dimension of word embeddings is 100. Because of the relatively small size of the training data compared with the number of parameters, we firstly pre-train RNSCN for 5 epochs with mini- batch size 30 and rmsprop initialized at 0.01. The joint model of RNSCN + -GRU is then trained with rmsprop initialized at 0.001 and mini-batch size 30. The trade-off parameter α, β and γ are set to be 1, 0.001 and 0.1, respectively. The hidden-layer dimension for GRU is 50, and the context win- dow size is 3 for input feature vectors of GRU. For the joint model of RNSCN-CRF, we implement SGD with a decaying learning rate initialized at 0.02. The context window size is also 3 in this case. Both joint models are trained for 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison &amp; Results</head><p>We compared our proposed model with several baselines and variants of the proposed model:</p><p>• RNCRF: A joint model of recursive neural network and CRF proposed by ( <ref type="bibr" target="#b25">Wang et al., 2016</ref>) for single-domain aspect and opinion terms extraction. We make all the parameters shared across domains for target prediction.</p><p>• RNGRU: A joint model of RNN and GRU. The hidden layer of RNN is taken as input for GRU. We share all the parameters across domains, similar to RNCRF.</p><p>• CrossCRF: A linear-chain CRF with hand- engineered features that are useful for cross- domain settings (Jakob and Gurevych, 2010), e.g., POS tags, dependency relations.</p><p>• RAP: The Relational Adaptive bootstraPping method proposed by ( <ref type="bibr" target="#b13">Li et al., 2012</ref>) that uses TrAdaBoost to expand lexicons.</p><p>• Hier-Joint: A recent deep model proposed by <ref type="bibr" target="#b6">Ding et al. (2017)</ref> that achieves state-of- the-art performance on aspect terms extraction across domains.</p><p>• RNSCN-GRU: Our proposed joint model in- tegrating auxiliary relation prediction task into RNN that is further combined with GRU.</p><p>• RNSCN-CRF: The second proposed model similar to RNSCN-GRU, which replace GRU with CRF.</p><p>• RNSCN + -GRU: Our final joint model with auto-encoders to reduce auxiliary label noise.</p><p>Note that we do not implement other recent deep adaptation models for comparison <ref type="bibr" target="#b3">(Chen et al., 2012;</ref><ref type="bibr" target="#b27">Yang and Hospedales, 2015)</ref>, because Hier- Joint ( <ref type="bibr" target="#b6">Ding et al., 2017</ref>) has already demonstrated better performances than these models. The overall comparison results with the baselines are shown in <ref type="table" target="#tab_3">Table 2</ref> with average F1 scores and standard deviations over three random splits. Clearly, the re- sults for aspect terms (AS) transfer are much lower than opinion terms (OP) transfer, which indicate that the aspect terms are usually quite different across domains, whereas the opinion terms could be more common and similar. Hence the ability to adapt the aspect extraction from the source do- main to the target domain becomes more crucial. On this behalf, our proposed model shows clear advantage over other baselines for this more dif- ficult transfer problem. Specifically, we achieve 6.77%, 5.88%, 10.55% improvement over the best- performing baselines for aspect extraction in R→L, L→D and D→L, respectively. By comparing with RNCRF and RNGRU, we show that the structural correspondence network is indeed effective when integrated into RNN.</p><p>To show the effect of the integration of the au- toencoder, we conduct experiments over different variants of the proposed model in <ref type="table" target="#tab_4">Table 3</ref>. RNSCN- GRU represents the model without autoencoder, which achieves much better F1 scores on most ex- periments compared with the baselines in <ref type="table" target="#tab_3">Table 2</ref>. RNSCN + -GRU outperforms RNSCN-GRU in al- most all experiments. This indicates the autoen- coder automatically learns data-dependent group- ings, which is able to reduce unnecessary label noise. To further verify that the autoencoder indeed reduces label noise when the parser is inaccurate, we generate new noisy parse trees by replacing some relations within each sentence with a random    relation. Specifically, in each source domain, for each relation that connects to any aspect or opin- ion word, it has 0.5 probability of being replaced by any other relation. In <ref type="table" target="#tab_4">Table 3</ref>, We denote the model with noisy relations with (r). Obviously, the performance of RNSCN-GRU without an autoen- coder significantly deteriorates when the auxiliary labels are very noisy. On the contrary, RNSCN + - GRU (r) achieves acceptable results compared to RNSCN + -GRU. This proves that the autoencoder makes the model more robust to label noise and helps to adapt the information more accurately to the target data. Note that a large drop for L → R in aspect extraction might be caused by a large por- tion of noisy replacements for this particular data which makes it too hard to train a good classifier. This may not greatly influence opinion extraction, as shown, because the two domains usually share many common opinion terms. However, the signif- icant difference in aspect terms makes the learning more dependent on common relations. The above comparisons are made using the test data from target domains which are not available during training (i.e., the inductive setting). For more complete comparison, we also conduct exper- iments in the transductive setting. We pick our best model RNSCN + -GRU, and show the effect of dif- ferent components. To do that, we first remove the sequential structure on top, resulting in RNSCN + . Moreover, we create another variant by removing opinion term labels to show the effect of the dou- ble propogation between aspect terms and opinion terms. The resulting model is named RNSCN + - GRU*. As shown in is, are, feels, believes, seems, like, will, would <ref type="table">Table 5</ref>: Case studies on word clustering robustness and the ability to learn well when test data is not presented during training. Without opin- ion labels, RNSCN + -GRU* still achieves better results than Hier-Joint most of the time. Its lower performance compared to RNSCN + -GRU also in- dicates that in the cross-domain setting, the dual information between aspects and opinions is bene- ficial to find appropriate and discriminative relation feature space. Finally, the results for RNSCN + by removing GRU are lower than the joint model, which proves the importance of combining syntac- tic tree structure with sequential modeling.</p><p>To qualitatively show the effect of the auxiliary task with auto-encoders for clustering syntactically similar words across domains, we provide some case studies on the predicted groups of some words in <ref type="table">Table 5</ref>. Specifically, for each relation in the dependency tree, we use (4) to obtain the most probable group to assign the word in the child node. The left column shows the predicted group index with the right column showing the corresponding words. Clearly, the words in the same group have similar syntactic functionalities, whereas the word types vary across groups.</p><p>In the end, we verify the robustness and capa- bility of the model by conducting sensitivity stud- ies and experiments with varying number of unla- beled target data for training, respectively. <ref type="figure">Figure 4</ref> shows the sensitivity test for L→D, which indi- cates that changing of the trade-off parameter γ or the number of groups |G| does not affect the model's performance greatly, i.e., less than 1% for aspect extraction and 2% for opinion extraction. This proves that our model is robust and stable against small variations. <ref type="figure">Figure 5</ref> compares the results of RNSCN + -GRU with Hier-Joint when increasing the proportion of unlabeled target train- ing data from 0 to 1. Obviously, our model shows steady improvement with the increasing number of unlabeled target data. This pattern proves our 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.58 0.59 0.60 0.61 0.62 0.63 0.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a novel dependency-tree-based RNN, namely RNSCN (or RNSCN + ), for domain adap- tation. The model integrates an auxiliary task into representation learning of nodes in the dependency tree. The adaptation takes place in a common re- lation feature space, which builds the structural correspondences using syntactic relations among the words in each sentence. We further develop a joint model to combine RNSCN/RNSCN + with a sequential labeling model for terms extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of two reviews with similar syntactic patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of RNSCN-GRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An autoencoder for relation grouping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Sensitivity studies for L→D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Comparisons with different baselines.</head><label>2</label><figDesc></figDesc><table>Models 
R→L 
R→D 
L→R 
L→D 
D→R 
D→L 
AS 
OP 
AS 
OP 
AS 
OP 
AS 
OP 
AS 
OP 
AS 
OP 
RNSCN-GRU 
37.77 
62.35 
33.02 
57.54 
53.18 
71.44 
35.65 
60.02 
49.62 
69.42 
45.92 
63.85 
RNSCN-GRU (r) 
32.97 
50.18 
26.21 
53.58 
35.88 
65.73 
32.87 
57.57 
40.03 
67.34 
40.06 
59.18 
RNSCN + -GRU 
40.43 
65.85 
35.10 
60.17 
52.91 
72.51 
40.42 
61.15 
48.36 
73.75 
51.14 
71.18 
RNSCN + -GRU (r) 
39.27 
59.41 
33.42 
57.24 
45.79 
69.96 
38.21 
59.12 
45.36 
72.84 
50.45 
68.05 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 : Comparisons with different variants of the proposed model.</head><label>3</label><figDesc></figDesc><table>R→L 
R→D 
L→R 
L→D 
D→R 
D→L 
AS 
OP 
AS 
OP 
AS 
OP 
AS 
OP 
AS 
OP 
AS 
OP 

OUT 

Hier-Joint 
33.66 
-
33.20 
-
48.10 
-
31.25 
-
47.97 
-
34.74 
-
RNSCN + -GRU* 
39.06 
-
34.07 
-
47.98 
-
38.51 
-
47.49 
-
48.49 
-
RNSCN + 
31.60 
65.89 
24.37 
60.01 
39.58 
71.03 
34.40 
60.47 
41.02 
71.23 
45.54 
69.00 
RNSCN + -GRU 
40.43 
65.85 
35.10 
60.17 
52.91 
72.51 
40.42 
61.15 
48.36 
73.75 
51.14 
71.18 

IN 

Hier-Joint 
32.41 
-
29.79 
-
47.04 
-
31.26 
-
47.41 
-
33.80 
-
RNSCN + -GRU* 
40.34 
-
30.75 
-
48.69 
-
37.40 
-
46.49 
-
48.50 
-
RNSCN + 
30.76 
63.65 
22.48 
59.24 
39.54 
70.25 
35.32 
60.00 
37.75 
70.64 
43.72 
68.27 
RNSCN + -GRU 
41.27 
65.44 
33.58 
60.28 
52.48 
72.10 
39.73 
60.18 
47.10 
72.19 
50.23 
70.21 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Comparisons with different transfer setting.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 ,</head><label>4</label><figDesc></figDesc><table>we denote by OUT 
</table></figure>

			<note place="foot" n="1"> Here, we use RNN to denote recursive neural networks, rather than recurrent neural networks.</note>

			<note place="foot" n="2"> http://www.yelp.com/dataset challenge 3 http://jmcauley.ucsd.edu/data/amazon/links.html</note>

			<note place="foot" n="4"> We omit standard deviation here due to the limit of space.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation for sentiment classification</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<biblScope unit="page" from="187" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain word representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Kawarabayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1627" to="1634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boosting for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Rong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The stanford typed dependencies representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><forename type="middle">C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CrossParser</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent neural networks with auxiliary labels for crossdomain opinion target extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3436" to="3442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting opinion targets in a single-and cross-domain setting with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1035" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A novel lexicalized hmm-based learning framework for web opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Hay</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structure-aware review mining and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying-Ju</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="653" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-domain co-extraction of sentiment and topic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ou</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finegrained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rated aspect summarization of short comments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neel</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="131" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SemEval-2015 task 12: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
	<note>Suresh Manandhar, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Opinion word expansion and target extraction through double propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<title level="m">Dumitru Erhan, and Andrew Rabinovich. 2015. Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Continuous Phrase Representations and Syntactic Parsing with Recursive Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling online reviews with multi-grain topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive neural conditional random fields for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Coupled multi-layer tensor network for co-extraction of aspect and opinion terms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3316" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A unified perspective on multi-domain and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised word and dependency path embeddings for aspect term extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaimeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2979" to="2985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Extracting and ranking product features in opinion documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk</forename><forename type="middle">Hwan</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eamonn O&amp;apos;brien-Strain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1462" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural networks for open domain targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy Tin</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bi-transferring deep neural networks for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Xiangji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
