<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single-Model Encoder-Decoder with Explicit Morphological Representation for Reinflection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
							<email>kann@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information &amp; Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information &amp; Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information &amp; Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single-Model Encoder-Decoder with Explicit Morphological Representation for Reinflection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="555" to="560"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Morphological reinflection is the task of generating a target form given a source form, a source tag and a target tag. We propose a new way of modeling this task with neural encoder-decoder models. Our approach reduces the amount of required training data for this architecture and achieves state-of-the-art results, making encoder-decoder models applicable to morphological reinflection even for low-resource languages. We further present a new automatic correction method for the outputs based on edit trees.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morphological analysis and generation of previ- ously unseen word forms is a fundamental prob- lem in many areas of natural language process- ing (NLP). Its accuracy is crucial for the success of downstream tasks like machine translation and question answering. Accordingly, learning mor- phological inflection patterns from labeled data is an important challenge.</p><p>The task of morphological reinflection (MRI) consists of producing an inflected form for a given source form, source tag and target tag. A spe- cial case is morphological inflection (MI), the task of finding an inflected form for a given lemma and target tag. An English example is "tree"+PLURAL → "trees". Prior work on MI and MRI includes machine learning models and mod- els that exploit the paradigm structure of the lan- guage ( <ref type="bibr" target="#b1">Ahlberg et al., 2015;</ref><ref type="bibr" target="#b8">Dreyer, 2011;</ref><ref type="bibr" target="#b21">Nicolai et al., 2015)</ref>.</p><p>In this work, we propose the neural encoder- decoder MED -Morphological Encoder-Decoder -a character-level sequence-to-sequence attention model that is a language-independent solution for MRI. In contrast to prior work, we train a single model that is trained on all source to target map- pings of the language that are attested in the train- ing set. This radically reduces the amount of train- ing data needed for the encoder-decoder because most MRI patterns occur in many source-target tag pairs. In our model design, what is learned for one pair can be transferred to others.</p><p>The key enabler for this single-model approach is a novel representation we use for MRI. We en- code the input as a single sequence of (i) the mor- phological tags of the source form, (ii) the mor- phological tags of the target form and (iii) the se- quence of letters of the source form. The output is the sequence of letters of the target form. As the decoder produces each letter, the attention mech- anism can focus on the input letter sequence for parts of the output that simply copy the input. For other parts of the output, e.g., an inflectional end- ing that is predicted using the target tags, the at- tention mechanism can focus on the target mor- phological tags. In more complex cases, simulta- neous attention can be paid to subsequences of all three input types -source tags, target tags and in- put letter sequence. We can train a single generic encoder-decoder per language on this represen- tation that can handle all tag pairs, thus making it possible to make efficient use of the available training data. MED outperformed other systems on the SIGMORPHON16 shared task 1 for all ten languages that were covered ( <ref type="bibr" target="#b18">Kann and Schütze, 2016;</ref><ref type="bibr">Cotterell et al., 2016)</ref>.</p><p>We also present POET -Prefer Observed Edit Trees -a new generic method for correcting the output of an MRI system. The combination of MED and POET is state-of-the-art or close to it on a CELEX-based evaluation of MRI even though this evaluation makes it difficult to exploit gener-alizations across tag pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Description</head><p>Neural network model. Our model is based on the network architecture proposed by  for machine translation. 2 They de- scribe the model in detail; unless we explicitly say so in the description of our model below, we use the same network configuration as . 's model is an extension of the recurrent neural network (RNN) encoder- decoder developed by  and <ref type="bibr" target="#b23">Sutskever et al. (2014)</ref>. The encoder of the latter consists of an RNN that reads an input sequence of vectors x and encodes it into a fixed-length context vector c, computing hidden states h t and c by</p><formula xml:id="formula_0">h t = f (x t , h t−1 ), c = q(h 1 , ..., h Tx )<label>(1)</label></formula><p>with nonlinear functions f and q. The decoder is trained to predict each output y t dependent on c and previous predictions y 1 , ..., y t−1 :</p><formula xml:id="formula_1">p(y) = Ty t=1 p(y t |{y 1 , ..., y t−1 }, c)<label>(2)</label></formula><p>with y = (y 1 , ..., y Ty ) and each conditional prob- ability being modeled with an RNN as</p><formula xml:id="formula_2">p(y t |{y 1 , ..., y t−1 }, c) = g(y t−1 , s t , c) (3)</formula><p>where g is a nonlinear function and s t is the hidden state of the RNN.  proposed an attention- based extension of this model that allows different vectors c t for each step by automatic learning of an alignment model. Additionally, they made the encoder bidirectional: each hidden state h j at time step j does not only depend on the preceding, but also on the following input:</p><formula xml:id="formula_3">h j = − → h T j ; ← − h T j T (4)</formula><p>The formula for p(y) changes as follows:</p><formula xml:id="formula_4">p(y) = Ty t=1 p(y t |{y 1 , ..., y t−1 }, x)<label>(5)</label></formula><formula xml:id="formula_5">= Ty t=1 g(y t−1 , s t , c t )<label>(6)</label></formula><p>with s t being an RNN hidden state for time t and c t being the weighted sum of the annota- tions (h 1 , ..., h Tx ) produced by the encoder, using the attention weights. Further descriptions can be found in ( ). The final model is a multilayer network with a single maxout ( <ref type="bibr" target="#b12">Goodfellow et al., 2013)</ref> hidden layer that computes the conditional probability of each element in the output sequence (a letter in our case, ( <ref type="bibr" target="#b22">Pascanu et al., 2014)</ref>). As MRI is less complex than machine translation, we reduce the number of hidden units and embedding size. Af- ter initial experiments, we fixed the hyperparame- ters of our system and did not further adapt them to a specific task or language. Encoder and de- coder RNNs have 100 hidden units each. For train- ing, we use stochastic gradient descent, Adadelta <ref type="bibr" target="#b25">(Zeiler, 2012</ref>) and a minibatch size of 20. We ini- tialize all weights in the encoder, decoder and the embeddings except for the GRU weights in the de- coder with the identity matrix as well as all biases with zero ( <ref type="bibr" target="#b19">Le et al., 2015</ref>). We train all models for 20,000 iterations. We settled on this number in early experimentation because training usually converged before that limit.</p><p>MED is an ensemble of five RNN encoder- decoders. The final decision is made by majority voting. In case of a tie, the answer is chosen ran- domly among the most frequent predictions.</p><p>Input and output format. We define the al- phabet Σ lang as the set of characters used in the application language. As each morphological tag consists of one or more subtags, e.g. "number" or "case", we further define Σ src and Σ trg as the set of morphological subtags seen during training as part of the source tag and target tag, respectively. Let S start and S end be predefined start and end sym- bols. Then each input of our system is of the for- mat</p><formula xml:id="formula_6">S start Σ src + Σ trg + Σ lang + S end .</formula><p>In the same way, we define the output format as</p><formula xml:id="formula_7">S start Σ lang + S end . A sample input for German is &lt;w&gt; IN=pos=ADJ IN=case=GEN IN=num=PL OUT=pos=ADJ OUT=case=ACC</formula><p>OUT=num=PL i s o l i e r t e r &lt;/w&gt;. The system should produce the corresponding output &lt;w&gt; i s o l i e r t e &lt;/w&gt;. The high-level structure of MED can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>POET. We now describe POET (Prefer Ob- served Edit Trees), a new generic method for cor- recting the output of an MRI system. We use it in combination with MED in this paper, but it can in principle be applied to any MRI system.</p><p>An edit tree e(σ, τ ) specifies a transforma- tion from a source string σ to a target string τ <ref type="bibr" target="#b4">(Chrupała, 2008)</ref>. To compute e(σ, τ ), we first determine the longest common substring (LCS) <ref type="bibr" target="#b15">(Gusfield, 1997</ref>) between σ and τ and then recur- sively model the prefix and suffix pairs of the LCS. If the length of LCS is zero for (σ, τ ), then e(σ, τ ) is simply the substitution operation that replaces σ with τ . <ref type="figure">Figure 2</ref> shows an example.</p><p>Let X be a training set for MRI. For each pair (s, t) of tags, we define:</p><formula xml:id="formula_8">E s,t = {e |∃x ∈ X : e = e(x), s = S(x), t = T (x)}</formula><p>where S(x) and T (x) are source and target tags of x and e(x) is e(σ(x), τ (x)), the edit tree that transforms the source form into the target form.</p><p>Let ρ be a target form predicted by the MRI system for the source form σ and let s and t be source and target tags. POET does not change ρ if e(σ, ρ) ∈ E s,t . Otherwise it replaces ρ with τ :</p><formula xml:id="formula_9">τ := τ if e(σ, τ ) ∈ E s,t , |ρ, τ | = 1</formula><p>ρ else where |ρ, τ | is the Levenshtein distance. If there are several forms τ with edit distance 1, we select the one with the most frequent edit tree. Ties are broken randomly. We observed that MED sometimes makes er- rors that are close to the target, but differ by one edit operation. Those errors are often not covered by edit trees that are observed in the training data whereas the correct form is. Thus, substituting a form not supported by an observed edit tree with a close one that is supported promises to reduce the error rate.</p><p>The effectiveness of POET depends on a train- ing set that is large enough to cover the possible edit trees that can occur in reinflection in a lan- guage. Thus, if the training set is not large enough in this respect, then POET will not be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We compare MED with the three models of <ref type="bibr" target="#b7">Dreyer et al. (2008)</ref> as well as with two recently pro- posed models: (i) discriminative string transduc- tion <ref type="bibr" target="#b9">(Durrett and DeNero, 2013;</ref><ref type="bibr" target="#b21">Nicolai et al., 2015)</ref>, the SIGMORPHON16 baseline, and (ii) <ref type="bibr" target="#b11">Faruqui et al. (2015)</ref>'s encoder-decoder model. <ref type="bibr">3</ref> We call the latter MODEL*TAG as it requires training as many models as there are target tags.</p><p>We evaluate MED on two MRI tasks: CELEX and SIGMORPHON16.</p><p>CELEX. This task is based on complete inflec- tion tables for German extracted from CELEX. For this experiment we follow <ref type="bibr" target="#b7">Dreyer et al. (2008)</ref>. We use four pairs of morphological tags and corre- sponding word forms from the German part of the CELEX morphological database. The 4 different transduction tasks are: 13SIA → 13SKE, 2PIE → 13PKE, 2PKE → z and rP → pA. <ref type="bibr">4</ref> An example for this task would be to produce the output ges- teuert (target tag pA) for the source steuert (source tag rP). To do so, the system has to learn that the prefix ge-, which is used for many participles in German, has to be added to the beginning of the original word form.</p><p>We use the same data splits as <ref type="bibr" target="#b7">Dreyer et al. (2008)</ref>, dividing the original 2500 samples for each tag into five folds, each consisting of 500 training and 1000 development and 1000 test sam- ples. We train a separate model for each fold and report exact match accuracy, averaged over the five folds, as our final result.  SIGMORPHON16. This task covers eight lan- guages and does not provide complete paradigms, but only a set of quadruples, each consisting of word form, source tag, target tag and target form. The main difference to CELEX is that the number of tag pairs is large, resulting in much less training data per tag pair. The number of tag pairs varies by language with Georgian being an extreme case; it has 28 tag pairs in dev that appear less than 10 times in train. For each language, we have around 12,800 training and 1600 development samples. We report exact match accuracy on the develop- ment set, as the final test data of the shared task is not publically available yet. <ref type="table">Table 1</ref> gives CELEX results. MED+POET is bet- ter than prior work on one task, close in perfor- mance on two and worse by a small amount on the third. Unlike <ref type="bibr" target="#b7">Dreyer et al. (2008)</ref>'s models, MED does not use any hand-crafted features. MED's re- sults are weakest on 13SIA. Typical errors on this task include epenthesis (e.g., zirkle vs. zirkele) and irregular verbs (e.g., abhing vs. abhängte).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>For SIGMORPHON16, <ref type="table" target="#tab_2">Table 2</ref> shows that MED outperforms the baseline for all eight lan- guages. Absolute performance and variance is probably influenced by type of morphology (e.g., templatic vs. agglutinative), regularity of the lan- guage, number of different tag pairs and other fac- tors. MED performs well even for complex and diverse languages like Arabic, Finnish, Navajo and Turkish, suggesting that the type of attention- based encoder-decoder we use -single-model, us- ing an explicit morphological representation -is a good choice for MRI.   We do not compare to MODEL*TAG here be- cause it requires training a large number of indi- vidual networks. This is a disadvantage compared to MED both in terms of the number of models that need to be trained and in terms of the effec- tive use of the small number of training examples that are available per tag pair.</p><note type="other">60.9 85.0 (1.1) 91.1 Russian 85.6 84.2 (0.3) 88.4 Spanish 95.6 96.3 (0.3) 97.5 Turkish 54.9 94.7 (1.3) 97.6</note><p>POET improves the results for all tag pairs for CELEX. However, initial experiments indicated that it is not effective for SIGMORPHON16 be- cause its training sets are not large enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>The main innovation of our work is that MED learns a single model of all MRI patterns of a lan- guage and thus can transfer what it has learned from one tag pair to another tag pair. Using CELEX, we now analyze how much our design contributes to better performance by conducting two experiments in which we gradually decrease the training set in two different ways. (i) Large general training set. We only reduce the number of training examples available for a tag pair (s, t) and retain all other training examples. (ii) Small training set. We reduce the number of training ex- amples available for all tag pairs, not just for one.</p><p>A typical example of the large general training set scenario is that familiar second person forms are rare in genres like encyclopedia and news. So a training set derived from these genres will be large, but it will have very few tag pairs whose target tag is familiar second person.</p><p>A typical example of the small training set sce- nario is that we are dealing with a low-resource language. In the following two experiments, we only re- duce the training set and do not change the test set.</p><p>Large general training set. We iteratively halve the training data for 2PIE → 13PKE until only 6.25% or 32 samples are left. <ref type="figure">Figure 3</ref> shows that MED performs well even if only 6.25% of the training examples for the tag pair remain. In con- trast, MODEL*TAG struggles to generalize cor- rectly. This is due to the fact that we train one single model for all tags, so it can learn from other tags and transfer what it has learned to the tag pair that has a small training set.</p><p>Small training set. <ref type="figure">Figure 4</ref> shows results for reducing the training data equally for all tags. MED performs much better than the baseline for less than 50% of the training data. This can be ex- plained by the fact that MED learns from all given data at once and thus is able to learn common pat- terns that apply across different tag pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Earlier work on morphology includes morpholog- ical segmentation <ref type="bibr" target="#b17">(Harris, 1955;</ref><ref type="bibr" target="#b16">Hafer and Weiss, 1974;</ref><ref type="bibr" target="#b6">Déjean, 1998)</ref> and different approaches for MRI ( <ref type="bibr" target="#b0">Ahlberg et al., 2014;</ref><ref type="bibr" target="#b9">Durrett and DeNero, 2013;</ref><ref type="bibr" target="#b10">Eskander et al., 2013;</ref><ref type="bibr" target="#b21">Nicolai et al., 2015)</ref>. Chrupała (2008) defined edit trees and Chrupała (2008) and <ref type="bibr">Müller et al. (2015)</ref> use them for mor- phological tagging and lemmatization.</p><p>In the last years, RNN encoder-decoder models and RNNs in general were applied to several NLP tasks. For example, they proved to be useful for machine translation <ref type="bibr" target="#b23">Sutskever et al., 2014;</ref>), parsing <ref type="bibr" target="#b24">(Vinyals et al., 2015)</ref> and speech recognition ( <ref type="bibr" target="#b13">Graves and Schmidhuber, 2005;</ref><ref type="bibr" target="#b14">Graves et al., 2013)</ref>.</p><p>MED bears some resemblance to <ref type="bibr" target="#b11">Faruqui et al. (2015)</ref>'s work. However, they train one network for every tag pair; this can negatively impact per- formance for low-resource languages and in gen- eral when training data are limited. In contrast, we train a single model for each language. This radi- cally reduces the amount of training data needed for the encoder-decoder because most MRI pat- terns occur in many tag pairs, so what is learned for one can be transferred to others. To be able to model all tag pairs of the language together, we introduce an explicit morphological represen- tation that enables the attention mechanism of the encoder-decoder to generalize MRI patterns across tag pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We have presented MED, a language independent neural sequence-to-sequence mapping approach, and POET, a method based on edit trees for cor- recting the output of an MRI system. MED ob- tains results comparable to state-of-the-art systems for CELEX and establishes the state-of-the-art for SIGMORPHON16. POET improves results fur- ther for large training sets. Our analysis showed that MED outperforms a neural encoder-decoder baseline system by a large margin, especially for small training sets.</p><p>In future work, we would like to make POET less dependent on the source tag and thus increase its accuracy for small training sets. Second, we will look into ways of taking advantage of ad- ditional information sources including unlabeled corpora.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of MED</figDesc><graphic url="image-1.png" coords="3,90.43,62.81,181.42,69.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>MED</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Results for the large general training set experiment: effect of reducing the training set for only 2PIE → 13PKE on the accuracy for 2PIE → 13PKE for MED and MODEL*TAG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Exact match accuracy of MRI on SIG-
MORPHON16; baseline: SIGMORPHON16 baseline; 
MED/average: average of five MED models (standard devia-
tion in parentheses); MED/ensemble: majority voting of five 
MED models. 

</table></figure>

			<note place="foot" n="1"> ryancotterell.github.io/ sigmorphon2016/</note>

			<note place="foot" n="2"> Our implementation of MED is based on github.com/mila-udem/blocks-examples/ tree/master/machine_translation.</note>

			<note place="foot" n="3"> For our experiments we ran the code available at github.com/mfaruqui/morph-trans. We used the enc-dec-attn model as overall results for the CELEX task were better than with the sep-morph model. 4 13SIA=1st/3rd sg. ind. past; 13SKE=1st/3rd sg. subjunct. pres.; 2PIE=2nd pl. ind. pres.; 13PKE=1st/3rd pl. subjunct. pres.; 2PKE=2nd. pl. subjunct. pres.; z=infinitive; rP=imperative pl.; pA=past part.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the financial support of Siemens for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of morphological paradigms and lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malin</forename><surname>Ahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Markus Forsberg, and Mans Hulden</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Proc. of EACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Paradigm classification in supervised learning of morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malin</forename><surname>Ahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Markus Forsberg, and Mans Hulden</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proc. of NAACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Towards a machinelearning architecture for lexical functional grammar parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jason Eisner, and Mans Hulden. 2016. The SIGMORPHON 2016 shared taskmorphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2016 Meeting of SIGMORPHON</title>
		<meeting>of the 2016 Meeting of SIGMORPHON</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Morphemes as necessary concept for structures discovery from untagged corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hervé Déjean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Joint Conferences on New Methods in Language Processing and CoNLL</title>
		<meeting>of the Joint Conferences on New Methods in Language essing and CoNLL</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Latent-variable modeling of string transductions with finite-state methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A non-parametric model for the discovery of inflectional paradigms from plain text using graphical models over strings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>Baltimore, MD</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised learning of complete morphological paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic extraction of morphological lexicons from morphologically annotated corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramy</forename><surname>Eskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Morphological inflection generation using character sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.06110</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proc of. ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Algorithms on strings, trees and sequences: computer science and computational biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gusfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Word segmentation by letter successor varieties. Information storage and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Margaret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="371" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From phoneme to morpheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="222" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MED: The LMU system for the SIGMORPHON 2016 shared task on morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2016 Meeting of SIGMORPHON</title>
		<meeting>of the 2016 Meeting of SIGMORPHON</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint lemmatization and morphological tagging with lemming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inflection generation as discriminative string transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Nicolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
