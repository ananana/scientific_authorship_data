<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TransG : A Generative Model for Knowledge Graph Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Science and Technology Dept. of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems National Lab. for Information</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Science and Technology Dept. of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems National Lab. for Information</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Science and Technology Dept. of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems National Lab. for Information</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TransG : A Generative Model for Knowledge Graph Embedding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2316" to="2325"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper proposes a novel gen-erative model (TransG) to address the issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples. The new model can discover latent semantics for a relation and leverage a mixture of relation-specific component vectors to embed a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, and at the first time, the issue of multiple relation semantics is formally discussed. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Abstract or real-world knowledge is always a ma- jor topic in Artificial Intelligence. Knowledge bases such as Wordnet <ref type="bibr" target="#b15">(Miller, 1995)</ref> and Free- base ( <ref type="bibr" target="#b0">Bollacker et al., 2008</ref>) have been shown very useful to AI tasks including question answering, knowledge inference, and so on. However, tra- ditional knowledge bases are symbolic and logic, thus numerical machine learning methods can- not be leveraged to support the computation over the knowledge bases. To this end, knowledge graph embedding has been proposed to project en- tities and relations into continuous vector spaces. Among various embedding models, there is a line * Correspondence author of translation-based models such as TransE <ref type="bibr" target="#b3">(Bordes et al., 2013</ref>), TransH ( <ref type="bibr" target="#b20">Wang et al., 2014</ref>), TransR ( <ref type="bibr" target="#b14">Lin et al., 2015b)</ref>, and other related mod- els ( <ref type="bibr" target="#b10">He et al., 2015</ref>) ( <ref type="bibr" target="#b13">Lin et al., 2015a</ref>). A dot denotes a triple and its position is decided by the difference vector between tail and head en- tity (t − h). Since TransE adopts the principle of t − h ≈ r, there is supposed to be only one cluster whose centre is the relation vector r. However, re- sults show that there exist multiple clusters, which justifies our multiple relation semantics assump- tion.</p><p>A fact of knowledge base can usually be rep- resented by a triple (h, r, t) where h, r, t indicate a head entity, a relation, and a tail entity, respec- tively. All translation-based models almost follow the same principle h r + r ≈ t r where h r , r, t r in-dicate the embedding vectors of triple (h, r, t), with the head and tail entity vector projected with respect to the relation space.</p><p>In spite of the success of these models, none of the previous models has formally discussed the issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples. As can be seen from <ref type="figure" target="#fig_0">Fig. 1</ref>, visualization results on embedding vectors obtained from TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013)</ref> show that, there are different clusters for a specific relation, and different clusters indicate different latent semantics. For example, the relation HasPart has at least two latent semantics: composition-related as <ref type="table">(Table,</ref> HasPart, Leg) and location-related as (Atlantics, HasPart, NewYorkBay). As one more example, in Freebase, (Jon Snow, birth place, Winter Fall) and (George R. R. Martin, birth place, U.S.) are mapped to schema /fic- tional universe/fictional character/place of birth and /people/person/place of birth respectively, indicating that birth place has different meanings. This phenomenon is quite common in knowledge bases for two reasons: artificial simplification and nature of knowledge. On one hand, knowledge base curators could not involve too many similar relations, so abstracting multiple similar relations into one specific relation is a common trick. On the other hand, both language and knowledge representations often involve ambiguous infor- mation. The ambiguity of knowledge means a semantic mixture. For example, when we mention "Expert", we may refer to scientist, businessman or writer, so the concept "Expert" may be ambigu- ous in a specific situation, or generally a semantic mixture of these cases.</p><p>However, since previous translation-based mod- els adopt h r + r ≈ t r , they assign only one trans- lation vector for one relation, and these models are not able to deal with the issue of multiple relation semantics. To illustrate more clearly, as showed in <ref type="figure">Fig.2</ref>, there is only one unique representation for relation HasPart in traditional models, thus the models made more errors when embedding the triples of the relation. Instead, in our proposed model, we leverage a Bayesian non-parametric in- finite mixture model to handle multiple relation se- mantics by generating multiple translation compo- nents for a relation. Thus, different semantics are characterized by different components in our em- bedding model. For example, we can distinguish the two clusters HasPart.1 or HasPart.2, where the relation semantics are automatically clustered to represent the meaning of associated entity pairs.</p><p>To summarize, our contributions are as follows:</p><p>• We propose a new issue in knowledge graph embedding, multiple relation semantics that a relation in knowledge graph may have dif- ferent meanings revealed by the associated entity pairs, which has never been studied previously.</p><p>• To address the above issue, we propose a novel Bayesian non-parametric infinite mix- ture embedding model, TransG. The model can automatically discover semantic clusters of a relation, and leverage a mixture of multi- ple relation components for translating an en- tity pair. Moreover, we present new insights from the generative perspective.</p><p>• Extensive experiments show that our pro- posed model obtains substantial improve- ments against the state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Translation-Based Embedding. Existing translation-based embedding methods share the same translation principle h + r ≈ t and the score function is designed as:</p><formula xml:id="formula_0">f r (h, t) = ||h r + r − t r || 2 2</formula><p>where h r , t r are entity embedding vectors pro- jected in the relation-specific space. TransE <ref type="bibr" target="#b3">(Bordes et al., 2013)</ref>, lays the entities in the original en- tity space: h r = h, t r = t. TransH ( <ref type="bibr" target="#b20">Wang et al., 2014</ref>) projects entities into a hyperplane for ad- dressing the issue of complex relation embedding: h r = h − w r hw r , t r = t − w r tw r . To address the same issue, TransR (Lin et al., 2015b), trans- forms the entity embeddings by the same relation- specific matrix: h r = M r h, t r = M r t. TransR also proposes an ad-hoc clustering-based method, CTransR, where the entity pairs for a relation are clustered into different groups, and the pairs in the same group share the same relation vec- tor. In comparison, our model is more elegant to address such an issue theoretically, and does not require a pre-process of clustering. Further- more, our model has much better performance than CTransR, as expected. TransM (Fan et al., <ref type="figure">Figure 2</ref>: Visualization of multiple relation semantics. The data are selected from Wordnet. The dots are correct triples that belong to HasPart relation, while the circles are incorrect ones. The point coor- dinate is the difference vector between tail and head entity, which should be near to the centre. (a) The correct triples are hard to be distinguished from the incorrect ones. (b) By applying multiple semantic components, our proposed model could discriminate the correct triples from the wrong ones.</p><p>2014) leverages the structure of the knowledge graph via pre-calculating the distinct weight for each training triple to enhance embedding. <ref type="bibr">KG2E (He et al., 2015</ref>) is a probabilistic embedding method for modeling the uncertainty in knowledge graph.</p><p>There are many works to improve translation- based methods by considering other information. For instance, ( ) aims at discov- ering the geometric structure of the embedding space to make it semantically smooth. ( <ref type="bibr" target="#b20">Wang et al., 2014</ref>) focuses on bridging the gap between knowledge and texts, with a loss function for jointly modeling knowledge graph and text re- sources. (  incorporates the rules that are related with relation types such as 1-N and N-1. PTransE (Lin et al., 2015a) takes into ac- count path information in knowledge graph.</p><p>Since the previous models are point-wise mod- eling methods, <ref type="bibr">ManifoldE (Xiao et al., 2016</ref>) pro- poses a novel manifold-based approach for knowl- edge graph embedding. In aid of kernel tricks, manifold-based methods can improve embedding performance substantially.</p><p>Structured &amp; Unstructured Embedding. The structured embedding model <ref type="bibr" target="#b1">(Bordes et al., 2011</ref>) transforms the entity space with the head-specific and tail-specific matrices. The score function is defined as f r (h, t) = ||M h,r h − M t,r t||. Ac- cording to <ref type="bibr" target="#b18">(Socher et al., 2013)</ref>, this model cannot capture the relationship between entities. Seman- tic Matching Energy (SME) ( ) ( <ref type="bibr" target="#b4">Bordes et al., 2014)</ref> can handle the correlations between entities and relations by matrix product and Hadamard product. The unstructured model ( ) may be a simplified version of TransE without considering any relation-related information. The score function is directly defined as f r (h, t) = ||h − t|| 2 2 . Neural Network based Embedding.</p><p>Sin- gle Layer Model (SLM) <ref type="bibr" target="#b18">(Socher et al., 2013)</ref> applies neural network to knowledge graph embedding.</p><p>The score function is defined as f r (h, t) = u r g(M r,1 h + M r,2 t) where M r,1 , M r,2 are relation-specific weight matri- ces. Neural Tensor Network (NTN) <ref type="bibr" target="#b18">(Socher et al., 2013</ref>) defines a very expressive score function by applying tensor:</p><formula xml:id="formula_1">f r (h, t) = u r g(h W ··r t + M r,1 h + M r,2 t + b r ),</formula><p>where u r is a relation-specific linear layer, g(·) is the tanh function, W ∈ R d×d×k is a 3-way tensor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TransG: A Generative Model for Embedding</head><p>As just mentioned, only one single translation vec- tor for a relation may be insufficient to model mul- tiple relation semantics. In this paper, we pro- pose to use Bayesian non-parametric infinite mix-ture embedding model ( <ref type="bibr" target="#b8">Griffiths and Ghahramani, 2011</ref>). The generative process of the model is as follows:</p><p>1. For an entity e ∈ E:</p><p>(a) Draw each entity embedding mean vec- tor from a standard normal distribution as a prior: u e N (0, 1).</p><p>2. For a triple (h, r, t) ∈ ∆:</p><p>(a) Draw a semantic component from Chi- nese Restaurant Process for this relation:</p><formula xml:id="formula_2">π r,m ∼ CRP (β). (b) Draw a head entity embedding vec- tor from a normal distribution: h N (u h , σ 2 h E). (c) Draw a tail entity embedding vec- tor from a normal distribution: t N (u t , σ 2 t E). (d) Draw a relation embedding vector for this semantics: u r,m = t − h N (u t − u h , (σ 2 h + σ 2 t )E).</formula><p>where u h and u t indicate the mean embedding vector for head and tail respectively, σ h and σ t indicate the variance of corresponding entity dis- tribution respectively, and u r,m is the m-th com- ponent translation vector of relation r. Chinese Restaurant Process (CRP) is a Dirichlet Process and it can automatically detect semantic compo- nents. In this setting, we obtain the score function as below:</p><formula xml:id="formula_3">P{(h, r, t)} ∝ Mr m=1 π r,m P(u r,m |h, t) = Mr m=1 π r,m e − ||u h +ur,m−u t || 2 2 σ 2 h +σ 2 t<label>(1)</label></formula><p>where π r,m is the mixing factor, indicating the weight of i-th component and M r is the number of semantic components for the relation r, which is learned from the data automatically by the CRP. Inspired by <ref type="figure" target="#fig_0">Fig.1</ref>, TransG leverages a mixture of relation component vectors for a specific re- lation. Each component represents a specific la- tent meaning. By this way, TransG could distin- guish multiple relation semantics. Notably, the CRP could generate multiple semantic compo- nents when it is necessary and the relation seman- tic component number M r is learned adaptively from the data. <ref type="table" target="#tab_1">Table 1: Statistics of datasets   Data  WN18  FB15K  WN11  FB13   #Rel  18  1,345  11  13  #Ent  40,943  14,951  38,696  75,043  #Train  141,442  483,142  112,581  316,232  #Valid  5,000  50,000  2,609  5,908  #Test</ref> 5,000 59,071 10,544 23,733</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Explanation from the Geometry Perspective</head><p>Similar to previous studies, TransG has geometric explanations. In the previous methods, when the relation r of triple (h, r, t) is given, the geometric representations are fixed, as h + r ≈ t. However, TransG generalizes this geometric principle to:</p><formula xml:id="formula_4">m * (h,r,t) = arg max m=1...Mr π r,m e − ||u h +ur,m−u t || 2 2 σ 2 h +σ 2 t h + u r,m * (h,r,t) ≈ t<label>(2)</label></formula><p>where m * (h,r,t) is the index of primary compo- nent. Though all the components contribute to the model, the primary one contributes the most due to the exponential effect (exp(·)). When a triple (h, r, t) is given, TransG works out the index of primary component then translates the head entity to the tail one with the primary translation vector.</p><p>For most triples, there should be only one com- ponent that have significant non-zero value as  is very large so that the expo- nential function value is very small. This is why the primary component could represent the corre- sponding semantics.</p><p>To summarize, previous studies make transla- tion identically for all the triples of the same re- lation, but TransG automatically selects the best translation vector according to the specific seman- tics of a triple. Therefore, TransG could focus on the specific semantic embedding to avoid much noise from the other unrelated semantic compo- nents and result in promising improvements than existing methods. Note that, all the components in TransG have their own contributions, but the pri- mary one makes the most.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Algorithm</head><p>The maximum data likelihood principle is applied for training. As to the non-parametric part, π r,m is generated from the CRP with Gibbs Sampling, similar to (He et al., 2015) and ( <ref type="bibr" target="#b8">Griffiths and Ghahramani, 2011)</ref>. A new component is sampled for a triple (h,r,t) with the below probability:</p><formula xml:id="formula_5">P(m r,new ) = βe − ||h−t|| 2 2 σ 2 h +σ 2 t +2 βe − ||h−t|| 2 2 σ 2 h +σ 2 t +2 + P{(h, r, t)}<label>(3)</label></formula><p>where P{(h, r, t)} is the current posterior prob- ability. As to other parts, in order to better distin- guish the true triples from the false ones, we max- imize the ratio of likelihood of the true triples to that of the false ones. Notably, the embedding vec- tors are initialized by <ref type="bibr" target="#b7">(Glorot and Bengio, 2010)</ref>. Putting all the other constraints together, the final objective function is obtained, as follows: </p><formula xml:id="formula_6">min u h ,ur,m,u t L L = −</formula><p>where ∆ is the set of golden triples and ∆ is the set of false triples. C controls the scaling degree. E is the set of entities and R is the set of relations.</p><p>Noted that the mixing factors π and the variances σ are also learned jointly in the optimization. SGD is applied to solve this optimization prob- lem. In addition, we apply a trick to control the parameter updating process during training. For those very impossible triples, the update process is skipped. Hence, we introduce a similar condition as TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013</ref>) adopts: the train- ing algorithm will update the embedding vectors only if the below condition is satisfied:</p><formula xml:id="formula_8">P{(h, r, t)} P{(h , r , t )} = Mr m=1 π r,m e − ||u h +ur,m−u t || 2 2 σ 2 h +σ 2 t M r m=1 π r ,m e − ||u h +u r ,m −u t || 2 2 σ 2 h +σ 2 t ≤ M r e γ<label>(5)</label></formula><p>where (h, r, t) ∈ ∆ and (h , r , t ) ∈ ∆ . γ con- trols the updating condition.</p><p>As to the efficiency, in theory, the time com- plexity of TransG is bounded by a small constant M compared to TransE, that is O(TransG) = O(M × O(TransE)) where M is the number of semantic components in the model. Note that TransE is the fastest method among translation- based methods. The experiment of Link Predic- tion shows that TransG and TransE would con- verge at around 500 epochs, meaning there is also no significant difference in convergence speed. In experiment, TransG takes 4.8s for one iteration on FB15K while TransR costs 136.8s and PTransE <ref type="table">Table 3</ref>: Evaluation results on FB15K by mapping properties of relations(%) Tasks Predicting Head(HITS@10) Predicting Tail(HITS@10) Relation Category 1-1 1-N N-1 N-N 1-1 1-N N-1 N-N Unstructured (Bordes et al., 2011) 34.5 2.5 6.1 6.6 34.3 4.2 1.9 6.6 SE( <ref type="bibr" target="#b1">Bordes et al., 2011)</ref> 35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3 SME(bilinear) <ref type="figure" target="#fig_0">(Bordes et al., 2012)</ref> 30.9 69.6 19.9</p><p>38.6 28.2 13.1 76.0 41.8 TransE ( <ref type="bibr" target="#b3">Bordes et al., 2013)</ref> 43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0 TransH ( <ref type="bibr" target="#b20">Wang et al., 2014)</ref> 66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2 TransR <ref type="figure" target="#fig_0">(Lin et al., 2015b)</ref> 78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1 CTransR <ref type="figure" target="#fig_0">(Lin et al., 2015b)</ref> 81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8 PTransE <ref type="figure" target="#fig_0">(Lin et al., 2015a)</ref> 90.1 92.0 58.7 86.1 90.1 70.7 87.5 88.7 KG2E <ref type="figure" target="#fig_0">(He et al., 2015)</ref> 92</p><note type="other">.3 93.7 66.0 69.6 92.6 67.9 94.4 73.4 TransG (this paper) 93.0 96.0 62.5 86.8 92.8 68.1 94.5 88.8</note><p>costs 1200.0s on the same computer for the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments are conducted on four public benchmark datasets that are the subsets of Word- net and Freebase, respectively. The statistics of these datasets are listed in Tab.1. Experiments are conducted on two tasks : Link Prediction and Triple Classification. To further demonstrate how the proposed model approaches multiple relation semantics, we present semantic component analy- sis at the end of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Link Prediction</head><p>Link prediction concerns knowledge graph com- pletion: when given an entity and a relation, the embedding models predict the other missing en- tity. More specifically, in this task, we predict t given (h, r, * ), or predict h given ( * , r, t). The WN18 and FB15K are two benchmark datasets for this task. Note that many AI tasks could be en- hanced by Link Prediction such as relation extrac- tion ( <ref type="bibr" target="#b11">Hoffmann et al., 2011</ref>). Evaluation Protocol. We adopt the same proto- col used in previous studies. For each testing triple (h, r, t), we corrupt it by replacing the tail t (or the head h) with every entity e in the knowledge graph and calculate a probabilistic score of this corrupted triple (h, r, e) (or (e, r, t)) with the score function f r (h, e). After ranking these scores in descend- ing order, we obtain the rank of the original triple. There are two metrics for evaluation: the averaged rank (Mean Rank) and the proportion of testing triple whose rank is not larger than 10 (HITS@10). This is called "Raw" setting. When we filter out the corrupted triples that exist in the training, val- idation, or test datasets, this is the"Filter" setting. If a corrupted triple exists in the knowledge graph, ranking it ahead the original triple is also accept- able. To eliminate this case, the "Filter" setting is preferred. In both settings, a lower Mean Rank and a higher HITS@10 mean better performance.</p><p>Implementation. As the datasets are the same, we directly report the experimental results of sev- eral baselines from the literature, as in ( <ref type="bibr" target="#b3">Bordes et al., 2013)</ref>, ( <ref type="bibr" target="#b20">Wang et al., 2014</ref>) and ( <ref type="bibr" target="#b14">Lin et al., 2015b</ref>). We have attempted several settings on the validation dataset to get the best configuration. For example, we have tried the dimensions of 100, 200, 300, 400. Under the "bern." sampling strat- egy, the optimal configurations are: learning rate α = 0.001, the embedding dimension k = 100, γ = 2.5, β = 0.05 on WN18; α = 0.0015, k = 400, γ = 3.0, β = 0.1 on FB15K. Note that all the symbols are introduced in "Methods". We train the model until it converges.</p><p>Results. Evaluation results on WN18 and FB15K are reported in Tab.2 and Tab.3 1 . We ob- serve that:  PartOf Location (Capital of Utah, Beehive State), (Hindustan, Bharat) ... Composition (Monitor, Television), (Bush, Adult Body), (Cell Organ, Cell)...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Religion</head><p>Catholicism </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Triple Classification</head><p>In order to testify the discriminative capability be- tween true and false facts, triple classification is conducted. This is a classical task in knowledge base embedding, which aims at predicting whether a given triple (h, r, t) is correct or not. WN11 and FB13 are the benchmark datasets for this task. Note that evaluation of classification needs nega- tive samples, and the datasets have already pro- vided negative triples. Evaluation Protocol. The decision process is very simple as follows: for a triple (h, r, t), if f r (h, t) is below a threshold σ r , then positive; oth- erwise negative. The thresholds {σ r } are deter- mined on the validation dataset. Implementation. As all methods use the same datasets, we directly re-use the results of different methods from the literature. We have attempted several settings on the validation dataset to find Results. Accuracies are reported in Tab.5 and <ref type="figure" target="#fig_4">Fig.3</ref>. The following are our observations:</p><p>1. TransG outperforms all the baselines remark- ably. Compared to TransR, TransG improves by 1.7% on WN11 and 5.8% on FB13, and the averaged semantic component number on WN11 is 2.63 and that on FB13 is 4.53. This result shows the benefit of capturing multiple relation semantics for a relation.</p><p>2. The relations, such as "Synset Domain" and "Type Of", which hold more semantic com- ponents, are improved much more. In com- parison, the relation "Similar" holds only one semantic component and is almost not pro- moted. This further demonstrates that cap- turing multiple relation semantics can benefit embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic Component Analysis</head><p>In this subsection, we analyse the number of se- mantic components for different relations and list the component number on the dataset WN18 and FB13 in <ref type="figure" target="#fig_5">Fig.4</ref>. Results. As <ref type="figure" target="#fig_5">Fig. 4</ref> and Tab. 4 show, we have the following observations:</p><p>1. Multiple semantic components are indeed necessary for most relations. Except for re- lations such as "Also See", "Synset Usage" and "Gender", all other relations have more than one semantic component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Different components indeed correspond</head><p>to different semantics, justifying the theoretical analysis and effectiveness of TransG. For example, "Profession" has at least three semantics: scientist- related as (ElLissitzky, Architect), businessman-related as (EnochPratt, Entrepreneur) and writer- related as (Vlad.Gardin, ScreenWriter).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WN11 and WN18 are different subsets of</head><p>Wordnet. As we know, the semantic compo- nent number is decided on the triples in the dataset. Therefore, It's reasonable that sim- ilar relations, such as "Synset Domain" and "Synset Usage" may hold different semantic numbers for WN11 and WN18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a generative Bayesian non-parametric infinite mixture embedding model, TransG, to address a new issue, multiple relation semantics, which can be commonly seen in knowl- edge graph. TransG can discover the latent se- mantics of a relation automatically and leverage a mixture of relation components for embedding. Extensive experiments show our method achieves substantial improvements against the state-of-the- art baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of TransE embedding vectors with PCA dimension reduction. Four relations (a ∼ d) are chosen from Freebase and Wordnet. A dot denotes a triple and its position is decided by the difference vector between tail and head entity (t − h). Since TransE adopts the principle of t − h ≈ r, there is supposed to be only one cluster whose centre is the relation vector r. However, results show that there exist multiple clusters, which justifies our multiple relation semantics assumption.</figDesc><graphic url="image-1.png" coords="1,307.28,288.34,218.27,236.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Factor Models.</head><label></label><figDesc>The latent factor models (Jenat- ton et al., 2012) (Sutskever et al., 2009) attempt to capturing the second-order correlations between entities by a quadratic form. The score function is defined as f r (h, t) = h W r t. RESCAL is a collective matrix factorization model which is also a common method in knowledge base embedding (Nickel et al., 2011) (Nickel et al., 2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and the others would be small enough, due to the exponential decay. This property reduces the noise from the other semantic components to better characterize mul- tiple relation semantics. In detail, (t − h) is al- most around only one translation vector u r,m * (h,r,t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>+C r∈R Mr m=1 ||u r,m || 2 2 + e∈E ||u e || 2 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracies of each relations in WN11 for triple classification. The right y-axis is the number of semantic components, corresponding to the lines.</figDesc><graphic url="image-3.png" coords="7,307.28,239.17,218.27,133.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Semantic component number on WN18 (left) and FB13 (right).</figDesc><graphic url="image-4.png" coords="8,106.02,62.81,385.53,165.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Evaluation results on link prediction 
Datasets 
WN18 
FB15K 

Metric 
Mean Rank 
HITS@10(%) 
Mean Rank 
HITS@10(%) 
Raw Filter Raw Filter 
Raw Filter Raw Filter 
Unstructured (Bordes et al., 2011) 
315 
304 
35.3 
38.2 
1,074 979 
4.5 
6.3 
RESCAL (Nickel et al., 2012) 
1,180 1,163 37.2 
52.8 
828 
683 28.4 
44.1 
SE(Bordes et al., 2011) 
1,011 
985 
68.5 
80.5 
273 
162 28.8 
39.8 
SME(bilinear) (Bordes et al., 2012) 
526 
509 
54.7 
61.3 
284 
158 31.3 
41.3 
LFM (Jenatton et al., 2012) 
469 
456 
71.4 
81.6 
283 
164 26.0 
33.1 
TransE (Bordes et al., 2013) 
263 
251 
75.4 
89.2 
243 
125 34.9 
47.1 
TransH (Wang et al., 2014) 
401 
388 
73.0 
82.3 
212 
87 
45.7 
64.4 
TransR (Lin et al., 2015b) 
238 
225 
79.8 
92.0 
198 
77 
48.2 
68.7 
CTransR (Lin et al., 2015b) 
231 
218 
79.4 
92.3 
199 
75 
48.4 
70.2 
PTransE (Lin et al., 2015a) 
N/A 
N/A N/A 
N/A 
207 
58 
51.4 
84.6 
KG2E (He et al., 2015) 
362 
348 
80.5 
93.2 
183 
69 
47.5 
71.5 
TransG (this paper) 
357 
345 
84.5 
94.9 
152 
50 
55.9 
88.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>1 .</head><label>1</label><figDesc></figDesc><table>TransG outperforms all the baselines obvi-
ously. Compared to TransR, TransG makes 
improvements by 2.9% on WN18 and 26.0% 
on FB15K, and the averaged semantic com-
ponent number on WN18 is 5.67 and that on 
FB15K is 8.77. This result demonstrates cap-
turing multiple relation semantics would ben-
efit embedding. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Different clusters in WN11 and FB13 relations. 
Relation 
Cluster 
Triples (Head, Tail) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Triple classification: accuracy(%) for dif-
ferent embedding methods. 
Methods WN11 FB13 AVG. 
LFM 
73.8 
84.3 
79.0 
NTN 
70.4 
87.1 
78.8 
TransE 
75.9 
81.5 
78.7 
TransH 
78.8 
83.3 
81.1 
TransR 
85.9 
82.5 
84.2 
CTransR 
85.7 
N/A 
N/A 
KG2E 
85.4 
85.3 
85.4 
TransG 
87.4 
87.3 
87.4 

</table></figure>

			<note place="foot" n="1"> Note that correctly regularized TransE can produce much better performance than what were reported in the ogirinal paper, see (García-Durán et al., 2015).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-fifth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint learning of words and meaning representations for open-text semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="233" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transition-based knowledge graph embedding with relational mapping properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Miao Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation</title>
		<meeting>the 28th Pacific Asia Conference on Language, Information, and Computation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Combining two and three-way embeddings models for link prediction in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Grandvalet</surname></persName>
		</author>
		<idno>abs/1506.00999</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The indian buffet process: An introduction and review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1185" to="1224" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantically smooth knowledge graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to represent knowledge graphs with gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A latent factor model for highly multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3167" to="3175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factorizing yago: scalable machine learning for linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on World Wide Web</title>
		<meeting>the 21st international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1821" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the TwentyEighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowledge base completion using embeddings and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From one point to a manifold: Knowledge graph embedding for precise link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
