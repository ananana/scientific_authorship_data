<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University ♣ Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University ♣ Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University ♣ Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University ♣ Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Macwhinney</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University ♣ Google DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University ♣ Google DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning the Curriculum with Bayesian Optimization for Task-Specific Word Representation Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="130" to="139"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We use Bayesian optimization to learn curricula for word representation learning, optimizing performance on downstream tasks that depend on the learned representations as features. The curricula are mod-eled by a linear ranking function which is the scalar product of a learned weight vector and an engineered feature vector that characterizes the different aspects of the complexity of each instance in the training corpus. We show that learning the curriculum improves performance on a variety of downstream tasks over random orders and in comparison to the natural corpus order.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is well established that in language acquisition, there are robust patterns in the order by which phenomena are acquired. For example, prototypi- cal concepts are acquired earlier; concrete words tend to be learned before abstract ones <ref type="bibr" target="#b29">(Rosch, 1978)</ref>. The acquisition of lexical knowledge in artificial systems proceeds differently. In gen- eral, models will improve during the course of pa- rameter learning, but the time course of acquisi- tion is not generally studied beyond generaliza- tion error as a function of training time or data size. We revisit this issue of choosing the order of learning-curriculum learning-framing it as an optimization problem so that a rich array of factors-including nuanced measures of difficulty, as well as prototypicality and diversity-can be exploited.</p><p>Prior research focusing on curriculum strate- gies in NLP is scarce, and has conventionally been following a paradigm of "starting small" <ref type="bibr" target="#b4">(Elman, 1993)</ref>, i.e., initializing the learner with "simple" examples first, and then gradually increasing data complexity ( <ref type="bibr">Bengio et al., 2009;</ref><ref type="bibr" target="#b38">Spitkovsky et al., 2010)</ref>. In language modeling, this prefer- ence for increasing complexity has been realized by curricula that increase the entropy of training data by growing the size of the training vocabu- lary from frequent to less frequent words <ref type="bibr">(Bengio et al., 2009)</ref>. In unsupervised grammar in- duction, an effective curriculum comes from in- creasing length of training sentences as training progresses ( <ref type="bibr" target="#b38">Spitkovsky et al., 2010)</ref>. These case studies have demonstrated that carefully designed curricula can lead to better results. However, they have relied on heuristics in selecting curricula or have followed the intuitions of human and animal learning <ref type="bibr" target="#b14">(Kail, 1990;</ref><ref type="bibr" target="#b35">Skinner, 1938)</ref>. Had differ- ent heuristics been chosen, the results would have been different. In this paper, we use curriculum learning to create improved word representations. However, rather than testing a small number of curricula, we search for an optimal curriculum us- ing Bayesian optimization. A curriculum is de- fined to be the ordering of the training instances, in our case it is the ordering of paragraphs in which the representation learning model reads the cor- pus. We use a linear ranking function to conduct a systematic exploration of interacting factors that affect curricula of representation learning models. We then analyze our findings, and compare them to human intuitions and learning principles.</p><p>We treat curriculum learning as an outer loop in the process of learning and evaluation of vector- space representations of words; the iterative pro- cedure is (1) predict a curriculum; (2) train word embeddings; (3) evaluate the embeddings on tasks that use word embeddings as the sole features. Through this model we analyze the impact of cur- riculum on word representation models and on ex- trinsic tasks. To quantify curriculum properties, we define three groups of features aimed at analyz- ing statistical and linguistic content and structure of training data: (1) diversity, (2) simplicity, and (3) prototypicality. A function of these features is computed to score each paragraph in the training data, and the curriculum is determined by sorting corpus paragraphs by the paragraph scores. We detail the model in §2. Word vectors are learned from the sorted corpus, and then evaluated on part- of-speech tagging, parsing, named entity recog- nition, and sentiment analysis ( §3). Our exper- iments confirm that training data curriculum af- fects model performance, and that models with op- timized curriculum consistently outperform base- lines trained on shuffled corpora ( §4). We analyze our findings in §5.</p><p>The contributions of this work are twofold. First, this is the first framework that formulates curriculum learning as an optimization problem, rather then shuffling data or relying on human in- tuitions. We experiment with optimizing the cur- riculum of word embeddings, but in principle the curriculum of other models can be optimized in a similar way. Second, to the best of our knowledge, this study is the first to analyze the impact of distri- butional and linguistic properties of training texts on the quality of task-specific word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Curriculum Learning Model</head><p>We are considering the problem of maximizing a performance of an NLP task through sequentially optimizing the curriculum of training data of word vector representations that are used as features in the task.</p><p>Let X = {x 1 , x 2 , . . . , x n } be the training cor- pus with n lines (sentences or paragraphs). The curriculum of word representations is quantified by scoring each of the paragraphs according to the linear function w φ(X ), where φ(X ) ∈ R ×1 is a real-valued vector containing linguistic features extracted for each paragraph, and w ∈ R ×1 de- note the weights learned for these features. The feature values φ(X ) are z-normalized across all paragraphs. These scores are used to specify the order of the paragraphs in the corpus-the curricu- lum: we sort the paragraphs by their scores.</p><p>After the paragraphs are curriculum-ordered, the reordered training corpus is used to generate word representations. These word representations are then used as features in a subsequent NLP task. We define the objective function eval : X → R, which is the quality estimation metric for this NLP task performed on a held-out dataset (e.g., corre- lation, accuracy, F 1 score, BLEU). Our goal is to define the features φ(X ) and to find the optimal weights w that maximize eval.</p><p>We optimize the feature weights using Bayesian optimization; we detail the model in §2.1. Distri- butional and linguistic features inspired by prior research in language acquisition and second lan- guage learning are described in §2.2. <ref type="figure" target="#fig_0">Figure 1</ref> shows the computation flow diagram. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bayesian Optimization for Curriculum Learning</head><p>As no assumptions are made regarding the form of eval(w), gradient-based methods cannot be ap- plied, and performing a grid search over param- eterizations of w would require a exponentially growing number of parameterizations to be tra- versed. Thus, we propose to use Bayesian Op- timization (BayesOpt) as the means to maximize eval(w). BayesOpt is a methodology to globally optimize expensive, multimodal black-box func- tions ( <ref type="bibr" target="#b33">Shahriari et al., 2016;</ref><ref type="bibr">Bergstra et al., 2011;</ref><ref type="bibr" target="#b36">Snoek et al., 2012)</ref>. It can be viewed as a se- quential approach to performing a regression from high-level model parameters (e.g., learning rate, number of layers in a neural network, and in our model-curriculum weights w) to the loss function or the performance measure (eval). An arbitrary objective function, eval, is treated as a black-box, and BayesOpt uses Bayesian infer- ence to characterize a posterior distribution over functions that approximate eval. This model of eval is called the surrogate model. Then, the BayesOpt exploits this model to make decisions about eval, e.g., where is the expected maximum of the function, and what is the expected improve- ment that can be obtained over the best iteration so far. The strategy function, estimating the next set of parameters to explore given the current beliefs about eval is called the acquisition function. The surrogate model and the acquisition function are the two key components in the BayesOpt frame- work; their interaction is shown in Algorithm 1.</p><p>The surrogate model allows us to cheaply ap- proximate the quality of a set of parameters w without running eval(w), and the acquisition function uses this surrogate to choose a new value of w. However, a trade-off must be made: should the acquisition function move w into a region where the surrogate believes an optimal value will be found, or should it explore regions of the space that reveal more about how eval behaves, per- haps discovering even better values? That is, acquisition functions balance a tradeoff between exploration-by selecting w in the regions where the uncertainty of the surrogate model is high, and exploitation-by querying the regions where the model prediction is high.</p><p>Popular choices for the surrogate model are Gaussian Processes <ref type="bibr" target="#b28">(Rasmussen, 2006;</ref><ref type="bibr">Snoek et al., 2012, GP)</ref>, providing convenient and powerful prior distribution on functions, and tree-structured Parzen estimators <ref type="bibr">(Bergstra et al., 2011, TPE)</ref>, tailored to handle conditional spaces. Choices of the acquisition functions include probability of improvement <ref type="bibr" target="#b17">(Kushner, 1964)</ref>, expected improve- ment (EI) <ref type="bibr">(Močkus et al., 1978;</ref><ref type="bibr" target="#b13">Jones, 2001</ref>), GP upper confidence bound ( <ref type="bibr" target="#b39">Srinivas et al., 2010</ref>), Thompson sampling <ref type="bibr" target="#b42">(Thompson, 1933)</ref>, entropy search ( <ref type="bibr" target="#b9">Hennig and Schuler, 2012)</ref>, and dynamic combinations of the above functions <ref type="bibr" target="#b10">(Hoffman et al., 2011)</ref>; see <ref type="bibr" target="#b33">Shahriari et al. (2016)</ref> for an ex- tensive comparison. <ref type="bibr" target="#b47">Yogatama et al. (2015)</ref> found that the combination of EI as the acquisition func- tion and TPE as the surrogate model performed favorably in Bayesian optimization of text repre- sentations; we follow this choice in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Distributional and Linguistic Features</head><p>To characterize and quantify a curriculum, we de- fine three categories of features, focusing on vari- ous distributional, syntactic, and semantic aspects of training data. We now detail the feature cate- gories along with motivations for feature selection.</p><p>DIVERSITY. Diversity measures capture the dis- tributions of types in data. Entropy is the best- known measure of diversity in statistical research, but there are many others ( <ref type="bibr" target="#b41">Tang et al., 2006;</ref><ref type="bibr" target="#b7">Gimpel et al., 2013)</ref>. Common measures of diversity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Bayesian optimization 1: H ← ∅ Initialize observation history 2: A ← EI</head><p>Initialize acquisition function 3: S 0 ← T P E Initialize surrogate model 4: for t ← 1 to T do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>w t ← argmax w A(w; S t−1 , H) Predict w t by optimizing acquisition function <ref type="bibr">6:</ref> eval(w t ) Evaluate w t on extrinsic task <ref type="bibr">7:</ref> H ← H ∪ (w t , eval(w t )) Update obser- vation history 8:</p><p>Estimate S t given H 9: end for 10: return H are used in many contrasting fields, from ecol- ogy and biology <ref type="bibr" target="#b30">(Rosenzweig, 1995;</ref><ref type="bibr" target="#b21">Magurran, 2013)</ref>, to economics and social studies <ref type="bibr" target="#b40">(Stirling, 2007)</ref>. Diversity has been shown effective in re- lated research on curriculum learning in language modeling, vision, and multimedia analysis <ref type="bibr">(Bengio et al., 2009;</ref><ref type="bibr" target="#b11">Jiang et al., 2014)</ref>.</p><p>Let p i and p j correspond to empirical frequen- cies of word types t i and t j in the training data. Let d ij correspond to their semantic similarity, calcu- lated as the cosine similarity between embeddings of t i and t j learned from the training data. We an- notate each paragraph with the following diversity features:</p><p>• Number of word types: #types</p><p>• Type-token ratio: #types #tokens <ref type="bibr" target="#b34">(Simpson, 1949)</ref>:</p><formula xml:id="formula_0">• Entropy: − i p i ln(p i ) • Simpson's index</formula><formula xml:id="formula_1">i p i 2</formula><p>• Quadratic entropy <ref type="bibr" target="#b27">(Rao, 1982)</ref>: <ref type="bibr" target="#b38">Spitkovsky et al. (2010)</ref> have val- idated the utility of syntactic simplicity in curricu- lum learning for unsupervised grammar induction by showing that training on sentences in order of increasing lengths outperformed other orderings. We explore the simplicity hypothesis, albeit with- out prior assumptions on specific ordering of data, and extend it to additional simplicity/complexity measures of training data. Our features are in- spired by prior research in second language acqui- sition, text simplification, and readability assess- ment ( <ref type="bibr" target="#b31">Schwarm and Ostendorf, 2005;</ref><ref type="bibr" target="#b8">Heilman et al., 2007;</ref><ref type="bibr" target="#b25">Pitler and Nenkova, 2008;</ref><ref type="bibr" target="#b45">Vajjala and Meurers, 2012)</ref>. We use an off-the-shelf syntac- tic parser 2 ( <ref type="bibr" target="#b48">Zhang and Clark, 2011</ref>) to parse our training corpus. Then, the following features are used to measure phonological, lexical, and syntac- tic complexity of training paragraphs:</p><formula xml:id="formula_2">1 i,j d ij p i p j SIMPLICITY.</formula><p>• Language model score</p><p>• Character language model score</p><p>• Average sentence length</p><p>• Verb-token ratio</p><p>• Noun-token ratio</p><p>• Parse tree depth  <ref type="bibr" target="#b29">(Rosch, 1978)</ref>, which posits that semantic cat- egories include more central (or prototypical) as well as less prototypical words. For example, in the ANIMAL category, dog is more prototypical than sloth (because dog is more frequent); dog is more prototypical than canine (because dog is more concrete); and dog is more prototypical than bull terrier (because dog is less specific). Accord- ing to the theory, more prototypical words are ac- quired earlier. We use lexical semantic databases to operationalize insights from the prototype the- ory in the following semantic features; the features are computed on token level and averaged over paragraphs:</p><p>• Age of acquisition (AoA) of words was ex- tracted from the crowd-sourced database, con- taining over 50 thousand English words <ref type="bibr" target="#b16">(Kuperman et al., 2012</ref>). For example, the AoA of run is 4.47 (years), of flee is 8.33, and of abscond is 13.36. If a word was not found in the database it was assigned the maximal age of 25.</p><p>• Concreteness ratings on the scale of 1-5 (1 is most abstract) for 40 thousand English lemmas <ref type="bibr">(Brysbaert et al., 2014</ref>). For example, cookie is rated as 5, and spirituality as 1.07.</p><p>• Imageability ratings are taken from the MRC psycholinguistic database <ref type="bibr" target="#b46">(Wilson, 1988)</ref>. Fol- lowing <ref type="bibr" target="#b44">Tsvetkov et al. (2014)</ref>, we used the MRC annotations as seed, and propagated the ratings to all vocabulary words using the word embed- dings as features in an 2 -regularized logistic re- gression classifier.</p><p>• Conventionalization features count the num- ber of "conventional" words and phrases in a paragraph. Assuming that a Wikipedia title is a proxy to a conventionalized concept, we counted the number of existing titles (from a database of over 4.5 million titles) in the para- graph.</p><p>• Number of syllables scores are also extracted from the AoA database; out-of-database words were annotated as 5-syllable words.</p><p>• Relative frequency in a supersense was com- puted by marginalizing the word frequencies in the training corpus over coarse semantic cate- gories defined in the WordNet <ref type="bibr">(Fellbaum, 1998;</ref><ref type="bibr" target="#b0">Ciaramita and Altun, 2006</ref> • Relative frequency in a synset was calculated similarly to the previous feature category, but word frequencies were marginalized over Word- Net synsets (more fine-grained synonym sets). For example, in the synset {vet, warhorse, vet- eran, oldtimer, seasoned stager}, veteran is the most prototypical word, scoring 0.87.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Benchmarks</head><p>We evaluate the utility of the pretrained word em- beddings as features in downstream NLP tasks. We choose the following off-the-shelf models that utilize pretrained word embeddings as features:</p><formula xml:id="formula_3">Sentiment Analysis (Senti). Socher et al.<label>(2013)</label></formula><p>created a treebank of sentences anno- tated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use the average of the word vectors of a given sentence as a feature vec- tor for classification <ref type="bibr" target="#b5">(Faruqui et al., 2015;</ref><ref type="bibr" target="#b32">Sedoc et al., 2016)</ref>. The 2 -regularized logistic regres- sion classifier is tuned on the development set and accuracy is reported on the test set.</p><p>Named Entity Recognition (NER </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Data. All models were trained on Wikipedia ar- ticles, split to paragraph-per-line. Texts were cleaned, tokenized, numbers were normalized by replacing each digit with "DG", all types that oc- cur less than 10 times were replaces by the "UNK" token, the data was not lowercased. We list data sizes in table 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># paragraphs # tokens # types 2,532,361</head><p>100,872,713 156,663 <ref type="table">Table 1</ref>: Training data sizes.</p><p>Setup. 100-dimensional word embeddings were trained using the cbow model implemented in the word2vec toolkit (Mikolov et al., 2013). 3 All training data was used, either shuffled or ordered by a curriculum. As described in §3, we modified the extrinsic tasks to learn solely from word em- beddings, without additional features. All models were learned under same conditions, across cur- ricula: in Parse, NER, and POS we limited the number of training iterations to 3, 3, and 1, re- spectively. This setup allowed us to evaluate the effect of curriculum without additional interacting factors.</p><p>Experiments. In all the experiments we first train word embedding models, then the word em- beddings are used as features in four extrinsic tasks ( §3). We tune the tasks on development data, and report results on the test data. The only com- ponent that varies across the experiments is order of paragraphs in the training corpus-the curricu- lum. We compare the following experimental se- tups:</p><p>• Shuffled baselines: the curriculum is defined by random shuffling the training data. We shuffled the data 10 times, and trained 10 word embed- dings models, each model was then evaluated on downstream tasks. Following <ref type="bibr">Bengio et al. (2009)</ref>, we report test results for the system that is closest to the median in dev scores. To evalu- ate variability and a range of scores that can be obtained from shuffling the data, we also report test results for systems that obtained the highest dev scores.</p><p>• Sorted baselines: the curriculum is defined by sorting the training data by sentence length in increasing/decreasing order, similarly to ( <ref type="bibr" target="#b38">Spitkovsky et al., 2010</ref>).</p><p>• Coherent baselines: the curriculum is defined by just concatenating Wikipedia articles. The goal of this experiment is to evaluate the im- portance of semantic coherence in training data.</p><p>Our intuition is that a coherent curriculum can improve models, since words with simi- lar meanings and similar contexts are grouped when presented to the learner.</p><p>• Optimized curriculum models: the curriculum is optimized using the BayesOpt. We evaluate and compare models optimized using features from one of the three feature groups ( §2.2). As in the shuffled baselines, we fix the number of trials (here, BayesOpt iterations) to 10, and we report test results of systems that obtained best dev scores.</p><p>Results. Experimental results are listed in ta- ble 2. Most systems trained with curriculum sub- stantially outperform the strongest of all baselines. These results are encouraging, given that all word embedding models were trained on the same set of examples, only in different order, and display the indirect influence of the data curriculum on downstream tasks. These results support our as- sumption that curriculum matters. Albeit not as pronounced as with optimized curriculum, sorting paragraphs by length can also lead to substantial improvements over random baselines, but there is no clear recipe on whether the models prefer cur- ricula sorted in an increasing or decreasing order. These results also support the advantage of a task- specific optimization framework over a general, intuition-guided recipe. An interesting result, also, that shuffling is not essential: systems trained on coherent data are on par (or better) than the shuf- fled systems. <ref type="bibr">4</ref> In the next section, we analyze these results qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>What are task-specific curriculum prefer- ences? We manually inspect learned features and curriculum-sorted corpora, and find that best systems are obtained when their embeddings are <ref type="bibr">4</ref> Note that in the shuffled NER baselines, best dev re- sults yield lower performance on the test data. This implies that in the standard development/test splits the development and test sets are not fully compatible or not large enough. We also observe this problem in the curriculum-optimized Parse-prototypicality and Senti-diversity systems. The dev scores for the Parse systems are 76.99, 76.47, 76.47 for di- versity, prototypicality, and simplicity, respectively, but the prototypicality-sorted parser performs poorly on test data. Similarly in the sentiment analysis task, the dev scores are 69.15, 69.04, 69.49 for diversity, prototypicality, and sim- plicity feature groups. Senti-diversity scores, however, are lower on the test data, although the dev results are better than in Senti-simplicity. This limitation of the standard dev/test splits is beyond the scope of this paper. learned from curricula appropriate to the down- stream tasks. We discuss below several examples.</p><p>POS and Parse systems converge to the same set of weights, when trained on features that provide various measures of syntactic simplicity. The fea- tures with highest coefficients (and thus the most important features in sorting) are #N P s, Parse tree depth, #V P s, and #P P s (in this order). The sign in the #N P s feature weight, however, is the opposite from the other three feature weights (i.e., sorted in different order). #N P s is sorted in the increasing order of the number of noun phrases in a paragraph, and the other features are sorted in the decreasing order. Since Wikipedia corpus con- tains a lot of partial phrases (titles and headings), such curriculum promotes more complex, full sen- tences, and demotes partial sentences.</p><p>Best Senti system is sorted by prototypicality features. Most important features (with the highest coefficients) are Concreteness, Relative frequency in a supersense, and the Number of syllables. First two are sorted in decreasing order (i.e. paragraphs are sorted from more to less concrete, and from more to less prototypical words), and the Num- ber of syllables is sorted in increasing order (this also promotes simpler, shorter words which are more prototypical). We hypothesize that this sor- ing reflects the type of data that Sentiment analysis task is trained on: it is trained on movie reviews, that are usually written in a simple, colloquial lan- guage.</p><p>Unlike POS, Parse, and Senti systems, all NER systems prefer curricula in which texts are sorted from short to long paragraphs. The most impor- tant features in the best (simplicity-sorted) system are #P P s and Verb-token ratio, both sorted from less to more occurrences of prepositional and verb phrases. Interestingly, most of the top lines in the NER system curricula contain named entities, al- though none of our features mark named entities explicitly. We show top lines in the simplicity- optimized system in <ref type="figure">figure 2</ref>.</p><p>Finally, in all systems sorted by prototypical- ity, the last line is indeed not a prototypical word Donaudampfschiffahrtselektrizitätenhaupt- betriebswerkbauunterbeamtengesellschaft, which is an actual word in German, frequently used as an example of compounding in synthetic languages, but rarely (or never?) used by German speakers.</p><p>Weighting examples according to curriculum. Another way to integrate curriculum in word em-    <ref type="figure">Figure 2</ref>: Most of the top lines in best-scoring NER system contain named entities, although our features do not annotate named entities explicitly.</p><p>bedding training is to weight training examples according to curriculum during word represen- tation training. We modify the cbow objective T t=1 log p(w t |w t−c ..w t+c ) as follows: 5</p><formula xml:id="formula_4">T t=1 ( 1 1 + e −weight(wt) + λ) log p(w t |w t−c ..w t+c )</formula><p>Here, weight(w t ) denotes the score attributed to the token w t , which is the z-normalized score of the paragraph; λ=0.5 is determined empirically. log p(w t )|w t−c ..w t+c ) computes the probability of predicting word w t , using the context of c words to the left and right of w t . Notice that this quan- tity is no longer a proper probability, as we are not normalizing over the weights weight(w t ) over all tokens. However, the optimization in word2vec is performed using stochastic gradient descent, op- timizing for a single token at each iteration. This yields a normalizer of 1 for each iteration, yielding the same gradient as the original cbow model. We retrain our best curriculum-sorted systems with the modified objective, also controlling for curriculum. The results are shown in table 3. We find that the benefit of integrating curricu- lum in training objective of word representations is not evident across tasks: Senti and NER systems trained on vectors with the modified objective sub- stantially outperform best results in table 2; POS and Parse perform better than the baselines but worse than the systems with the original objective.  <ref type="table">Table 3</ref>: Evaluation of the impact of curriculum integrated in the cbow objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Senti NER POS Parse</head><p>Are we learning task-specific curricula? One way to assess whether we learn meaningful task- specific curriculum preferences is to compare cur- ricula learned by one downstream task across dif- ferent feature groups. If learned curricula are sim- ilar in, say, NER system, despite being optimized once using diversity features and once using proto- typicality features-two disjoint feature sets-we can infer that the NER task prefers word embed- dings learned from examples presented in a cer- tain order, regardless of specific optimization fea- tures. For each downstream task, we thus measure Spearman's rank correlation between the curricula optimized using diversity (D), or prototypicality (P), or simplicity (S) feature sets. Prior to measur- ing correlations, we remove duplicate lines from the training corpora <ref type="table" target="#tab_6">. Correlation results across  tasks and across feature sets are shown in table 4</ref>.</p><p>The general pattern of results is that if two sys- tems score higher than baselines, training sen- tences of their feature embeddings have similar curricula (i.e., the Spearman's ρ is positive), and if two systems disagree (one is above and one is be- low the baseline), then their curricula also disagree (i.e., the Spearman's ρ is negative or close to zero). NER systems all outperform the baselines and their curricula have high correlations. Moreover, NER sorted by diversity and simplicity have bet- ter scores than NER sorted by prototypicality, and in line with these results ρ(S,D) N ER &gt; ρ(P,S) N ER and ρ(S,D) N ER &gt; ρ(D,P) N ER . Similar pattern of results is in POS correlations. In Parse systems, also, diversity and simplicity features yielded best parsing results, and ρ(S,D) P arse has high positive correlation. The prototypicality-optimized parser performed poorly, and its correlations with better systems are negative. The best parser was trained using the diversity-optimized curriculum, and thus ρ(D,P) P arse is the lowest. Senti results follow sim- ilar pattern of curricula correlations.  Curriculum learning vs. data selection. We compare the task of curriculum learning to the task of data selection (reducing the set of training in- stances to more important or cleaner examples). We reduce the training data to the subset of 10% of tokens, and train downstream tasks on the reduced training sets. We compare system performance trained using the top 10% of tokens in the best curriculum-sorted systems (Senti-prototypicality, NER-implicity, POS-simplicity, Parse-diversity) to the systems trained using the top 10% of tokens in a corpus with randomly shuffled paragraphs. <ref type="bibr">6</ref> The results are listed in table 5. The curriculum-based systems are better in POS <ref type="bibr">6</ref> Top n% tokens are used rather than top n% paragraphs because in all tasks except NER curriculum-sorted corpora begin with longer paragraphs. Thus, with top n% paragraphs our systems would have an advantage over random systems due to larger vocabulary sizes and not necessarily due to a better subset of data.  <ref type="table">Table 5</ref>: Data selection results. and in Parse systems, mainly because these tasks prefer vectors trained on curricula that promote well-formed sentences (as discussed above). Con- versely, NER prefers vectors trained on corpora that begin with named entities, so most of the to- kens in the reduced training data are constituents in short noun phrases. These results suggest that the tasks of data selection and curriculum learning are different. Curriculum is about strong initializa- tion of the models and time-course learning, which is not necessarily sufficient for data reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Senti NER POS Parse</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Senti NER POS Parse</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Two prior studies on curriculum learning in NLP are discussed in the paper ( <ref type="bibr">Bengio et al., 2009;</ref><ref type="bibr" target="#b38">Spitkovsky et al., 2010)</ref>. Curriculum learning and related research on self-paced learning has been explored more deeply in computer vision ( <ref type="bibr">Bengio et al., 2009;</ref><ref type="bibr" target="#b15">Kumar et al., 2010;</ref><ref type="bibr" target="#b20">Lee and Grauman, 2011)</ref> and in multimedia analysis ( <ref type="bibr" target="#b12">Jiang et al., 2015)</ref>. Bayesian optimization has also received little attention in NLP. GPs were used in the task of machine translation quality estimation <ref type="bibr" target="#b1">(Cohn and Specia, 2013)</ref> and in temporal analysis of so- cial media texts <ref type="bibr" target="#b26">(Preotiuc-Pietro and Cohn, 2013</ref>); TPEs were used by <ref type="bibr" target="#b47">Yogatama et al. (2015)</ref> for optimizing choices of feature representations-n- gram size, regularization choice, etc.-in super- vised classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We used Bayesian optimization to optimize curric- ula for training dense distributed word representa- tions, which, in turn, were used as the sole features in NLP tasks. Our experiments confirmed that bet- ter curricula yield stronger models. We also con- ducted an extensive analysis, which sheds better light on understanding of text properties that are beneficial for model initialization. The proposed novel technique for finding an optimal curriculum is general, and can be used with other datasets and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>137</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Curriculum optimization framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Shuffled</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Evaluation of the impact of the curriculum of word embeddings on the downstream tasks. 

Trimingham " Golf " ball . 
Adélie penguin 
" Atriplex " leaf UNK UNK 
Hồng Lĩnh mountain 
Anneli Jäätteenmäki UNK cabinet 
Gävle goat 
Early telescope observations . 
Scioptric ball 
Matryoshka doll 
Luxembourgian passport 
Australian Cashmere goat 
Plumbeous water redstart 
Dagebüll lighthouse 
Vecom FollowUs . tv 
Syracuse Junction railroad . 
San Clemente Island goat 
Tychonoff plank 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Curricula correlations across feature 
groups. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>random 63.97 82.35 96.22 69.11 curriculum 64.47 76.96 96.55 72.93</figDesc><table></table></figure>

			<note place="foot" n="1"> Intuitively, this feature promotes paragraphs that contain semantically similar high-probability words.</note>

			<note place="foot" n="2"> http://http://people.sutd.edu.sg/ ~yue_zhang/doc</note>

			<note place="foot" n="3"> To evaluate the impact of curriculum learning, we enforced sequential processing of data organized in a predefined order of training examples. To control for sequential processing, word embedding were learned by running the cbow using a single thread for one iteration.</note>

			<note place="foot" n="5"> The modified word2vec tool is located at https:// github.com/wlin12/wang2vec .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Science Foundation through award IIS-1526745. We are grateful to Nathan Schneider, Guillaume Lample, Waleed Ammar, Austin Matthews, and the anony-mous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modelling annotator bias with multi-task Gaussian processes: An application to machine translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning and development in neural networks: The importance of starting small</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="99" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">WordNet: an electronic lexical database</title>
		<editor>Christiane Fellbaum</editor>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A systematic exploration of diversity in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1100" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combining lexical and grammatical features to improve readability measures for first and second language texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevyn</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Entropy search for information-efficient global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1809" to="1837" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Portfolio allocation for Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Matthew D Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. UAI</title>
		<meeting>UAI</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2078" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A taxonomy of global optimization methods based on response surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald R Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="345" to="383" />
		</imprint>
	</monogr>
	<note>Journal of global optimization</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The development of memory in children</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kail</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Freeman and Company</publisher>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Age-of-acquisition ratings for 30,000 english words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Stadthagen-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="978" to="990" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kushner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="106" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning the easy things first: Self-paced visual category discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1721" to="1728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Measuring biological diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anne E Magurran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On Bayesian methods for seeking the extremum</title>
	</analytic>
	<monogr>
		<title level="m">Jonas Močkus, Vytautas Tiesis, and Antanas Žilinskas</title>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting readability: A unified framework for predicting text quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A temporal model of text periodicities using gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preotiuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Pietro</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="977" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Diversity and dissimilarity coefficients: a unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Radhakrishna</forename><surname>Rao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="24" to="43" />
		</imprint>
	</monogr>
	<note>Theoretical population biology</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gaussian Processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Edward Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Principles of categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleanor</forename><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognition and categorization</title>
		<editor>Eleanor Rosch and Barbara B. Lloyd</editor>
		<imprint>
			<date type="published" when="1978" />
			<biblScope unit="page" from="28" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael L Rosenzweig</surname></persName>
		</author>
		<title level="m">Species diversity in space and time</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reading level assessment using support vector machines and statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Schwarm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Semantic word clusters using signed normalized graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Sedoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Gallier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Foster</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.05403</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Taking the human out of the loop: A review of Bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bobak</forename><surname>Shahriari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="148" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Measurement of diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simpson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The behavior of organisms: an experimental analysis. An Experimental Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Burrhus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skinner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Practical Bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From baby steps to leapfrog: How less is more in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Jurafsky</forename><surname>Alshawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gaussian process optimization in the bandit setting: No regret and experimental design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A general framework for analysing diversity in science, technology and society</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Stirling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Society Interface</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="707" to="719" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An analysis of diversity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>E Ke Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponnuthurai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="247" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William R Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Metaphor detection with cross-lingual model transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On improving the accuracy of readability classification using insights from second language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sowmya</forename><surname>Vajjala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BEA</title>
		<meeting>BEA</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="163" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">MRC psycholinguistic database: Machine-usable dictionary, version 2.00. Behavior Research Methods, Instruments, &amp; Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bayesian optimization of text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2100" to="2105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
