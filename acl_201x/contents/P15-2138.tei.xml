<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised extractive summarization via coverage maximization with syntactic and semantic concepts</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised extractive summarization via coverage maximization with syntactic and semantic concepts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="840" to="844"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Coverage maximization with bigram concepts is a state-of-the-art approach to un-supervised extractive summarization. It has been argued that such concepts are adequate and, in contrast to more linguistic concepts such as named entities or syntactic dependencies, more robust, since they do not rely on automatic processing. In this paper, we show that while this seems to be the case for a commonly used newswire dataset, use of syntactic and semantic concepts leads to significant improvements in performance in other domains .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>State-of-the-art approaches to extractive summa- rization are based on the notion of coverage max- imization <ref type="bibr" target="#b0">(Berg-Kirkpatrick et al., 2011</ref>). The assumption is that a good summary is a selec- tion of sentences from the document that contains as many of the important concepts as possible. The importance of concepts is implemented by as- signing weights w i to each concept i with binary variable c i , yielding the following coverage maxi- mization objective, subject to the appropriate con- straints:</p><formula xml:id="formula_0">N i w i c i<label>(1)</label></formula><p>In proposing bigrams as concepts for their system, <ref type="bibr" target="#b4">Gillick and Favre (2009)</ref> explain that:</p><p>[c]oncepts could be words, named enti- ties, syntactic subtrees or semantic re- lations, for example. While deeper se- mantics make more appealing concepts, their extraction and weighting are much more error-prone. Any error in concept extraction can result in a biased objec- tive function, leading to poor sentence selection. ( <ref type="bibr" target="#b4">Gillick and Favre, 2009)</ref> Several authors, e.g., <ref type="bibr">Woodsend and Lapata (2012)</ref>, and <ref type="bibr" target="#b7">Li et al. (2013)</ref>, have followed <ref type="bibr" target="#b4">Gillick and Favre (2009)</ref> in assuming that bigrams would lead to better practical performance than more syntactic or semantic concepts, even though bi- grams serve as only an approximation of these.</p><p>In this paper, we revisit this assumption and evaluate the maximum coverage objective for ex- tractive text summarization with syntactic and se- mantic concepts. Specifically, we replace bigram concepts with new ones based on syntactic depen- dencies, semantic frames, as well as named enti- ties. We show that using such concepts can lead to significant improvements in text summariza- tion performance outside of the newswire domain. We evaluate coverage maximization incorporating syntactic and semantic concepts across three dif- ferent domains: newswire, legal judgments, and Wikipedia articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Concept coverage maximization for extractive summarization</head><p>In extractive summarization, the unsupervised ver- sion of the task is sometimes set up as that of find- ing a subset of sentences in a document, within some relatively small budget, that covers as many of the important concepts in the document as pos- sible. In the maximum coverage objective, con- cepts are considered as independent of each other. Concepts are weighted by the number of times they appear in a document. Moreover, due the NP-hardness of coverage maximization, for an ex- act solution to the concept coverage optimization problem, we resort to fast solvers for integer linear programming, under some appropriate constraints.</p><p>Bigrams. <ref type="bibr" target="#b4">Gillick and Favre (2009)</ref> proposed to use bigrams as concepts, and to weight their con- tribution to the objective function in Equation <ref type="formula" target="#formula_0">(1)</ref> by the frequency with which they occur in the doc- ument. Some pre-processing is first carried out to these bigrams: all bigrams consisting uniquely of stop-words are removed from consideration, and each word is stemmed. They also require bigrams to occur with a minimal frequency (cf. Section 3.2).</p><p>Named entities. We consider three new types of concepts, all suggested, but subsequently rejected by <ref type="bibr" target="#b4">Gillick and Favre (2009)</ref>. The first is simply to use named entities, e.g., Court of Justice of the European Union, as concepts. This reflects the in- tuition that persons, organizations, and locations are particularly important for extractive summa- rization. We use an NER maximum entropy tag- ger 1 to augment documents with named entities.</p><p>Syntactic dependencies. The second type of concept is dependency subtrees. In particular, we extract labeled and unlabeled syntactic depen- dencies, e.g., DEPENDENCY(walks,John) or SUB- JECT(walks,John), from sentences and represent them by such syntactic concepts. We use the Stan- ford parser 2 to augment documents with syntac- tic dependencies. As was done for bigrams, each word in the dependency is stemmed. Syntactic dependency-based concepts are intuitively a closer approximation than bigrams to concepts in gen- eral.</p><p>Semantic frames. The intuition behind our use of frame semantics is that a summary should rep- resent the most central semantic frames <ref type="bibr" target="#b3">(Fillmore, 1982;</ref><ref type="bibr" target="#b2">Fillmore et al., 2003</ref>) present in the cor- responding document-indeed, we consider these frames to be actual types of concepts. We ex- tract frame names from sentences for a further type of concepts under consideration. We use SE- MAFOR 3 to augment documents with semantic frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>In order to investigate the importance of concept types across different domains, we evaluate our systems across three distinct domains, which we refer to as ECHR, TAC08, and WIKIPEDIA. ECHR consists of judgment-summary pairs scraped from the European Court of Hu-man Rights case-law website, HUDOC 4 . The document-summary pairs were split into training, development and test sets, consisting of 1018, 117, and 138 pairs, respectively. In the training set (pruning sentences of length less than 5), the aver- age document length is 13,184 words or 455 sen- tences. The average summary length is 806 words or 28 sentences. For both documents and sum- maries, the average sentence length is 29 words.</p><p>TAC08 consists of 48 queries and 2 newswire document sets for each query, each set contain- ing 10 documents. Document sets contain 235 in- put sentences on average, and the mean sentence length is 25 words. Summaries consist of 4 sen- tences or 100 words on average.</p><p>WIKIPEDIA consists of 992 Wikipedia articles (all labeled "good article" 5 ) from a comprehen- sive dump of English language Wikipedia arti- cles <ref type="bibr">6</ref> . We use the Wikipedia abstracts (the leading paragraphs before the table of contents) as sum- maries. The (document,summary) pairs were split into training, development and test sets, consist- ing of 784, 97, and 111 pairs, respectively. In the training set (pruning sentences of length less than 5), the average document length is around 8918 words or 339 sentences. The average summary length is 335 words or 13 sentences. For both documents and summaries, the average sentence length is around 26 words.</p><p>In our main experiments, we use unsupervised summarization techniques, and we only use the training summaries (and not the documents) to de- termine output summary lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baseline and systems</head><p>Our baseline is the bigram-based extraction sum- marization system of <ref type="bibr" target="#b4">Gillick and Favre (2009)</ref>, icsisumm 7 . Their system was originally in- tended for multi-document update summarization, and summaries are extracted from document sen- tences that share more than k content words with some query. We follow this approach for the TAC08 data. For ECHR and WIKIPEDIA, the task is single document summarization, and the now irrelevant topic-document intersection pre- processing step is eliminated.</p><p>The original system uses the GNU linear pro- gramming kit 8 with a time limit of 100 sec- onds. For all experiments presented in this pa- per, we double this time limit; we experimented with longer time limits on the development set for the ECHR data, without any performance im- provements. Once the summarizer reaches the time limit, a summary is output based on the cur- rent feasible solution, whether the solution is op- timal or not. Moreover, the current icsisumm (v1) distribution prunes sentences shorter than 10 words. We note that we also tried replacing glpk by gurobi 9 , for which no time limit was neces- sary, but found poorer results on the development set of the ECHR data.</p><p>The original system takes several important in- put parameters.</p><p>1. Summary length, for TAC08, is specified by the TAC 2008 conference guidelines as 100 words. For WIKIPEDIA and ECHR, we have access to training sets which gave an average summary length of around 335 and 805 words respectively, which we take as the standard output summary length.</p><p>2. Concept count cut-off is the minimum fre- quency of concepts from the document (set) that qualifies them for consideration in cov- erage maximization. For bigrams of the orig- inal system on TAC08, there are two types of document sets: 'A' and 'B'. For 'A' type documents, <ref type="bibr" target="#b4">Gillick and Favre (2009)</ref> set this threshold to 3 and for 'B' type documents, they set this to 4. For WIKIPEDIA and ECHR, we take the bigram threshold to be 4. In our extension of the system to other concepts, we do not use any threshold.</p><p>3. First concept weighting: in multi-document summarization, there is the possibility for repeated sentences. Concepts from first- encountered sentences may be weighted higher: these concept counts from first- encountered sentences are doubled for 'B' documents and remain unchanged for 'A' documents in the original system on TAC08. For other concepts, we do not alter frequen- cies in this manner, which is justified by the task change to single-document summariza- tion.</p><p>4. Query-sentence intersection threshold, is set to 1 for 'A' documents and 0 to 'B' documents in the original system on TAC08. This threshold is only for the update summa- rization task and therefore does not concern ECHR and WIKIPEDIA. In addition to our baseline, we consider five single-concept systems using (a) named entities, (b) labeled dependencies, (c) unlabeled dependen- cies, (d) semantic frame names, and (e) seman- tic frame dependencies, as well as the five sys- tems combining each of these new concept types with bigrams. For the combination of these new concepts with bigrams, we extend the objective function to maximise in, Equation (1), into two sums-one for bigram concepts and the other for the new concept type-with their relative impor- tance controlled by a parameter Î±. N 1 and N 2 are the number of bigram and other concept types oc- curring with the permitted threshold frequency in the document, relatively. Given that we are carry- ing out unsupervised summarization, rather than tune Î±, we set Î± = 0.5, so the concepts are con- sidered in their totality (i.e., N 1 + N 2 concepts to- gether) with no explicit favouring of one over the other that does not naturally fall out of concept fre- quency.</p><p>(1âÎ±)</p><formula xml:id="formula_1">N 1 i w i bigram i +Î± N 2 j w j new concept j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We evaluate output summaries using ROUGE-1, ROUGE-2, and ROUGE-SU4 <ref type="bibr" target="#b8">(Lin, 2004</ref>), with no stemming and retaining all stopwords. These measures have been shown to correlate best with human judgments in general, but among the au- tomatic measures, ROUGE-1 and ROUGE-2 also correlate best with the Pyramid (Nenkova and <ref type="bibr">Passonneau, 2004;</ref><ref type="bibr">Nenkova et al., 2007)</ref> and Re- sponsiveness manual metrics ( <ref type="bibr">Louis and Nenkova, 2009)</ref>. Moreover, ROUGE-1 has been shown to best reflect human-automatic summary compar- isons ( <ref type="bibr">Owczarzak et al., 2012</ref>).</p><p>For single concept systems, the results are shown in <ref type="table" target="#tab_1">Table 1</ref>, and concept combination sys- tem results are given in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>We first note that our runs of the current dis- tribution of icsisumm yield significantly worse ROUGE-2 results than reported in <ref type="bibr" target="#b4">(Gillick and Favre, 2009</ref>) (see <ref type="table" target="#tab_1">Table 1</ref>, BIGRAMS): 0.081 com- pared to 0.110 respectively.</p><p>On the TAC08 data, we observe no improve- ments over the baseline BIGRAM system for any ROUGE metric here. Hence, <ref type="bibr" target="#b4">Gillick and Favre (2009)</ref> were right in their assumption that syntac- tic and semantic concepts would not lead to perfor- mance improvements, when restricting ourselves to this dataset. However, when we change domain to the legal judgments or Wikipedia articles, using syntactic and semantic concepts leads to signifi- cant gains across all the ROUGE metrics.</p><p>For ECHR, replacing bigrams by frame names (FRAME) results in an increase of +0.1 in ROUGE-1, +0.031 in ROUGE-2 and +0.046 in ROUGE-SU4. We note that FrameNet 1.5 covers the legal domain quite well, which may explain why these concepts are particularly useful for the ECHR dataset. However, labeled (LDEP) and unla- beled (UDEP) dependencies also significantly out- perform the baseline.</p><p>For WIKIPEDIA, replacing bigrams by labeled or unlabeled syntactic dependencies results in sig- nificant improvements: an increase of +0.088 for ROUGE-1, +0.015 for ROUGE-2, and +0.03 for ROUGE-SU4. Interestingly, the NER sys- tem also yields significantly better performance over the baseline, which may reflect the nature of Wikipedia articles, often being about historical figures, famous places, organizations, etc.</p><p>We observe in <ref type="table" target="#tab_2">Table 2</ref>, that for concept combi- nation systems as well, ROUGE scores on TAC08 do not indicate any improvement in performance. However, best ROUGE-1 scores are produced for both ECHR and WIKIPEDIA data with sys- tems that incorporate semantic frame names. For WIKIPEDIA, best ROUGE-2 and ROUGE-SU4 scores incorporate named-entity information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Most researchers have used bigrams as concepts in coverage maximization-based approaches to unsu- pervised extractive summarization. <ref type="bibr" target="#b1">Filatova and Hatzivassiloglou (2004)</ref>, however,use relations be- tween named entities as concepts in extractive summarization. They use slightly different extrac- tion algorithms, but their work is similar in spirit to ours. <ref type="bibr">Nishikawa et al. (2010)</ref>, also, use opin- ions -tuples of targets, aspects, and polarity - as concepts in opinion summarization. In early work on summarization, <ref type="bibr">Silber and McCoy (2000)</ref> used WordNet synsets as concepts. <ref type="bibr" target="#b6">Kitajima and Kobayashi (2011)</ref> replace words by syntactic de- pendencies in the Maximal Marginal Relevance  Multidocument measure first proposed by <ref type="bibr" target="#b5">Goldstein et al. (2000)</ref> for evaluating the importance of sentences in query-based extractive summa- rization, yielding improvements for their Japanese newswire dataset.</p><formula xml:id="formula_2">ECHR R-1 R-2 R-SU4 concept (95% conf.) (95% conf.) (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper challenges the assumption that bigrams make better concepts for unsupervised extractive summarization than syntactic and semantic con- cepts relying on automatic processing. We show that using concepts relying on syntactic dependen- cies or semantic frames instead of bigrams leads to significant performance improvements of cover- age maximization summarization across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ECHR</head><p>R-1 R-2 R-SU4 concept (95% conf.) (95% conf.) (  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Single concept results on ECHR, TAC08, 
and WIKIPEDIA. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results for systems combining bi-
grams with new concepts, on ECHR, TAC08 and 

WIKIPEDIA. </table></figure>

			<note place="foot" n="1"> http://www.nltk.org/ 2 http://nlp.stanford.edu/software/ lex-parser.shtml 3 http://www.ark.cs.cmu.edu/SEMAFOR/</note>

			<note place="foot" n="4"> http://hudoc.echr.coe.int/ 5 http://en.wikipedia.org/wiki/ Wikipedia:Good_articles 6 https://dumps.wikimedia.org/ enwiki/latest/enwiki-latest-pages-articles-multistream.xml.bz2 7 https://code.google.com/p/icsisumm/</note>

			<note place="foot" n="8"> http://www.gnu.org/software/glpk/ 9 http://www.gurobi.com/</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACL</title>
		<meeting>of ACL<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Event-based extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Filatova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Workshop on Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Background to framenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><forename type="middle">R L</forename><surname>Petruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<title level="m">Linguistics in the Morning Calm, chapter Frame Semantics</title>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="page" from="111" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A scalable global model for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ILP</title>
		<meeting>of ILP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-document summarization by sentence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kantrowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of the ANLP/NAACL Workshop on Automatic Summarization</title>
		<meeting>of the ANLP/NAACL Workshop on Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A latent topic extracting method based on events in a document and its application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risa</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACL-HLT Student Session</title>
		<meeting>of ACL-HLT Student Session<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using supervised bigram-based ilp for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ACL</title>
		<meeting>of ACL<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of WAS</title>
		<meeting>of WAS<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
