<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting Depression for Japanese Blog Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misato</forename><surname>Hiraga</surname></persName>
							<email>mhiraga@indiana.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Indiana University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting Depression for Japanese Blog Text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of ACL 2017, Student Research Workshop</title>
						<meeting>ACL 2017, Student Research Workshop <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="107" to="113"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-3018</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This study aims to predict clinical depression , a prevalent mental disorder, from blog posts written in Japanese by using machine learning approaches. The study focuses on how data quality and various types of linguistic features (characters, tokens , and lemmas) affect prediction outcome. Depression prediction achieved 95.5% accuracy using selected lemmas as features.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The World Health Organization (WHO) recog- nizes that depression is a leading cause of ill health and disability <ref type="bibr">(2017)</ref>. In Japan, it is also the most frequent reason for sick leave from work <ref type="bibr" target="#b3">(Kitanaka, 2012</ref>). However, many people with de- pression may not be aware that their mood change and fatigue are due to depression. In order to offer help for those who need it, we first need to iden- tify them. This study examines whether linguistic features in written texts can help predict whether the author is depressed by using a supervised ma- chine learning approach. Specifically, we examine the effectiveness of morphological (character n- grams), syntactic (token n-grams), and (syntactic- )semantic (lemmas of selected POS categories) features. In addition, we remove the topic bias so that the methods can be used to predict depression in people who do not know they are depressed and thus do not write about depression. The results show that lemmas from verb and adverb categories improve performance in classifying authors. Ad- ditionally, the selected words include words not typically thought of as related to depression. Thus, the study suggests that feature engineering should not be constrained by our notion of what would be related to certain mental conditions or personali- ties as changes in people's language use may be very subtle.</p><p>Section 2 discusses previous work on author profiling and depression detection. In Section 3, we describe data acquisition, topic modeling, and classifications with different features. Section 4 summarizes the results, and Section 5 discusses the results. Finally, Section 6 concludes the paper with a summary and an outlook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Language and social media activities have been utilized for author profiling including personality prediction ( <ref type="bibr" target="#b0">Bachrach et al., 2012;</ref><ref type="bibr" target="#b1">Golbeck et al., 2011</ref>). Although depression is not a personality, the studies in personality prediction can be ex- tended to predicting depression since one's mental state is often reflected in his or her language and social activities.</p><p>Character n-grams are reported to do well in gender prediction in English blog text <ref type="bibr" target="#b10">(Sarawgi et al., 2011</ref>) and personality prediction in Dutch ( <ref type="bibr" target="#b7">Noecker et al., 2013)</ref>. However, the PAN 2014 challenge ( <ref type="bibr" target="#b9">Rangel et al., 2014</ref>) reports that charac- ter n-grams are not useful in author profiling (age and gender) in English and Spanish social media, Twitter, blogs, and hotel reviews. Japanese is dif- ferent from Germanic or Romance languages in terms of how much information can be encoded in one character. Japanese basic characters, hira- gana and katakana, represent one mora, which is a sound unit similar to a syllable, and kanji (Chi- nese characters) can encode more than one mora. Moreover, there are many characters: 50 each for hiragana and katakata, and approximately 2000 kanji for everyday writing. The current study thus examines the effectiveness of character n-grams as features in Japanese text classification. <ref type="bibr" target="#b5">Matsumoto et al. (2012)</ref> built a classifier to Class # of Author Word/Author # of Document Word/ <ref type="table" target="#tab_0">Doc   All  Depressed  51  3666  842  222  Non-Depressed  60  3692  1020  220   Topic  Depressed  49  2630  739  192  Non-Depressed  59  2890  904  191   Table 1</ref>: Number of authors and documents and average word count before and after topic modeling predict whether a blog text is written by a de- pressed author or a non-depressed author, using approximately 1800 blog texts written by 30 de- pressed authors and 30 non-depressed authors.</p><p>They experimented with bag-of-word (BOW) fea- tures and also examined whether words that are associated with emotions from the JAppraisal dic- tionary (Sato, 2011) were useful in classification. Their classifier using Naive Bayes and BOW per- formed the best, resulting in average of 88.1% ac- curacy with 10-fold cross validation. One would question whether their prediction with BOW was based on topic, however. By browsing blogs writ- ten by depressed authors, we find many blogs are about depression. Therefore, texts written by de- pressed authors are biased towards the topic of de- pression. Unless Matsumoto et al. (2012) con- trolled topic-bias, their system may be heavily af- fected by topic. The goal of this study is to pre- dict depression from topic-general texts. Thus, the current study controls topic by performing topic modeling (Section 3.2) before classification and examines its effect. As their model using emotion- related words did worse (&lt;70%) than BOW, the current study examines various features that are not obviously related to depression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>Blog texts are collected from a blog ranking site 1 and by searching on blog provider websites (Ya- hoo Japan, Livedoor, Hatena, FC2, Seesaa, Nifty, Muragon, and Ameblo) 2 . The blog ranking site ranks registered blogs by the number of votes from readers who visit the blogs. The site is di- vided into categories which include "depression". In this category, most authors state that they are diagnosed with or have been suffering from de- pression. Blogs are chosen to be included in the study if the authors report that they themselves are suffering from depression and have written their blogs for at least three months. Some authors write only a little in a month, but most write at least 10 entries within three months. Thus, for each au- thor, three months' worth of blog posts are col- lected. This "depressed" group consists of 51 au- thors. Three months' worth of texts for 60 "non- depressed" authors are also collected. They are randomly chosen among those who have a sim- ilar profile as the depressed authors. For exam- ple, if a depressed author is a male in his 40's, a blog author who had the similar profile is chosen. Moreover, non-depressed authors with the same interest as the depressed authors are collected. Their interests include pets, food, and sports. 3 As many blogs written by depressed authors are re- trieved from a blog ranking site, they include fixed phrases, such as "Please vote", to encourage their readers to vote for their blogs. These fixed phrases that appear repeatedly are removed as they do not appear in blogs written by the non-depressed au- thors. Moreover, a document whose file size is smaller than 100 bytes is removed as it contained very few words. The average number of words and characters per author and per document are in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Removing Blog entries on depression</head><p>Before performing classification, topic model- ing is performed to divide documents into topic classes. The purpose of topic modeling is to re- move blog entries that are biased towards the topic of depression so that classification will not clas- sify documents based on topics (see Section 2). MALLET <ref type="bibr" target="#b6">(McCallum, 2002</ref>) is used for topic- modeling. Dividing documents into 5 topics with a hyperparameter optimization value of 50 is found to work the best by manually testing different val- ues. Two of the five topics included topic keys that are related to depression, and thus the doc- uments in those two topics are excluded in the study. The numbers are summarized in <ref type="table">Table 1</ref>. <ref type="table" target="#tab_0">Train  Depressed  39  2404  590  193  Non-Depressed  47  2466  722  195   Test  Depressed  10  3512  149  186  Non-Depressed  12  4233  182  177   Table 2</ref>: Number of authors and documents and average word count in training and test sets</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of Author Word/Author # of Document Word/Doc</head><p>The number of authors is reduced to 49 depressed and 59 non-depressed authors. The reason for the reduction in non-depressed authors seemed to be because documents that are related to companies or work are classified together with the topic of depression. For example, one topic class which include the topic key "depression" also includes keys, such as "company", and "investment".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Classification</head><p>We perform classification of texts into whether the author is depressed or not. Our system learns from texts written by a group of both depressed and non-depressed authors, and classifies unseen texts written by a different group of depressed and non- depressed authors. We perform classification of texts per author (henceforth, author-level classifi- cation) and per document (document-level classi- fication). In author-level classification, one docu- ment contains all the blog entries written by one author. In document-level classification, one doc- ument contains one blog entry. For both experi- ments, data are divided into a training and a testing set. In document-level classification, documents written by the authors in the training set do not ap- pear in the test set, and thus none of the authors have their documents in both training and testing sets. The number of documents are summarized in <ref type="table">Table 2</ref>. Classification is performed using Multinomial NaiveBayes (NB), Linear Support Vector Ma- chines (SVM), and Logistic Regression (LR) in scikit-learn <ref type="bibr" target="#b8">(Pedregosa et al., 2011</ref>). Multinomi- nal NB is used with the default alpha value (al- pha=1.0), and SVM and LR classifiers are both used with the default regularization value (C=1). Univariate and model-based feature selection is performed for each experiment. In univariate fea- ture selection, features that are above the 75 per- centile are chosen. For model-based feature selec- tion, SVM and LR, both with penalty of L1 and C=1 are used to select features with non-zero co- efficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Character n-grams</head><p>The first feature set is character n-grams. Char- acter n-grams worked well in other languages as discussed in Section 2, and they provide a generic cross-linguistic way of getting at morphological units. To test the effects of the writing system, two types of character n-grams, Japanese char- acter n-grams and Romanized Japanese n-grams, are used. Japanese has three types symbols (hi- ragana, katakana, and kanji (Chinese characters)), and thus one word could be written in several ways. For example, the word kawaii "cute" could be written all in hiragana or katakana, or combi- nation of hiragana and katakana, or combination of kanji and hiragana. Without Romanization, all of them would be treated differently despite their shared meaning. However, Romanization can also collapse words that are pronounced the same but written differently with different meanings. hashi can mean "chopsticks", "bridge", and "edge" de- pending on which Chinese characters are used and in what context they are used. To experiment with Romanized Japanese character n-grams, Japanese characters are converted to Romaji (Roman alpha- bets) using jConverter <ref type="bibr">4</ref> . The value of n ranges be- tween 1 and 10, and only features of one n value are used as features in one experiment, and fea- tures of different values of n are not combined to- gether, to avoid the number of features from be- coming too large. For instance, an experiment with trigrams only uses trigram features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Token n-grams</head><p>The next feature set is token n-grams with vary- ing values of n (1-10). Tokens retain inflections and conjugation, so token n-grams represent syn- tactic properties of written text. The Japanese text is tokenized with Cabocha ( <ref type="bibr" target="#b4">Kudo and Matsumoto, 2002</ref>), as Japanese does not use a space to indicate word boundaries. <ref type="table">Document  Features  NB  SVM  LR  NB  SVM  LR  CharUni</ref> 68.2 54.5 54.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="65.3">63.8 67.2 CharUni+FS 63.6 (U) 68.2 (U) 68.2 (U) 65.6 (U) 65.9 (L) 67.2 (L) CharN(n) 81.8 (3) 59.1 (5) 54.5 (3) 70.0 (3) 69.7 (10) 70.3 (7) CharN+FS 86.4 (U) 59.1 (U) 72.7 (U) 75.5 (U) 73.4 (U) 75.2 (U) RomUni</head><p>56.5 52.5 69.6 50.2 56.8 56.8 RomUni+FS 65.2 (L) 60.9 (S) 69.6 (L) 52.6 (L) 57.1 (L) 57.1 (S) RomN(n) 82.6 (5) 60.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(3) 78.3 (2) 71.6 (5) 68.3 (8) 70.4 (8) RomN+FS</head><p>82.6 (U) 60.9 (L,U) 65.2 (L,S) 71.9 (U) 68.9 (U) 70.7 (U)  <ref type="table">Table 4</ref>: Accuracies (%) for Token and Token n-grams. The value in parentheses indicates the value of n for n-grams or model of feature selection (L:Logistic Regression, S:SVM, U:Univariate)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Lemmas and selected lemmas</head><p>The next feature set is lemmas. As lemmatiza- tion suppresses inflections, lemmas represent use of words regardless of their form in a sentence. Given this semantic nature and the results of token n-grams (Section 4.2), we only examine lemma unigrams. In addition, certain types of words may convey more relevant information than others, and thus in order to find whether certain categories of words are more informative in classification, POS categories are used to extract groups of words. First, words are POS-tagged with Cabocha ( <ref type="bibr" target="#b4">Kudo and Matsumoto, 2002</ref>), and words from each POS category (e.g. Noun) are used as features. Then, all possible combinations of 13 POS categories (Noun, Verb, Auxiliary, Adverb, Adjective, Par- ticle, Symbol, Filler, Rentaishi 5 , Conjunction, Af- fix, Interjection, Other) are created and a feature set containing words from each set of combined POS categories is evaluated. <ref type="bibr">5</ref> Rentaishi is a category of words that are not adjectives but modify nouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Character n-grams</head><p>The accuracy scores for Japanese character n- grams and Romanized character n-grams are sum- marized in <ref type="table" target="#tab_0">Table 3</ref>. Selected trigrams (97,992 fea- tures) achieved an accuracy of 86.4% with NB in author-level classification. Trigrams selected by univariate feature selection (114,303 features) worked best for the document-level classification, resulting in 75.5% accuracy. Romaji n-grams achieved similar accuracies, but they were below Japanese character n-grams. <ref type="table">Table 4</ref> shows the results of classification with to- ken n-grams as features. Token unigrams with the NB classifier yielded 86.4% with (14,656 fea- tures) or without feature selection (10,992 fea- tures) in author-level classification, which was the same accuracy as the model with selected charac- ter trigrams. For the document-level classification, SVM with selected token trigrams (124,528 fea- tures) worked the best (68.8%) though it was not as good as the accuracy obtained from character trigrams.  <ref type="table">Table 5</ref>: Accuracies (%) for lemmas and lemmas of POS categories with the highest accuracy. The char- acter in parentheses shows a model of feature selection (L:Logistic Regression, S:SVM, U:Univariate)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Token n-grams</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author</head><formula xml:id="formula_0">Depressed Non-Depressed Verb iru</formula><p>"there is (someone)" agaru "go up" naru "become" aru "there is (something)" dekiru "can do" wakaru "understand" suru "do" yaru "do" kangaeru "think" motsu "have" tsukareru "get tired" yomu "read" shinu "die" yaru "do" Adv nandaka "somehow" itsumo "always" sukoshi "a little" maa "relatively" <ref type="table">Table 6</ref>: Some selected verbs and adverbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Lemmas and Selected Lemmas</head><p>The accuracy score for each classifier with the lemma feature set is shown in <ref type="table">Table 5</ref>. The NB classifier resulted in the highest accuracy of 95.5% with verbs and adverbs as features (2,627 fea- tures). After feature selection within the set of verbs and adverbs (2,007 features), the accuracy stayed the same. With different set of features, the SVM achieved 90.9% accuracy after further fea- ture selection. For document-level classification, the highest accuracy was 69.0% with selected lem- mas of four POS categories (2,579 features). Even though the accuracy improved from the baseline, it did worse than the character n-grams. Some of the selected lemmas from the best resulting author- level classification are shown in <ref type="table">Table 6</ref>. Words that appear more frequently in one class are listed under that class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification and features</head><p>In all the experiments, author-level classification is better than document-level classification. This may be because each document contains around 200 words in document-level classification, and many features may not appear in one document, leaving feature vectors sparse.</p><p>Lemmas of verb and adverb categories give the best accuracy for the author-level classification. This suggests that frequency of words, regardless of their inflection or the surrounding context, is most useful when provided with sufficient amount of text. Although some of the selected lemmas are related to symptoms of depression (fatigue, sui- cidal thoughts), lemmas appearing frequently in depressed authors' documents are not necessar- ily related to emotions or mood (e.g. somehow, always). This suggests that there are subtle dif- ferences in choice of words by depressed authors which we may not immediately associate with de- pression.</p><p>Morphological and syntactic information such as inflection and word order, may be useful, but they do not provide accuracies that are as good as lemmas in the author-level classification. However, the experiment with character trigrams results into having the best accuracies for the document-level classification. This is likely be- cause within a limited amount of text, character n-grams appear more often than lemmas. Roman- izing characters do not improve the performance. Representing Japanese language with Romaji sup- presses homonyms and kanji that may otherwise Author Document Before After Before After CharUni 81.8 (NB) 68.2 (NB) 75.3 (LR) 67.2 (LR) TokenUni 72.7 (NB) 86.4 (NB) 71.5 (LR) 63.2 (NB) LemmaUni 72.7 (NB) 81.8 (NB) 71.5 (LR) 66.8 (NB) <ref type="table">Table 7</ref>: Accuracies before and after topic modeling be informative (see Section 3.4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Topic bias</head><p>We now take a closer look at the effect of topic bias, i.e., we compare the results when the entries on depression have been removed (see Section 3.2 for details) to the condition when these entries are kept in the training and the test set. The latter con- dition corresponds to the settings that have been used by <ref type="bibr" target="#b5">Matsumoto et al. (2012)</ref>. The classifica- tion on the document-level with the full data set does worse than the classification with the cleaned data (see <ref type="table">Table 7</ref>). This is expected because topic bias is factored out after topic-modeling. How- ever, on the author-level, it is a more complex picture: for word-based units (token and lemma), accuracy actually goes up once topic bias is re- moved. As a blog entry tends to focus on one topic, and depressed authors' documents contain more words about depression, the document-level classification seems to be affected by the topic. We will investigate why the author-level classification improves with the cleaned data in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future work</head><p>This study showed that selected lemmas can pre- dict whether authors of written texts are depressed or not with an accuracy of 95.5%. This is higher than <ref type="bibr" target="#b5">Matsumoto et al. (2012)</ref> though it is difficult to compare because of different data sets. The bet- ter performance of author-level classification sug- gests that documents should contain enough text to be classified correctly. The next step will in- volve finding out how much text per document is necessary to achieve such high accuracy.</p><p>As the current study only tested default param- eters for SVM and LR in classification and fea- ture selection, different parameter settings will be tested in the future work.</p><p>As the study is small-scale, it is necessary to ex- amine how the results extend to larger data. More- over, expanding the scope of study to other men- tal conditions may reveal the nature of language use in relation to mental health. Further investiga- tion of selected lemmas in connection with clini- cal studies may provide us insights on why these words work well as features.</p><p>Finally, the methods of the current study can easily be adapted to other languages with different character systems if a language can be tokenized and POS-tagged. It would be worth exploring how depression can be detected from texts in different languages and performing a cross-linguistic com- parison of characteristics found in depressed au- thors' writings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>http://mental.blogmura.com/utsu/ 2 Data by Matsumoto et al. (2012) was not available</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracies (%) for character n-grams. CharUni and CharN: Japanese character uni-and n-
grams. RomUni and RomN: Romanized character uni-and n-grams. FS:Feature Selection. The value 
in parentheses indicates the best value of n for n-gram and a method for feature selection (L:Logistic 
Regression, S:SVM, U:Univariate) 

Author 
Document 
NB 
SVM 
LR 
NB 
SVM 
LR 
TokenUni 
86.4 
54.5 
59.1 
63.2 
58.9 
62.0 
TokenUni+FS 86.4 (U) 72.7 (U) 72.7 (U) 64.8 (L) 62.6 (S) 65.1 (S) 
TokenN (n) 
77.3 (2) 54.5 (8) 59.1 (2) 66.0 (2) 63.2 (3) 66.7 (3) 
TokenN+FS 
81.8 (U) 72.7 (S) 81.8 (U) 65.7 (U) 68.8 (U) 67.3 (U) 

</table></figure>

			<note place="foot" n="3"> We are aware that we cannot guarantee that nondepressed authors are not depressed.</note>

			<note place="foot" n="4"> http://jprocessing.readthedocs.io/en/ latest/#id2</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Personality and patterns of facebook usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Stillwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Annual ACM Web Science Conference</title>
		<meeting>the 4th Annual ACM Web Science Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="24" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Predicting personality from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Golbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michon</forename><surname>Edmondson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">IEEE Third International Conference on. IEEE</title>
		<imprint>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Depression in Japan: Psychiatric cures for a society in distress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junko</forename><surname>Kitanaka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Japanese dependency analysis using cascaded chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL 2002: Proceedings of the 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="63" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Utsu key phrase to kanjo hyogen hendo ni motozuku blog kara no utsu kensyutsu syuho</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuhiro</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuji</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Natural Language Processing 18th Conference Proceedings</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1126" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mallet: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="Http://mallet.cs.umass.edu" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Psychological profiling through textual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Noecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Juola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Literary and Linguistic Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="382" to="387" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of the 2nd author profiling task at pan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Trenkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Verhoeven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Daeleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEUR Workshop Proceedings. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1180</biblScope>
			<biblScope unit="page" from="898" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gender attribution: tracing stylometric evidence beyond topic and genre</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruchita</forename><surname>Sarawgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailash</forename><surname>Gajulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Japanese dictionary of appraisalattitude-(jappraisal dictionary). JAppraisal Dictionary ver1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Sato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
