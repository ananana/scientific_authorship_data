<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Haonan</roleName><forename type="first">Yu</forename><forename type="middle">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Laboratory for Deep Learning Technology and Applications</orgName>
								<address>
									<country>Beijing China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Baidu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Research -Institue of Deep Learning</orgName>
								<address>
									<settlement>Sunnyvale</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2609" to="2619"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2609</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Building intelligent agents that can communicate with and learn from humans in natural language is of great value. Supervised language learning is limited by the ability of capturing mainly the statistics of training data, and is hardly adaptive to new scenarios or flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting. We highlight the perspective that conversational interaction serves as a natural interface both for language learning and for novel knowledge acquisition and propose a joint imitation and reinforcement approach for grounded language learning through an interactive conversational game. The agent trained with this approach is able to actively acquire information by asking questions about novel objects and use the just-learned knowledge in subsequent conversations in a one-shot fashion. Results compared with other methods verified the effectiveness of the proposed approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language is one of the most natural forms of com- munication for human and is typically viewed as fundamental to human intelligence; therefore it is crucial for an intelligent agent to be able to use lan- guage to communicate with human as well. While supervised training with deep neural networks has led to encouraging progress in language learning, it suffers from the problem of capturing mainly the statistics of training data, and from a lack of adaptiveness to new scenarios and being flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting. Moreover, supervised training of deep neural network mod- els needs a large number of training samples while many interesting applications require rapid learn- ing from a small amount of data, which poses an even greater challenge to the supervised setting.</p><p>In contrast, humans learn in a way very different from the supervised setting <ref type="bibr" target="#b26">(Skinner, 1957;</ref><ref type="bibr" target="#b15">Kuhl, 2004</ref>). First, humans act upon the world and learn from the consequences of their actions <ref type="bibr" target="#b26">(Skinner, 1957;</ref><ref type="bibr" target="#b15">Kuhl, 2004;</ref><ref type="bibr" target="#b22">Petursdottir and Mellor, 2016)</ref>. While for mechanical actions such as movement, the consequences mainly follow geometrical and mechanical principles, for language, humans act by speaking, and the consequence is typically a response in the form of verbal and other behav- ioral feedback (e.g., nodding) from the conversa- tion partner (i.e., teacher). These types of feed- back typically contain informative signals on how to improve language skills in subsequent conver- sations and play an important role in humans' language acquisition process <ref type="bibr" target="#b15">(Kuhl, 2004;</ref><ref type="bibr" target="#b22">Petursdottir and Mellor, 2016)</ref>. Second, humans have shown a celebrated ability to learn new concepts from small amount of data ( <ref type="bibr" target="#b5">Borovsky et al., 2003)</ref>. From even just one example, children seem to be able to make inferences and draw plausible bound- aries between concepts, demonstrating the ability of one-shot learning <ref type="bibr" target="#b16">(Lake et al., 2011</ref>).</p><p>The language acquisition process and the one- shot learning ability of human beings are both impressive as a manifestation of human intelli- gence, and are inspiring for designing novel set- tings and algorithms for computational language learning. In this paper, we leverage conversation as both an interactive environment for language learning <ref type="bibr" target="#b26">(Skinner, 1957</ref>) and a natural interface for acquiring new knowledge ( <ref type="bibr" target="#b4">Baker et al., 2002</ref>). We propose an approach for interactive language acquisition with one-shot concept learning ability. The proposed approach allows an agent to learn grounded language from scratch, acquire the trans-ferable skill of actively seeking and memorizing information about novel objects, and develop the one-shot learning ability, purely through conver- sational interaction with a teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Supervised Language Learning. Deep neural network-based language learning has seen great success on many applications, including machine translation ( <ref type="bibr" target="#b7">Cho et al., 2014b)</ref>, dialogue genera- tion ( <ref type="bibr" target="#b35">Wen et al., 2015;</ref><ref type="bibr" target="#b25">Serban et al., 2016)</ref>, image captioning and visual question answering <ref type="bibr" target="#b0">(?Antol et al., 2015)</ref>. For training, a large amount of la- beled data is needed, requiring significant efforts to collect. Moreover, this setting essentially cap- tures the statistics of training data and does not re- spect the interactive nature of language learning, rendering it less flexible for acquiring new knowl- edge without retraining or forgetting <ref type="bibr" target="#b28">(Stent and Bangalore, 2014</ref>).</p><p>Reinforcement Learning for Sequences. Some recent studies used reinforcement learning (RL) to tune the performance of a pre-trained language model according to certain metrics ( <ref type="bibr" target="#b23">Ranzato et al., 2016;</ref><ref type="bibr">Bahdanau et al., 2017;</ref>. Our work is also related to RL in natural language action space ( <ref type="bibr" target="#b13">He et al., 2016)</ref> and shares a similar motivation with <ref type="bibr" target="#b36">Weston (2016)</ref> and <ref type="bibr" target="#b18">Li et al. (2017)</ref>, which explored language learning through pure textual dialogues. However, in these works ( <ref type="bibr" target="#b13">He et al., 2016;</ref><ref type="bibr" target="#b36">Weston, 2016;</ref><ref type="bibr" target="#b18">Li et al., 2017)</ref>, a set of candidate sequences is provided and the action is to select one from the set. Our main focus is rather on learning language from scratch: the agent has to learn to generate a sequence action rather than to simply select one from a provided candidate set. Communication and Emergence of Language. Recent studies have examined learning to com- municate ( <ref type="bibr" target="#b11">Foerster et al., 2016;</ref><ref type="bibr" target="#b30">Sukhbaatar et al., 2016</ref>) and invent language ( <ref type="bibr" target="#b17">Lazaridou et al., 2017;</ref><ref type="bibr" target="#b21">Mordatch and Abbeel, 2018)</ref>. The emerged lan- guage needs to be interpreted by humans via post- processing <ref type="bibr" target="#b21">(Mordatch and Abbeel, 2018)</ref>. We, however, aim to achieve language learning from the dual perspectives of understanding and gener- ation, and the speaking action of the agent is read- ily understandable without any post-processing. Some studies on language learning have used a guesser-responder setting in which the guesser tries to achieve the final goal (e.g., classification) by collecting additional information through ask- ing the responder questions ( <ref type="bibr" target="#b29">Strub et al., 2017;</ref><ref type="bibr" target="#b8">Das et al., 2017)</ref>. These works try to optimize the question being asked to help the guesser achieve the final goal, while we focus on transferable speaking and one-shot ability.</p><p>One-shot Learning and Active Learning. One- shot learning has been investigated in some re- cent works <ref type="bibr" target="#b16">(Lake et al., 2011;</ref><ref type="bibr" target="#b24">Santoro et al., 2016;</ref><ref type="bibr" target="#b37">Woodward and Finn, 2016)</ref>. The memory- augmented network ( <ref type="bibr" target="#b24">Santoro et al., 2016</ref>) stores visual representations mixed with ground truth class labels in an external memory for one-shot learning. A class label is always provided follow- ing the presentation of an image; thus the agent receives information from the teacher in a passive way. <ref type="bibr" target="#b37">Woodward and Finn (2016)</ref> present efforts toward active learning, using a vanilla recurrent neural network (RNN) without an external mem- ory. Both lines of study focus on image classi- fication only, meaning the class label is directly provided for memorization. In contrast, we tar- get language and one-shot learning via conversa- tional interaction, and the learner has to learn to extract important information from the teacher's sentences for memorization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Conversational Game</head><p>We construct a conversational game inspired by experiments on language development in infants from cognitive science <ref type="bibr" target="#b34">(Waxman, 2004</ref>). The game is implemented with the XWORLD simula- tor ( <ref type="bibr" target="#b38">Yu et al., 2018;</ref> and is pub- licly available online. 1 It provides an environment for the agent 2 to learn language and develop the one-shot learning ability. One-shot learning here means that during test sessions, no further training happens to the agent and it has to answer teacher's questions correctly about novel images of never- before-seen classes after being taught only once by the teacher, as illustrated in <ref type="figure">Figure 1</ref>. To suc- ceed in this game, the agent has to learn to 1) speak by generating sentences, 2) extract and memo- rize useful information with only one exposure and use it in subsequent conversations, and 3) be- have adaptively according to context and its own knowledge (e.g., asking questions about unknown objects and answering questions about something known), all achieved through interacting with the <ref type="figure">Figure 1</ref>: Interactive language and one-shot concept learning. Within a session S l , the teacher may ask questions, answer learner's questions, make statements, or say nothing. The teacher also provides reward feedback based on learner's responses as (dis-)encouragement. The learner alternates between in- terpreting teacher's sentences and generating a response through interpreter and speaker. Left: Initially, the learner can barely say anything meaningful. Middle: Later it can produce meaningful responses for interaction. Right: After training, when confronted with an image of cherry, which is a novel class that the learner never saw before during training, the learner can ask a question about it ("what is it") and generate a correct statement ("this is cherry") for another instance of cherry after only being taught once.</p><formula xml:id="formula_0">S 1 - Train Teacher Learner S l - Test (novel data)</formula><p>teacher. This makes our game distinct from other seemingly relevant games, in which the agent can- not speak ( <ref type="bibr" target="#b32">Wang et al., 2016</ref>) or "speaks" by se- lecting a candidate from a provided set ( <ref type="bibr" target="#b13">He et al., 2016;</ref><ref type="bibr" target="#b36">Weston, 2016;</ref><ref type="bibr" target="#b18">Li et al., 2017</ref>) rather than generating sentences by itself, or games mainly focus on slow learning ( <ref type="bibr" target="#b8">Das et al., 2017;</ref><ref type="bibr" target="#b29">Strub et al., 2017)</ref> and falls short on one-shot learning.</p><p>In this game, sessions (S l ) are randomly in- stantiated during interaction. Testing sessions are constructed with a separate dataset with concepts that never appear before during training to eval- uate the language and one-shot learning ability. Within a session, the teacher randomly selects an object and interacts with the learner about the ob- ject by randomly 1) posing a question (e.g., "what is this"), 2) saying nothing (i.e., "") or 3) mak- ing a statement (e.g., "this is monkey"). When the teacher asks a question or says nothing, i) if the learner raises a question, the teacher will pro- vide a statement about the object asked (e.g., "it is frog") with a question-asking reward (+0.1); ii) if the learner says nothing, the teacher will still pro- vide an answer (e.g., "this is elephant") but with an incorrect-reply reward (−1) to discourage the learner from remaining silent; iii) for all other in- correct responses from the learner, the teacher will provide an incorrect-reply reward and move on to the next random object for interaction. When the teacher generates a statement, the learner will re- ceive no reward if a correct statement is gener- ated otherwise an incorrect-reply reward will be given. The session ends if the learner answers the teacher's question correctly, generates a cor- rect statement when the teacher says nothing (re- ceiving a correct-answer reward +1), or when the maximum number of steps is reached. The sen- tence from teacher at each time step is generated using a context-free grammar as shown in <ref type="table">Table 1</ref>.</p><p>A success is reached if the learner behaves cor- rectly during the whole session: asking questions about novel objects, generating answers when asked, and making statements when the teacher says nothing about objects that have been taught within the session. Otherwise it is a failure. <ref type="table">Table 1</ref>: Grammar for the teacher's sentences.</p><formula xml:id="formula_1">start → question | silence | statement question → Q1 | Q2 | Q3 silence → " " statement → A1 | A2 | A3 | A4 | A5 | A6 | A7 | A8 Q1 → "what" Q2</formula><p>→ "what" M Q3</p><p>→ "tell what" N M → "is it" | "is this" | "is there" | "do you see" | "can you see" | "do you observe" | "can you observe" N → "it is" | "this is" | "there is" | "you see" | "you can see" | "you observe" | "you can observe"</p><formula xml:id="formula_2">A1 → G A2 → "it is" G A3 → "this is" G A4 → "there is" G A5 → "i see" G A6 → "i observe" G A7 → "i can see" G A8 → "i can observe" G G → object name</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Interactive Language Acquisition via Joint Imitation and Reinforcement</head><p>Motivation. The goal is to learn to converse and develop the one-shot learning ability by convers- ing with a teacher and improving from teacher's feedback. We propose to use a joint imitation and reinforce approach to achieve this goal. Imitation helps the agent to develop the basic ability to generate sensible sentences. As learning is done by observing the teacher's behaviors during con- version, the agent essentially imitates the teacher from a third-person perspective (Stadie et al., 2017) rather than imitating an expert agent who is conversing with the teacher ( <ref type="bibr" target="#b8">Das et al., 2017;</ref><ref type="bibr" target="#b29">Strub et al., 2017)</ref>. During conversations, the agent perceives sentences and images without any explicit labeling of ground truth answers, and it has to learn to make sense of raw perceptions, extract useful information, and save it for later use when generating an answer to teacher's ques- tion. While it is tempting to purely imitate the teacher, the agent trained this way only devel- ops echoic behavior <ref type="bibr" target="#b26">(Skinner, 1957)</ref>, i.e., mimicry.</p><p>Reinforce leverages confirmative feedback from the teacher for learning to converse adaptively be- yond mimicry by adjusting the action policy. It enables the learner to use the acquired speaking ability and adapt it according to reward feedback. This is analogous to some views on the babies' language-learning process that babies use the ac- quired speaking skills by trial and error with par- ents and improve according to the consequences of speaking actions <ref type="bibr" target="#b26">(Skinner, 1957;</ref><ref type="bibr" target="#b22">Petursdottir and Mellor, 2016)</ref>. The fact that babies don't fully de- velop the speaking capabilities without the ability to hear <ref type="bibr" target="#b14">(Houston and Miyamoto, 2011)</ref>, and that it is hard to make a meaningful conversation with a trained parrot signifies the importance of both im- itation and reinforcement in language learning.</p><p>Formulation. The agent's response can be mod- eled as a sample from a probability distribu- tion over the possible sequences. Specifically, for one session, given the visual input v t and conversation history H t ={w 1 , a 1 , · · · , w t }, the agent's response a t can be generated by sampling from a distribution of the speaking action a t ∼ p S θ (a|H t , v t ). The agent interacts with the teacher by outputting the utterance a t and receives feed- back from the teacher in the next step, with w t+1 a sentence as verbal feedback and r t+1 reward feed- back (with positive values as encouragement while negative values as discouragement, according to a t , as described in Section 3). Central to the goal is learning p S θ (·). We formulate the problem as the minimization of a cost function as:</p><formula xml:id="formula_3">L θ =E W − t log p I θ (w t |·) Imitation L I θ +E p S θ − t [γ] t−1 · r t Reinforce L R θ</formula><p>where E W (·) is the expectation over all the sen- tences W from teacher, γ is a reward discount factor, and [γ] t denotes the exponentiation over γ. While the imitation term learns directly the predic- tive distribution p I θ (w t |H t−1 , a t ), it contributes to p S θ (·) through parameter sharing between them.</p><p>Architecture. The learner comprises four ma- jor components: external memory, interpreter, speaker, and controller, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Ex- ternal memory is flexible for storing and retriev- ing information ( <ref type="bibr" target="#b12">Graves et al., 2014;</ref><ref type="bibr" target="#b24">Santoro et al., 2016)</ref>, making it a natural component of our net- work for one-shot learning. The interpreter is re- sponsible for interpreting the teacher's sentences, extracting information from the perceived signals, and saving it to the external memory. The speaker is in charge of generating sentence responses with reading access to the external memory. The re- sponse could be a question asking for informa- tion or a statement answering a teacher's question, leveraging the information stored in the external memory. The controller modulates the behavior of the speaker to generate responses according to context (e.g., the learner's knowledge status).</p><p>At time step t, the interpreter uses an interpreter-RNN to encode the input sentence w t from the teacher as well as historical conversa- tional information into a state vector h t I . h t I is then passed through a residue-structured network, which is an identity mapping augmented with a learnable controller f (·) implemented with fully connected layers for producing c t . Finally, c t is used as the initial state of the speaker-RNN for generating the response a t . The final state h t last of the speaker-RNN will be used as the initial state of the interpreter-RNN at the next time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Imitation with Memory Augmented Neural Network for Echoic Behavior</head><p>The teacher's way of speaking provides a source for the agent to imitate. For example, the syn- tax for composing a sentence is a useful skill the agent can learn from the teacher's sentences, which could benefit both interpreter and speaker. Imitation is achieved by predicting teacher's future sentences with interpreter and parameter sharing between interpreter and speaker. For prediction, we can represent the probability of the next sen- tence w t conditioned on the image v t as well as previous sentences from both the teacher and the   </p><note type="other">External Memory External Memory External Memory External Memory External Memory External Memory mix mix t −→ t + 1 −→ t + 2 −→ w t v t h t</note><formula xml:id="formula_4">learner {w 1 , a 1 , · · · , w t−1 , a t−1 } as p I θ (w t |H t−1 , a t−1 , v t ) = i p I θ (w t i |w t 1:i−1 , h t−1 last , v t ),<label>(1)</label></formula><p>where h t−1 last is the last state of the RNN at time step t−1 as the summarization of {H t−1 , a t−1 } (c.f., <ref type="figure" target="#fig_0">Figure 2</ref>), and i indexes words within a sentence.</p><p>It is natural to model the probability of the i-th word in the t-th sentence with an RNN, where the sentences up to t and words up to i within the t-th sentence are captured by a fixed-length state vec- tor h t i = RNN(h t i−1 , w t i ). To incorporate knowl- edge learned and stored in the external memory, the generation of the next word is adaptively based on i) the predictive distribution of the next word from the state of the RNN to capture the syntac- tic structure of sentences, and ii) the information from the external memory to represent the previ- ously learned knowledge, via a fusion gate g:</p><formula xml:id="formula_5">p I θ (w t i |h t i , v t ) = (1 − g) · p h + g · p r ,<label>(2)</label></formula><p>where p h = softmax E T f MLP (h t i ) and p r = softmax E T r . E∈R d×k is the word embedding table, with d the embedding dimension and k the vocabulary size. r is a vector read out from the external memory using a visual key as detailed in the next section. f MLP (·) is a multi-layer Multi- Layer Perceptron (MLP) for bridging the seman- tic gap between the RNN state space and the word embedding space. The fusion gate g is computed as g = f (h t i , c), where c is the confidence score c=max(E T r), and a well-learned concept should have a large score by design (Appendix A.2).</p><p>Multimodal Associative Memory. We use a mul- timodal memory for storing visual (v) and sen- tence (s) features with each modality while pre- serving the correspondence between them (Badde- ley, 1992). Information organization is more struc- tured than the single modality memory as used in <ref type="bibr" target="#b24">Santoro et al. (2016)</ref> and cross modality re- trieval is straightforward under this design. A vi- sual encoder implemented as a convolutional neu- ral network followed by fully connected layers is used to encode the visual image v into a visual key k v , and then the corresponding sentence fea- ture can be retrieved from the memory as:</p><formula xml:id="formula_6">r ← READ(k v , M v , M s ).<label>(3)</label></formula><p>M v and M s are memories for visual and sen- tence modalities with the same number of slots (columns). Memory read is implemented as r = M s α with α a soft reading weight obtained through the visual modality by calculating the co- sine similarities between k v and slots of M v . Memory write is similar to Neural Turing Ma- chine ( <ref type="bibr" target="#b12">Graves et al., 2014</ref>), but with a content im- portance gate g mem to adaptively control whether the content c should be written into memory:</p><formula xml:id="formula_7">M m ← WRITE(M m , c m , g mem ), m ∈ {v, s}.</formula><p>For the visual modality c v k v . For the sentence modality, c s has to be selectively extracted from the sentence generated by the teacher. We use an attention mechanism to achieve this by c s =Wη, where W denotes the matrix with columns be- ing the embedding vectors of all the words in the sentence. η is a normalized attention vector representing the relative importance of each word in the sentence as measured by the cosine sim- ilarity between the sentence representation vec- tor and each word's context vector, computed us- ing a bidirectional-RNN. The scalar-valued con- tent importance gate g mem is computed as a func- tion of the sentence from the teacher, meaning that the importance of the content to be written into memory depends on the content itself (c.f., Ap- pendix A.3 for more details). The memory write is achieved with an erase and an add operation:</p><formula xml:id="formula_8">˜ M m = M m − M m (g mem · 1 · β T ), M m = ˜ M m + g mem · c m · β T , m∈{v, s}.</formula><p>denotes Hadamard product and the write loca- tion β is determined with a Least Recently Used Access mechanism (Santoro et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Context-adaptive Behavior Shaping through Reinforcement Learning</head><p>Imitation fosters the basic language ability for generating echoic behavior <ref type="bibr" target="#b26">(Skinner, 1957)</ref>, but it is not enough for conversing adaptively with the teacher according to context and the knowl- edge state of the learner. Thus we leverage re- ward feedback to shape the behavior of the agent by optimizing the policy using RL. The agent's re- sponse a t is generated by the speaker, which can be modeled as a sample from a distribution over all possible sequences, given the conversation history H t ={w 1 , a 1 , · · · , w t } and visual input v t :</p><formula xml:id="formula_9">a t ∼ p S θ (a|H t , v t ).<label>(4)</label></formula><p>As H t can be encoded by the interpreter-RNN as h t I , the action policy can be represented as p S θ (a|h t I , v t ). To leverage the language skill that is learned via imitation through the interpreter, we can generate the sentence by implementing the speaker with an RNN, sharing parameters with the interpreter-RNN, but with a conditional signal modulated by a controller network <ref type="figure" target="#fig_0">(Figure 2)</ref>:</p><formula xml:id="formula_10">p S θ (a t |h t I , v t ) = p I θ (a t |h t I + f (h t I , c), v t ). (5)</formula><p>The reason for using a controller f (·) for modula- tion is that the basic language model only offers the learner the echoic ability to generate a sen- tence, but not necessarily the adaptive behavior according to context (e.g. asking questions when facing novel objects and providing an answer for a previously learned object according to its own knowledge state). Without any additional module or learning signals, the agent's behaviors would be the same as those of the teacher because of param- eter sharing; thus, it is difficult for the agent to learn to speak in an adaptive manner.</p><p>To learn from consequences of speaking ac- tions, the policy p S θ (·) is adjusted by maximizing expected future reward as represented by L R θ . As a non-differentiable sampling operation is involved in Eqn. <ref type="formula" target="#formula_9">(4)</ref>, policy gradient theorem <ref type="bibr" target="#b31">(Sutton and Barto, 1998</ref>) is used to derive the gradient for up- dating p S θ (·) in the reinforce module:</p><formula xml:id="formula_11">θ L R θ = E p S θ t A t · θ log p S θ (a t |c t ) ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_12">A t = V (h t I , c t ) − r t+1 − γV (h t+1 I , c t+1 )</formula><p>is the advantage <ref type="bibr" target="#b31">(Sutton and Barto, 1998</ref>) estimated using a value network V (·). The imitation mod- ule contributes by implementing L I θ with a cross- entropy loss ( <ref type="bibr" target="#b23">Ranzato et al., 2016</ref>) and minimizing it with respect to the parameters in p I θ (·), which are shared with p S θ (·). The training signal from imita- tion takes the shortcut connection without going through the controller. More details on f (·), V (·) are provided in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments with comparison to base- line approaches. We first experiment with a word- level task in which the teacher and the learner communicate a single word each time. We then investigate the impact of image variations on con- cept learning. We further perform evaluation on the more challenging sentence-level task in which the teacher and the agent communicate in the form of sentences with varying lengths.</p><p>Setup. To evaluate the performance in learning a transferable ability, rather than the ability of fit- ting a particular dataset, we use an Animal dataset for training and test the trained models on a Fruit dataset <ref type="figure">(Figure 1</ref>). More details on the datasets are provided in Appendix A.1. Each session consists of two randomly sampled classes, and the maxi- mum number of interaction steps is six. Baselines. The following methods are compared:</p><p>• Reinforce: a baseline model with the same network structure as the proposed model and trained using RL only, i.e. minimizing L R θ ;</p><p>• Imitation: a recurrent encoder decoder ( <ref type="bibr" target="#b25">Serban et al., 2016</ref>) model with the same structure as ours and trained via imitation (minimizing L I θ ); • Imitation+Gaussian-RL: a joint imitation and reinforcement method using a Gaussian pol- icy ( <ref type="bibr" target="#b9">Duan et al., 2016</ref>) in the latent space of the control vector c t ( ). The pol- icy is changed by modifying the control vector c t the action policy depends upon.</p><p>Training Details. The training algorithm is imple- mented with the deep learning platform PaddlePad- dle. 3 The whole network is trained from scratch in an end-to-end fashion. The network is randomly initialized without any pre-training and is trained with decayed Adagrad ( <ref type="bibr" target="#b10">Duchi et al., 2011</ref>). We use a batch size of 16, a learning rate of 1×10 −5 and a weight decay rate of 1.6 × 10 −3 . We also exploit experience replay ( <ref type="bibr" target="#b38">Yu et al., 2018)</ref>. The reward discount factor γ is 0.99, the word embedding dimension d is 1024 and the dictionary size k is 80. The visual im- age size is 32 × 32, the maximum length of gen- erated sentence is 6 and the memory size is 10. Word embedding vectors are initialized as random vectors and remain fixed during training. A sam- pling operation is used for sentence generation during training for exploration while a max op- eration is used during testing both for Proposed and for Reinforce baseline. The max operation is   used in both training and testing for Imitation and Imitation+Gaussian-RL baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word-Level Task</head><p>In this experiment, we focus on a word-level task, which offers an opportunity to analyze and under- stand the underlying behavior of different algo- rithms while being free from distracting factors. Note that although the teacher speaks a word each time, the learner still has to learn to generate a full- sentence ended with an end-of-sentence symbol. <ref type="figure" target="#fig_1">Figure 3</ref> shows the evolution curves of the re- wards during training for different approaches. It is observed that Reinforce makes very little progress, mainly due to the difficulty of explo- ration in the large space of sequence actions. Imitation obtains higher rewards than Reinforce during training, as it can avoid some penalty by generating sensible sentences such as ques- tions. Imitation+Gaussian-RL gets higher re- wards than both Imitation and Reinforce, indi- cating that the RL component reshapes the action policy toward higher rewards. However, as the Gaussian policy optimizes the action policy indi- rectly in a latent feature space, it is less efficient for exploration and learning. Proposed achieves the highest final reward during training.</p><p>We train the models using the Animal dataset and evaluate them on the Fruit dataset; <ref type="figure" target="#fig_3">Figure 4</ref> sum- The speaker uses the fusion gate g to adaptively switch between signals from RNN (small g) and external memory (large g) to generate sentence responses.</p><p>marizes the success rate and average reward over 1K testing sessions. As can be observed, Rein- force achieves the lowest success rate (0.0%) and reward (−6.0) due to its inherent inefficiency in learning. Imitation performs better than Rein- force in terms of both its success rate (28.6%) and reward value (−2.7). Imitation+Gaussian- RL achieves a higher reward (−1.2) during test- ing, but its success rate (32.1%) is similar to that of Imitation, mainly due to the rigorous criteria for success. Proposed reaches the highest success rate (97.4%) and average reward (+1.1) 4 , outper- forming all baseline methods by a large margin. From this experiment, it is clear that imitation with a proper usage of reinforcement is crucial for achieving adaptive behaviors (e.g., asking ques- tions about novel objects and generating answers or statements about learned objects proactively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learning with Image Variations</head><p>To evaluate the impact of within-class image vari- ations on one-shot concept learning, we train mod- els with and without image variations, and during testing compare their performance under different image variation ratios (the chance of a novel image instance being present within a session) as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. It is observed that the performance of the model trained without image variations drops significantly as the variation ratio increases. We also evaluate the performance of models trained under a variation ratio of 0.5. <ref type="figure" target="#fig_4">Figure 5</ref> clearly shows that although there is also a performance drop, which is expected, the performance degrades more gradually, indicating the importance of im- age variation for learning one-shot concepts. <ref type="figure">Fig- ure 6</ref> visualizes sampled training and testing im- ages represented by their corresponding features extracted using the visual encoder trained with- out and with image variations. Clusters of visu- ally similar concepts emerge in the feature space when trained with image variations, indicating that a more discriminative visual encoder was obtained for learning generalizable concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sentence-Level Task</head><p>We further evaluate the model on sentence-level tasks. Teacher's sentences are generated using the grammar as shown in <ref type="table">Table 1</ref> and have a number of variations with sentence lengths ranging from one to five. Example sentences from the teacher are presented in Appendix A.1. This task is more challenging than the word-level task in two ways: i) information processing is more difficult as the learner has to learn to extract useful information which could appear at different locations of the sentence; ii) the sentence generation is also more difficult than the word-level task and the learner has to adaptively fuse information from RNN and external memory to generate a complete sentence.</p><p>Comparison of different approaches in terms of their success rates and average rewards on the novel test set are shown in <ref type="figure" target="#fig_6">Figure 8</ref>. As can be observed from the figure, Proposed again outper- forms all other compared methods in terms of both success rate (82.8%) and average reward (+0.8), demonstrating its effectiveness even for the more complex sentence-level task. We also visualize the information extraction and the adaptive sentence composing process of the proposed approach when applied to a test set. As shown in <ref type="figure">Figure 7</ref>, the agent learns to extract use- ful information from the teacher's sentence and use the content importance gate to control what content is written into the external memory. Con- cretely, sentences containing object names have a larger g mem value, and the word corresponding to object name has a larger value in the attention vec- tor η compared to other words in the sentence. The combined effect of η and g mem suggests that words corresponding to object names have higher likelihoods of being written into the external mem- ory. The agent also successfully learns to use the external memory for storing the information extracted from the teacher's sentence, to fuse it adaptively with the signal from the RNN (captur- ing the syntactic structure) and to generate a com- plete sentence with the new concept included. The value of the fusion gate g is small when gener- ating words like "what,", "i," "can," and "see," meaning it mainly relies on the signal from the RNN for generation (c.f., Eqn.(2) and <ref type="figure">Figure 7</ref>). In contrast, when generating object names (e.g., "banana," and "cucumber"), the fusion gate g has a large value, meaning that there is more emphasis on the signal from the external memory. This ex- periment showed that the proposed approach is ap- plicable to the more complex sentence-level task for language learning and one-shot learning. More interestingly, it learns an interpretable operational process, which can be easily understood. More re- sults including example dialogues from different approaches are presented in Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We have presented an approach for grounded lan- guage acquisition with one-shot visual concept learning in this work. This is achieved by purely interacting with a teacher and learning from feed- back arising naturally during interaction through joint imitation and reinforcement learning, with a memory augmented neural network. Experimental results show that the proposed approach is effec- tive for language acquisition with one-shot visual concept learning across several different settings compared with several baseline approaches.</p><p>In the current work, we have designed and used a computer game (synthetic task with synthetic language) for training the agent. This is mainly due to the fact that there is no existing dataset to the best of our knowledge that is adequate for de- veloping our addressed interactive language learn- ing and one-shot learning problem. For our cur- rent design, although it is an artificial game, there is a reasonable amount of variations both within and across sessions, e.g., the object classes to be learned within a session, the presentation order of the selected classes, the sentence patterns and im- age instances to be used etc. All these factors con- tribute to the increased complexity of the learning task, making it non-trivial and already very chal- lenging to existing approaches as shown by the experimental results. While offering flexibility in training, one downside of using a synthetic task is its limited amount of variation compared with real-world scenarios with natural languages. Al- though it might be non-trivial to extend the pro- posed approach to real natural language directly, we regard this work as an initial step towards this ultimate ambitious goal and our game might shed some light on designing more advanced games or performing real-world data collection. We plan to investigate the generalization and application of the proposed approach to more realistic environ- ments with more diverse tasks in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Network structure. (a) Illustration of the overall architecture. At each time step, the learner uses the interpreter module to encode the teacher's sentence. The visual perception is also encoded and used as a key to retrieve information from the external memory. The last state of the interpreter-RNN will be passed through a controller. The controller's output will be added to the input and used as the initial state of the speaker-RNN. The interpreter-RNN will update the external memory with an importance (illustrated with transparency) weighted information extracted from the perception input. 'Mix' denotes a mixture of word embedding vectors. (b) The structures of the interpreter-RNN (top) and the speakerRNN (bottom). The interpreter-RNN and speaker-RNN share parameters.</figDesc><graphic url="image-19.png" coords="5,438.87,69.24,81.16,150.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Evolution of reward during training for the word-level task without image variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3</head><label></label><figDesc>https://github.com/PaddlePaddle/Paddle</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Test performance for the word-level task without image variations. Models are trained on the Animal dataset and tested on the Fruit dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Test success rate and reward for the word-level task on the Fruit dataset under different test image variation ratios for models trained on the Animal dataset with a variation ratio of 0.5 (solid lines) and without variation (dashed lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Visualization of the CNN features with t-SNE. Ten classes randomly sampled from (a-b) the Animal dataset and (c-d) the Fruit dataset, with features extracted using the visual encoder trained without (a, c) and with (b, d) image variations on the the Animal dataset. Teacher Learner Interpreter Speaker</figDesc><graphic url="image-788.png" coords="8,114.75,236.99,394.77,61.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Test performance for sentence-level task with image variations (variation ratio=0.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>it is monkey it is monkey monkey write what is this what is this &lt; , mix&gt; mix read read monkey write read read monkey tiger this is tiger monkey write read read monkey</head><label></label><figDesc></figDesc><table>&lt; , tiger&gt; 

tiger 

controller 

+ 

controller 

+ 

controller 

+ 

&lt; , monkey&gt; 

</table></figure>

			<note place="foot" n="1"> https://github.com/PaddlePaddle/XWorld 2 We use the term agent interchangeably with learner.</note>

			<note place="foot" n="4"> The testing reward is higher than the training reward mainly due to the action sampling in training for exploration.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers and PC members for theirs efforts in helping improving the paper. We thank Xiaochen Lian and Xiao Chu for their discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Working memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Baddeley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">255</biblScope>
			<biblScope unit="issue">5044</biblScope>
			<biblScope unit="page" from="556" to="559" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<pubPlace>Aaron C</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Conversational Learning: An Experiential Approach to Knowledge Creation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">C</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Kolb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Copley Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to use words: Event related potentials index single-shot contextual word learning. Cognzition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arielle</forename><surname>Borovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Kutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Glehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning cooperative visual dialog agents with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Benchmarking deep reinforcement learning for continuous control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to communicate with deep multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">N</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><forename type="middle">M</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>abs/1410.5401</idno>
		<title level="m">Neural turing machines. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with a natural language action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effects of early auditory experience on word learning and speech perception in deaf children with cochlear implants: Implications for sensitive periods of language development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">T</forename><surname>Miyamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Otol Neurotol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1248" to="1253" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Early language acquisition: cracking the speech code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><forename type="middle">K</forename><surname>Kuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Rev Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="831" to="843" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th Annual Meeting of the Cognitive Science Society</title>
		<meeting>the 33th Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-agent cooperation and the emergence of (natural) language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Peysakhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning through dialogue interactions by asking questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcaurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Playing Atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Emergence of grounded compositional language in multi-agent populations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reinforcement contingencies in language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">Ingeborg</forename><surname>Petursdottir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Mellor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Policy Insights from the Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="32" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Metalearning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Verbal Behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Skinner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<publisher>Copley Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Third-person imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradly</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Natural Language Generation in Interactive Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end optimization of goal-driven and visually grounded dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning language games through interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sample efficient actor-critic with experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Everything had a name, and each name gave birth to a new thought: links between early word learning and conceptual organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waxman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantically conditioned LSTM-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dialog-based language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Active oneshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Woodward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Reinforcement Learning Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interactive grounded language acquisition and generalization in a 2D world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SeqGAN: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Listen, interact and talk: Learning to speak via interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Visually-Grounded Interaction and Language</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
