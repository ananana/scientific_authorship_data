<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating Machine Translation Systems with Second Language Proficiency Tests</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
							<email>matuzaki@nuee.nagoya-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering and Computer Science</orgName>
								<orgName type="laboratory">‡ Information and Society Research Division</orgName>
								<orgName type="institution" key="instit1">Nagoya University</orgName>
								<orgName type="institution" key="instit2">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fujita</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering and Computer Science</orgName>
								<orgName type="laboratory">‡ Information and Society Research Division</orgName>
								<orgName type="institution" key="instit1">Nagoya University</orgName>
								<orgName type="institution" key="instit2">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoya</forename><surname>Todo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering and Computer Science</orgName>
								<orgName type="laboratory">‡ Information and Society Research Division</orgName>
								<orgName type="institution" key="instit1">Nagoya University</orgName>
								<orgName type="institution" key="instit2">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><forename type="middle">H</forename><surname>Arai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering and Computer Science</orgName>
								<orgName type="laboratory">‡ Information and Society Research Division</orgName>
								<orgName type="institution" key="instit1">Nagoya University</orgName>
								<orgName type="institution" key="instit2">National Institute of Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating Machine Translation Systems with Second Language Proficiency Tests</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="145" to="149"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A lightweight, human-in-the-loop evaluation scheme for machine translation (MT) systems is proposed. It extrinsically evaluates MT systems using human subjects&apos; scores on second language ability test problems that are machine-translated to the subjects&apos; native language. A large-scale experiment involving 320 subjects revealed that the context-unawareness of the current MT systems severely damages human performance when solving the test problems, while one of the evaluated MT systems performed as good as a human translation produced in a context-unaware condition. An analysis of the experimental results showed that the extrinsic evaluation captured a different dimension of translation quality than that captured by manual and automatic intrinsic evaluation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic evaluation metrics, such as the BLEU score ( <ref type="bibr" target="#b10">Papineni et al., 2002</ref>), were crucial ingredi- ents for the advances of machine translation tech- nology in the last decade. Meanwhile, the short- comings of BLEU and similar n-gram proximity- based metrics have been pointed out by many au- thors including <ref type="bibr" target="#b2">Callison-Burch et al. (2006)</ref>. The main criticisms include: 1) unreliability in evalu- ating short translations, 2) non-interpretability of the scores beyond numerical comparison, and 3) bias towards statistical MT systems.</p><p>Manual evaluation of translation quality is more reliable in many regards, but it is costly. Further- more, it is not necessarily easy to analyze the char- acteristics of MT systems based solely on the eval- uation results such as a 5-point scale evaluation of adequacy/fluency and a ranking of the outputs of different systems.</p><p>A remedy for some of the above-raised issues is task-based evaluation of MT systems <ref type="bibr" target="#b5">(Jones et al., 2005;</ref><ref type="bibr" target="#b8">Laoudi et al., 2006;</ref><ref type="bibr" target="#b6">Jones et al., 2007;</ref><ref type="bibr" target="#b11">Schneider et al., 2010;</ref><ref type="bibr" target="#b0">Berka et al., 2011</ref>), which measures the human perfor- mance in a task such as information extraction from a machine-translated text. The main bur- den of conducting task-based evaluation is also its cost; the development of a sizable amount of test materials and the gathering of appropriate human subjects is time consuming and expensive.</p><p>This paper proposes to utilize second-language proficiency tests (SLPTs), such as TOEIC, as the source of the specimens for extrinsic evaluation of MT systems. For evaluating, e.g., English-to- Japanese MT systems, a set of English test prob- lems is translated by the systems and the trans- lation qualities are evaluated by the test scores achieved by native Japanese speakers on the trans- lated problems.</p><p>In many languages, a large collection of SLPT problems is available. More than 130 standard- ized tests for 32 languages are listed in the English Wikipedia page for 'List of language proficiency tests' as of April 30, 2015. They are carefully designed to evaluate various aspects of language ability with objective criteria. We can thus obtain an easy-to-use test set that focuses on a certain as- pect of MT system performance by appropriately choosing the problem types and levels. Moreover, SLPTs are primarily designed to assess the test- takers' language ability but not their general intel- ligence. Hence, as evidenced later in the paper, the proposed scheme is expected to be robust against the heterogeneity of the subjects, as long as they are native speakers of the target language. This is a desirable property for conducting a large-scale experiment, possibly with crowdsourcing.</p><p>In the current paper, we utilize a typical for- mat of multiple-choice dialogue completion prob- lems ( <ref type="figure">Figure 1</ref>). The subjects are given a machine-INSTRUCTION Choose the most suitable utterance for the blank in the following dialogue from choices 1, 2, 3, and 4. DIALOGUE A: Hello. Can I help you? B: Yes.</p><p>[BLANK] A: I'm sorry, I can't find that name on the reservation list. B: Oh, really? Then give me a new reservation, please. OPTIONS 1. I'd like to make a reservation for Flight 502. 2. I have a reservation under the name Hashimoto. 3. I'm sure you can find my name on the list. 4. I wonder if you could tell me how to make a reservation. <ref type="figure">Figure 1</ref>: Example of multiple-choice dialogue completion problem translated conversation and asked to choose an ap- propriate utterance from several options, which are also machine-translated, to fill in a blank in the conversation.</p><p>We evaluated four translation methods in the ex- periment including both machine-translation and manual-translation. The extrinsic evaluation re- vealed that one of the MT systems is comparable to the human translation produced by randomly presenting the individual sentences to the trans- lator without any context, but the translation pro- duced by the best MT system is still far worse than that produced by a human translator working on the entire dialogue at once. Furthermore, we ex- amined the relations between the extrinsic metric based on the subjects' scores and various intrin- sic metrics including automatic scores such as the BLEU score and manual evaluation. The test ma- terial is available on request for research purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of Experiment</head><p>We extrinsically evaluated four different transla- tions of the same material, namely multiple-choice dialogue completion problems taken from second language ability tests. The original problems were in English, and we translated them into Japanese. Two of the translations were produced by MT sys- tems, and the other two were produced by a hu- man translator with and without considering the contexts of the individual sentences in the dia- logues. The human subjects solved the translated problems without knowing whether a machine or a human produced them. Finally, the translation quality was evaluated based on the rate of correct answers given by the human subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Participants</head><p>The subjects of the experiment included 320 Japanese junior high school students (12-15 years old) from two schools (schools A and B). The participants from school A consisted of 80 first- year students, 80 second-year students, and 78 third-year students. All the students from school B (82 students) were first-year students. Thus, the participants had varying levels of English and scholastic abilities. We will examine the effect of these factors on the experimental results later in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Materials</head><p>All the problems used in the experiment consisted of a short conversation between two people, where part of an utterance is hidden. The subject was presented with four options and asked to complete the dialogue with the most appropriate one.</p><p>We randomly extracted 40 English dialogue completion problems from mock National Center Test for University Admissions conducted by one of the largest preparatory schools in Japan. In the extracted problems, the number of utterances in one dialogue ranged from two to four, with each utterance consisting of one to three sentences, and an option including one or two sentences. All 40 problems contained 327 sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Translation Systems</head><p>The English dialogue completion problems were translated by four methods: 1 G: Automatic translation by Google Translate 2 Y : Automatic translation by Yahoo Translate 3 H S : Human translation produced by providing in- dividual sentences from the problems to a translator in random order H O : Human translation produced by a translator working on the entire dialogue at once</p><p>The subscripts of H S and H O stand for the translations of the sentences in "shuffled order" and "original order", respectively. The translations by H S were created by first preparing a file con- taining all the sentences from the 40 problems in a randomized order and then asking a translator to translate the file sentence-by-sentence, without assuming any specific context. H S thus provides an estimate of the performance upper-bound of the current MT systems since most current systems translate each sentence individually.</p><p>We asked three native Japanese speakers who are fluent in English to first produce the sentence- by-sentence translations by method H S and then translate all the dialogue problems in the normal way (i.e., by H O ). We randomly chose one of the translators and used his translations as the test ma- terial that the subjects solved. The other human translations were used as the reference translations for the automatic evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Procedure</head><p>Each subject was given 12 different problems that consisted of an equal number (3) of translated problems produced by the four translation meth- ods. Although the sets of problems were different among the subjects, they were designed such that the number of subjects who solve each translated problem was roughly the same. Each subject was given 12 sheets of paper, each of which showed a problem and its answer choices, and was given one minute to complete each problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Extrinsic Evaluation Metric</head><p>The translation systems were evaluated by the av- erage of the rate of correct answers made on the translated problems. Let P = {p i } be the set of original problems and M (p) be the translation of problem p produced by method M . The correct answer rate (CAR) on M (p) is defined as: CAR M (p) = # of subjects that correctly answered M(p) # of subjects who solved M(p) .</p><p>The extrinsic evaluation score of translation method M is the average of CAR over P :</p><formula xml:id="formula_0">Avg-CAR M = 1 |P | ∑ p∈P CAR M (p).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Intrinsic Evaluations</head><p>Automatic Evaluation Metrics We also evalu- ated the translation quality using BLEU, BLEU+1 ( <ref type="bibr" target="#b9">Lin and Och, 2004</ref>  binary relations. For each relation "A &gt; B" found in the broken-down relations, one point was added to system A. The final ranking among the systems for a problem was determined by the total points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary Analysis: Robustness against the Heterogeneity of the Human Subjects</head><p>We divided the participants from school A into three groups according to grade level, and then ex- amined the differences in the rate of correct an- swers for each problem among each group. We also compared the correct answer rates between the participants in the 1st grade at schools A and B. The two-way analysis of variance (ANOVA) re- vealed that the grades and schools had no signifi- cant effect on the correct answer rate for 38 out of the 40 problems (p &gt; 0.05). The results showed that the participants' grade levels and scholastic abilities (including English ability) did not affect the test results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System-level Evaluation</head><p>We first present the system-level evaluation results for the four translation methods. <ref type="figure" target="#fig_0">Figure 2</ref> shows the min/max and the quartiles of the correct an- swer rates (CARs) for the 40 problems translated by each system. The averages of the correct an- swer rates are 0.524, 0.696, 0.693, and 0.875 for each translation system G, Y , H S , and H O , re-  <ref type="table">Table 1</ref> lists the five automatic evaluation scores for each translation method measured against the two reference translation sets. The averages of the CARs over the 40 problems are also listed in the bottom row of the table. There are several notice- able facts. First, despite the significantly better av- erage CAR for Y over G, BLEU, BLEU+1, and TER prefer G to Y . Second, while the average CARs for Y and H S are almost equal, there are large differences between their automatic evalua- tion scores across all metrics. Third, a comparison of the corresponding automatic evaluation scores using Ref S and Ref O reveals that G, Y , and H S are more similar to the manual translations that were produced without referring to the contexts of the individual sentences than those produced tak- ing the contexts into consideration. This is not surprising. However, the large difference in the correct answer rates for H S and H O suggests that ignorance of the context in the current MT sys- tems severely degrades the comprehensibility of the translations of texts like daily conversations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Agreement between Intrinsic and Extrinsic Evaluation Metrics</head><p>We examined how often an intrinsic metric cor- rectly predicts the difference of the subjects' test performance on a problem. Specifically, for two translation methods A and B, we say an intrinsic metric M agrees with the CAR by the subjects on problem p i iff metric M scores A's translation of</p><formula xml:id="formula_1">p i (= A(p i )) better than B's translation (= B(p i ))</formula><p>and the CAR is higher on A(p i ) than on B(p i ).  between two intrinsic metrics is defined similarly. <ref type="figure">Figure 3</ref> shows the rates of agreements between the automatic metrics and CARs and between the human evaluation and CARs. As <ref type="figure">Figure 3</ref> shows, all the agreement rates between the automatic met- rics and CARs were less than 0.65. When consid- ering a random baseline of 0.5, we may conclude that the automatic metrics are not very good pre- dictors of the CARs. This is unfortunate since the CARs directly indicate the comprehensibility of the translated dialogues. The disagreements can- not be attributed only to the unreliability of au- tomatic metrics on short translations. <ref type="figure">Figure 4</ref> shows the rate of agreements between the auto- matic metrics and the human evaluation. <ref type="figure">As Fig- ure 4</ref> shows, BLEU, BLEU+1, and TER agree with human evaluation on nearly 90% of the prob- lems when comparing Y and H S .</p><p>The human evaluation agrees with the CAR slightly better than the automatic metrics. How- ever, the agreement rates are still less than 0.7 for all pairs of compared systems. These findings sug- gest that there is an inherent discrepancy between the assessment of the overall translation quality of a problem and the CAR. It is presumably because the CAR can be critically damaged by a subtle translation mistake that spoils a coherent under- standing of a dialogue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>We have presented the results of an experiment, in which machine-and human-translated second lan- guage proficiency test (SLPT) problems were used for extrinsic evaluation of the translation quality. Comparison of four translation methods revealed, most notably, the crucial importance of consider- ing contexts of individual sentences in translating dialogues. The analysis on the experimental re- sults suggests that the extrinsic evaluation based on SLPT problems captures a different dimension of translation quality than the manual/automatic intrinsic metrics. The robustness against the het- erogeneity of human subjects and the abundance of existing SLPT problems enable easy adaption of the proposed evaluation scheme in addition to the traditional intrinsic evaluations. Our future work includes experiments with other types of SLPT problems that focus on different aspects of translation quality and language understanding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Boxplots of Correct Answer Rates for 40 Problems</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Agreement Rates between Intrinsic Evaluation Metrics and Correct Answer Rate</figDesc></figure>

			<note place="foot" n="1"> The two MT results were produced on June 11th, 2014. 2 https://translate.google.co.jp/?hl=ja 3 http://honyaku.yahoo.co.jp/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are grateful to all the participants in the experiments for their time and patience and also grateful to Yoyogi Seminar for their allowance to use their problems in the experi-ments. This study is conducted as a part of the Todai Robot Project (http://21robot.org/ ?lang=english).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cern`y, and Ondřej Bojar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Berka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martiň</forename><surname>Cern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quiz-based evaluation of machine translation</title>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Re-evaluation the role of bleu in machine translation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">F</forename><surname>Zaidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="17" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic evaluation of translation quality for distant language pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-2010)</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP-2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="944" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measuring translation quality by testing english speakers with a new defense language proficiency test for arabic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Granoien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 International Conference on Intelligence Analysis</title>
		<meeting>the 2005 International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ilr-based mt comprehension test with multi-level questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussny</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Jairam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Emonts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">Short Papers</title>
		<imprint>
			<biblScope unit="page" from="77" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Task-based mt evaluation: From who/when/where extraction to event understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal</forename><surname>Laoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calandra</forename><forename type="middle">R</forename><surname>Tate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC2006)</title>
		<meeting>the Fifth International Conference on Language Resources and Evaluation (LREC2006)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2048" to="2053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Orange: A method for evaluating automatic evaluation metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics (COLING-2004)</title>
		<meeting>the 20th International Conference on Computational Linguistics (COLING-2004)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="501" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002)</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparing intrinsic and extrinsic evaluation of mt output in a dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><forename type="middle">H</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ielka</forename><surname>Van Der Sluis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saturnino</forename><surname>Luz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation (IWSLT-2010)</title>
		<meeting>the International Workshop on Spoken Language Translation (IWSLT-2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="329" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwaltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas (AMTA-2006)</title>
		<meeting>Association for Machine Translation in the Americas (AMTA-2006)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Taskbased evaluation of machine translation (mt) engines: Measuring how well people extract who, when, where-type elements in mt output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calandra</forename><forename type="middle">R</forename><surname>Tate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 11th Annual Conference of the European Association for Machine Translation (EAMT-2006)</title>
		<meeting>11th Annual Conference of the European Association for Machine Translation (EAMT-2006)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
