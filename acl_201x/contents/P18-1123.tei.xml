<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exemplar Encoder-Decoder for Neural Conversation Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Pandey</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI New Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Contractor</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI New Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI New Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachindra</forename><surname>Joshi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research AI New Delhi</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exemplar Encoder-Decoder for Neural Conversation Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1329" to="1338"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1329</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses. Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model. The retrieved responses are used to create exemplar vectors that are used by the decoder to generate the response. The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context. We present detailed experiments on two large data sets and find that our method out-performs state of the art sequence to sequence generative models on several recently proposed evaluation metrics. We also observe that the responses generated by the proposed EED model are more informative and diverse compared to existing state-of-the-art method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the availability of large datasets and the recent progress made by neural meth- ods, variants of sequence to sequence learning (seq2seq) ( <ref type="bibr" target="#b24">Sutskever et al., 2014</ref>) architectures have been successfully applied for building con- versational systems <ref type="bibr" target="#b20">(Serban et al., , 2017b</ref>). However, despite these methods being the state- of-the art frameworks for conversation generation, they suffer from problems such as lack of diver- sity in responses and generation of short, repetitive and uninteresting responses ( <ref type="bibr" target="#b9">Liu et al., 2016;</ref><ref type="bibr" target="#b20">Serban et al., , 2017b</ref>. A large body of recent literature has focused on overcoming such chal- lenges ( <ref type="bibr" target="#b7">Li et al., 2016a;</ref><ref type="bibr" target="#b11">Lowe et al., 2017)</ref>.</p><p>In part, such problems arise as all information required to generate responses needs to be cap- tured as part of the model parameters learnt from the training data. These model parameters alone may not be sufficient for generating natural con- versations. Therefore, despite providing enormous amount of data, neural generative systems have been found to be ineffective for use in real world applications ( <ref type="bibr" target="#b9">Liu et al., 2016)</ref>.</p><p>In this paper, we focus our attention on closed domain conversations. A characteristic feature of such conversations is that over a period of time, some conversation contexts 1 are likely to have oc- curred previously ( <ref type="bibr" target="#b13">Lu et al., 2017b</ref>). For instance, <ref type="table">Table 1</ref> shows some contexts from the Ubuntu dia- log corpus. Each row presents an input dialog con- text with its corresponding gold response followed by a similar context and response seen in train- ing data -as can be seen, contexts for "installing dms", "sharing files", "blocking ufw ports" have all occurred in training data. We hypothesize that being able to refer to training responses for pre- viously seen similar contexts could be a helpful signal to use while generating responses.</p><p>In order to exploit this aspect of closed do- main conversations we build our neural encoder- decoder architecture called the Exemplar Encoder Decoder (EED), that learns to generate a response for a given context by exploiting similar contexts from training conversations. Thus, instead of hav- ing the seq2seq model learn patterns of language only from aligned parallel corpora, we assist the model by providing it closely related (similar) samples from the training data that it can refer to while generating text.</p><p>Specifically, given a context c, we retrieve a set Input Context Gold Response Similar Context in training data Associated Response U1 if you want autologin install a dm of some sort lightdm, gdm, kdm, xdm, slim, etc. U1 if you're running a dm, it will probably restart x e.g. gdm, kdm, xdm U2 what is a dm U2 whats a dm? <ref type="table" target="#tab_12">U1  is it possible to share a file in one user's  home directory with another user?   so chmod 777  should do it, right?  U1  howto set right permission for my home  directory?</ref> chmod and chown? u mean that sintax U2 if you set permissions (to 'group','other' or with an acl) U2 but which is the syntax to set permission for my user in my home user directory ? of context-response pairs (c (k) , r (k) ), 1 ≤ k ≤ K using an inverted index of training data. We create an exemplar vector e (k) by encoding the response r (k) (also referred to as exemplar response) along with an encoded representation of the current con- text c. We then learn the importance of each ex- emplar vector e (k) based on the likelihood of it be- ing able to generate the ground truth response. We believe that e (k) may contain information that is helpful in generating the response. <ref type="table">Table 1</ref> high- lights the words in exemplar responses that appear in the ground truth response as well.</p><p>Contributions: We present a novel Exemplar Encoder-Decoder (EED) architecture that makes use of similar conversations, fetched from an index of training data. The retrieved context- response pairs are used to create exemplar vec- tors which are used by the decoder in the EED model, to learn the importance of train- ing context-response pairs, while generating re- sponses. We present detailed experiments on the publicly benchmarked Ubuntu dialog corpus data set ( <ref type="bibr" target="#b10">Lowe et al., 2015)</ref> as well a large collection of more than 127,000 technical support conversa- tions. We compare the performance of the EED model with the existing state of the art generative models such as HRED ( ) and VHRED ( <ref type="bibr" target="#b20">Serban et al., 2017b</ref>). We find that our model out-performs these models on a wide vari- ety of metrics such as the recently proposed Activ- ity Entity metrics ( <ref type="bibr" target="#b19">Serban et al., 2017a</ref>) as well as Embedding-based metrics ( <ref type="bibr" target="#b10">Lowe et al., 2015</ref>). In addition, we present qualitative insights into our results and we find that exemplar based responses are more informative and diverse. The rest of the paper is organized as follows. Section 2 briefly describes the recent works in neu- ral dialogue generation The details of the proposed EED model for dialogue generation are described in detail in Section 3. In Section 4, we describe the datasets as well as the details of the models used during training. We present quantitative and qualitative results of EED model in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we compare our work against other data-driven end-to-end conversation models. End- to-end conversation models can be further classi- fied into two broad categories -generation based models and retrieval based models.</p><p>Generation based models cast the problem of dialogue generation as a sequence to sequence learning problem. Initial works treat the entire context as a single long sentence and learn an encoder-decoder framework to generate response word by word ( <ref type="bibr" target="#b21">Shang et al., 2015;</ref><ref type="bibr" target="#b26">Vinyals and Le, 2015</ref>). This was followed by work that mod- els context better by breaking it into conversation history and last utterance ( <ref type="bibr" target="#b23">Sordoni et al., 2015b</ref>). Context was further modeled effectively by us- ing a hierarchical encoder decoder (HRED) model which first learns a vector representation of each utterance and then combines these representations to learn vector representation of context ( . Later, an alternative hierarchical model called VHRED ( <ref type="bibr" target="#b20">Serban et al., 2017b</ref>) was proposed, where generated responses were condi- tioned on latent variables. This leads to more in-formative responses and adds diversity to response generation. Models that explicitly incorporate di- versity in response generation have also been stud- ied in literature ( <ref type="bibr" target="#b8">Li et al., 2016b;</ref><ref type="bibr" target="#b25">Vijayakumar et al., 2016;</ref><ref type="bibr" target="#b0">Cao and Clark, 2017;</ref>.</p><p>Our work differs from the above as none of these above approaches utilize similar conversa- tion contexts observed in the training data explic- itly.</p><p>Retrieval based models on the other hand treat the conversation context as a query and obtain a set of responses using information retrieval (IR) techniques from the conversation logs ( <ref type="bibr" target="#b5">Ji et al., 2014</ref>). There has been further work where the responses are further ranked using a deep learn- ing based model ( <ref type="bibr">Yan et al., 2016a,b;</ref><ref type="bibr" target="#b16">Qiu et al., 2017)</ref>. On the other hand of the spectrum, end- to-end deep learning based rankers have also been employed to generate responses ( <ref type="bibr" target="#b27">Wu et al., 2017;</ref><ref type="bibr" target="#b3">Henderson et al., 2017)</ref>. Recently a framework has also been proposed that uses a discriminative di- alog network that ranks the candidate responses received from a response generator network and trains both the networks in an end to end manner ( <ref type="bibr" target="#b12">Lu et al., 2017a)</ref>.</p><p>In contrast to the above models, we use the in- put contexts as well as the retrieved responses for generating the final responses. Contemporaneous to our work, a generative model for machine trans- lation that employs retrieved translation pairs has also been proposed ( <ref type="bibr" target="#b2">Gu et al., 2017)</ref>. We note that while the underlying premise of both the papers remains the same, the difference lies in the mech- anism of incorporating the retrieved data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Exemplar Encoder Decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>A conversation consists of a sequence of utter- ances. At a given point in the conversation, the ut- terances expressed prior to it are jointly referred to as the context. The utterance that immediately fol- lows the context is referred to as the response. As discussed in Section 1, given a conversational con- text, we wish to to generate a response by utiliz- ing similar context-response pairs from the train- ing data. We retrieve a set of K exemplar context- response pairs from an inverted index created us- ing the training data in an off-line manner. The input and the retrieved context-response pairs are then fed to the Exemplar Encoder Decoder (EED) network. A schematic illustration of the EED net- work is presented in <ref type="figure" target="#fig_0">Figure 1</ref>. The EED encoder combines the input context and the retrieved re- sponses to create a set of exemplar vectors. The EED decoder then uses the exemplar vectors based on the similarity between the input context and re- trieved contexts to generate a response. We now provide details of each of these modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrieval of Similar Context-Response Pairs</head><p>Given a large collection of conversations as (context, response) pairs, we index each re- sponse and its corresponding context in tf − idf vector space. We further extract the last turn of a conversation and index it as an additional attribute of the context-response document pairs so as to al- low directed queries based on it.</p><p>Given an input context c, we construct a query that weighs the last utterance in the context twice as much as the rest of the context and use it to retrieve the top-k similar context-response pairs from the index based on a BM25 ( <ref type="bibr" target="#b17">Robertson et al., 2009</ref>) retrieval model. These retrieved pairs form our exemplar context-response pairs</p><formula xml:id="formula_0">(c (k) , r (k) ), 1 ≤ k ≤ K.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Exemplar Encoder Network</head><p>Given the exemplar pairs (c (k) , r (k) ), 1 ≤ k ≤ K and an input context-response pair (c, r), we feed the input context c and the exemplar contexts c (1) , . . . , c <ref type="bibr">(K)</ref> through an encoder to generate the embeddings as given below:</p><formula xml:id="formula_1">c e = Encode c (c) c (k) e = Encode c (c (k) ), 1 ≤ k ≤ K</formula><p>Note that we do not constrain our choice of en- coder and that any parametrized differentiable ar- chitecture can be used as the encoder to gener- ate the above embeddings. Similarly, we feed the exemplar responses r (1) , . . . , r (K) through a re- sponse encoder to generate response embeddings r</p><formula xml:id="formula_2">(1) e , . . . , r (K) e , that is, r (k) e = Encode r (r (k) ), 1 ≤ k ≤ K<label>(1)</label></formula><p>Next, we concatenate the exemplar response en- coding r <ref type="bibr">(k)</ref> e with an encoded representation of cur- rent context c e as shown in equation 2 to create the exemplar vector e (k) . This allows us to include in- </p><formula xml:id="formula_3">(c (k) , r (k) ), 1 ≤ k ≤ K.</formula><p>formation about similar responses along with the encoded input context representation.</p><formula xml:id="formula_4">e (k) = [c e ; r (k) e ], 1 ≤ k ≤ K<label>(2)</label></formula><p>The exemplar vectors e (k) , 1 ≤ k ≤ K are fur- ther used by the decoder for generating the ground truth response as described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Exemplar Decoder Network</head><p>Recall that we want the exemplar responses to help generate the responses based on how similar the corresponding contexts are with the input context. More similar an exemplar context is to the input context, higher should be its effect in generating the response. To this end, we compute the similar- ity scores s (k) , 1 ≤ k ≤ K using the encodings computed in Section 3.3 as shown below.</p><formula xml:id="formula_5">s (k) = exp(c T e c (k) e ) K l=1 exp(c T e c (l) e )<label>(3)</label></formula><p>Next, each exemplar vector e (k) computed in Section 3.3, is fed to a decoder, where the decoder is responsible for predicting the ground truth re- sponse from the exemplar vector. Let p dec (r|e <ref type="bibr">(k)</ref> ) be the distribution of generating the ground truth response given the exemplar embedding. The ob- jective function to be maximized, is expressed as a function of the scores s (k) , the decoding distribu- tion p dec and the exemplar vectors e (k) as shown below:</p><formula xml:id="formula_6">ll = K k=1 s (k) log p dec (r|e (k) )<label>(4)</label></formula><p>Note that we weigh the contribution of each exem- plar vector to the final objective based on how sim- ilar the corresponding context is to the input con- text. Moreover, the similarities are differentiable function of the input and hence, trainable by back propagation. The model should learn to assign higher similarities to the exemplar contexts, whose responses are helpful for generating the correct re- sponse.</p><p>The model description uses encoder and de- coder networks that can be implemented using any differentiable parametrized architecture. We dis- cuss our choices for the encoders and decoder in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The Encoders and Decoder</head><p>In this section, we discuss the various encoders and the decoder used by our model. The con- versation context consists of an ordered sequence of utterances and each utterance can be further viewed as a sequence of words. Thus, con- text can be viewed as having multiple levels of hierarchies-at the word level and then at the ut- terance (sentence) level. We use a hierarchical re- current encoder-popularly employed as part of the HRED framework for generating responses and query suggestions ( <ref type="bibr" target="#b22">Sordoni et al., 2015a;</ref><ref type="bibr" target="#b20">Serban et al., , 2017b</ref>). The word-level encoder encodes the vector representations of words of an utterance to an utterance vector. Finally, the utterance-level encoder encodes the utterance vec- tors to a context vector.</p><p>Let (u 1 , . . . , u N ) be the utterances present in the context. Furthermore, let (w n1 , . . . , w nMn ) be the words present in the n th utterance for 1 ≤ n ≤ N . For each word in the utterance, we retrieve its corresponding embedding from an embedding matrix. The word embedding for w nm will be de- noted as w enm . The encoding of the n th utterance can be computed iteratively as follows:</p><formula xml:id="formula_7">h nm = f 1 (h nm−1 , w enm ), 1 ≤ m ≤ M n (5)</formula><p>We use an LSTM <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber, 1997</ref>) to model the above equation. The last hid- den state h nMn is referred to as the utterance en- coding and will be denoted as h n .</p><p>The utterance-level encoder takes the utterance encodings h 1 , . . . , h N as input and generates the encoding for the context as follows:</p><formula xml:id="formula_8">c en = f 2 (c en−1 , h n ), 1 ≤ n ≤ N<label>(6)</label></formula><p>Again, we use an LSTM to model the above equa- tion. The last hidden state c eN is referred to as the context embedding and is denoted as c e .</p><p>A single level LSTM is used for embedding the response. In particular, let (w 1 , . . . , w M ) be the sequence of words present in the response. For each word w, we retrieve the corresponding word embedding w e from a word embedding matrix. The response embedding is computed from the word embeddings iteratively as follows:</p><formula xml:id="formula_9">r em = g(r em−1 , w em ), 1 ≤ m ≤ M<label>(7)</label></formula><p>Again, we use an LSTM to model the above equa- tion. The last hidden state r em is referred to as the response embedding and is denoted as r e .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Tech Support Dataset</head><p>We also conduct our experiments on a large tech- nical support dataset with more than 127K con- versations. We will refer to this dataset as Tech Support dataset in the rest of the paper. Tech Support dataset contains conversations pertaining to an employee seeking assistance from an agent (technical support) -to resolve problems such as password reset, software installation/licensing, and wireless access. In contrast to Ubuntu dataset, this dataset has clearly two distinct users -em- ployee and agent. In our experiments we model the agent responses only. For each conversation in the tech support data, we sample context and response pairs to create a dataset similar to the Ubuntu dataset format. Note that multiple context-response pairs can be gen- erated from a single conversation. For each con- versation, we sample 25% of the possible context- response pairs. We create validation pairs by selecting 5000 conversations randomly and sam- pling context response pairs). Similarly, we create test pairs from a different subset of 5000 conver- sations. The remaining conversations are used to create training context-response pairs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model and Training Details</head><p>The EED and HRED models were implemented using the PyTorch framework ( <ref type="bibr" target="#b15">Paszke et al., 2017)</ref>. We initialize the word embedding matrix as well as the weights of context and response en- coders from the standard normal distribution with mean 0 and variance 0.01. The biases of the en- coders and decoder are initialized with 0. The word embedding matrix is shared by the context and response encoders. For Ubuntu dataset, we use a word embedding size of 600, whereas the size of the hidden layers of the LSTMs in context and response encoders and the decoder is fixed at 1200. For Tech support dataset, we use a word em- bedding size of 128. Furthermore, the size of the hidden layers of the multiple LSTMs in context and response encoders and the decoder is fixed at 256. A smaller embedding size was chosen for the Tech Support dataset since we observed much less diversity in the responses of the Tech Support dataset as compared to Ubuntu dataset. Two different encoders are used for encoding the input context (not shown in <ref type="figure" target="#fig_0">Figure 1</ref> for sim- plicity). The output of the first context encoder is concatenated with the exemplar response vectors to generate exemplar vectors as detailed in Sec- tion 3.3. The output of the second context encoder is used to compute the scoring function as detailed in Section 3.4. For each input context, we re- trieve 5 similar context-response pairs for Ubuntu dataset and 3 context-response pairs for Tech sup- port dataset using the tf-idf mechanism discussed in Section 3.2.</p><p>We use the Adam optimizer ( <ref type="bibr" target="#b6">Kingma and Ba, 2014</ref>) with a learning rate of 1e − 4 for training the model. A batch size of 20 samples was used during training. In order to prevent overfitting, we use early stopping with log-likelihood on valida- tion set as the stopping criteria. In order to gen- erate the samples using the proposed EED model, we identify the exemplar context that is most sim- ilar to the input context based on the learnt scor- ing function discussed in Section 3.4. The cor- responding exemplar vector is fed to the decoder to generate the response. The samples are gener- ated using a beam search with width 5. The av- erage per-word log-likelihood is used to score the beams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results &amp; Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Activity and Entity Metrics</head><p>A traditional and popular metric used for compar- ing a generated sentence with a ground truth sen- tence is BLEU ( <ref type="bibr" target="#b14">Papineni et al., 2002</ref>) and is fre- quently used to evaluate machine translation. The metric has also been applied to compute scores for predicted responses in conversations, but it has been found to be less indicative of actual perfor- mance ( <ref type="bibr" target="#b9">Liu et al., 2016;</ref><ref type="bibr" target="#b22">Sordoni et al., 2015a;</ref><ref type="bibr" target="#b19">Serban et al., 2017a</ref>), as it is extremely sensitive to the exact words in the ground truth response, and gives equal importance to stop words/phrases and informative words. <ref type="bibr" target="#b19">Serban et al. (2017a)</ref> recently proposed a new set of metrics for evaluating dialogue responses for the Ubuntu corpus. It is important to highlight that these metrics have been specifically designed for the Ubuntu corpus and evaluate a generated response with the ground truth response by com- paring the coarse level representation of an utter- ance (such as entities, activities, Ubuntu OS com- mands). Here is a brief description of each metric:</p><p>• Activity: Activity metric compares the ac- tivities present in a predicted response with the ground truth response. Activity can be thought of as a verb. Thus, all the verbs in a response are mapped to a set of manually identified list of 192 verbs.</p><p>• Entity: This compares the technical entities that overlap with the ground truth response. A total of 3115 technical entities is identified using public resources such as Debian pack- age manager APT.  • Tense: This measure compares the time tense of ground truth with predicted response.</p><p>• Cmd: This metric computes accuracy by comparing commands identified in ground truth utterance with a predicted response.  <ref type="figure" target="#fig_0">(Serban et al., 2017a)</ref>, as MRNN explic- itly utilizes the activities and entities during the generation process. In contrast, the proposed EED model and the other models used for comparison are agnostic to the activity and entity information. We use the standard script 3 to compute the met- rics.</p><p>The EED model scores better than generative models on almost all of the metrics, indicating that we generate more informative responses than other state-of-the-art generative based approaches for Ubuntu corpus. The results show that re- sponses associated with similar contexts may con- tain the activities and entities present in the ground truth response, and thus help in response genera- tion. This is discussed further in Section 5.2. Ad- ditionally, we compared our proposed EED with a retrieval only baseline. The retrieval baseline achieves an activity F1 score of 4.23 and entity F1 score of 2.72 compared to 4.87 and 2.99 re- spectively achieved by our method on the Ubuntu corpus.</p><p>The Tech Support dataset is not evaluated using the above metrics, since activity and entity infor- mation is not available for this dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Embedding Metrics</head><p>Embedding metrics ( <ref type="bibr" target="#b11">Lowe et al., 2017)</ref> were pro- posed as an alternative to word by word compar- ison metrics such as BLEU. We use pre-trained Google news word embeddings 4 similar to <ref type="bibr" target="#b20">Serban et al. (2017b)</ref>, for easy reproducibility as these metrics are sensitive to the word embeddings used. The three metrics of interest utilize the word vec- tors in ground truth response and a predicted re- sponse and are discussed below:</p><p>• Average: Average word embedding vec- tors are computed for the candidate response and ground truth. The cosine similarity is computed between these averaged embed- dings. High similarity gives as indication that ground truth and predicted response have similar words.</p><p>• Greedy: Greedy matching score finds the most similar word in predicted response to ground truth response using cosine similarity.</p><p>• Extrema: Vector extrema score computes the maximum or minimum value of each dimen- sion of word vectors in candidate response and ground truth.</p><p>Of these, the embedding average metric is the most reflective of performance for our setup. The extrema representation, for instance, is very sensi- tive to text length and becomes ineffective beyond single length sentences( <ref type="bibr" target="#b1">Forgues et al., 2014</ref>   nope not yet. grub install cd is not working GT: no i did not.</p><p>3 C: any ideas on my mplayer problem ? → reinstall. apt-get purge mplayer then reinstall R: but half my system depends on this package ...</p><p>HRED: I did that , but it didn't work VHRED: I dont know how to do that EED: does n t work. package does n t work GT: that does n't seem as a solution 4 C: any of you guys in here know how to take a package on launchpad and recompile it and submit to your own ppa ? https : //launchpad.net/ mojocode/+archive /ppa ¡ ¡ i 'm trying to take the nginx package and install the rtmp module → could n't you download the source and just commit it to your own ? R: i need prop . drivers HRED: I don't know how to do that , I just want to know if there is a way to do it VHRED: no , I just want to make it so I can use it EED: i need to get the source package. package manager is not available GT: so i figured how how to copy into my private ppa . any idea how to download those source files maybe and make modifications to resubmit and recompile ? https : //launchpad.net / blendedbyus/+archive/master/+packages  own implementation of HRED. <ref type="table" target="#tab_9">Table 5</ref> compares our model with HRED, and depicts that our model scores better on all metrics for Technical Support dataset, and on majority of the metrics for Ubuntu dataset.</p><p>We note that the improvement achieved by the EED model on activity and entity metrics are much more significant than those on embedding metrics. This suggests that the EED model is bet- ter able to capture the specific information (objects and actions) present in the conversations. Finally, we evaluate the diversity of the gener- ated responses for EED against HRED by count- ing the number of unique tokens, token-pairs and token-triplets present in the generated responses on Ubuntu and Tech Support dataset. The results are shown in <ref type="table" target="#tab_10">Table 6</ref>. As can be observed, the re- sponses in EED have a larger number of distinct tokens, token-pairs and token-triplets than HRED, and hence, are arguably more diverse. <ref type="table" target="#tab_12">Table 7</ref> presents the responses generated by HRED, VHRED and the proposed EED for a few selected contexts along with the corresponding similar exemplar responses. As can be observed from the table, the responses generated by EED tend to be more specific to the input context as compared to the responses of HRED and VHRED. For example, in conversations 1 and 2 we find that both HRED and VHRED generate simple generic responses whereas EED generates responses with additional information such as the type of disk par- tition used or a command not working. This is also confirmed by the quantitative results obtained using activity and entity metrics in the previous section. We further observe that the exemplar re- sponses contain informative words that are utilized by the EED model for generating the responses as highlighted in <ref type="table" target="#tab_12">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we propose a deep learning method, Exemplar Encoder Decoder (EED), that given a conversation context uses similar contexts and cor- responding responses from training data for gen- erating a response. We show that by utilizing this information the system is able to outperform state of the art generative models on publicly available Ubuntu dataset. We further show improvements achieved by the proposed method on a large col- lection of technical support conversations.</p><p>While in this work, we apply the exemplar en- coder decoder network on conversational task, the method is generic and could be used with other tasks such as question answering and machine translation. In our future work we plan to extend the proposed method to these other applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A schematic illustration of the EED network. The input context-response pair is (c, r), while the exemplar context-response pairs are (c (k) , r (k) ), 1 ≤ k ≤ K.</figDesc><graphic url="image-1.png" coords="4,94.68,62.81,408.18,262.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3</head><label></label><figDesc>https://github.com/julianser/Ubuntu-Multiresolution- Tools/blob/master/ActEntRepresentation/eval file.sh</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Dataset statistics for Ubuntu Dialog Cor-
pus v2.0 (Lowe et al., 2015), where |V | represents 
the size of vocabulary. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 de- picts some statistics for this dataset:</head><label>3</label><figDesc></figDesc><table>Size 

Conversations 
127,466 

Training Pairs 
204,808 
Validation Pairs 
8,738 
Test Pairs 
8,756 

|V | 
293,494 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Dataset statistics for Tech Support 
dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Activity &amp; Entity metrics for the Ubuntu corpus. LSTM*, HRED* &amp; VHRED* as reported by 
Serban et al. (2017a). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 compares</head><label>4</label><figDesc></figDesc><table>our model with other re-
cent generative models (Serban et al., 2017a) -
LSTM (Shang et al., 2015), HRED (Serban et al., 
2016) &amp; VHRED (Serban et al., 2017b).We do not 
compare our model with Multi-Resolution RNN 
(MRNN) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 5 : Embedding Metrics (Lowe et al., 2015) for Ubuntu and Technical Support Corpus.</head><label>5</label><figDesc></figDesc><table>Ubuntu 
Tech Support 

Model distinct-1 distinct-2 distinct-3 
distinct-1 distinct-2 distinct-3 

HRED 1154 
8668 
22417 
645 
3510 
6365 
EED 
1173 
9572 
29447 
732 
4149 
7877 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>The number of unique tokens, token-pairs and token-triplets for Ubuntu and Technical Support 
Corpus. 

Input Context (C) and exemplar response (R) 
Generated/Ground Truth Responses 

1 

C: i want to check ntfs file systems. is fsck ok with ntfs. 
→ no , you 'd need to use chkdsk from windows. → 
R: i already symlinked ntfsfix to fsck. ntfs and am 
scanning the unmounted devices 

HRED: i know , but i dont know how to do that 
VHRED: how can i do that ? 
EED: i am trying to use ntfs 3g i have a windows partition. 
GT: i dont have windows installed 

2 

C: anyone help me ? → seems that its detecting grub , 
have you tried checking continue without installing 
grub ? 
R: try the live cd... you ca n't boot into linux 
unless grub install 

HRED: yes i did , but i have no idea how to fix it 
VHRED: yes i did. i dont know how to do it 
EED: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Contexts, exemplar responses and responses generated by HRED, VHRED and the proposed 
EED model. We use the published responses for HRED and VHRED. GT indicates the ground truth 
response. The change of turn is indicated by →. The highlighted words in bold are common between 
the exemplar response and the response predicted by EED. 

</table></figure>

			<note place="foot" n="1"> We use the phrase &quot;dialog context&quot;, &quot;conversation context&quot; and &quot;context&quot; interchangeably throughout the paper.</note>

			<note place="foot" n="2"> https://github.com/rkadlec/ ubuntu-ranking-dataset-creator</note>

			<note place="foot" n="4"> GoogleNews-vectors-negative300.bin from https:// code.google.com/archive/p/word2vec/ 5 https://github.com/julianser/ hed-dlg-truncated/blob/master/ Evaluation/embedding_metrics.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to the anonymous reviewers for their comments that helped in improving the pa-per.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent variable dialogue models and their diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bootstrapping dialog systems with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Forgues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marie</forename><surname>Larchevêque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réal</forename><surname>Tremblay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nips, modern machine learning and natural language processing workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Search engine guided nonparametric neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07267</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficient natural language response suggestion for smart reply</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">László</forename><surname>Lukács</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balint</forename><surname>Miklos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<idno>abs/1705.00652</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An information retrieval approach to short text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongcheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1408.6988</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08023</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.08909</idno>
		<title level="m">The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training end-to-end dialogue systems with the ubuntu dialogue corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Thomas Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nissan</forename><surname>Pow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="65" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Best of both worlds: Transferring knowledge from discriminative learning to a generative visual dialog model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anitha</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A practical approach to dialogue response generation in closed domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Keung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaonan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Bhardwaj</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09439</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Alime chat: A sequence to sequence and rerank based chatbot engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Lin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiresolution recurrent neural networks: An application to dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3288" to="3294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3295" to="3301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A hierarchical recurrent encoderdecoder for generative context-aware query suggestion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Vahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><forename type="middle">Grue</forename><surname>Simonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="553" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A neural network approach to contextsensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1506.05869</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A sequential matching framework for multi-turn response selection in retrievalbased chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1710.11344</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning to respond with deep neural networks for retrievalbased human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<editor>SIGIR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">shall i be your chat companion?&quot;: Towards an online human-computer conversation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning discourse-level diversity for neural dialog models using conditional variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="654" to="664" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
