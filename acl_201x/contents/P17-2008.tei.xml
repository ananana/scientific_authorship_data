<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Uncertainty into Deep Learning for Spoken Language Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Ragni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><forename type="middle">M</forename><surname>Knill</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
						</author>
						<title level="a" type="main">Incorporating Uncertainty into Deep Learning for Spoken Language Assessment</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="45" to="50"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2008</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>There is a growing demand for automatic assessment of spoken English proficiency.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Systems for automatic assessment of spontaneous spoken language proficiency ( <ref type="figure" target="#fig_0">Fig. 1)</ref> are becom- ing increasingly important to meet the demand for English second language learning. Such systems are able to provide throughput and consistency which are unachievable with human examiners. This is a challenging task. There is a large vari- ation in the quality of spoken English across all proficiency levels. In addition, candidates of the same skill level will have different accents, voices, mispronunciations, and sentence construction er- rors. All of which are heavily influenced by the candidate's L1 language and compounded by ASR errors. It is therefore impossible in practice to ob- serve all these variants in training. At test time, the predicted grade's validity will decrease the more the candidate is mismatched to the data used to train the system. For deployment of these systems to high-stakes tests the performance on all candi- dates needs to be consistent and highly correlated with human graders. To achieve this it is impor- tant that these systems can detect outlier speakers who need to be examined by, for example, human graders.</p><p>Previously, separate models were used to fil- ter out "non-scorable" candidates ( <ref type="bibr" target="#b15">Yoon and Xie, 2014;</ref><ref type="bibr" target="#b16">Zechner et al., 2009;</ref><ref type="bibr" target="#b6">Higgins et al., 2011;</ref><ref type="bibr" target="#b14">Xie et al., 2012</ref>). However, such models reject candidates based on whether they can be scored at all, rather than an automatic grader's uncertainty 1 in its predictions. It was shown by van <ref type="bibr" target="#b13">Dalen et al. (2015)</ref> that Gaussian Process (GP) graders give state-of-the-art performance for automatic assess- ment and yield meaningful uncertainty estimates for rejection of candidates. There are, however, computational constraints on training set sizes for GPs. In contrast, Deep Neural Networks (DNNs) are able to scale to large data sets, but lack a na- tive measure of uncertainty. However, <ref type="bibr" target="#b4">Gal and Ghahramani (2016)</ref> have shown that Monte-Carlo Dropout (MCD) can be used to derive an uncer- tainty estimate for a DNN.</p><p>Alternatively, a Deep Density Network (DDN), which is a Mixture Density Network <ref type="bibr" target="#b1">(Bishop, 1994)</ref> with only one mixture component, may be used to yield a mean and variance corresponding to the predicted grade and the uncertainty in the prediction. Similar to GP and DNNs with MCD, a standard DDN provides an implicit modelling of uncertainty in its prediction. This implicit model may not be optimal for the task at hand. Hence, a novel approach to explicitly model uncertainty is proposed in which the DDN is trained in a multi- task fashion to model a low variance real data dis- tribution and a high variance artificial data dis- tribution which represents candidates with unseen characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prediction Uncertainty</head><p>The principled method for dealing with uncer- tainty in statistical modelling is the Bayesian ap- proach, where a conditional posterior distribution over grades, g, given inputs, x, and training data D = {ˆg{ˆg, ˆ x} is computed by marginalizing over all models:</p><formula xml:id="formula_0">p(g|x, D) = ∫ p(g|x, M)p(M|D)dM (1)</formula><p>where p(M|D) is a prior over a model given the data. Given the posterior, the predictive mean and the variance (uncertainty) can be computed using:</p><formula xml:id="formula_1">µ g (x) = ∫ p(g|x, D)gdg σ 2 g (x) = ∫ p(g|x, D)g 2 dg − µ 2 g (x)<label>(2)</label></formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gaussian Processes</head><p>Eq. 2, 3 can be analytically solved for a class of models called Gaussian Processes (GP) <ref type="bibr" target="#b11">(Rasmussen and Williams, 2006</ref>), a powerful non- parametric model for regression. The GP induces a conditional posterior in the form of a normal dis- tribution over grades g given an input x and train- ing data D:</p><formula xml:id="formula_2">p(g|x; D) = N (g; µ g (x|D), σ 2 g (x|D))<label>(4)</label></formula><p>With mean function µ g (x|D) and variance func- tion σ 2 g (x|D), which is a function of the similarity of an input x to the training data inputsˆxinputsˆ inputsˆx, where the similarity metric is defined by a covariance function k(., .). The nature of GP variance means that the model is uncertain in predictions for inputs far away from the training data, given appropriate choice of k(., .). Unfortunately, without sparsifi- cation approaches, the computational and mem- ory requirements of GPs become prohibitively ex- pensive for large data sets. Furthermore, GPs are known to scale poorly to higher dimensional fea- tures ( <ref type="bibr" target="#b11">Rasmussen and Williams, 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Monte-Carlo Dropout</head><p>Alternatively, a grader can be constructed using Deep Neural Networks (DNNs) which have a very flexible architecture and scale well to large data sets. DNNs, however, lack a native measure of un- certainty. Uncertainty estimates for DNNs can be computed using a Monte-Carlo ensemble approx- imation to Eq. 2, 3:</p><formula xml:id="formula_3">ˆ µ g (x) = 1 N N ∑ i=1 f (x; M (i) ) ˆ σ 2 g (x) = 1 N N ∑ i=1 ( f (x; M (i) ) ) 2 − ˆ µ 2 g (x)<label>(5)</label></formula><p>where there are N DNN models in the ensemble, M (i) is a DNN with a particular architecture and parameters sampled from p(M|D) using Monte Carlo Dropout (MCD) ( <ref type="bibr" target="#b12">Srivastava et al., 2014)</ref>, and f (x; M (i) ) are the DNN predictions. Recent work by <ref type="bibr" target="#b4">Gal and Ghahramani (2016)</ref> showed that MCD is equivalent to approximate variational in- ference in GPs, and can be used to yield mean- ingful uncertainty estimates for DNNs. Further- more, <ref type="bibr" target="#b4">Gal and Ghahramani (2016)</ref> show that dif- ferent choices of DNN activation functions corre- spond to different GP covariance functions. MCD uncertainty assumes that for inputs further from the training data, different subnets will produce in- creasingly differing outputs, leading to larger vari- ances. Unfortunately, it is difficult to know before- hand which activation functions accomplish this in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Density Networks</head><p>Instead of relying on a Monte Carlo approximation to Eq. 1, a DNN can be modified to produce a prediction of both a mean and a variance:</p><formula xml:id="formula_5">µ g (x) = f µ (x; M) σ 2 g (x) = f σ 2 (x; M)<label>(7)</label></formula><p>parametrising a normal distribution over grades conditioned on the input, similar to a GP. This architecture is a Deep Density Network (DDN), which is a Mixture Density Network (MDN) <ref type="bibr" target="#b1">(Bishop, 1994)</ref> with only one mixture component. DDNs are trained by maximizing the likelihood of the training data. The variance of the DDN rep- resents the natural spread of grades at a given in- put. This is an implicit measure of uncertainty, like GP and MCD variance, because it is learned au- tomatically as part of the model. However, this doesn't enforce higher variance further away from training points in DDNs. It is possible to explic- itly teach a DDN to predict a high or low vari- ance for inputs which are unlike or similar to the training data, respectively <ref type="figure" target="#fig_1">(Fig. 2)</ref>. This requires a novel training procedure. Two normal distribu- tions are constructed: a low-variance real (train- ing) data distribution p D and a high-variance arti- ficial data distribution p N , which models data out- side the real training data region. The DDN needs to model both distributions in a multi-task (MT) fashion. The loss function for training the DDN with explicitly specified uncertainty is the expec- tation over the training data of the KL divergence between the distribution it parametrizes and both the real and artificial data distributions:</p><formula xml:id="formula_7">L = E ˆ x [KL(p D ||p(g|ˆxg|ˆx; M)] + α · E ˜ x [KL(p N ||p(g|˜xg|˜x; M)]<label>(9)</label></formula><p>where α is the multi-task weight.</p><p>The DDN with explicit uncertainty is trained in a two stage fashion. First, a standard DDN M 0 is trained, then a DDN M is instantiated using the parameters of M 0 and trained in a multi-task fash- ion. The real data distribution p D is defined by M 0 (Eq. 7, 8). The artificial data distribution p N is con- structed by generating artificial inputs˜xinputs˜ inputs˜x and the associated mean and variance targets µ(˜ x), σ 2 (˜ x):</p><formula xml:id="formula_8">p N = N (g; f µ (˜ x; M 0 ), σ 2 (˜ x))<label>(10)</label></formula><p>The predictions of M 0 are used as the targets for µ(˜ x). </p><formula xml:id="formula_9">˜ x ∼ N (W z + µ, γΨ), z ∼ N (0, γI) (11)</formula><p>where W is the loading matrix, Ψ the diagonal residual noise variance, µ the mean, all derived fromˆxfromˆ fromˆx, and γ is used to control the distance of the generated data from the real training data re- gion. During training the artificial inputs are sam- pled from the FA model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><formula xml:id="formula_10">AUC RR = AUC var AUC max<label>(12)</label></formula><p>As previously stated, the operating scenario is to use a model's estimate of the uncertainty in its prediction to reject candidates to be assessed by human graders for high-stakes tests, maximiz- ing the increase in performance while rejecting the least number of candidates. The rejection process is illustrated using a rejection plot <ref type="figure" target="#fig_2">(Fig. 3)</ref>. As the rejection fraction is increased, model predictions are replaced with human scores in some particular order, increasing overall correlation with human graders. <ref type="figure" target="#fig_2">Fig. 3</ref> has 3 curves representing differ- ent orderings: expected random rejection, optimal rejection and model rejection. The expected ran- dom performance curve is a straight line from the base predictive performance to 1.0, representing rejection in a random order. The optimal rejec- tion curve is constructed by rejecting predictions in order of decreasing mean square error relative to human graders. A rejection curve derived from a model should sit between the random and op- timal curves. In this work, model rejection is in order of decreasing predicted variance.</p><p>The following metrics are used to assess and compare models: Pearson Correlation Coefficient (PCC) with human graders, the standard perfor- mance metric in assessment ( <ref type="bibr" target="#b16">Zechner et al., 2009;</ref><ref type="bibr" target="#b6">Higgins et al., 2011</ref>); 10% rejection PCC, which illustrates the predictive performance at a partic- ular operating point, i.e. rejecting 10% of candi- dates; and Area under a model's rejection curve (AUC) <ref type="figure" target="#fig_2">(Fig 3)</ref>. However, AUC is influenced by the base PCC of a model, making it difficult to compare the rejection performance. Thus, a metric independent of predictive performance is needed. The proposed metric, AUC RR (Eq. 12), is the ratio of the areas under the actual (AUC var ) and optimal (AUC max ) rejection curves relative to the random re- jection curve. Ratios of 1.0 and 0.0 correspond to perfect and random rejection, respectively.</p><p>All experiments were done using 33- dimensional pronunciation, fluency and acoustic features derived from audio and ASR transcrip- tions of responses to questions from the BULATS exam ( <ref type="bibr" target="#b2">Chambers and Ingham, 2011</ref>). The ASR system has a WER of 32% on a development set. The training and test sets have 4300 and 224 candidates, respectively. Each candidate provided a response to 21 questions, and the features used are aggregated over all 21 questions into a single feature vector. The test data was graded by expert graders at Cambridge English. These experts have inter-grader PCCs in the range 0.95-0.97. Candidates are equally distributed across CEFR grade levels <ref type="bibr">(Europe, 2001</ref>).</p><p>The input features where whitened by subtract- ing the mean and dividing by the standard devia- tion for each dimension computed on all training speakers. The Adam optimizer ( <ref type="bibr" target="#b7">Kingma and Ba, 2015)</ref>, dropout ( <ref type="bibr" target="#b12">Srivastava et al., 2014</ref>) regulariza- tion with a dropout keep probability of 0.6 and an exponentially decaying learning rate are used with decay factor of 0.86 per epoch, batch size 50. All networks have 2 hidden layers with 180 rectified linear units (ReLU) in each layer. DNN and DDN models were implemented in <ref type="bibr">Tensorflow (Abadi et al., 2015)</ref>. Models were initialized using the Xavier Initializer ( <ref type="bibr" target="#b5">Glorot and Bengio, 2010)</ref>. A validation set of 100 candidates was selected from the training data to tune the model and hyper- parameters. GPs were run using Scikit-Learn (Pe- dregosa et al., 2011) using a squared exponential covariance function. The Gaussian Process grader, GP, is a com- petitive baseline (Tab. 1). GP variance clearly yields uncertainty which is useful for rejection. A DNN with ReLU activation, MCD, achieves grad- ing performance similar to the GP. However, MCD fails to yield an informative uncertainty for rejec- tion, with performance barely above random. If the tanh activation function, MCD tanh , is used in- stead, then a DNN is able to provide a meaningful measure of uncertainty using MCD, at the cost of grading performance. It is likely that ReLU ac- tivations correspond to a GP covariance function which is not suited for rejection on this data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grader</head><p>The standard DDN has comparable grading per- formance to the GP and DNNs. AUC RR of the DDN is on par with the GP, but the 10% rejection PCC is lower, indicating that the DDN is not as effec- tive at rejecting the worst outlier candidates. The approach proposed in this work, a DDN trained in a multi-task fashion (DDN+MT), achieves signif- icantly higher rejection performance, resulting in the best AUC RR and 10% rejection PCC, showing its better capability to detect outlier candidates. Note, AUC reflects similar trends to AUC RR , but not as clearly, which is demonstrated by <ref type="figure" target="#fig_3">Fig. 4</ref>. The model was found to be insensitive to the choice of hyper-parameters α and γ, but λ needed to be set to produce target noise variances σ 2 (˜ x) larger than data variances σ 2 (ˆ x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>A novel method for explicitly training DDNs to yield uncertainty estimates is proposed. A DDN is a density estimator which is trained to model two distributions in a multi-task fashion (1) the low variance (uncertainty) true data distribution and (2) a generated high variance artificial data distribution. The model is trained by minimizing the KL divergence between the DDN and the true data distribution (1) and between the DDN and the artificial data distribution <ref type="bibr">(2)</ref>. The DDN should assign its prediction of low or high variance (un- certainty) if the input is similar or dissimilar to the true data respectively. The artificial data distribu- tion is given by a factor analysis model trained on the real data. During training the artificial data is sampled from this distribution.</p><p>This method outperforms GPs and Monte-Carlo Dropout in uncertainty based rejection for auto- matic assessment. However, the effect of the nature of artificial data on rejection performance should be further investigated and other data generation methods, such as Variational Auto- Encoders ( <ref type="bibr" target="#b8">Kingma and Welling, 2014)</ref>, and met- rics to assess similarity between artificial and real training data should be examined. The proposed approach must also be assessed on other tasks and datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Automatic Assessment System</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Desired variance characteristic</figDesc><graphic url="image-1.png" coords="3,72.00,354.48,227.78,98.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example Rejection Plot</figDesc><graphic url="image-2.png" coords="3,321.49,524.13,189.84,135.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Rejection Plots for models</figDesc><graphic url="image-5.png" coords="4,130.44,220.25,166.97,126.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>, they should represent candidates with unseen characteristics, such as L1, accent and proficiency. A simple approach to generating˜xgenerating˜ generating˜x is to use a Fac- tor Analysis (FA) (Murphy, 2012) model trained onˆxonˆ onˆx. The generative model of FA is:</figDesc><table>The target variance σ 2 (˜ x) should depend 
on the similarity of˜xof˜ of˜x to the training data. Here, 
this variance is modelled by the squared normal-
ized Euclidean distance from the mean ofˆxofˆ ofˆx, with 
a diagonal covariance matrix, scaled by a hyper-
parameter λ. The artificial inputs˜xinputs˜ inputs˜x need to be 
different to, but related to the real datâ 
x. Ide-
ally</table></figure>

			<note place="foot" n="1"> Uncertainty is used in the sense of the inverse of confidence to be consistent with Gal and Ghahramani (2016) and van Dalen et al. (2015).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was funded under the ALTA Insti-tute, University of Cambridge as well as the En-gineering and Physical Sciences Research Coun-cil. Thanks to Cambridge English, University of Cambridge, for support and access to the BULATS data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
		<title level="m">TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow</title>
		<meeting><address><addrLine>Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mixture density networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<idno>NCRG 4288</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Neural Computing Research Group, Department of Computer Science, Aston University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The BULATS online speaking test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Ingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research Notes</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="21" to="25" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Common European framework of reference for languages: Learning, teaching, assessment</title>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Cambridge, U.K</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Council of Europe ; Press Syndicate of the University of Cambridge</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML-16)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML-16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks. In Aistats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A three-stage approach to the automated scoring of spontaneous spoken responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="282" to="306" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations</title>
		<meeting>the 2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">Edward</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatically Grading Learners&apos; English Using a Gaussian Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rogier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><forename type="middle">M</forename><surname>Van Dalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">J F</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISCA Workshop on Speech and Language Technology for Education (SLaTE)</title>
		<meeting>the ISCA Workshop on Speech and Language Technology for Education (SLaTE)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring Content Features for Automated Speech Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keelan</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SimilarityBased Non-Scorable Response Detection for Automated Speech Scoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Su-Youn Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Ninth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic scoring of non-native spontaneous speech in tests of spoken english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="883" to="895" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Spoken Language Technology for Education Spoken Language</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
