<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dialogue Management based on Sentence Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendong</forename><surname>Ge</surname></persName>
							<email>wendong.ge@ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Automation</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<email>xubo@ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dialogue Management based on Sentence Clustering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="800" to="805"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Dialogue Management (DM) is a key issue in Spoken Dialogue System (SDS). Most of the existing studies on DM use Dialogue Act (DA) to represent semantic information of sentence, which might not represent the nuanced meaning sometimes. In this paper, we model DM based on sentence clusters which have more powerful semantic representation ability than DAs. Firstly, sentences are clustered not only based on the internal information such as words and sentence structures, but also based on the external information such as context in dialogue via Recurrent Neural Networks. Additionally, the DM problem is modeled as a Partially Observable Markov Decision Processes (POMD-P) with sentence clusters. Finally, experimental results illustrate that the proposed DM scheme is superior to the existing one.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialogue Management (DM) is an important is- sue in Spoken Dialogue Systems (SDS). <ref type="bibr" target="#b15">(Paek et al., 2008</ref>) Most of the existing studies on DM use the abstract semantic representation such as Dia- logue Act (DA) to represent the sentence intention. In ( <ref type="bibr" target="#b0">Bohus et al., 2009)</ref>, authors propose a plan- based, task-independent DM framework, called RavenClaw, which isolates the domain-specific as- pects of the dialogue control logic from domain- independent conversational skills. ( <ref type="bibr">Daubigney et al., 2010)</ref> proposes a Kalman Temporal Differ- ences based algorithm to learn efficiently in an off- policy manner a strategy for a large scale dialogue system. In ( <ref type="bibr" target="#b1">Emmanuel et al., 2013)</ref>, authors pro- pose a scheme to utilize a socially-based reward function for reinforcement learning and use it to fit the user adaptation issue for DM. ( <ref type="bibr" target="#b12">Young et al., 2013</ref>) provides an overview of the current state of the art in the development of POMDP-based spo- ken dialog systems. ( <ref type="bibr" target="#b16">Hao et al., 2014</ref>) presents a dialog manager based on a log-linear probabilistic model and uses context-free grammars to impart hierarchical structure to variables and features.</p><p>As we know, sentences in human-human dia- logues are extremely complicated. The sentences labeled with the same DA might contain differ- ent extra meanings. Thus, it is difficult for DA to represent the nuanced meaning of sentence in dialogue. In this paper, we propose a novel DM scheme based on sentence clustering. The contri- butions of this work are as follows.</p><p>• Semantic representation of sentence in dia- logue is defined as sentence cluster which could represent more nuanced semantic in- formation than DA. Sentence similarity for clustering is calculated via internal informa- tion such as words and sentence structures and external information such as the dis- tributed representation of sentence (vector) from Recurrent Neural Networks (RNN).</p><p>• The DM problem is modeled as a POMD- P, where state is defined as sequence of sen- tence clusters, reward is defined as slot-filling efficiency and sentence popularity, and state transition probability is calculated by the pre- diction model based on RNN, considering historical dialogue information sufficiently.</p><p>The rest of this paper is organized as follows. In Section 2, system model is introduced. Sec- tion 3 describes sentence clustering and prediction model based on RNN, and Section 4 models the DM problem as a POMDP. Extensive experimen- tal results are provided in Section 5 to illustrate the performance comparison, and Section 6 concludes this study. A1: I need to record the quantity of clients. B1 <ref type="table" target="#tab_0">: Perhaps 3 persons.    A2: please tell me the number of clients.  B2: Is it necessary?  A3: Yes, I need to record this.  B3: OK, 3 persons, maybe.</ref> request (client_quantity) I have to know how many persons will live in. It is necessary to record the number of clients. ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DA1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cluster 1 request (client_quantity) DA1</head><p>Do you mind telling me the quantity of clients? Please let me know how many persons will live in. ... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System Model</head><p>In this paper, we establish a SDS via human- human dialogue corpus, where sentence cluster rather than DA is utilized to represent sentence in- tention due to its ability of catching finer-grained semantic information. For example, <ref type="figure">Fig. 1</ref> shows some dialogue segments in hotel reserva- tion. Both A1 and A2 could be labeled with "re- quest (client quantity)", because the aims of them are requesting the quantity of clients. Howev- er, A1 has an extra meaning that it is a necessity for the reception to record the quantity of clients, while A2 not, which might lead to different evo- lutions of dialogues. Probably, we could add this necessity to the DA corresponding to A1 manual- ly, but it is infeasible for all the sentences to dis- tinguish the fine-grained semantic information by adding abstract symbol to DA. Thus, in this paper, we automatically cluster all the sentences in dia- logues, and utilize sentence clusters to represent sentence intentions, which has more powerful ca- pability to capture semantic information.</p><p>The SDS based on sentence clustering could be divided into offline stage and online stage, illus- trated in <ref type="figure">Fig. 2</ref>.</p><p>In offline stage: Sentence Clustering: The sentence similarity is calculated based on not only internal informa- tion such as words and sentence structure, but also external information such as the distributed rep- resentation from RNN. And then the sentences in dialogue corpus are clustered into different tiny groups, which will be discussed in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hello, I want to reserve a double room</head><p>Two double room. Your check-in time?</p><p>I need only one double room. User:</p><p>Machine:</p><p>User:</p><p>Machine:</p><p>Figure 3: an online example</p><p>Dialogue Policy Training: We label the dia- logues in corpus with the sentence clusters gen- erated in the previous process. Thus, these labeled dialogues could be utilized to train the optimal dia- logue policy with Reinforcement Learning, which will be introduced in section 4.</p><p>In online stage: Automatic Speech Recognition (ASR): When receiving user voice, ASR module transforms it in- to text <ref type="bibr" target="#b10">(Vinyals et al., 2012</ref>). As there might be ambiguity and errors in ASR, it is difficult to ob- tain the exact text corresponding to the input voice. Thus, the distribution over possible texts is used to represent the result of ASR.</p><p>Sentence Matching (SM): the function of SM is to establish a mapping from the distribution over possible texts to the distribution over possible sen- tence clusters.</p><p>DM: Based on the distribution of clusters, D- M model updates the belief state in POMDP and selects the optimal action, namely the optimal ma- chine sentence cluster, according to the dialogue policy. The relevant slots are also filled based on the user and machine sentence clusters.</p><p>Sentence Selection: This module selects the most appropriate sentence from the output ma- chine sentence cluster according to the user profile such as personality character ( <ref type="bibr" target="#b3">Ball et al., 2000</ref>).</p><p>Text To Speech (TTS): This model transforms the selected sentence text into the output voice as a response <ref type="bibr" target="#b4">(Zen et al., 2007)</ref>. <ref type="figure">Fig. 3</ref> is a human-machine dialogue example in online stage.</p><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4: Please tell me your phone number.</head><p>B4: Well, my cellphone is broken. A5: I am sorry. We need a phone number to contact you.</p><p>Could you please give me your friend's phone number? … ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6: Please give me your phone number.</head><p>B6: Unfortunately, I lost my cellphone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7: I am sorry. We need a phone number to contact you.</head><p>Could you please tell me your friend's phone number?</p><p>... <ref type="figure">Figure 4</ref>: an example of sentence similarity and sentence parsing might be used for this cal- culation.) Additionally, for DM-based sentence clustering, the sentences that we intend to put into the same cluster are not only the sentences with similar surface meaning, but also the sentences with similar intention (Semantics or Pragmatics), even if they might be different in surface meaning sometimes. For example, illustrated in <ref type="figure">Fig. 4, B4</ref> and B6 are different in surface meaning, but they have similar intention, namely he or she might not provide his or her phone number right now. Thus, in the sentence clustering for DM modeling, they should be clustered into the same group. It is diffi- cult to give a high similarity score between B4 and B6 only according to the internal information, but we could observe that the sentences around them in the context are similar. Thus, external informa- tion is also important to the sentence clustering for DM. In the following, we will discuss the cluster- ing process.</p><p>We denote the sentence cluster set as</p><formula xml:id="formula_0">C k = { c k 1 , c k 2 , · · · , c k N k C }</formula><p>, and the dialogue set as</p><formula xml:id="formula_1">D k = { d k 1 , d k 2 , · · · , d k N k D }</formula><p>in the k-th iteration. Thus, the steps of sentence clustering are:</p><p>Step 1: Initially, we only utilize the internal information to cluster the sentences via Affinity Propagation (AP) algorithm <ref type="bibr" target="#b2">(Brendan et al., 2007)</ref> and denote the clustering result as C 0 . If C 0 is used to label the sentences in dialogues, the j-th dialogue could be denoted as a sequence of clus-</p><formula xml:id="formula_2">ters, namely d 0 j = { c 0 1 , c 0 2 , · · · , c 0 N d j } .</formula><p>Step 2: In the k-th iteration, we use cluster set C k to label dialogue set D k .</p><p>Step 3: We utilize RNN to obtain the distribut- ed representation of sentence, illustrated in <ref type="figure" target="#fig_1">Fig. 5</ref>. The input of RNN is sentence cluster in each turn, namely c k t . The input layer I (t) is the one-hot rep- resentation of c k t . (Turian et al., 2010) (The size of I (t) is equivalent to C k . There is only one 1 in I (t) corresponding to the c k t position, and other elements are zeros.) H (t) is defined as the hidden layer. The output layer O (t) is the distribution over possible c k t+1 , which could be calculated as </p><formula xml:id="formula_3">O (t) = g (VH (t))<label>(1)</label></formula><p>where</p><formula xml:id="formula_4">f (x) = 1/(1 + e −x ) and g (x i ) = e x i / ∑ Ne i=1 e x i .</formula><p>The parameters of this RNN could be trained by the Back Propagation Through Time (BPTT) algorithm. (Mikolov, 2012) From RNN, we could obtain two significant results: one is the distributed representation (vectors) of the sentence clusters (U), which is used for sentence clustering; the other is the prediction model for sentence clus- ters, which is used for DM.</p><p>Step 4: we calculate the sentence similarity based on vectors obtained in Step 3, and combine it with the sentence similarity from internal infor- mation (weighted mean), in order to cluster the set C k via AP algorithm, which is denoted as C k+1 .</p><p>Step 5: ¯ N C = ∑ k+1 i=k−k th +2 N i C is defined as the average number of clusters in the last k th iter- ation. If ∑ k+1</p><p>i=k−k th +2 N i C − ¯ N C &lt; N th , stop the iteration of clustering, or go to Step 2, where N th is the variation threshold of quantity of clusters.</p><p>Thus, in the last iteration, we get the cluster set</p><formula xml:id="formula_5">C ¯ k = { c ¯ k 1 , c ¯ k 2 , · · · , c ¯ k N k C }</formula><p>and prediction model for these sentence clusters. We divide all the sen- tences in dialogue corpus into the sentence set spo- ken by customers and the sentence set spoken by customer service representatives, and then utilize C ¯ k to label them respectively, which is denoted as</p><formula xml:id="formula_6">C u = { c u 1 , c u 2 , · · · , c u Nu }</formula><p>, namely the clusters of user sentences, and</p><formula xml:id="formula_7">C m = { c m 1 , c m 2 , · · · , c m Nm } ,</formula><p>namely the clusters of machine sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DM based on Sentence Clustering</head><p>The dialogue process mentioned in section 2 could be formulized as follows, illustrated in <ref type="figure">Fig. 6</ref>  <ref type="figure">Figure 6</ref>: dialogue process C u . E = {e 1 , · · · , e T } is defined as the input voice, which is observable to infer x t in each turn. Y = {y 1 , · · · , y T } is defined as the output clus- ter of machine, where y t ∈ C m . Thus, the DM problem is to find out the optimal y t according to {e 1 , y 1 , · · · , e t }. In the following, the DM prob- lem is modeled as a POMDP.</p><p>State in the t-th epoch is defined as the sequence of clusters, namely s t = {x t−τ , y t−τ , · · · , x t−1 , y t−1 , x t }, where s t ∈ S . Action in the t-th epoch is defined as a t = y t , where a t ∈ A . The state transition probability Pr {s t+1 |s t , a t } could be shown as</p><formula xml:id="formula_8">Pr {s t+1 |s t , a t } = Pr {x t+1 |y t , x t , · · · , y t−τ , x t−τ }<label>(2)</label></formula><p>which is calculated by the prediction model based on RNN in section 3. Observation is defined as o t = {e t−τ , · · · , e t }, where o t ∈ O. As {x t−τ , · · · , x t } in state s t is unobservable, belief state is defined to represent the distribution over possible states, which is de- noted as b (t) ∈ B. According to <ref type="bibr" target="#b8">(Kaelbling et al., 1998</ref>), the belief state updating could be repre- sented as</p><formula xml:id="formula_9">b t+1 (s t+1 ) = Pr {o t+1 |s t+1 , a t } p s t+1 Pr {o t+1 |b t , a t }<label>(3)</label></formula><p>where p s t+1 = ∑ st∈S Pr {s t+1 |s t , a t } b t (s t ). According to <ref type="figure" target="#fig_1">Fig. 5</ref>, Pr {o t+1 |s t+1 , a t } could be shown as</p><formula xml:id="formula_10">Pr {o t+1 |s t+1 , a t } = Pr {o t+1 |s t+1 } = Pr {e t−τ +1 , · · · , e t+1 |x t−τ +1 , · · · , y t , x t+1 } = Pr {e t−τ +1 , · · · , e t+1 |x t−τ +1 , · · · , x t+1 } = t+1 ∏ i=t−τ +1</formula><p>Pr {e i |x i } (4) However, it is difficult to obtain the probabili- ty Pr {e t |x t }, as different people have different habits of expression and pronunciation. Fortunate- ly, Pr {x t |e t } could be estimated based on ASR and SM. Thus, based on Bayes Rules, we have the following equation.</p><formula xml:id="formula_11">Pr {e i |x i } = Pr {x i |e i } Pr {e i } Pr {x i }<label>(5)</label></formula><p>where Pr {x t } is the prior distribution of x t and could be counted by corpus. With <ref type="formula">(4)</ref> and <ref type="formula" target="#formula_11">(5)</ref>, <ref type="formula" target="#formula_9">(3)</ref> could be rewritten as</p><formula xml:id="formula_12">b t+1 (s t+1 ) = κ · p s t+1 · t+1 ∏ i=t−τ +1 Pr {x i |e i } t+1 ∏ i=t−τ +1 Pr {x i }<label>(6)</label></formula><p>where</p><formula xml:id="formula_13">κ = ∏ t+1 i=t−τ +1</formula><p>Pr {e i } / Pr {o t+1 |b t , a t } <ref type="formula">(7)</ref> is a normalization constant. The reward function is defined as</p><formula xml:id="formula_14">r t (s t , a t , s t+1 ) = λ f r f (st,at,s t+1 ) + λ p r p (st,at,s t+1 )<label>(8)</label></formula><p>where λ f + λ p = 1 and r t (s t , a t , s t+1 ) ∈ R. Firstly, r f (st,at,s t+1 ) stands for the number of un- filled slots that are filled by the sequence of sen- tence clusters corresponding to (s t , a t , s t+1 ). This slot-filling process could be achieved by a clas- sifier trained by the dialogues labeled with sen- tence clusters and slot-filling information. (Input- s are cluster sequences, and outputs are filled s- lots.) Additionally, r p (st,at,s t+1 ) is defined as the normalized quantity of s t+1 conditioned by s t and a t , which could be counted in corpus and stands for the popularity features of human-human dia- logues. Thus, for the belief state, the reward func- tion could be represented as</p><formula xml:id="formula_15">r t (b t , a t ) = ∑ s t+1 ∈S ∑ st∈S r t (s t , a t , s t+1 ) · Pr (s t+1 |s t , a t ) b t (s t ) (9)</formula><p>Therefore, if we define the policy as a mapping from belief state to action, namely ζ ∈ Z : B → A , the POMDP-based DM problem is shown as</p><formula xml:id="formula_16">max ζ∈S E ζ [ T ∑ t=1 βr t (b t , a t ) ] s.t. b t+1 (s t+1 ) = κ t+1 ∏ i=t−τ +1 Pr{x i |e i } t+1 ∏ i=t−τ +1 Pr{x i } · ∑ st∈S Pr {s t+1 |s t , a t } b t (s t ) (10)</formula><p>where β is the time discount factor and 0 &lt; β &lt; 1. This problem is a MDP problem with continuous states, which could be solved by the Natural Actor and Critic algorithm ( <ref type="bibr" target="#b7">Peters et al., 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we compare the performances of the proposed Sentence Clustering based Dialogue Management (SCDM) scheme and the existing D- M scheme. The existing scheme is designed ac- cording to <ref type="bibr" target="#b12">(Young et al., 2013)</ref>, where DA is uti- lized to represent the semantic information of sen- tence and the dialogue policy is trained via Rein- forcement Learning. It is also an extrinsic (or end- to-end) evaluation to compare the semantic repre- sentation ability between sentence cluster and DA. In order to compare the performances of the DM schemes, we collect 171 human-human di- alogues in hotel reservation and utilize 100 dia- logues of them to establish a SDS. The residual 71 dialogues are used to establish a simulated user for testing ( <ref type="bibr" target="#b5">Schatzmann et al., 2006</ref>). We define the slots requested from machine to user as "room type", "room quantity", "checkin time", "check- out time", "client name" and "client phone". We also define the slots requested from users to ma- chine as "hotel address = No.95 East St.", "room type set = single room, double room, and deluxe room", "single room price = $80", "double room price = $100", "deluxe room price = $150". The hotel reservation task could be considered as a pro- cess of exchanging the slot information between machine and user to some extent. <ref type="figure">Fig. 7</ref> illustrates the dialogue turn in the DM schemes, using different training corpus. Here, we vary the size of training corpus from 10 dia- logues to 100 dialogues and define average turn as the average dialogue turn cost to complete the task. From this picture, we find out that the SCD- M scheme has lower average turn than the existing scheme, partly because the sentence are automati- cally clustered into many small groups that could represent more nuanced semantic information than DAs, partly because RNN could estimate next sen- tence cluster according to the vector in hidden lay- er that contains abundant historical dialogue in- formation. As the number of sentence clusters is greater than number of DAs, RNN could also solve the scarcity problem and smoothing problem in the predicting process. Additionally, with the incre- ment of training dialogue size, the average turn quantity of training dialogues average turns of testing dialogues the existing DM scheme the SCDM scheme <ref type="figure">Figure 7</ref>: comparison of average turn of dialogue decreases, which ought to be ascribed to the fact that more training data could let SD- S reach more states with more times and increase the accuracy of the parameter estimation in RNN and POMDP. Furthermore, with the increment of training dialogue size, the dialogue turn improve- ment of the proposed scheme turns less obvious, because the number of new sentence pattern de- ceases with the training size increment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we focused on the DM scheme based on sentence clustering. Firstly, sentence cluster is defined as the semantic representation of sentence in dialogue, which could describe more naunced sentence intention than DA. Secondly, RNN is es- tablished for sentence clustering, where sentence similarity is calculated not only based on the inter- nal information such as words and sentence struc- ture, but also based on the external information such as context in dialogue. Thirdly, the DM prob- lem is modeled as a POMDP, where the state is defined as the sequence of sentence clusters and the state transition probability is estimated by RN- N, considering the whole information of historical dialogue. Finally, the experimental results illus- trated that the proposed DM scheme is superior to the existing one.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: sentence cluster vs. DA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: RNN for sentence clustering</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>I am sorry. One double room. OK.</head><label>I</label><figDesc></figDesc><table>C27: 0.63 
C15: 0.37 

C15: 0.88 
C79: 0.12 

ASR+SM 

ASR+SM 

C283 

C125 

TTS+SS 

TTS+SS 

... 

DM 

room type 
double room 
room num 
2 
... 
... 

room type 
double room 
room num 
1 
... 
... 

slot filling 

slot filling 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>1 t x 1 t y 1 t x 1 t y 1 t e t e 1 t e</head><label></label><figDesc></figDesc><table>. It 
is defined X = {x 1 , · · · , x T } as inner (or exac-
t) sentence cluster corresponding to the user in-
put in each turn, which is unobservable and x t ∈ t 

x 

t 

y 

</table></figure>

			<note place="foot" n="3"> Sentence Clustering based on RNN In this section, we cluster the sentences for DM modeling, which might be different from general sentence clustering. Sentence similarity for clustering are calculated from two aspects. Firstly, it is calculated traditionally based on internal information such as words and sentence structures, which is widely researched in (Li et al., 2006) (Achananuparp et al., 2008). (Word embedding</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The RavenClaw dialog management framework: Architecture and systems Computer Speech and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Bohus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rudnicky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="332" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Social signal and user adaptation in reinforcement learningbased dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Lefvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLIS &apos;13</title>
		<imprint>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Clustering by Passing Messages Between Data Points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delbert</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Emotion and personality in a conversational agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Breese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="189" to="219" />
		</imprint>
	</monogr>
	<note>Embodied conversational agents</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The HMM-based speech synthesis system version 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sako</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Masuko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th ISCA Workshop on Speech Synthesis</title>
		<meeting>6th ISCA Workshop on Speech Synthesis</meeting>
		<imprint>
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies. The knowledge engineering review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stuttle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="97" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural actor-critic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputinge</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1180" to="1190" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassandr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Offpolicy learning in large-scale POMDP-based dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daubigney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2012-03" />
			<biblScope unit="page" from="4989" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Revisiting Recurrent Neural Networks for robust ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The evaluation of sentence similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achananuparp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Warehousing and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="305" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks. Presentation at Google, Mountain View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model. 11th Annual Conference of the International Speech Communication Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karafit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automating spoken dialogue management design using machine learning: An industry perspective. Speech communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pieraccini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="716" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Log-linear dialog manager</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sentence similarity based on semantic nets and corpus statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z A</forename><surname>Bandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1138" to="1150" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Knowledge and Data Engineering</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
