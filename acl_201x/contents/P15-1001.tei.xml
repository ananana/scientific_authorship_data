<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Using Very Large Target Vocabulary for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal Yoshua Bengio Université de Montréal CIFAR Senior Fellow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal Yoshua Bengio Université de Montréal CIFAR Senior Fellow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal Yoshua Bengio Université de Montréal CIFAR Senior Fellow</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On Using Very Large Target Vocabulary for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1" to="10"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases out-perform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore , when we use an ensemble of a few models with very large target vocabularies , we achieve performance comparable to the state of the art (measured by BLEU) on both the English→German and English→French translation tasks of WMT&apos;14.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) is a recently introduced approach to solving machine transla- tion <ref type="bibr" target="#b12">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b21">Sutskever et al., 2014</ref>). In neural ma- chine translation, one builds a single neural net- work that reads a source sentence and generates its translation. The whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence, us- ing the bilingual corpus. The NMT models have shown to perform as well as the most widely used conventional translation systems <ref type="bibr" target="#b21">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>.</p><p>Neural machine translation has a number of advantages over the existing statistical machine translation system, specifically, the phrase-based system ( <ref type="bibr" target="#b13">Koehn et al., 2003)</ref>. First, NMT requires a minimal set of domain knowledge. For instance, all of the models proposed in <ref type="bibr" target="#b21">(Sutskever et al., 2014</ref>), ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> or <ref type="bibr" target="#b12">(Kalchbrenner and Blunsom, 2013)</ref> do not assume any linguis- tic property in both source and target sentences except that they are sequences of words. Sec- ond, the whole system is jointly trained to maxi- mize the translation performance, unlike the exist- ing phrase-based system which consists of many separately trained features whose weights are then tuned jointly. Lastly, the memory footprint of the NMT model is often much smaller than the exist- ing system which relies on maintaining large ta- bles of phrase pairs.</p><p>Despite these advantages and promising results, there is a major limitation in NMT compared to the existing phrase-based approach. That is, the number of target words must be limited. This is mainly because the complexity of training and us- ing an NMT model increases as the number of tar- get words increases.</p><p>A usual practice is to construct a target vo- cabulary of the K most frequent words (a so- called shortlist), where K is often in the range of 30k ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) to 80k <ref type="bibr" target="#b21">(Sutskever et al., 2014</ref>). Any word not included in this vocab- ulary is mapped to a special token representing an unknown word <ref type="bibr">[UNK]</ref>. This approach works well when there are only a few unknown words in the target sentence, but it has been observed 1 that the translation performance degrades rapidly as the number of unknown words increases ( <ref type="bibr" target="#b5">Cho et al., 2014a</ref>; <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>.</p><p>In this paper, we propose an approximate train- ing algorithm based on (biased) importance sam- pling that allows us to train an NMT model with a much larger target vocabulary. The proposed al- gorithm effectively keeps the computational com- plexity during training at the level of using only a small subset of the full vocabulary. Once the model with a very large target vocabulary is trained, one can choose to use either all the target words or only a subset of them.</p><p>We compare the proposed algorithm against the baseline shortlist-based approach in the tasks of English→French and English→German transla- tion using the NMT model introduced in <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. The empirical results demon- strate that we can potentially achieve better trans- lation performance using larger vocabularies, and that our approach does not sacrifice too much speed for both training and decoding. Further- more, we show that the model trained with this al- gorithm gets the best translation performance yet achieved by single NMT models on the WMT'14 English→French translation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation and Limited Vocabulary Problem</head><p>In this section, we briefly describe an approach to neural machine translation proposed recently in ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>). Based on this descrip- tion we explain the issue of limited vocabularies in neural machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>Neural machine translation is a recently proposed approach to machine translation, which uses a sin- gle neural network trained jointly to maximize the translation performance (Forcada and˜Necoand˜ and˜Neco, 1997; <ref type="bibr" target="#b12">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b6">Cho et al., 2014b;</ref><ref type="bibr" target="#b21">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. Neural machine translation is often imple- mented as the encoder-decoder network. The en- coder reads the source sentence x = (x 1 , . . . , x T ) and encodes it into a sequence of hidden states h = (h 1 , · · · , h T ):</p><formula xml:id="formula_0">h t = f (x t , h t−1 ) .<label>(1)</label></formula><p>Then, the decoder, another recurrent neural net- work, generates a corresponding translation y = (y 1 , · · · , y T ) based on the encoded sequence of hidden states h:</p><formula xml:id="formula_1">p(y t | y &lt;t , x) ∝ exp {q (y t−1 , z t , c t )} ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_2">z t = g (y t−1 , z t−1 , c t ) ,<label>(3)</label></formula><formula xml:id="formula_3">c t = r (z t−1 , h 1 , . . . , h T ) ,<label>(4)</label></formula><p>and y &lt;t = (y 1 , . . . , y t−1 ).</p><p>The whole model is jointly trained to maximize the conditional log-probability of the correct trans- lation given a source sentence with respect to the parameters θ of the model:</p><formula xml:id="formula_4">θ * = arg max θ N n=1 Tn t=1 log p(y n t | y n &lt;t , x n ),</formula><p>where (x n , y n ) is the n-th training pair of sen- tences, and T n is the length of the n-th target sen- tence (y n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Detailed Description</head><p>In this paper, we use a specific implementation of neural machine translation that uses an attention mechanism, as recently proposed in ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>). In ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, the encoder in Eq. (1) is implemented by a bi-directional recur- rent neural network such that</p><formula xml:id="formula_5">h t = ← − h t ; − → h t ,</formula><p>where</p><formula xml:id="formula_6">← − h t = f x t , ← − h t+1 , − → h t = f x t , − → h t−1 .</formula><p>They used a gated recurrent unit for f (see, e.g., <ref type="bibr" target="#b6">(Cho et al., 2014b)</ref>).</p><p>The decoder, at each time, computes the con- text vector c t as a convex sum of the hidden states (h 1 , . . . , h T ) with the coefficients α 1 , . . . , α T computed by</p><formula xml:id="formula_7">α t = exp {a (h t , z t−1 )} k exp {a (h k , z t−1 )} ,<label>(5)</label></formula><p>where a is a feedforward neural network with a single hidden layer. A new hidden state z t of the decoder in Eq. <ref type="formula" target="#formula_2">(3)</ref> is computed based on the previous hidden state z t−1 , previous generated symbol y t−1 and the computed context vector c t . The decoder also uses the gated recurrent unit, as the encoder does.</p><p>The probability of the next target word in Eq. <ref type="formula" target="#formula_1">(2)</ref> is then computed by</p><formula xml:id="formula_8">p(y t | y &lt;t , x) = 1 Z exp w t φ (y t−1 , z t , c t ) + b t ,<label>(6)</label></formula><p>where φ is an affine transformation followed by a nonlinear activation, and w t and b t are respec- tively the target word vector and the target word bias. Z is the normalization constant computed by</p><formula xml:id="formula_9">Z = k:y k ∈V exp w k φ (y t−1 , z t , c t ) + b k , (7)</formula><p>where V is the set of all the target words.</p><p>For the detailed description of the implementa- tion, we refer the reader to the appendix of (Bah- danau et al., 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Limited Vocabulary Issue and Conventional Solutions</head><p>One of the main difficulties in training this neu- ral machine translation model is the computational complexity involved in computing the target word probability (Eq. <ref type="formula" target="#formula_8">(6)</ref>). More specifically, we need to compute the dot product between the feature φ (y t−1 , z t , c t ) and the word vector w t as many times as there are words in a target vocabulary in order to compute the normalization constant (the denominator in Eq. <ref type="formula" target="#formula_8">(6)</ref>). This has to be done for, on average, 20-30 words per sentence, which eas- ily becomes prohibitively expensive even with a moderate number of possible target words. Fur- thermore, the memory requirement grows linearly with respect to the number of target words. This has been a major hurdle for neural machine trans- lation, compared to the existing non-parametric approaches such as phrase-based translation sys- tems.</p><p>Recently proposed neural machine translation models, hence, use a shortlist of 30k to 80k most frequent words ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b21">Sutskever et al., 2014</ref>). This makes training more feasible, but comes with a number of problems. First of all, the performance of the model degrades heavily if the translation of a source sentence requires many words that are not included in the shortlist ( <ref type="bibr" target="#b5">Cho et al., 2014a</ref>). This also affects the performance evaluation of the system which is often measured by BLEU. Second, the first issue becomes more problematic with languages that have a rich set of words such as German or other highly inflected languages.</p><p>There are two model-specific approaches to this issue of large target vocabulary. The first approach is to stochastically approximate the target word probability. This has been proposed recently in <ref type="bibr" target="#b18">(Mnih and Kavukcuoglu, 2013;</ref><ref type="bibr" target="#b17">Mikolov et al., 2013</ref>) based on noise-contrastive estimation <ref type="bibr" target="#b11">(Gutmann and Hyvarinen, 2010)</ref>. In the second ap- proach, the target words are clustered into multi- ple classes, or hierarchical classes, and the target probability p(y t |y &lt;t , x) is factorized as a product of the class probability p(c t |y &lt;t , x) and the intra- class word probability p(y t |c t , y &lt;t , x). This re- duces the number of required dot-products into the sum of the number of classes and the words in a class. These approaches mainly aim at reducing the computational complexity during training, but do not often result in speed-up when decoding a translation during test time. <ref type="bibr">1</ref> Other than these model-specific approaches, there exist translation-specific approaches. A translation-specific approach exploits the proper- ties of the rare target words. For instance, Luong et al. proposed such an approach for neural ma- chine translation ( <ref type="bibr" target="#b16">Luong et al., 2015)</ref>. They re- place rare words (the words that are not included in the shortlist) in both source and target sentences into corresponding OOV n tokens using the word alignment model. Once a source sentence is trans- lated, each OOV n in the translation will be re- placed based on the source word marked by the corresponding OOV n .</p><p>It is important to note that the model- specific approaches and the translation-specific approaches are often complementary and can be used together to further improve the translation performance and reduce the computational com- plexity.</p><p>tational complexity of training becomes constant with respect to the size of the target vocabulary. Furthermore, the proposed approach allows us to efficiently use a fast computing device with lim- ited memory, such as a GPU, to train a neural ma- chine translation model with a much larger target vocabulary.</p><p>As mentioned earlier, the computational inef- ficiency of training a neural machine translation model arises from the normalization constant in Eq. (6). In order to avoid the growing complex- ity of computing the normalization constant, we propose here to use only a small subset V of the target vocabulary at each update. The proposed approach is based on the earlier work of <ref type="bibr" target="#b2">(Bengio and Sénécal, 2008)</ref>.</p><p>Let us consider the gradient of the log- probability of the output in Eq. (6). The gradient is composed of a positive and negative part:</p><formula xml:id="formula_10">log p(y t | y &lt;t , x)<label>(8)</label></formula><formula xml:id="formula_11">=E(y t ) − k:y k ∈V p(y k | y &lt;t , x)E(y k ),</formula><p>where we define the energy E as</p><formula xml:id="formula_12">E(y j ) = w j φ (y j−1 , z j , c j ) + b j .</formula><p>The second, or negative, term of the gradient is in essence the expected gradient of the energy:</p><formula xml:id="formula_13">E P [E(y)] ,<label>(9)</label></formula><p>where P denotes p(y | y &lt;t , x).</p><p>The main idea of the proposed approach is to approximate this expectation, or the negative term of the gradient, by importance sampling with a small number of samples. Given a predefined pro- posal distribution Q and a set V of samples from Q, we approximate the expectation in Eq. <ref type="formula" target="#formula_13">(9)</ref> with</p><formula xml:id="formula_14">E P [E(y)] ≈ k:y k ∈V ω k k :y k ∈V ω k E(y k ),<label>(10)</label></formula><p>where</p><formula xml:id="formula_15">ω k = exp {E(y k ) − log Q(y k )} .<label>(11)</label></formula><p>This approach allows us to compute the normal- ization constant during training using only a small subset of the target vocabulary, resulting in much lower computational complexity for each param- eter update. Intuitively, at each parameter update, we update only the vectors associated with the cor- rect word w t and with the sampled words in V . Once training is over, we can use the full target vo- cabulary to compute the output probability of each target word.</p><p>Although the proposed approach naturally ad- dresses the computational complexity, using this approach naively does not guarantee that the num- ber of parameters being updated for each sen- tence pair, which includes multiple target words, is bounded nor can be controlled. This becomes problematic when training is done, for instance, on a GPU with limited memory.</p><p>In practice, hence, we partition the training cor- pus and define a subset V of the target vocabu- lary for each partition prior to training. Before training begins, we sequentially examine each tar- get sentence in the training corpus and accumulate unique target words until the number of unique tar- get words reaches the predefined threshold τ . The accumulated vocabulary will be used for this par- tition of the corpus during training. We repeat this until the end of the training set is reached. Let us refer to the subset of target words used for the i-th partition by V i . This may be understood as having a separate proposal distribution Q i for each partition of the training corpus. The distribution Q i assigns equal probability mass to all the target words included in the subset V i , and zero probability mass to all the other words, i.e.,</p><formula xml:id="formula_16">Q i (y k ) =    1 |V i | if y t ∈ V i 0 otherwise.</formula><p>This choice of proposal distribution cancels out the correction term − log Q(y k ) from the impor- tance weight in Eqs. (10)-(11), which makes the proposed approach equivalent to approximating the exact output probability in Eq. (6) with</p><formula xml:id="formula_17">p(y t | y &lt;t , x) = exp w t φ (y t−1 , z t , c t ) + b t k:y k ∈V exp w k φ (y t−1 , z t , c t ) + b k .</formula><p>It should be noted that this choice of Q makes the estimator biased.</p><p>The proposed procedure results in speed up against usual importance sampling, as it exploits the advantage of modern computers in doing matrix-matrix vs matrix-vector multiplications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Informal Discussion on Consequence</head><p>The parametrization of the output probability in Eq. (6) can be understood as arranging the vectors associated with the target words such that the dot product between the most likely, or correct, target word's vector and the current hidden state is max- imized. The exponentiation followed by normal- ization is simply a process in which the dot prod- ucts are converted into proper probabilities.</p><p>As learning continues, therefore, the vectors of all the likely target words tend to align with each other but not with the others. This is achieved ex- actly by moving the vector of the correct word in the direction of φ (y t−1 , z t , c t ), while pushing all the other vectors away, which happens when the gradient of the logarithm of the exact output prob- ability in Eq. <ref type="formula" target="#formula_8">(6)</ref> is maximized. Our approximate approach, instead, moves the word vectors of the correct words and of only a subset of sampled tar- get words (those included in V ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoding</head><p>Once the model is trained using the proposed ap- proximation, we can use the full target vocabulary when decoding a translation given a new source sentence. Although this is advantageous as it al- lows the trained model to utilize the whole vocab- ulary when generating a translation, doing so may be too computationally expensive, e.g., for real- time applications.</p><p>Since training puts the target word vectors in the space so that they align well with the hidden state of the decoder only when they are likely to be a correct word, we can use only a subset of candi- date target words during decoding. This is similar to what we do during training, except that at test time, we do not have access to a set of correct tar- get words.</p><p>The most na¨ıvena¨ıve way to select a subset of candi- date target words is to take only the top-K most frequent target words, where K can be adjusted to meet the computational requirement. This, how- ever, effectively cancels out the whole purpose of training a model with a very large target vocabu- lary. Instead, we can use an existing word align- ment model to align the source and target words in the training corpus and build a dictionary. With the dictionary, for each source sentence, we construct a target word set consisting of the K-most fre- quent words (according to the estimated unigram probability) and, using the dictionary, at most K likely target words for each source word. K and K may be chosen either to meet the computa- tional requirement or to maximize the translation performance on the development set. We call a subset constructed in either of these ways a candi- date list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Source Words for Unknown Words</head><p>In the experiments, we evaluate the proposed ap- proach with the neural machine translation model called RNNsearch ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) (see Sec. 2.1.1). In this model, as a part of decoding process, we obtain the alignments between the tar- get words and source locations via the alignment model in Eq. (5).</p><p>We can use this feature to infer the source word to which each target word was most aligned (in- dicated by the largest α t in Eq. <ref type="formula" target="#formula_7">(5)</ref>). This is especially useful when the model generated an <ref type="bibr">[UNK]</ref> token. Once a translation is generated given a source sentence, each <ref type="bibr">[UNK]</ref> may be re- placed using a translation-specific technique based on the aligned source word. For instance, in the experiment, we try replacing each <ref type="bibr">[UNK]</ref> token with the aligned source word or its most likely translation determined by another word alignment model. Other techniques such as transliteration may also be used to further improve the perfor- mance <ref type="bibr" target="#b14">(Koehn, 2010</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed approach in English→French and English→German trans- lation tasks. We trained the neural machine translation models using only the bilingual, paral- lel corpora made available as a part of WMT'14. For each pair, the datasets we used are:</p><formula xml:id="formula_18">• English→French: 2 -Common Crawl -News Commentary -Gigaword -Europarl v7 -UN • English→German: -Common Crawl -News Commentary -Europarl v7</formula><p>English-French English- <ref type="table">German  Train  Test  Train  Test  15k</ref> 93  <ref type="table">Table 1</ref>: Data coverage (in %) on target-side cor- pora for different vocabulary sizes. "All" refers to all the tokens in the training set.</p><p>To ensure fair comparison, the English→French corpus, which comprises approximately 12 mil- lion sentences, is identical to the one used in <ref type="bibr" target="#b12">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b21">Sutskever et al., 2014</ref>). As for English→German, the corpus was preprocessed, in a manner similar to ( <ref type="bibr" target="#b15">Li et al., 2014)</ref>, in order to remove many poorly translated sentences.</p><p>We evaluate the models on the WMT'14 test set (news-test 2014), 3 while the concatenation of news-test-2012 and news-test-2013 is used for model selection (development set). <ref type="table">Table 1</ref> presents data coverage w.r.t. the vocabulary size, on the target side.</p><p>Unless mentioned otherwise, all reported BLEU scores ( <ref type="bibr" target="#b19">Papineni et al., 2002</ref>) are computed with the multi-bleu.perl script 4 on the cased tokenized translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>As a baseline for English→French translation, we use the RNNsearch model proposed by <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>, with 30k source and target words. 5 Another RNNsearch model is trained for English→German translation with 50k source and target words.</p><p>For each language pair, we train another set of RNNsearch models with much larger vocab- ularies of 500k source and target words, using the proposed approach. We call these models RNNsearch-LV. We vary the size of the short- list used during training (τ in Sec. 3.1). We tried 15k and 30k for English→French, and 15k and 50k for English→German. We later report the re- sults for the best performance on the development set, with models generally evaluated every twelve hours. The training speed is approximately the same as for RNNsearch. Using a 780 Ti or Titan Black GPU, we could process 100k mini-batches of 80 sentences in about 29 and 39 hours respec- tively for τ = 15k and τ = 50k.</p><p>For both language pairs, we also trained new models, with τ = 15k and τ = 50k, by reshuffling the dataset at the beginning of each epoch. While this causes a non-negligible amount of overhead, such a change allows words to be contrasted with different sets of other words each epoch.</p><p>To stabilize parameters other than the word em- beddings, at the end of the training stage, we freeze the word embeddings and tune only the other parameters for approximately two more days after the peak performance on the development set is observed. This helped increase BLEU scores on the development set.</p><p>We use beam search to generate a translation given a source. During beam search, we keep a set of 12 hypotheses and normalize probabili- ties by the length of the candidate sentences, as in ( <ref type="bibr" target="#b5">Cho et al., 2014a</ref>). <ref type="bibr">6</ref> The candidate list is chosen to maximize the performance on the development set, for K ∈ {15k, 30k, 50k} and K ∈ {10, 20}. As explained in Sec. 3.2, we test using a bilin- gual dictionary to accelerate decoding and to re- place unknown words in translations. The bilin- gual dictionary is built using fast align <ref type="bibr" target="#b8">(Dyer et al., 2013)</ref>. We use the dictionary only if a word starts with a lowercase letter, and otherwise, we copy the source word directly. This led to better performance on the development sets.</p><p>Note on ensembles For each language pair, we began training four models from each of which two points corresponding to the best and second- best performance on the development set were col- lected. We continued training from each point, while keeping the word embeddings fixed, until the best development performance was reached, and took the model at this point as a single model in an ensemble. This procedure resulted in a to- tal of eight models from which we averaged the length-normalized log-probabilities. Since much of training had been shared, the composition of  <ref type="table">Table 2</ref>: The translation performances in BLEU obtained by different models on (a) English→French and (b) English→German translation tasks. RNNsearch is the model proposed in ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, RNNsearch-LV is the RNNsearch trained with the approach proposed in this paper, and Google is the LSTM-based model proposed in <ref type="bibr" target="#b21">(Sutskever et al., 2014)</ref>. Unless mentioned otherwise, we report single- model RNNsearch-LV scores using τ = 30k (English→French) and τ = 50k (English→German). such ensembles may be sub-optimal. This is sup- ported by the fact that higher cross-model BLEU scores ( ) are observed for mod- els that were partially trained together.</p><note type="other">RNNsearch RNNsearch-LV Google Phrase-based SMT Basic</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translation Performance</head><p>In <ref type="table">Table 2</ref>, we present the results obtained by the trained models with very large target vocabular- ies, and alongside them, the previous results re- ported in <ref type="bibr" target="#b21">(Sutskever et al., 2014)</ref>, <ref type="bibr" target="#b16">(Luong et al., 2015)</ref>, <ref type="bibr">(Buck et al., 2014</ref>) and ( ). Without translation-specific strategies, we can clearly see that the RNNsearch-LV outper- forms the baseline RNNsearch.</p><p>In the case of the English→French task, RNNsearch-LV approached the performance level of the previous best single neural machine transla- tion (NMT) model, even without any translation- specific techniques (Sec. 3.2-3.3). With these, however, the RNNsearch-LV outperformed it. The performance of the RNNsearch-LV is also better than that of a standard phrase-based translation system ( <ref type="bibr" target="#b6">Cho et al., 2014b)</ref>. Furthermore, by com- bining 8 models, we were able to achieve a trans- lation performance comparable to the state of the art, measured in BLEU.</p><p>For English→German, the RNNsearch-LV out- performed the baseline before unknown word re- placement, but after doing so, the two systems per- formed similarly. We could reach higher large- vocabulary single-model performance by reshuf- fling the dataset, but this step could potentially also help the baseline. In this case, we were able to surpass the previously reported best translation result on this task by building an ensemble of 8 models.</p><p>With τ = 15k, the RNNsearch-LV performance worsened a little, with best BLEU scores, with- out reshuffling, of 33.76 and 18.59 respectively for English→French and English→German.</p><p>The English→German ensemble described in this paper has also been used for the shared trans- lation task of the 10 th Workshop on Statistical Ma- chine Translation (WMT'15), where it was ranked first in terms of BLEU score. The translations by this ensemble can be found online. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Decoding Speed</head><p>In <ref type="table" target="#tab_2">Table 3</ref>, we present the timing information of decoding for different models. Clearly, decoding from RNNsearch-LV with the full target vocab-CPU GPU • RNNsearch 0.09 s 0.02 s RNNsearch-LV 0.80 s 0.25 s RNNsearch-LV 0.12 s 0.05 s +Candidate list A potential issue with using a candidate list is that for each source sentence, we must re-build a target vocabulary and subsequently replace a part of the parameters, which may easily become time- consuming. We can address this issue, for in- stance, by building a common candidate list for multiple source sentences. By doing so, we were able to match the decoding speed of the baseline RNNsearch model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Decoding Target Vocabulary</head><p>For English→French (τ = 30k), we evaluate the influence of the target vocabulary when translat- ing the test sentences by using the union of a fixed set of 30k common words and (at most) K likely candidates for each source word according to the dictionary. Results are presented in <ref type="figure">Figure 1</ref>. With K = 0 (not shown), the performance of the sys- tem is comparable to the baseline when not replac- ing the unknown words <ref type="bibr">(30.12)</ref>, but there is not as much improvement when doing so <ref type="bibr">(31.14)</ref>. As the large vocabulary model does not predict <ref type="bibr">[UNK]</ref> as much during training, it is less likely to generate it when decoding, limiting the effectiveness of the post-processing step in this case. With K = 1, which limits the diversity of allowed uncommon words, BLEU is not as good as with moderately larger K , which indicates that our models can, to some degree, correctly choose between rare alter- natives. If we rather use K = 50k, as we did for testing based on validation performance, the improvement over K = 1 is approximately 0.2 BLEU.</p><p>When validating the choice of K, we found it to be correlated with the value of τ used during With UNK replacement Without UNK replacement <ref type="figure">Figure 1</ref>: Single-model test BLEU scores (English→French) with respect to the number of dictionary entries K allowed for each source word.</p><p>training. For example, on the English→French validation set, with τ = 15k (and K = 10), the BLEU score is 29.44 with K = 15k, but drops to 29.19 and 28.84 respectively for K = 30k and 50k. For τ = 30k, the score increases moder- ately from K = 15k to K = 50k. A similar effect was observed for English→German and on the test sets. As our implementation of importance sampling does not apply the usual correction to the gradient, it seems beneficial for the test vocabular- ies to resemble those used during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a way to extend the size of the target vocabulary for neural machine trans- lation. The proposed approach allows us to train a model with much larger target vocabulary with- out any substantial increase in computational com- plexity. It is based on the earlier work in <ref type="bibr" target="#b2">(Bengio and Sénécal, 2008</ref>) which used importance sam- pling to reduce the complexity of computing the normalization constant of the output word proba- bility in neural language models. On English→French and English→German translation tasks, we observed that the neural ma- chine translation models trained using the pro- posed method performed as well as, or better than, those using only limited sets of target words, even when replacing unknown words. As per- formance of the RNNsearch-LV models increased when only a selected subset of the target vocab- ulary was used during decoding, this makes the proposed learning algorithm more practical.</p><p>When measured by BLEU, our models showed translation performance comparable to the state-of-the-art translation systems on both the English→French task and English→German task. On the English→French task, a model trained with the proposed approach outperformed the best single neural machine translation (NMT) model from ( <ref type="bibr" target="#b16">Luong et al., 2015</ref>) by approximately 1 BLEU point. The performance of the ensemble of multiple models, despite its relatively less diverse composition, is approximately 0.3 BLEU points away from the best system ( <ref type="bibr" target="#b16">Luong et al., 2015</ref>). On the English→German task, the best performance of 21.59 BLEU by our model is higher than that of the previous state of the art (20.67) reported in ( <ref type="bibr">Buck et al., 2014</ref>).</p><p>Finally, we release the source code used in our experiments to encourage progress in neural ma- chine translation. 8</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For the experiments we have run ourselves, we show the scores on the development set as well in the brackets. () (Sutskever et al., 2014), (•) (Luong et al., 2015), (•) (Durrani et al., 2014), ( * ) Standard Moses Setting (Cho et al., 2014b), () (Buck et al., 2014).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The average per-word decoding time. 
Decoding here does not include parameter load-
ing and unknown word replacement. The baseline 
uses 30k words. The candidate list is built with 
K = 30k and K = 10. () i7-4820K (single 
thread), (•) GTX TITAN Black 

ulary is slowest. If we use a candidate list for 
decoding each translation, the speed of decoding 
substantially improves and becomes close to the 
baseline RNNsearch. 
</table></figure>

			<note place="foot" n="3"> Approximate Learning Approach to Very Large Target Vocabulary 3.1 Description In this paper, we propose a model-specific approach that allows us to train a neural machine translation model with a very large target vocabulary. With the proposed approach, the compu1 This is due to the fact that the beam search requires the conditional probability of every target word at each time step regardless of the parametrization of the output probability.</note>

			<note place="foot" n="2"> The preprocessed data can be found and downloaded from http://www-lium.univ-lemans.fr/ ˜ schwenk/nnmt-shared-task/README.</note>

			<note place="foot" n="3"> To compare with previous submissions, we use the filtered test sets. 4 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl 5 The authors of (Bahdanau et al., 2015) gave us access to their trained models. We chose the best one on the validation set and resumed training.</note>

			<note place="foot" n="6"> These experimental details differ from (Bahdanau et al., 2015).</note>

			<note place="foot" n="7"> http://matrix.statmt.org/matrix/ output/1774?run_id=4079</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the developers of Theano ( <ref type="bibr" target="#b3">Bergstra et al., 2010;</ref><ref type="bibr" target="#b1">Bastien et al., 2012</ref>). We acknowledge the support of the fol-lowing agencies for research funding and comput-ing support: NSERC, Calcul Québec, Compute Canada, the Canada Research Chairs, CIFAR and Samsung.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive importance sampling to accelerate training of a neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Sénécal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="713" to="722" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<publisher>Oral Presentation</publisher>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">N-gram counts and language models from the common crawl</title>
		<ptr target="https://github.com/sebastien-j/LV_groundhogChristianBuck" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference</title>
		<meeting>the Language Resources and Evaluation Conference<address><addrLine>Reykjavík, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
	<note>Kenneth Heafield, and Bas van Ooyen</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: EncoderDecoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Edinburgh&apos;s phrase-based machine translation systems for WMT-14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM Model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recursive hetero-associative memories for translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><forename type="middle">L</forename><surname>Forcada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biological and Artificial Computation: From Neuroscience to Technology</title>
		<editor>José Mira, Roberto Moreno-Díaz, and Joan Cabestany</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">1240</biblScope>
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Eu-bridge MT: Combined machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Statistical Machine Translation</title>
		<meeting>of the Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS&apos;10)</title>
		<meeting>The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, NAACL &apos;03</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, NAACL &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The DCUICTCAS MT system at WMT 2014 on GermanEnglish translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><forename type="middle">Cortes</forename><surname>Vaillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="136" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations: Workshops Track</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The RWTH Aachen GermanEnglish machine translation system for WMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="157" to="162" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
