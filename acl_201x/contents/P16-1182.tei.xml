<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MUTT: Metric Unit TesTing for Language Generation Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Boag</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
								<address>
									<addrLine>198 Riverside St</addrLine>
									<postCode>01854</postCode>
									<settlement>Lowell</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renan</forename><surname>Campos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
								<address>
									<addrLine>198 Riverside St</addrLine>
									<postCode>01854</postCode>
									<settlement>Lowell</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
								<address>
									<addrLine>198 Riverside St</addrLine>
									<postCode>01854</postCode>
									<settlement>Lowell</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell</orgName>
								<address>
									<addrLine>198 Riverside St</addrLine>
									<postCode>01854</postCode>
									<settlement>Lowell</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MUTT: Metric Unit TesTing for Language Generation Tasks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1935" to="1943"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Precise evaluation metrics are important for assessing progress in high-level language generation tasks such as machine translation or image captioning. Historically , these metrics have been evaluated using correlation with human judgment. However, human-derived scores are often alarmingly inconsistent and are also limited in their ability to identify precise areas of weakness. In this paper, we perform a case study for metric evaluation by measuring the effect that systematic sentence transformations (e.g. active to passive voice) have on the automatic metric scores. These sentence &quot;corruptions&quot; serve as unit tests for precisely measuring the strengths and weaknesses of a given metric. We find that not only are human annotations heavily inconsistent in this study, but that the Metric Unit TesT analysis is able to capture precise shortcomings of particular metrics (e.g. comparing passive and active sentences) better than a simple correlation with human judgment can.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of high-level language generation tasks such as machine translation (MT), paraphrasing and image/video captioning depends on the existence of reliable and precise automatic evaluation metrics. <ref type="figure">Figure 1</ref>: A few select entries from the SICK dataset. All of these entries follow the same "Negated Subject" transfor- mation between sentence 1 and sentence 2, yet humans anno- tated them with an inconsistently wide range of scores (from 1 to 5). Regardless of whether the gold labels for this partic- ular transformation should score this high or low, they should score be scored consistently.</p><p>Efforts have been made to create standard met- rics ( <ref type="bibr" target="#b8">Papineni et al., 2001;</ref><ref type="bibr" target="#b3">Lin, 2004;</ref><ref type="bibr" target="#b2">Denkowski and Lavie, 2014;</ref><ref type="bibr" target="#b13">Vedantam et al., 2014</ref>) to help advance the state-of-the-art. However, most such popular metrics, despite their wide use, have seri- ous deficiencies. Many rely on ngram matching and assume that annotators generate all reasonable refer- ence sentences, which is infeasible for many tasks. Furthermore, metrics designed for one task, e.g., MT, can be a poor fit for other tasks, e.g., video cap- tioning.</p><p>To design better metrics, we need a principled approach to evaluating their performance. Histori- cally, MT metrics have been evaluated by how well they correlate with human annotations <ref type="bibr">(CallisonBurch et al., 2010;</ref><ref type="bibr" target="#b4">Machacek and Bojar, 2014</ref>). However, as we demonstrate in Sec. 5, human judgment can result in inconsistent scoring. This presents a serious problem for determining whether a metric is "good" based on correlation with incon- sistent human scores. When "gold" target data is unreliable, even good metrics can appear to be inac- curate.</p><p>Furthermore, correlation of system output with human-derived scores typically provides an overall score but fails to isolate specific errors that met- rics tend to miss. This makes it difficult to dis- cover system-specific weaknesses to improve their performance. For instance, an ngram-based metric might effectively detect non-fluent, syntactic errors, but could also be fooled by legitimate paraphrases whose ngrams simply did not appear in the training set. Although there has been some recent work on paraphrasing that provided detailed error analysis of system outputs <ref type="bibr" target="#b12">(Socher et al., 2011;</ref><ref type="bibr" target="#b5">Madnani et al., 2012</ref>), more often than not such investigations are seen as above-and-beyond when assessing metrics.</p><p>The goal of this paper is to propose a process for consistent and informative automated analysis of evaluation metrics. This method is demonstrably more consistent and interpretable than correlation with human annotations. In addition, we extend the SICK dataset to include un-scored fluency-focused sentence comparisons and we propose a toy metric for evaluation.</p><p>The rest of the paper is as follows: Section 2 introduces the corruption-based metric unit testing process, Section 3 lists the existing metrics we use in our experiments as well as the toy metric we propose, Section 4 describes the SICK dataset we used for our experiments, Section 5 motivates the need for corruption-based evaluation instead of cor- relation with human judgment, Section 6 describes the experimental procedure for analyzing the met- ric unit tests, Section 7 analyzes the results of our experiments, and in Section 8 we offer concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Metric Unit TesTs</head><p>We introduce metric unit tests based on sentence corruptions as a new method for automatically eval- uating metrics developed for language generation tasks. Instead of obtaining human ranking for system output and comparing it with the metric- based ranking, the idea is to modify existing ref- erences with specific transformations, and exam- ine the scores assigned by various metrics to such corruptions. In this paper, we analyze three broad categories of transformations -meaning-altering, meaning-preserving, and fluency-disrupting sen- tence corruptions -and we evaluate how success- fully several common metrics can detect them.</p><p>As an example, the original sentence "A man is playing a guitar." can be corrupted as follows:</p><p>Meaning-Altering: A man is not playing guitar. Meaning-Preserving: A guitar is being played by a man. Fluency-Disrupting: A man a guitar is playing.</p><p>Examples for each corruption type we consider are shown in <ref type="table" target="#tab_0">Tables 1 and 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Meaning-altering corruptions</head><p>Meaning-altering corruptions modify the seman- tics of a sentence, resulting in a new sentence that has a different meaning. Corruptions (1- 2) check whether a metric can detect small lex- ical changes that cause the sentence's semantics to entirely change. Corruption (3) is designed to fool distributed and distributional representations of words, whose vectors often confuse synonyms and antonyms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Meaning-preserving corruptions</head><p>Meaning-preserving corruptions change the lexical presentation of a sentence while still preserving meaning and fluency. For such transformations, the "corruption" is actually logically equivalent to the original sentence, and we would expect that consis- tent annotators would assign roughly the same score to each. These transformations include changes such as rephrasing a sentence from active voice to passive voice (4) or paraphrasing within a sentence (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Fluency disruptions</head><p>Beyond understanding semantics, metrics must also recognize when a sentence lacks fluency and gram- mar. Corruptions (7-9) were created for this reason, and do so by generating ungrammatical sentences.</p><p>Meaning Altering 1 negated subject <ref type="formula">(337)</ref> "A man is playing a harp" "There is no man playing a harp" 2 negated action <ref type="bibr">(202)</ref> "A jet is flying" "A jet is not flying" 3 antonym replacement <ref type="formula">(246)</ref> "a dog with short hair" "a dog with long hair" Meaning Preserving 4 active-to-passive <ref type="formula">(238)</ref> "A man is cutting a potato" "A potato is being cut by a man" 5 synonymous phrases <ref type="formula">(240)</ref> "A dog is eating a doll" "A dog is biting a doll" 6 determiner substitution (65) "A cat is eating food" "The cat is eating food"  <ref type="formula">(500)</ref> "A boy walks at night" "A boy walks at night at night" 8 remove head from PP (500) "A man danced in costume" "A man danced costume" 9 re-order chunked phrases (500) "A woman is slicing garlics" "Is slicing garlics a woman" <ref type="table">Table 2</ref>: Generated corruptions. The first column gives the total number of generated corruptions in parentheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Metrics Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Existing Metrics</head><p>Many existing metrics work by identifying lexical similarities, such as n-gram matches, between the candidate and reference sentences. Commonly-used metrics include BLEU, CIDEr, and TER:</p><p>• BLEU, an early MT metric, is a precision- based metric that rewards candidates whose words can be found in the reference but pe- nalizes short sentences and ones which overuse popular n-grams ( <ref type="bibr" target="#b8">Papineni et al., 2001</ref>).</p><p>• CIDEr, an image captioning metric, uses a consensus-based voting of tf-idf weighted ngrams to emphasize the most unique seg- ments of a sentence in comparison ( <ref type="bibr" target="#b13">Vedantam et al., 2014</ref>).</p><p>• TER (Translation Edit Rate) counts the changes needed so the surface forms of the out- put and reference match <ref type="bibr" target="#b10">(Snover et al., 2006</ref>).</p><p>Other metrics have attempted to capture similar- ity beyond surface-level pattern matching:</p><p>• METEOR, rather than strictly measuring ngram matches, accounts for soft similari- ties between sentences by computing synonym and paraphrase scores between sentence align- ments <ref type="bibr" target="#b2">(Denkowski and Lavie, 2014</ref>).</p><p>• BADGER takes into account the contexts over the entire set of reference sentences by using a simple compression distance calculation af- ter performing a series of normalization steps <ref type="bibr" target="#b9">(Parker, 2008</ref>).</p><p>• TERp (TER-plus) minimizes the edit dis- tance by stem matches, synonym matches, and phrase substitutions before calculating the TER score, similar to BADGER's normaliza- tion step <ref type="bibr" target="#b11">(Snover et al., 2009</ref>).</p><p>We evaluate the strengths and weaknesses of these existing metrics in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Toy Metric: W2V-AVG</head><p>To demonstrate how this paper's techniques can also be applied to measure a new evaluation metric, we create a toy metric, W2V-AVG, using the cosine of the centroid of a sentence's word2vec embeddings ( <ref type="bibr" target="#b7">Mikolov et al., 2013</ref>). The goal for this true bag- of-words metric is to serve as a sanity check for how corruption unit tests can identify metrics that cap- ture soft word-level similarities, but cannot handle directed relationships between entities.</p><p>which contains entries consisting of a pair of sen- tences and a human-estimated semantic relatedness score to indicate the similarity of the sentence pair ( <ref type="bibr" target="#b6">Marelli et al., 2014</ref>). The reason we use this data is twofold:</p><p>1. it is a well-known and standard dataset within semantic textual similarity community.</p><p>2. it contains many common sentence transfor- mation patterns, such as those described in Ta- ble 1.</p><p>The SICK dataset was built from the 8K Image- Flickr dataset 1 and the SemEval 2012 STS MSR- Video Description corpus 2 . Each of these origi- nal datasets contain human-generated descriptions of images/videos -a given video often has 20-50 reference sentences describing it. These reference sets prove very useful because they are more-or-less paraphrases of one another; they all describe the same thing. The creators of SICK selected sentence pairs and instructed human annotators to ensure that all sentences obeyed proper grammar. The creators of SICK ensured that two of the corruption types -meaning-altering and meaning-preserving -were generated in the annotated sentence pairs. We then filtered through SICK using simple rule-based tem- plates <ref type="bibr">3</ref> to identify each of the six corruption types listed in <ref type="table" target="#tab_0">Table 1</ref>. Finally, we matched the sentences in the pair back to their original reference sets in the Flickr8 and MSR-Video Description corpora to ob- tain reference sentences for our evaluation metrics experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SICK+</head><p>Since all of the entries in the SICK dataset were created for compositional semantics, every sentence was manually checked by annotators to ensure flu- ency. For our study, we also wanted to measure the effects of bad grammar between sentences, so we automatically generated our own corruptions to the SICK dataset to create SICK+, a set of fluency-disrupting corruptions. The rules to gener- ate these corruptions were simple operations involv- ing chunking and POS-tagging. Fortunately, these corruptions were, by design, meant to be ungram- matical, so there was no need for (difficult) auto- matic correctness checking for fluency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Inconsistencies in Human Judgment</head><p>A major issue with comparing metrics against hu- man judgment is that human judgments are often inconsistent. One reason for this is that high-level semantic tasks are difficult to pose to annotators. Consider SICK's semantic relatedness annotation as a case study for human judgment. Annotators were shown two sentences, were asked "To what extent are the two sentences expressing related meaning?", and were instructed to select an integer from 1 (com- pletely unrelated) to 5 (very related). We can see the difficulty annotators faced when estimating seman- tic relatedness, especially because the task descrip- tion was intentionally vague to avoid biasing anno- tator judgments with strict definitions. In the end, "the instructions described the task only through [a handful of] examples of relatedness" <ref type="bibr" target="#b6">(Marelli et al., 2014)</ref>.</p><p>As a result of this hands-off annotation guideline, the SICK dataset contains glaring inconsistencies in semantic relatedness scores, even for sentence pairs where the only difference between two sentences is due to the same known transformation. <ref type="figure">Figure 1</ref> demonstrates the wide range of human-given scores for the pairs from SICK that were created with the Negated Subject transformation. Since the guide- lines did not establish how to handle the effect of this transformation, some annotators rated it high for describing the same actions, while others rated it low for having completely opposite subjects.</p><p>To better appreciate the scope of these annota- tion discrepancies, <ref type="figure" target="#fig_0">Figure 2</ref> displays the distribution of "gold" human scores for every instance of the "Negated Subject" transformation. We actually find that the relatedness score approximately follows a normal distribution centered at 3.6 with a standard  deviation of about .45. The issue with this distri- bution is that regardless of whether the annotators rank this specific transformation as low or high, they should be ranking it consistently. Instead, we see that their annotations span all the way from 2.5 to 4.5 with no reasonable justification as to why.</p><p>Further, a natural question to ask is whether all sentence pairs within this common Negated Sub- ject transformation do, in fact, share a structure of how similar their relatedness scores "should" be.</p><p>To answer this question, we computed the similar- ity between the sentences in an automated manner using three substantially different evaluation met- rics: METEOR, BADGER, and TERp. These three metrics were chosen because they present three very different approaches for quantifying semantic sim- ilarity, namely: sentence alignments, compression redundancies, and edit rates. We felt that these dif- ferent approaches for processing the sentence pairs would allow for different views of their underlying relatedness.</p><p>To better understand how similar an automatic metric would rate these sentence pairs, <ref type="figure" target="#fig_1">Figure 3</ref> shows the distribution over scores predicted by the METEOR metric. The first observation is that the metric produces scores that are far more peaky than the gold scores in <ref type="figure" target="#fig_0">Figure 2</ref>, which indicates that they have a significantly more consistent structure about them.</p><p>In order to see how each metric's scores com- pare, <ref type="table" target="#tab_2">Table 3</ref> lists all pairwise correlations between the gold and the three metrics. As a sanity check, we can see that the 1.0s along the diagonal indicate perfect correlation between a prediction and itself. More interestingly, we can see that the three met- rics have alarmingly low correlations with the gold scores: 0.09, 0.03, and 0.07. However, we also see that the three metrics all have significantly higher correlations amongst one another: 0.80, 0.80, and 0.91. This is a very strong indication that the three metrics all have approximate agreement about how the various sentences should be scored, but this con- sensus is not at all reflected by the human judg- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">MUTT Experiments</head><p>In our Metric Unit TesTing experiments, we wanted to measure the fraction of times that a given metric is able to appropriately handle a particular corrup- tion type. Each (original,corruption) pair is consid- ered a trial, which the metric either gets correct or   incorrect. We report the percent of successful trials for each metric in <ref type="table">Tables 4, 5</ref>, and 6. Experiments were run using 5, 10, and 20 reference sentences to understand which metrics are able perform well without much data and also which metrics are able to effectively use more data to improve. An accu- racy of 75% would indicate that the metric is able to assign appropriate scores 3 out of 4 times. <ref type="bibr">4</ref> For Meaning-altering and Fleuncy-disrupting corruptions, the corrupted sentence will be truly dif- ferent from the original and reference sentences. A trial would be successful when the score of the orig- inal s orig is rated higher than the score of the cor- ruption s corr : s orig &gt; s corr Alternatively, Meaning-preserving transforma- tions create a "corruption" sentence which is just as correct as the original. To reflect this, we consider a trial to be successful when the score of the corrup- tion s corr is within 15% of the score of the original s orig :</p><formula xml:id="formula_0">s orig − s corr s orig + ≤ 0.15</formula><p>where is a small constant (10 −9 ) to prevent divi- sion by zero. We refer to this alternative trial formu- lation as the Difference formula. <ref type="bibr">4</ref> Our code is made available at https://github. com/text-machine-lab/MUTT <ref type="figure">Figure 5</ref>: Results for the Active-to-Passive corruption (using Difference formula scores).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Meaning-altering corruptions</head><p>As shown by the middle figure in <ref type="table">Table 4</ref>, it is CIDEr which performs the best for Antonym Re- placement. Even with only a few reference sen- tences, it is already able to score significantly higher than the other metrics. We believe that a large contributing factor for this is CIDEr's use of tf-idf weights to emphasize the important aspects of each sentence, thus highlighting the modified when com- pared against the reference sentences.</p><p>The success of these metrics reiterates the earlier point about metrics being able to perform more con- sistently and reliably than human judgment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Meaning-preserving corruptions</head><p>The graph in <ref type="figure" target="#fig_2">Figure 4</ref> of the determiner substitu- tion corruption shows an interesting trend: as the number of references increase, all of the metrics in- crease in accuracy. This corruption replaces "a" in the candidate with a "the', or vice versa. As the ref- erences increase, there we tend to see more exam- ples which use these determiners interchanagably while keeping the rest of the sentence's meaning the same. Since a large number of references results in far more for the pair to agree on, the two scores are very close.</p><p>Conversely, the decrease in accuracy in the  <ref type="table">Table 4</ref>: Meaning-altering corruptions. These % accuracies represent the number of times that a given metric was able to correctly score the original sentence higher than the corrupted sentence. Numbers refer- enced in the prose analysis are highlighted in bold.  <ref type="table">Table 5</ref>: Meaning-preserving corruptions. These % accuracies represent the number of times that a given metric was able to correctly score the semantically-equaivalent "corrupted" sentence within 15% of the original sentence. Numbers referenced in the prose analysis are highlighted in bold.  <ref type="table">Table 6</ref>: Fluency-disrupting corruptions. These % accuracies represent the number of times that a given metric was able to correctly score the original sentence higher than the corrupted sentence. Numbers refer- enced in the prose analysis are highlighted in bold.</p><p>Active-to-Passive table reflects how adding more (mostly active) references makes the system more (incorrectly) confident in choosing the active origi- nal. As the graph in <ref type="figure">Figure 5</ref> shows, METEOR per- formed the best, likely due to its sentence alignment approach to computing scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Fluency disruptions</head><p>All of the metrics perform well at identifying the duplicate prepositional phrase corruption, except for BADGER which has noticeably lower accuracy scores than the rest. These lower scores may be at- tributed to the compression algorithm that it uses to compute similarity. Because BADGER's algorithm works by compressing the candidate and references jointly, we can see why a repeated phrase would be of little effort to encode -it is a compression algo- rithm, after all. The result of easy-to-compress re- dundancies is that the original sentence and its cor- ruption have very similar scores, and BADGER gets fooled.</p><p>Unlike the other two fluency-disruptions, none of the accuracy scores of the "Remove Head from PP" corruption reach 90%, so this corruption could be seen as one that metrics could use improvement on. BLEU performed the best on this task. This is likely due to its ngram-based approach, which is able to identify that deleting a word breaks the fluency of a sentence.</p><p>All of the metrics perform well on the "Re-order Chunks" corruption. METEOR, however, does slightly worse than the other metrics. We believe this to be due to its method of generating an align- ment between the words in the candidate and refer- ence sentences. This alignment is computed while minimizing the number of chunks of contiguous and identically ordered tokens in each sentence pair <ref type="bibr" target="#b1">(Chen et al., 2015)</ref>. Both the original sentence and the corruption contain the same chunks, so it makes sense that METEOR would have more trouble dis- tinguishing between the two than the n-gram based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">W2V-AVG</head><p>The results for the W2V-AVG metric's success on each corruption are shown in <ref type="table" target="#tab_5">Table 7</ref>. "Shuffled Chunks" is one of the most interesting corruptions for this metric, because it achieves an accuracy of 0% across the board. The reason for this is that W2V-AVG is a pure bag-of-words model, meaning that word order is entirely ignored, and as a result the model cannot distinguish between the original sentence and its corruption, and so it can never rank the original greater than the corruption.</p><p>Surprisingly, we find that W2V-AVG is far less fooled by active-to-passive than most other metrics. Again, we believe that this can be attributed to its bag-of-words approach, which ignores the word or- der imposed by active and passive voices. Because each version of the sentence will contain nearly all of the same tokens (with the exception of a few "is" and "being" tokens), the two sentence representa- tions are very similar. In a sense, W2V-AVG does well on passive sentences for the wrong reasons - rather than understanding that the semantics are un- changed, it simply observes that most of the words are the same. However, we still see the trend that performance goes down as the number  sentences increases. Interestingly, we can see that although W2V-AVG achieved 98% accuracy on "Negated Subject", it scored only 75% on "Negated Action". This ini- tially seems quite counter intuitive -either the model should be good at the insertion of a negation word, or it should be bad. The explanation for this reveals a bias in the data itself: in every instance where the "Negated Subject" corruption was applied, the sentence was transformed from "A/The [subject] is" to "There is no <ref type="bibr">[subject]</ref>". This is differs from the change in "Negated Action", which is simply the insertion of "not" into the sentence before an ac- tion. Because one of these corruptions resulted in 3x more word replacements, the model is able to identify it fairly well.</p><p>To confirm this, we added two final entries to <ref type="table" target="#tab_5">Table 7</ref> where we applied the Difference formula to the "Negated Subject" and "Negated Action" corruptions to see the fraction of sentence pairs whose scores are within 15% of one another. We found that, indeed, the "Negated Action" corruption scored 100% (meaning that the corruption embed- dings were very similar to the original embeddings), while the "Negated Subject" corruption pairs were only similar about 85% of the time. By analyz- ing these interpretable errors, we can see that stop words play a larger role than we'd want in our toy metric. To develop a stronger metric, we might change W2V-AVG so that it considers only the con- tent words when computing the centroid embed- dings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>The main contribution of this work is a novel ap- proach for analyzing evaluation metrics for lan- guage generation tasks using Metric Unit TesTs. Not only is this evaluation procedure able to high- light particular metric weaknesses, it also demon- strates results which are far more consistent than correlation with human judgment; a good metric will be able to score well regardless of how noisy the human-derived scores are. Finally, we demonstrate the process of how this analysis can guide the devel- opment and strengthening of newly created metrics that are developed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Human annotations for the Negated Subject corruption.</figDesc><graphic url="image-2.png" coords="5,87.08,119.99,198.42,142.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Metric predictions for the Negated Subject corruption.</figDesc><graphic url="image-3.png" coords="5,87.08,308.38,198.43,145.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results for the Determiner Substitution corruption (using Difference formula scores).</figDesc><graphic url="image-4.png" coords="6,89.91,119.98,192.76,152.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Corruptions from the SICK dataset. The left column lists the number of instances for each corruption type. 

Fluency disruptions 
7 double PP </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Pairwise correlation between the predictions of three evaluation metrics and the gold standard.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>1 .</head><label>1</label><figDesc></figDesc><table>Negated Subject 
num refs 
5 
10 
20 
CIDEr 
99.4 99.4 99.4 
BLEU 
99.1 99.7 99.7 
METEOR 97.0 98.5 98.2 
BADGER 97.9 97.6 98.2 
TERp 
99.7 99.7 99.4 

2. Negated Action 
num refs 
5 
10 
20 
CIDEr 
98.5 98.5 98.5 
BLEU 
97.5 97.5 98.0 
METEOR 96.0 96.0 97.0 
BADGER 93.6 95.5 96.5 
TERp 
95.5 97.0 95.0 

3. Antonym Replacement 
num refs 
5 
10 
20 
CIDEr 
86.2 92.7 93.5 
BLEU 
76.4 85.4 88.6 
METEOR 80.9 86.6 91.5 
BADGER 76.0 85.8 88.6 
TERp 
75.2 79.7 80.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>7 .</head><label>7</label><figDesc></figDesc><table>Duplicate PP 
num refs 
5 
10 
20 
CIDEr 
100 
99.0 100 
BLEU 
100 
100 
100 
METEOR 95.1 98.5 99.5 
BADGER 63.5 70.0 74.9 
TERp 
96.6 99.0 99.0 

8. Remove Head From PP 
num refs 
5 
10 
20 
CIDEr 
69.5 76.8 80.8 
BLEU 
63.5 81.3 87.7 
METEOR 60.6 72.9 84.2 
BADGER 63.1 67.0 71.4 
TERp 
52.7 66.5 70.4 

9. Re-order Chunks 
num refs 
5 
10 
20 
CIDEr 
91.4 95.6 96.6 
BLEU 
83.0 91.4 94.2 
METEOR 81.2 89.6 92.4 
BADGER 95.4 96.6 97.8 
TERp 
91.0 93.4 93.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Performance of the AVG-W2V metric. These 

% accuracies represent the number of successful trials. 
Numbers referenced in the prose analysis are highlighted 
in bold. * indicates the scores computed with the Differ-
ence formula. 

</table></figure>

			<note place="foot" n="4"> Datasets 4.1 SICK All of our experiments are run on the Sentences Involving Compositional Knowledge (SICK) dataset,</note>

			<note place="foot" n="1"> http://nlp.cs.illinois.edu/ HockenmaierGroup/data.html 2 http://www.cs.york.ac.uk/ semeval-2012/task6/index.php?id=data 3 For instance, the &quot;Antonym Replacement&quot; template checked to see if the two sentences were one word apart, and if so whether they had a SICK-annotated NOT ENTAILMENT relationsip.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Zaidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="17" to="53" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rouge: a package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="25" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Results of the wmt14 metrics shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matous</forename><surname>Machacek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA, June</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="293" to="301" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Re-examining machine translation metrics for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="182" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zamparelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fondazione Bruno</forename><surname>Kessler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-09" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Badger: A new machine translation metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Parker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Ter-plus: Paraphrase, semantic, and alignment enhancements to translation edit rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">G</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cider: Consensusbased image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1411.5726</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
