<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Attention with an Ensemble of Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchan</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">Domain Attention with an Ensemble of Experts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="643" to="653"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1060</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>An important problem in domain adaptation is to quickly generalize to a new domain with limited supervision given K existing domains. One approach is to retrain a global model across all K + 1 domains using standard techniques, for instance Daumé III (2009). However, it is desirable to adapt without having to re-estimate a global model from scratch each time a new domain with potentially new intents and slots is added. We describe a solution based on attending an ensemble of domain experts. We assume K domain-specific intent and slot models trained on respective domains. When given domain K + 1, our model uses a weighted combination of the K domain experts&apos; feedback along with its own opinion to make predictions on the new domain. In experiments, the model significantly outperforms base-lines that do not use domain adaptation and also performs better than the full retraining approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An important problem in domain adaptation is to quickly generalize to a new domain with limited supervision given K existing domains. In spo- ken language understanding, new domains of in- terest for categorizing user utterances are added on a regular basis <ref type="bibr">1</ref> . For instance, we may <ref type="bibr">1</ref> A scenario frequently arising in practice is having a re- quest for creating a new virtual domain targeting a specific application. One typical use case is that of building natural language capability through intent and slot modeling (with- out actually building a domain classifier) targeting a specific application.</p><p>add ORDERPIZZA domain and desire a domain- specific intent and semantic slot tagger with a lim- ited amount of training data. Training only on the target domain fails to utilize the existing resources in other domains that are relevant (e.g., labeled data for PLACES domain with place name, location as the slot types), but naively training on the union of all domains does not work well since different domains can have widely varying distributions.</p><p>Domain adaptation offers a balance between these extremes by using all data but simultane- ously distinguishing domain types. A common approach for adapting to a new domain is to re- train a global model across all K + 1 domains us- ing well-known techniques, for example the fea- ture augmentation method of <ref type="bibr" target="#b6">Daumé III (2009)</ref> which trains a single model that has one domain- invariant component along with K + 1 domain- specific components each of which is specialized in a particular domain. While such a global model is effective, it requires re-estimating a model from scratch on all K + 1 domains each time a new do- main is added. This is burdensome particularly in our scenario in which new domains can arise fre- quently.</p><p>In this paper, we present an alternative solu- tion based on attending an ensemble of domain experts. We assume that we have already trained K domain-specific models on respective domains. Given a new domain K +1 with a small amount of training data, we train a model on that data alone but queries the K experts as part of the training procedure. We compute an attention weight for each of these experts and use their combined feed- back along with the model's own opinion to make predictions. This way, the model is able to selec- tively capitalize on relevant domains much like in standard domain adaptation but without explicitly re-training on all domains together.</p><p>In experiments, we show clear gains in a do- main adaptation scenario across 7 test domains, yielding average error reductions of 44.97% for intent classification and 32.30% for slot tagging compared to baselines that do not use domain adaptation. Moreover we have higher accuracy than the full re-training approach of <ref type="bibr" target="#b22">Kim et al. (2016c)</ref>, a neural analog of Daumé III (2009).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain Adaptation</head><p>There is a venerable history of research on do- main adaptation <ref type="bibr" target="#b7">(Daume III and Marcu, 2006;</ref><ref type="bibr" target="#b6">Daumé III, 2009;</ref><ref type="bibr" target="#b2">Blitzer et al., 2006</ref><ref type="bibr" target="#b1">Blitzer et al., , 2007</ref><ref type="bibr" target="#b32">Pan et al., 2011</ref>) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daumé III <ref type="formula">(2009)</ref> whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domain- specific patterns, the model is forced to learn from all domains yet preserve domain-specific knowl- edge. The method is generalized to the neu- ral paradigm by <ref type="bibr" target="#b22">Kim et al. (2016c)</ref> who jointly use a domain-specific LSTM and also a global LSTM shared across all domains. In the con- text of SLU, <ref type="bibr" target="#b14">Jaech et al. (2016)</ref> proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; <ref type="bibr" target="#b22">Kim et al. (2016c)</ref> instead employed K + 1 LSTMs.  proposed to employ a sequence-to-sequence model by introducing a fic- titious symbol at the end of an utterance of which tag represents the corresponding domain and in- tent.</p><p>All these methods require one to re-train a model from scratch to make it learn the correlation and invariance between domains. This becomes difficult to scale when there is a new domain com- ing in at high frequency. We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts. This gives a clear computational advantage over the feature augmentation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spoken Language Understanding</head><p>Recently, there has been much investment on the personal digital assistant (PDA) technology in in- dustry <ref type="bibr" target="#b33">(Sarikaya, 2015;</ref>. Ap- ples Siri, Google Now, Microsofts Cortana, and Amazons Alexa are some examples of personal digital assistants. Spoken language understanding (SLU) is an important component of these exam- ples that allows natural communication between the user and the agent <ref type="bibr" target="#b38">(Tur, 2006;</ref><ref type="bibr" target="#b8">El-Kahky et al., 2014</ref>). PDAs support a number of scenarios in- cluding creating reminders, setting up alarms, note taking, scheduling meetings, finding and consum- ing entertainment (i.e. movie, music, games), find- ing places of interest and getting driving directions to them <ref type="bibr" target="#b17">(Kim et al., 2016a</ref>).</p><p>Naturally, there has been an extensive line of prior studies for domain scaling problems to eas- ily scale to a larger number of domains: pre- training ( <ref type="bibr" target="#b20">Kim et al., 2015c</ref>), transfer learning ( <ref type="bibr" target="#b24">Kim et al., 2015d</ref>), constrained decoding with a sin- gle model <ref type="bibr" target="#b17">(Kim et al., 2016a)</ref>, multi-task learn- ing ( <ref type="bibr" target="#b14">Jaech et al., 2016)</ref>, neural domain adap- tation ( <ref type="bibr" target="#b22">Kim et al., 2016c)</ref>, domainless adapta- tion ( <ref type="bibr" target="#b21">Kim et al., 2016b</ref>), a sequence-to-sequence model , adversary do- main training <ref type="bibr" target="#b18">(Kim et al., 2017</ref>) and zero-shot learning <ref type="bibr" target="#b9">Ferreira et al., 2015)</ref>.</p><p>There are also a line of prior works on enhanc- ing model capability and features: jointly mod- eling intent and slot predictions <ref type="bibr" target="#b15">(Jeong and Lee, 2008;</ref><ref type="bibr" target="#b39">Xu and Sarikaya, 2013;</ref><ref type="bibr" target="#b11">Guo et al., 2014;</ref><ref type="bibr" target="#b40">Zhang and Wang, 2016;</ref><ref type="bibr">Liu and Lane, 2016a,b)</ref>, modeling SLU models with web search click logs ( <ref type="bibr" target="#b27">Li et al., 2009;</ref><ref type="bibr" target="#b16">Kim et al., 2015a</ref>) and enhancing features, including representations ( <ref type="bibr" target="#b0">Anastasakos et al., 2014;</ref><ref type="bibr" target="#b4">Celikyilmaz et al., , 2010</ref><ref type="bibr" target="#b23">Kim et al., 2016d</ref>) and lexicon ( <ref type="bibr" target="#b19">Kim et al., 2015b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We use an LSTM simply as a mapping φ :    <ref type="figure">Figure 1</ref>: The overall network architecture of the individual model.</p><formula xml:id="formula_0">R d × R d → R d that</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Individual Model Architecture</head><p>Let C denote the set of character types and W the set of word types. Let ⊕ denote the vector concatenation operation. A wildly successful ar- chitecture for encoding a sentence (w 1 . . . w n ) ∈ W n is given by bidirectional LSTMs (BiLSTMs) <ref type="bibr" target="#b36">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b10">Graves, 2012)</ref>. Our model first constructs a network over an utterance closely following <ref type="bibr" target="#b26">Lample et al. (2016)</ref>. The model parameters Θ associated with this BiLSTM layer are</p><formula xml:id="formula_1">• Character embedding e c ∈ R 25 for each c ∈ C • Character LSTMs φ C f , φ C b : R 25 × R 25 → R 25 • Word embedding e w ∈ R 100 for each w ∈ W • Word LSTMs φ W f , φ W b : R 150 ×R 100 → R 100 Let w 1 .</formula><p>. . w n ∈ W denote a word sequence where word w i has character w i (j) ∈ C at position j. First, the model computes a character-sensitive word representation v i ∈ R 150 as</p><formula xml:id="formula_2">f C j = φ C f e w i (j) , f C j−1 ∀j = 1 . . . |w i | b C j = φ C b e w i (j) , b C j+1 ∀j = |w i | . . . 1 v i = f C |w i | ⊕ b C 1 ⊕ e w i</formula><p>for each i = 1 . . . n. 2 Next, the model computes</p><formula xml:id="formula_3">f W i = φ W f v i , f W i−1 ∀i = 1 . . . n b W i = φ W b v i , b W i+1 ∀i = n . . . 1</formula><p>and induces a character-and context-sensitive word representation h i ∈ R 200 as</p><formula xml:id="formula_4">h i = f W i ⊕ b W i (1)</formula><p>for each i = 1 . . . n. These vectors can be used to perform intent classification or slot tagging on the utterance.</p><p>Intent Classification We can predict the intent of the utterance using (h 1 . . . h n ) ∈ R 200 in <ref type="formula" target="#formula_14">(1)</ref> as follows. Let I denote the set of intent types. We introduce a single-layer feedforward network g i : R 200 → R |I| whose parameters are denoted by Θ i . We compute a |I|-dimensional vector</p><formula xml:id="formula_5">µ i = g i n i=1</formula><p>h i and define the conditional probability of the cor- rect intent τ as</p><formula xml:id="formula_6">p(τ |h 1 . . . h n ) ∝ exp µ i τ (2)</formula><p>The intent classification loss is given by the nega- tive log likelihood:</p><formula xml:id="formula_7">L i Θ, Θ i = − l log p τ (l) |h (l)<label>(3)</label></formula><p>where l iterates over intent-annotated utterances.</p><p>Slot Tagging We predict the semantic slots of the utterance using (h 1 . . . h n ) ∈ R 200 in (1) as follows. Let S denote the set of semantic types and L the set of corresponding BIO label types 3 that is, L = {B-e : e ∈ E} ∪ {I-e : e ∈ E} ∪ {O}. We add a transition matrix T ∈ R |L|×|L| and a single- layer feedforward network g t : R 200 → R |L| to the network; denote these additional parameters by Θ t . The conditional random field (CRF) tag- ging layer defines a joint distribution over label sequences of y 1 . . . y n ∈ L of w 1 . . . w n as</p><formula xml:id="formula_8">p(y 1 . . .y n |h 1 . . . h n ) ∝ exp n i=1 T y i−1 ,y i × g t y i (h i )<label>(4)</label></formula><p>The tagging loss is given by the negative log like- lihood:</p><formula xml:id="formula_9">L t Θ, Θ t = − l log p y (l) |h (l)<label>(5)</label></formula><p>where l iterates over tagged sentences in the data. Alternatively, we can optimize the local loss:</p><formula xml:id="formula_10">L t−loc Θ, Θ t = − l i log p y (l) i |h (l) i<label>(6)</label></formula><p>where p(y i |h i ) ∝ exp g t y i (h i ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Domain Attention Architecture</head><p>Now we assume that for each of the K domains we have an individual model described in Section 3.1. Denote these domain experts by Θ (1) . . . Θ (K) .</p><p>We now describe our model for a new domain K + 1. Given an utterance w 1 . . . w n , it uses a BiLSTM layer to induce a feature representation h 1 . . . h n as specified in <ref type="formula" target="#formula_14">(1</ref>  </p><formula xml:id="formula_11">q dot i,k = h i h (k)<label>(7)</label></formula><p>in the simplest case. We also experiment with the bilinear function</p><formula xml:id="formula_12">q bi i,k = h i Bh (k)<label>(8)</label></formula><p>where B is an additional model parameter, and also the feedforward function</p><formula xml:id="formula_13">q feed i,k = W tanh U h i + V h (k) + b 1 + b 2 (9)</formula><p>where U, V, W, b 1 , b 2 are additional model param- eters. The final attention weights a</p><formula xml:id="formula_14">(1) i . . . a<label>(1)</label></formula><p>i are obtained by using a softmax layer</p><formula xml:id="formula_15">a i,k = exp(q i,k ) K k=1 exp(q i,k )<label>(10)</label></formula><p>The weighted combination of the experts' feed- back is given by</p><formula xml:id="formula_16">h experts i = K k=1 a i,k h (k) i<label>(11)</label></formula><p>and the model makes predictions by using ¯ h 1 . . . ¯ h n where</p><formula xml:id="formula_17">¯ h i = h i ⊕ h experts i<label>(12)</label></formula><p>These vectors replace the original feature vectors h i in defining the intent or tagging losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Domain Attention Variants</head><p>We also consider two variants of the domain atten- tion architecture in Section 4.1.</p><p>Label Embedding In addition to the state vec- tors h (1) . . . h (K) produced by K experts, we fur- ther incorporate their final (discrete) label predic- tions using pre-trained label embeddings. We in- duce embeddings e y for labels y from all domains using the method of <ref type="bibr" target="#b24">Kim et al. (2015d)</ref>. At the i-th word, we predict the most likely label y (k) under the k-th expert and compute an attention weight as</p><formula xml:id="formula_18">¯ q dot i,k = h i e y (k)<label>(13)</label></formula><p>Then we compute an expectation over the experts' predictions</p><formula xml:id="formula_19">¯ a i,k = exp(¯ q i,k ) K k=1 exp(¯ q i,k )<label>(14)</label></formula><formula xml:id="formula_20">h label i = K k=1 ¯ a i,k e y (k) i<label>(15)</label></formula><p>and use it in conjunction with ¯ h i . Note that this makes the objective a function of discrete decision and thus non-differentiable, but we can still opti- mize it in a standard way treating it as learning a stochastic policy.</p><p>Selective Attention Instead of computing atten- tion over all K experts, we only consider the top K ≤ K that predict the highest label scores. We only compute attention over these K vectors. We experiment with various values of K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we describe the set of experi- ments conducted to evaluate the performance of our model. In order to fully assess the contri- bution of our approach, we also consider several baselines and variants besides our primary expert model.  <ref type="table">Table 2</ref>: The overlapping percentage of intent types and slot types with experts or source do- mains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|I| |S|</head><note type="other">Description EVENTS 10 12 Buy event tickets FITNESS 10 9 Track health M-TICKET 8 15 Buy movie tickets ORDERPIZZA 19 27 Order pizza REMINDER 19 20 Remind task TAXI 8 13 Find/book an cab TV 7 5 Control TV</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Test domains and Tasks</head><p>To test the effectiveness of our proposed approach, we apply it to a suite of 7 Microsoft Cortana do- mains with 2 separate tasks in spoken language un- derstanding: (1) intent classification and (2) slot (label) tagging. The intent classification task is a multi-class classification problem with the goal of determining to which one of the |I| intents a user utterance belongs within a given domain. The slot tagging task is a sequence labeling problem with the goal of identifying entities and chunk- ing of useful information snippets in a user utter- ance. For example, a user could say "reserve a table at joeys grill for thursday at seven pm for five people". Then the goal of the first task would be to classify this utterance as "make reservation" intent given the places domain, and the goal of the second task would be to tag "joeys grill" as restaurant, "thursday" as date, "seven pm" as time, and "five" as number people. The short descriptions on the 7 test domains are shown in <ref type="table" target="#tab_2">Table 1</ref>. As the table shows, the test domains have different granularity and diverse se- mantics. For each personal assistant test domain, we only used 1000 training utterances to simulate scarcity of newly labeled data. The amount of de- velopment and test utterance was 100 and 10k re- spectively.</p><p>The similarities of test domains, represented by overlapping percentage, with experts or source domains are represented in <ref type="table">Table 2</ref>. The in- tent overlapping percentage ranges from 30% on FITNESS domain to 70% on EVENTS, which av- erages out at 51.49%. And the slots for test do- mains overlaps more with those of source domains ranging from 60% on TV domain to 100% on both M-TICKET and TAXI domains, which averages out at 81.69%.  In testing our approach, we consider a domain adaptation (DA) scenario, where a target domain has a limited training data and the source domain has a sufficient amount of labeled data. We further consider a scenario, creating a new virtual domain targeting a specific scenario given a large inven- tory of intent and slot types and underlying models build for many different applications and scenar- ios. One typical use case is that of building natural language capability through intent and slot model- ing (without actually building a domain classifier) targeting a specific application. Therefore, our ex- perimental settings are rather different from previ- ously considered settings for domain adaptation in two aspects:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>• Multiple source domains: In most previous works, only a pair of domains (source vs. tar- get) have been considered, although they can be easily generalized to K &gt; 2. Here, we experiment with K = 25 domains shown in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>• Variant output: In a typical setting for do- main adaptation, the label space is invariant across all domains. Here, the label space can be different in different domains, which is a more challenging setting. See <ref type="bibr" target="#b24">Kim et al. (2015d)</ref> for details of this setting.</p><p>For this DA scenario, we test whether our ap- proach can effectively make a system to quickly generalize to a new domain with limited supervi- sion given K existing domain experts shown in 3 .</p><p>In summary, our approach is tested with 7 Mi- crosoft Cortana personal assistant domains across 2 tasks of intent classification and slot tagging. Below shows more detail of our baselines and vari- ants used in our experiments.</p><p>Baselines: All models below use same underly- ing architecture described in Section 3.1</p><p>• TARGET: a model trained on a targeted do- main without DA techniques.</p><p>• UNION: a model trained on the union of a tar- geted domain and 25 domain experts.</p><p>• DA: a neural domain adaptation method of <ref type="bibr" target="#b22">Kim et al. (2016c)</ref> which trains domain spe- cific K LSTMs with a generic LSTM on all domain training data.</p><p>Domain Experts (DE) variants: All models be- low are based on attending on an ensemble of 25 domain experts (DE) described in Section 4.1, where a specific set of intent and slots models are trained for each domain. We have two feedback from domain experts: (1) feature representation from LSTM, and (2) label embedding from feed- foward described in Section 4.1 and Section 4.2, respectively.</p><p>• DE B : DE without domain attention mecha- nism. It uses the unweighted combination of first feedback from experts like bag-of-word model.</p><p>• DE 1 : DE with domain attention with the weighted combination of the first feedbacks from experts.</p><p>• DE 2 : DE 1 with additional weighted combina- tion of second feedbacks.</p><p>• DE S2 : DE 2 with selected attention mecha- nism, described in Section 4.2.</p><p>In our experiments, all the models were imple- mented using Dynet ( <ref type="bibr" target="#b31">Neubig et al., 2017)</ref> and were trained using Stochastic Gradient Descent (SGD) with Adam ( <ref type="bibr" target="#b25">Kingma and Ba, 2015</ref>)-an adaptive learning rate algorithm. We used the ini- tial learning rate of 4 × 10 −4 and left all the other hyper parameters as suggested in <ref type="bibr" target="#b25">Kingma and Ba (2015)</ref>. Each SGD update was computed with- out a minibatch with Intel MKL (Math Kernel Li- brary) <ref type="bibr">4</ref> . We used the dropout regularization <ref type="bibr" target="#b37">(Srivastava et al., 2014</ref>) with the keep probability of 0.4 at each LSTM layer.</p><p>To encode user utterances, we used bidirec- tional LSTMs (BiLSTMs) at the character level and the word level, along with 25 dimensional character embedding and 100 dimensional word embedding. The dimension of both the input and output of the character LSTMs were 25, and the dimensions of the input and output of the word LSTMs were 150 5 and 100, respectively. The di- mension of the input and output of the final feed- forward network for intent, and slot were 200 and the number of their corresponding task. Its activa- tion was rectified linear unit (ReLU).</p><p>To initialize word embedding, we used word embedding trained from ( <ref type="bibr" target="#b26">Lample et al., 2016</ref>). In the following sections, we report intent classifica- tion results in accuracy percentage and slot results in F1-score. To compute slot F1-score, we used the standard CoNLL evaluation script <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>We show our results in the DA setting where we had a sufficient labeled dataset in the 25 source domains shown in <ref type="table" target="#tab_4">Table 3</ref>, but only 1000 labeled data in the target domain. The performance of the baselines and our domain experts DE variants are shown in <ref type="table" target="#tab_6">Table 4</ref>. The top half of the table shows the results of intent classification and the results of slot tagging is in the bottom half.</p><p>The baseline which trained only on the target domain (TARGET) shows a reasonably good per- formance, yielding on average 87.7% on the in- tent classification and 83.9% F1-score on the slot tagging. Simply training a single model with ag- gregated utterance across all domains (UNION) brings the performance down to 77.4% and 75.3%. Using DA approach of <ref type="bibr" target="#b22">Kim et al. (2016c)</ref> shows a significant increase in performance in all 7 do- mains, yielding on average 90.3% intent accuracy and 86.2%.</p><p>The DE without domain attention (DE B ) shows similar performance compared to DA. Using DE model with domain attention (DE 1 ) shows an- other increase in performance, yielding on aver- age 90.9% intent accuracy and 86.9%. The per- formance increases again when we use both fea- ture representation and label embedding (DE 2 ), yielding on average 91.4% and 88.2% and observe nearly 93.6% and 89.1% when using selective at- tention (DE S2 ). Note that DE S2 selects the appro- priate number of experts per layer by evaluation on a development set.</p><p>The results show that our expert variant ap- proach (DE S2 ) achieves a significant performance gain in all 7 test domains, yielding average er- ror reductions of 47.97% for intent classification and 32.30% for slot tagging. The results suggest that our expert approach can quickly generalize to a new domain with limited supervision given K existing domains by having only a handful more data of 1k newly labeled data points. The poor performance of using the union of both source and target domain data might be due to the rela- tively very small size of the target domain data, overwhelmed by the data in the source domain. For example, a word such as "home" can be la- beled as place type under the TAXI domain, but in the source domains can be labeled as ei- ther home screen under the PHONE domain or contact name under the CALENDAR domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training Time</head><p>The <ref type="figure">Figure 3</ref> shows the time required for training DE S2 and DA of <ref type="bibr" target="#b22">Kim et al. (2016c</ref>   <ref type="figure">Figure 4</ref>: Learning curves in accuracy across all seven test domains as the number of expert do- mains increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Learning Curve</head><p>We also measured the performance of our methods as a function of the number of domain experts. For each test domain, we consider all possible sizes of experts ranging from 1 to 25 and we then take the average of the resulting performances obtained from the expert sets of all different sizes. <ref type="figure">Figure 4</ref> shows the resulting learning curves for each test domain. The overall trend is clear: as the more ex- pert domains are added, the more the test perfor- mance improves. With ten or more expert domains added, our method starts to get saturated achiev- ing more than 90% in accuracy across all seven domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Attention weights</head><p>From the heatmap shown in <ref type="figure" target="#fig_3">Figure 5</ref>, we can see that the attention strength generally agrees with common sense. For example, the M-TICKET and TAXI domain selected MOVIE and PLACES as their top experts, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Oracle Expert</head><p>Domain  <ref type="table">Table 5</ref>: Intent classification accuracy with an or- acle expert in the expert pool.</p><p>The results in <ref type="table">Table 5</ref> show the intent classi- fication accuracy of DE 2 when we already have the same domain expert in the expert pool. To simulate such a situation, we randomly sampled 1,000, 100, and 100 utterances from each domain as training, development and test data, respec- tively. In both ALARM and HOTEL domains, the trained models only on the 1,000 training utter- ances (TARGET) achieved only 70.1%and 65.2% in accuracy, respectively. Whereas, with our method (DE 2 ) applied, we reached almost the full training performance by selectively paying at- tention to the oracle expert, yielding 98.2% and 96.9%, respectively. This result again confirms that the behavior of the trained attention network indeed matches the semantic closeness between different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Selective attention</head><p>The results in  <ref type="table" target="#tab_8">Table 6</ref>: Accuracies of DE S2 using different num- ber of experts.</p><p>number of experts in the pool. The rationale be- hind DE S2 is to alleviate the downside of soft at- tention, namely distributing probability mass over all items even if some are bad items. To deal with such issues, we apply a hard cut-off at top k do- mains. From the result, a threshold at top 3 or 5 yielded better results than that of either 1 or 25 ex- perts. This matches our common sense that their are only a few of domains that are close enough to be of help to a test domain. Thus it is advisable to find the optimal k value through several rounds of experiments on a development dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a solution for scal- ing domains and experiences potentially to a large number of use cases by reusing existing data la- beled for different domains and applications. Our solution is based on attending an ensemble of do- main experts. When given a new domain, our model uses a weighted combination of domain experts' feedback along with its own opinion to make prediction on the new domain. In both in- tent classification and slot tagging tasks, the model significantly outperformed baselines that do not use domain adaptation and also performed better than the full re-training approach. This approach enables creation of new virtual domains through a weighted combination of domain experts' feed- back reducing the need to collect and annotate the similar intent and slot types multiple times for dif- ferent domains. Future work can include an exten- sion of domain experts to take into account dialog history aiming for a holistic framework that can handle contextual interpretation as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>takes an input vector x and a state vector h to output a new state vector h = φ(x, h). See Hochreiter and Schmidhuber (1997) for a de- tailed description. At a high level, the individ- ual model consists of builds on several ingredients shown in Figure 1: character and word embed- ding, a bidirectional LSTM (BiLSTM) at a charac- ter layer, a BiLSTM at word level, and feedfoward network at the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>…</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall network architecture of the domain attention, which consists of three components: (1) K domain experts + 1 target BiLSTM layer to induce a feature representation, (2) K domain experts + 1 target feedfoward layer to output pre-trained label embedding (3) a final feedforward layer to output an intent or slot. We have two separate attention mechanisms to combine feedback from domain experts.</figDesc><graphic url="image-61.png" coords="4,307.28,62.81,218.25,192.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Heatmap visualizing attention weights.</figDesc><graphic url="image-64.png" coords="9,124.04,63.29,166.13,85.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The number of intent types (|I|), the num-
ber of slot types (|S|), and a short description of 
the test domains. 

Overlapping 
Domain 
Intents 
Slots 
EVENTS 
70.00% 75.00% 
FITNESS 
30.00% 77.78% 
M-TICKET 
37.50% 100.00% 
ORDERPIZZA 47.37% 74.07% 
REMINDER 
68.42% 85.00% 
TAXI 
50.00% 100.00% 
TV 
57.14% 60.00% 
AVG 
51.49% 81.69% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Overview of experts or source domains: 
Domain categories which have been created based 
on the label embeddings. These categorizations 
are solely for the purpose of describing domains 
because of the limited space and they are com-
pletely unrelated to the model. The number of 
sentences in each domain is in the range of 50k 
to 660k and the number of unique intents and slots 
are 200 and 500 respectively. In total, we have 25 
domain-specific expert models. For the average 
performance, intent accuracy is 98% and slot F1 
score is 96%. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>). The training time for DE S2 stays almost constant as the number of source domains increases. However, the train- ing time for DA grows exponentially in the num- ber of source domains. Specifically, when trained</figDesc><table>Task 

Domain 
TARGET UNION DA DE B DE 1 DE 2 DE S2 

Intent 

EVENTS 
88.3 
78.5 
89.9 93.1 92.5 92.7 94.5 
FITNESS 
88.0 
77.7 
92.0 92.0 91.2 91.8 94.0 
M-TICKET 
88.2 
79.2 
91.9 94.4 91.5 92.7 93.4 
ORDERPIZZA 
85.8 
76.6 
87.8 89.3 89.4 90.8 92.8 
REMINDER 
87.2 
76.3 
91.2 90.0 90.5 90.2 93.1 
TAXI 
87.3 
76.8 
89.3 89.9 89.6 89.2 93.7 
TV 
88.9 
76.4 
90.3 81.5 91.5 92.0 94.0 
AVG 
87.7 
77.4 
90.3 90.5 90.9 91.4 93.6 

Slot 

EVENTS 
84.8 
76.1 
87.1 87.4 88.1 89.4 90.2 
FITNESS 
84.0 
75.6 
86.4 86.3 87.0 88.1 88.9 
M-TICKET 
84.2 
75.6 
86.4 86.1 86.8 88.4 89.7 
ORDERPIZZA 
82.3 
73.6 
84.2 84.4 85.0 86.3 87.1 
REMINDER 
83.5 
75.0 
85.9 86.3 87.0 88.3 89.2 
TAXI 
83.0 
74.6 
85.6 85.5 86.3 87.5 88.6 
TV 
85.4 
76.7 
87.7 87.6 88.3 89.3 90.1 
AVG 
83.9 
75.3 
86.2 86.2 86.9 88.2 89.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Intent classification accuracy (%) and slot tagging F1-score (%) of our baselines and variants of 
DE. The numbers in boldface indicate the best performing methods. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 6 examines how the intent pre- diction accuracy of DE S2 varies with respect to the</head><label>6</label><figDesc></figDesc><table>Domain Top 1 Top 3 Top 5 Top 25 
EVENTS 98.1 
98.8 
99.2 
96.4 
TV 
81.4 
82.0 
81.7 
80.9 
AVG 
89.8 
90.4 
90.5 
88.7 

</table></figure>

			<note place="foot" n="2"> For simplicity, we assume some random initial state vectors such as f C 0 and b C |w i |+1 when we describe LSTMs.</note>

			<note place="foot" n="4"> https://software.intel.com/en-us/articles/intelr-mkl-andc-template-libraries 5 We concatenated last two outputs from the character LSTM and word embedding, resulting in 150 (25+25+100) 6 http://www.cnts.ua.ac.be/conll2000/chunking/output.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task specific continuous word representations for mono and multi-lingual spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasos</forename><surname>Anastasakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3246" to="3250" />
		</imprint>
	</monogr>
	<note>Acoustics, Speech and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 conference on empirical methods in natural language processing</title>
		<meeting>the 2006 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical investigation of word class-based features for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="994" to="1005" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>TASLP)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Convolutional neural network based semantic tagging with entity embeddings. genre</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silicon</forename><surname>Valley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkanitur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zero-shot learning of intent embeddings for expansion by convolutional deep structured semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6045" to="6049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno type="arXiv">arXiv:0907.1815</idno>
		<title level="m">Frustratingly easy domain adaptation</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="101" to="126" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Extending domain coverage of language understanding systems via intent transfer between domains using knowledge graphs and search query click logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>El-Kahky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zero-shot semantic parser for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Jabaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="15" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint semantic utterance classification and slot filling with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="554" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-domain joint semantic frame parsing using bi-directional rnn-lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 17th Annual Meeting of the International Speech Communication Association</title>
		<meeting>The 17th Annual Meeting of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Jaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00117</idno>
		<title level="m">Domain adaptation of recurrent neural networks for natural language understanding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Triangular-chain conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary Geunbae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1287" to="1302" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weakly supervised slot tagging with partially labeled sequences from web search click logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL. Association for Computational Linguistics</title>
		<meeting>the NAACL. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Natural language model reusability for scaling to different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rochette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the Empiricial Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial adaptation of synthetic or stale data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchan</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Compact lexicon selection with spectral methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pre-training of hidden-unit crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="192" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domainless adaptation by constrained decoding on a schema lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 26th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Frustratingly easy neural domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 26th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scalable semi-supervised query classification using matrix sketching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 54th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>page 8</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">New transfer learning techniques for disparate label sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL. Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Extracting structured information from user queries with semi-supervised conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="685" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint online spoken language understanding and language modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics</title>
		<meeting>the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A discriminative model based entity dictionary weighting approach for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="195" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
	</analytic>
	<monogr>
		<title level="m">The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The technology powering personal digital assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Keynote at Interspeech</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shrinkage based features for slot tagging with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="268" to="272" />
		</imprint>
	</monogr>
	<note>Anoop Deoras, and Minwoo Jeong</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An overview of endto-end language understanding and dialog management for personal digital assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Robichaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rochette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">Zia</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuahu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Spoken Language Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multitask learning for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICASSP</title>
		<meeting>the ICASSP<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular crf for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A joint model of intent determination and slot filling for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
