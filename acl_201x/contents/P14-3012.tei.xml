<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Text Normalization via Unsupervised Model and Discriminative Reranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 22-27 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dallas Computer Science Department</orgName>
								<orgName type="institution">The University of Texas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dallas Computer Science Department</orgName>
								<orgName type="institution">The University of Texas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Text Normalization via Unsupervised Model and Discriminative Reranking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the ACL 2014 Student Research Workshop</title>
						<meeting>the ACL 2014 Student Research Workshop <address><addrLine>Baltimore, Maryland USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="86" to="93"/>
							<date type="published">June 22-27 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Various models have been developed for normalizing informal text. In this paper, we propose two methods to improve nor-malization performance. First is an unsu-pervised approach that automatically identifies pairs of a non-standard token and proper word from a large unlabeled corpus. We use semantic similarity based on continuous word vector representation, together with other surface similarity measurement. Second we propose a reranking strategy to combine the results from different systems. This allows us to incorporate information that is hard to model in individual systems as well as consider multiple systems to generate a final rank for a test case. Both word-and sentence-level optimization schemes are explored in this study. We evaluate our approach on data sets used in prior studies, and demonstrate that our proposed methods perform better than the state-of-the-art systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been a lot of research efforts recently on analysis of social media text (e.g., from Twit- ter and Facebook) ( <ref type="bibr" target="#b25">Ritter et al., 2011;</ref><ref type="bibr" target="#b21">Owoputi et al., 2013;</ref>). One challenge in processing social media text is how to deal with the frequently occurring non-standard words, such as bday (meaning birthday), snd (meaning sound) and gl (meaning girl) . Normalizing informal text (changing non-standard words to standard ones) will ease subsequent language processing mod- ules.</p><p>Text normalization has been an important topic for the text-to-speech field. See ( <ref type="bibr" target="#b26">Sproat et al., 2001</ref>) for a good report of this problem. Recently, much research on normalization has been done for social text domain, which has many abbrevi- ations or non-standard tokens. A simple approach for normalization would be applying traditional spell checking model, which is usually based on edit distance <ref type="bibr" target="#b6">(Damerau, 1964;</ref><ref type="bibr" target="#b13">Levenshtein, 1966)</ref>. However, this model can not well handle the non- standard words in social media text due to the large variation in generating them.</p><p>Another line of work in normalization adopts a noisy channel model. For a non-standard to- ken A, this method finds the most possible stan- dard wordˆSwordˆ wordˆS based on the Bayes rule: ˆ S = argmaxP (S|A) = argmaxP (A|S) * P (S).</p><p>Different methods have been used to compute P (A|S). Pennell and <ref type="bibr" target="#b22">Liu (2010)</ref> used a CRF se- quence modeling approach for deletion-based ab- breviations.  further extended this work by considering more types of non-standard words without explicit pre-categorization for non- standard tokens.</p><p>In addition, the noisy channel model has also been utilized on the sentence level. <ref type="bibr" target="#b2">Choudhury et al. (2007)</ref> used a hidden Markov model to sim- ulate SMS message generation, considering the non-standard tokens in the input sentence as emis- sion states in HMM and labeling results as pos- sible candidates. <ref type="bibr" target="#b5">Cook and Stevenson (2009)</ref> ex- tended work by adding several more subsystems in this error model according to the most common non-standard token's formation process.</p><p>Machine translation (MT) is another commonly chosen method for text normalization. It is also used on both the token and the sentence level. <ref type="bibr" target="#b0">Aw et al. (2006)</ref> treated SMS as another language, and used MT methods to translate this 'foreign lan- guage' to regular English. <ref type="bibr" target="#b4">Contractor et al. (2010)</ref> used an MT model as well but the focus of their work is to utilize an unsupervised method to clean noisy text. Pennell and Liu (2011) firstly intro- duced an MT method at the token level which translates an unnormalized token to a possible cor-rect word.</p><p>Recently, a new line of work surges relying on the analysis of huge amount of twitter data, of- ten in an unsupervised fashion. By using con- text information from a large corpus, <ref type="bibr" target="#b8">Han et al. (2012)</ref> generated possible variant and normaliza- tion pairs, and constructed a dictionary of lexical variants of known words, which are further ranked by string similarity. This dictionary can facilitate lexical normalization via simple string substitu- tion. <ref type="bibr" target="#b9">Hassan and Menezes (2013)</ref> proposed an ap- proach based on the random walk algorithm on a contextual similarity bipartite graph, constructed from n-gram sequences on a large unlabeled text corpus. <ref type="bibr">Yang and Eisenstein (2013)</ref> presented a unified unsupervised statistical model for text nor- malization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Normalization Methods Used in Reranking</head><p>In this work we adopt several normalization meth- ods developed in previous studies. The following briefly describes these previous approaches. Next section will introduce our proposed methods using unsupervised learning and discriminative rerank- ing for system combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Character-block level MT</head><p>Pennell and Liu (2011) proposed to use a character-level MT model for text normalization. The idea is similar to traditional translation, except that the translation unit is characters, not words. Formally, for a non-standard word A = a 1 a 2 ...a n , the MT method finds the most likely standard word S = s 1 s 2 ...s m (a i and s i are the characters in the words): S = argmaxP (S|A) = argmaxP (A|S)P (S) = argmaxP (a 1 a 2 ...a n |s 1 s 2 ...</p><formula xml:id="formula_0">s m )P (s 1 s 2 ...s m )</formula><p>where P (a 1 a 2 ...a n |s 1 s 2 ...s m ) is from a character- level translation model, and P (s 1 s 2 ...s m ) is from a character-level language model. (Li and Liu, 2012a) modified this approach to perform the translation at the character-block level in order to generate better alignment between characters (analogous to the word vs. phrase based alignment in traditional MT). This system generates one ranked list of word candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Character-level Two-step MT</head><p>Li and Liu (2012b) extended the character-level MT model by incorporating the pronunciation in- formation. They first translate non-standard words to possible pronunciations, which are then trans- lated to standard words in the second step. This method has been shown to yield high coverage (high accuracy in its n-best hypotheses). There are two candidate lists generated by this two-step MT method. The first one is based on the pronuncia- tion list produced in the first step (some phonetic sequences directly correspond to standard words). The second list is generated from the second trans- lation step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Character-Block level Sequence Labeling</head><p>Pennell and <ref type="bibr" target="#b22">Liu (2010)</ref> used sequence labeling model (CRF) for normalizing deletion-based ab- breviation at the character-level. The model labels every character in a standard word as 'Y' or 'N' to represent whether it appears or not in a possible abbreviation token. The features used for the clas- sification task represent the character's position, pronunciation and context information. Using the sequence labeling model, a standard word can generate many possible non-standard words. A re- verse look-up table is used to store the correspond- ing possible standard words for the non-standard words for reverse lookup during testing.  extended the above model to handle other types of non-standard words. (Li and Liu, 2012a) used character-blocks (same ones as that in the character-block MT method above) as the units in this sequence labeling framework. There is one list of word candidates from this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Spell Checker</head><p>The forth normalization subsystem is the Jazzy Spell Checker 1 , which is based on edit distance and integrates a phonetic matching algorithm as well. This provides one list of hypotheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised Corpus-based Similarity for Normalization</head><p>Previous work has shown that unlabeled text can be used to induce unsupervised word clusters that can improve performance of many supervised NLP tasks ( <ref type="bibr" target="#b12">Koo et al., 2008;</ref><ref type="bibr">Turian et al., 2010;</ref><ref type="bibr">Täckström et al., 2012)</ref>. We investigate using a large unlabeled Twitter corpus to automatically identify pairs of non-standard words and their cor- responding standard words. We use the Edinburgh Twitter corpus <ref type="bibr" target="#b24">(Petrovic et al., 2010)</ref>, and a dictionary obtained from http://ciba.iciba.com/ to identify all the in- vocabulary and out-of-vocabulary (OOV) words in the corpus. The task is then to automatically find the corresponding OOV words (if any) for each dictionary word, and the likelihood of each pair.</p><p>The key question is how to compute this likelihood or similarity.</p><p>We propose to use an unsupervised method based on the large corpus to induce dense real- valued low-dimension word embedding and then use the inner product as a measure of semantic similarity. We use the continuous bag-of-words model that is similar to the feedforward neural network language model to compute vector rep- resentations of words. This model was first in- troduced by <ref type="bibr" target="#b19">(Mikolov et al., 2013</ref>). We use the tool word2vec 2 to implement this model. Two constraints are used in order to eliminate unlikely word pairs: (I) OOV words need to begin with the same letter as the dictionary standard word; (II) OOV words can only consist of English letter and digits.</p><p>In addition to considering the above semantic similarity, for the normalization task, we use other information including the surface character level similarity based on longest common sequence be- tween the two tokens, and the frequency of the to- ken. The final score between a dictionary word w and an OOV word t is: sim(w, t) = longest common string(w, t) length(t) * log(T ermF req(t)) * inner product(vec(w), vec(t)) * longest common seq(w, t) length(t)</p><p>The first and second term share the same property of visual prime value used in ( ).</p><p>The third term is the vector-based semantic simi- larity of the two words, calculated by our proposed model. The last term is the length of longest com- mon sequence between the two words divided by the length of the OOV word. Using this method, we can identify all the pos- sible OOV words for each dictionary word based on an unlabeled large corpus. Each pair has a similarity score. Then a reverse lookup table is created to store the corresponding possible stan- dard words for each non-standard word, which is used during testing. This framework is similar to the sequence labeling method described in Sec- tion 2.3 in the sense of creating the mapping ta- ble between the OOV and dictionary words. How- ever, the difference is that this is an unsupervised method whereas the sequence labeling uses super- vised learning to generate possible candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reranking for System Combination</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Word Level Reranking</head><p>Each of the above systems has its own strength and weakness. The MT model and the sequence la- beling models have better precision, the two-step MT model has a broader coverage of candidates, and the spell checker has a high confidence for simple non-standard words. Therefore combining these systems is expected to yield better overall results. We propose to use a supervised maximum entropy reranking model to combine our proposed unsupervised method with those described in Sec- tion 2 (4 systems that have 5 candidate lists). The features we used in the normalization reranking model are shown in <ref type="table" target="#tab_0">Table 1</ref>. This maxent rerank- ing method has shown success in many previous work such as <ref type="bibr" target="#b1">(Charniak and Johnson, 2005;</ref><ref type="bibr" target="#b10">Ji et al., 2006</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features:</head><p>1.Boolean value to indicate whether a candidate is on the list of each system. There are 6 lists and thus 6 such fea- tures. 2.A concatenation of the 6 boolean features above. 3.The position of this candidate in each candidate list. If this candidate is not on a list, the value of this feature is -1 for that list. 4.The unigram language model probability of the candi- date. 5.Boolean value to indicate whether the first character of the candidate and non-standard word is the same. 6.Boolean value to indicate whether the last character of the candidate and non-standard word is the same. The first three features are related to the indi-vidual systems, and the last three features com- pare the candidate with the non-standard word. It is computationally expensive to include informa- tion represented in the last three features in the in- dividual systems since they need to consider more candidates in the normalization step; whereas in reranking, only a small set of word candidates are evaluated, thus it is more feasible to use such global features in the reranking model. We also tried some other lexical features such as the length difference of the non-standard word and the can- didate, whether non-standard word contains num- bers, etc. But they did not obtain performance gain. Another advantage of the reranker is that we can use information about multiple systems, such as the first three features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sentence Level Reranking and Decoding</head><p>In the above reranking method, we only use infor- mation about the individual words. When contex- tual words are available (in sentences or Tweets), we can use that information. If a sentence con- taining OOV words is given during testing, we can perform standard sentence level Viterbi decod- ing to combine information from the normaliza- tion candidates and language model scores. Furthermore, if sentences are available during training (not just isolated word pairs as used in all the previous supervised individual systems and the Maxent reranking above), we can also use contex- tual information for training the reranker. This can be achieved in two different ways. First, we add the Language Model score from context words as features in the reranker. In this work, in addition to the features in <ref type="table" target="#tab_0">Table 1</ref>, we add a trigram probabil- ity to represent the context information. For every candidate of a non-standard word, we use trigram probability from the language model. The trigram consists of this candidate, and the previous and the following token of the non-standard word. If the previous/following word is also a non-standard to- ken, then we calculate the trigram using all of their candidates and then take the average. After adding the additional LM probability feature, the same Maxent reranking method as above is used, which optimizes the word level accuracy.</p><p>The second method is to change the training ob- jective and perform the optimization at the sen- tence level. The feature set can be the same as the word level reranker, or with the additional contex- tual LM score features. To train the model (feature weights), we perform sentence level Viterbi de- coding on the training set to find the best hypoth- esis for each non-standard word. If the hypothe- sis is incorrect, we update the feature weight us- ing structured perceptron strategy <ref type="bibr" target="#b3">(Collins, 2002</ref>). We will explore these different feature and train- ing configurations for reranking in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>The following data sets are used in our experi- ments. We use Data 1 and Data 2 as test data, and Data 3 as training data for all the supervised mod- els.</p><p>• Data 1: 558 pairs of non-standard tokens and standard words collected from 549 tweets in 2010 by <ref type="bibr" target="#b7">(Han and Baldwin, 2011</ref>).</p><p>• Data 2: 3,962 pairs of non-standard tokens and standard words collected from 6,160 tweets between 2009 and 2010 by (Liu et al., 2011).</p><p>• Data 3: 2,333 unique pairs of non-standard tokens and standard words, collected from 2,577 Twitter messages (selected from the Edinburgh Twitter corpus) used in (Pennell and Liu, 2011). We made some changes on this data, removing the pairs that have more than one proper words, and sentences that only contain such pairs. 3</p><p>• Data 4: About 10 million twitter messages selected from the the Edinburgh Twitter cor- pus mentioned above, consisting of 3 million unique tokens. This data is used by the un- supervised method to create the mapping ta- ble, and also for building the word-based lan- guage model needed in sentence level nor- malization.</p><p>The dictionary we used is obtained from http://ciba.iciba.com/, which includes 75,262 En- glish word entries and their corresponding pho- netic symbols (IPA symbols). This is used in var- ious modules in the normalization systems. The number of the final standard words used to create the look-up table is 10,105 because we only use the words that have the same number of character- block segments and phones. These 10,105 words cover 90.77% and 93.74% standard words in Data set 1 and Data set 2 respectively. For the non- standard words created in the CRF model, they cover 80.47% and 86.47% non-standard words in Data set1 and Data set 2. This coverage using the non-standard words identified by the new unsuper- vised model is 91.99% and 92.32% for the two data sets, higher than that by the CRF model. During experiments, we use CRF++ toolkit 4 for our sequence labeling model, SRILM toolkit <ref type="bibr" target="#b27">(Stolcke, 2002</ref>) to build all the language models, <ref type="bibr">Giza++ (Och and Ney, 2003)</ref> for automatic word alignment, and Moses ( <ref type="bibr" target="#b11">Koehn et al., 2007</ref>) for translation decoding in three MT systems. <ref type="table">Table 2</ref> shows the isolated word normalization re- sults on the two test data sets for various systems. The performance metrics include the accuracy for the top-1 candidate and other top-N candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Isolated Word Normalization Experiments</head><p>Coverage means how many test cases correct an- swers can be obtained in the final list regardless of its positions. The top part presents the results on Data Set 1 and the bottom shows the results on Data Set 2. We can see that our proposed unsu- pervised corpus similarity model achieves better top-1 accuracy than the other individual systems described in Section 2. Its top-n coverage is not always the best -the 2-step MT method has advan- tages in its coverage. The results in the table also show that reranking improves system performance over any of the used individual systems, which is expected. After reranking, on Data set 1, our sys- tem yields better performance than previously re- ported ones. On Data set 2, it has better top-1 ac- curacy than ( ), but slightly worse top-N coverage. However, the method in ( ) has higher computational cost because of the calculation of the prime visual values for each non-standard word on the fly during testing. In addition, they also used more training data than ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentence Level Normalization Results</head><p>We have already seen that after reranking we ob- tain better word-level normalization performance, for both top-1 and other top-N candidates. One follow-up question is whether this improved per- formance carries over to sentence level normaliza-System Accuracy % <ref type="table" target="#tab_0">Top1 Top3 Top10 Top20 Cover  Data 1  MT  61.81</ref>  73.04 n/a n/a n/a n/a <ref type="table">Table 2:</ref> MT: Character-block Level MT; MT21&amp;MT22: First&amp;Second step in Character- level Two-step MT; SL: Sequence Labeling sys- tem; SC: Spell Checker; UCS: Unsupervised Cor- pus Similarity Model; Sys1 is from ( ); Sys2 is from (Li and Liu, 2012a); Sys3 is from <ref type="bibr">(Yang and Eisenstein, 2013</ref>). tion when context information is used via the in- corporation of a language model. Since detecting which tokens need normalization in the first place is a hard task itself in social media text and is an open question currently, similar to some previous work, we assume that we already know the non- standard words that need to be normalized for a given sentence. Then the sentence-level normal- ization task is just to find which candidate from the n-best lists for each of those already 'detected' non-standard words is the best one. We use the tweets in the Data set 1 described above because Data set 2 only has token pairs but not sentences. <ref type="table" target="#tab_3">Table 3</ref> shows the sentence level normaliza- tion results using different reranking configura- tions with respect to the features used in the reranker and the training process. Regarding fea- tures, reranker 1 and 3 use the features described in Section 3.2.1, i.e., features based on the words only, without the additional trigram LM probabil- ity feature; reranker 2 and 4 use the additional LM probability feature. About training, reranker 1 and 2 use the Maxent reranking that is trained and op- timized for the word level; reranker 3 and 4 use structure perceptron training at the sentence level. Note that all of the systems perform Viterbi decod- ing during testing to determine the final top one candidate for each non-standard word in the sen- tence. The scores from the reranked normalization output and the LM probabilities are combined in decoding. From the results, we can see that adding contextual information (LM probabilities) as fea- tures in the reranker is useful. When this feature is not used, using sentence-level training objec- tive benefits (reranker 3 outperforms 1); however, when this feature is used, performing sentence- level training via structure perceptron is not useful (reranker 2 outperforms 4), partly because the con- textual information is incorporated in the features already and using it in sentence-level decoding for training is redundant and does not bring additional gain. Finally compared to the previously report results, our system performs the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Acc   <ref type="bibr">and Eisenstein, 2013)</ref>. Acc % is the top one accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Impact of Unsupervised Corpus Similarity Model</head><p>Our last question is regarding unsupervised model importance in the reranking system and contribu- tions of its different similarity measure compo- nents. We conduct the following two experiments: First, we removed the new model and just use the other remaining models in reranking (five candi- date lists). Second, we kept this new model but changed the corpus similarity measure (removed the third item in Eq(1) that represents the seman- tic similarity). This way we can evaluate the im- pact of the semantic similarity measure based on the continuous word vector representation. <ref type="table">Table 4</ref> shows the word level and sentence re- sults on Data set 1 and 2 using these different setups. Because of space limit, we only present the top one accuracy. The other top-n results have similar patterns. Sentence level normaliza- tion uses the Reranker 2 described above. We can see that there is a degradation in both of the new setups, suggesting that the unsupervised method itself is beneficial, and in particular the word vec- tor based semantic similarity component is crucial to the system performance.  <ref type="table">Table 4</ref>: Word level and Sentence level normaliza- tion results (top-1 accuracy in %) after reranking on Data Set 1 and 2. System-A is without using the unsupervised model, system-B is without its semantic similarity measure, and system-C is our proposed system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we proposed a novel normalization system by using unsupervised methods in a large corpus to identify non-standard words and their corresponding proper words. We further combine it with several previously developed normalization systems by a reranking strategy. In addition, we explored different sentence level reranking meth- ods to evaluate the impact of context information. Our experiments show that the reranking system not only significantly improves the word level nor- malization accuracy, but also helps the sentence level decoding. In the future work, we plan to ex- plore more useful features and also leverage pair- wise and link reranking strategy.</p><p>Oscar Täckström, Ryan McDonald, and Jakob Uszko- reit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of NAACL.</p><p>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceed- ings of ACL.</p><p>Yi Yang and Jacob Eisenstein. 2013. A log-linear model for unsupervised text normalization. In Pro- ceedings of EMNLP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Features for Reranking.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>73.53 78.50 79.57 80.00 MT21 39.61 52.93 63.59 65.36 65.72 MT22 53.64 68.56 77.44 80.46 88.</figDesc><table>10 
SL 
53.29 61.99 69.09 71.92 75.85 
SC 
50.27 56.31 56.84 57.02 57.02 
UCS 
61.81 69.98 74.60 76.55 82.17 
Rerank 77.14 86.96 93.04 94.82 95.90 
Sys1 
75.69 n/a 
n/a 
n/a 
n/a 
Sys2 
73 
81.9 86.7 89.2 94.2 
Data 2 
MT 
55.02 63.3 66.99 67.77 68.00 
MT21 35.64 47.65 54.67 56.01 56.4 
MT22 49.02 62.49 70.99 74.86 80.07 
SL 
46.52 55.05 61.21 62.97 66.21 
SC 
51.16 55.48 55.88 55.88 55.88 
UCS 
57.29 65.75 70.55 72.64 80.84 
Rerank 74.44 84.57 90.25 92.37 93.5 
Sys1 
69.81 82.51 92.24 93.79 95.71 
Sys2 
62.6 75.1 84 
87.5 90.7 
Sys3 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Sentence level normalization results on 
Data Set 1 using different reranking setups. Sys1 
is from (Liu et al., 2012a); Sys2 is from (Yang </table></figure>

			<note place="foot" n="3"> Proposed Method All the above models except the Spell Checker are supervised methods that need labeled data consisting of pairs of non-standard words and proper words. In this paper we propose an unsupervised method to create the lookup table of the nonstandard words and their corresponding proper words offline. We further propose to use different discriminative reranking approaches to combine multiple individual systems. 1 http://jazzy.sourceforge.net</note>

			<note place="foot" n="2"> https://code.google.com/p/word2vec/</note>

			<note place="foot" n="3"> http://www.hlt.utdallas.edu/∼chenli/normalization</note>

			<note place="foot" n="4"> http://crfpp.googlecode.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the NSF for travel and conference sup-port for this paper. The work is also partially sup-ported by DARPA Contract No. FA8750-13-2-0041. Any opinions, findings, and conclusions or recommendations expressed are those of the au-thor and do not necessarily reflect the views of the funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A phrase-based statistical model for sms text normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiti</forename><surname>Aw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processing of COLING/ACL</title>
		<meeting>essing of COLING/ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd ACL</title>
		<meeting>the 43rd ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Investigation and modeling of the structure of texting language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijit</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeshna</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Basu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>IJDAR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised cleansing of noisy text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danish</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tanveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Faruquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An unsupervised model for text message normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A technique for computer detection and correction of spelling errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Damerau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="171" to="176" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lexical normalisation of short text messages: Makn sens a #twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of 49th ACL</title>
		<meeting>eeding of 49th ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatically constructing a normalisation dictionary for microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 EMNLP</title>
		<meeting>the 2012 EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Social text normalization using contextual graph random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Re-ranking algorithms for name tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing</title>
		<meeting>the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst, and Evan Herbst</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">707</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving text normalization using character-blocks based models and system combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Normalization of text messages using character-and phone-based machine translation approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th Interspeech</title>
		<meeting>13th Interspeech</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Insertion, deletion, or substitution?: normalizing text messages without pre-categorization nor supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th ACL: short papers</title>
		<meeting>the 49th ACL: short papers</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A broad-coverage normalization system for social media language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th ACL</title>
		<meeting>the 50th ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint inference of named entity recognition and normalization for tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Normalization of text messages for text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>In ICASSP</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A character-level machine translation approach for normalization of sms abbreviations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deana</forename><surname>Pennell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th IJCNLP</title>
		<meeting>5th IJCNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The edinburgh twitter corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lavrenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Normalization of non-standard words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="333" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Spoken Language Processing</title>
		<meeting>International Conference on Spoken Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
