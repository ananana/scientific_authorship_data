<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint CTC/attention decoding for end-to-end speech recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
						</author>
						<title level="a" type="main">Joint CTC/attention decoding for end-to-end speech recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="518" to="529"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1048</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>End-to-end automatic speech recognition (ASR) has become a popular alternative to conventional DNN/HMM systems because it avoids the need for linguistic resources such as pronunciation dictionary, tokenization, and context-dependency trees, leading to a greatly simplified model-building process. There are two major types of end-to-end archi-tectures for ASR: attention-based methods use an attention mechanism to perform alignment between acoustic frames and recognized symbols, and connection-ist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential problems by dynamic programming. This paper proposes a joint decoding algorithm for end-to-end ASR with a hybrid CTC/attention architecture, which effectively utilizes both advantages in decoding. We have applied the proposed method to two ASR benchmarks (spontaneous Japanese and Mandarin Chi-nese), and showing the comparable performance to conventional state-of-the-art DNN/HMM ASR systems without linguistic resources.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Automatic speech recognition (ASR) is currently a mature set of technologies that have been widely deployed, resulting in great success in interface applications such as voice search. A typical ASR system is factorized into several modules includ- ing acoustic, lexicon, and language models based on a probabilistic noisy channel model <ref type="bibr">(Jelinek, 1976)</ref>. Over the last decade, dramatic improve- ments in acoustic and language models have been driven by machine learning techniques known as deep learning <ref type="bibr">(Hinton et al., 2012</ref>).</p><p>However, current systems lean heavily on the scaffolding of complicated legacy architectures that grew up around traditional techniques. For example, when we build an acoustic model from scratch, we have to first build hidden Markov model (HMM) and Gaussian mixture model (GMM) followed by deep neural networks (DNN). In addition, the factorization of acoustic, lexicon, and language models is derived by conditional in- dependence assumptions (especially Markov as- sumptions), although the data do not necessarily follow such assumptions leading to model mis- specification. This factorization form also yields a local optimum since the above modules are optimized separately. Further, to well factorize acoustic and language models, the system requires linguistic knowledge based on a lexicon model, which is usually based on a hand-crafted pronun- ciation dictionary to map word to phoneme se- quence. In addition to the pronunciation dictio- nary issue, some languages, which do not ex- plicitly have a word boundary, need language- specific tokenization modules ( <ref type="bibr">Kudo et al., 2004;</ref><ref type="bibr">Bird, 2006</ref>) for language modeling. Finally, in- ference/decoding has to be performed by integrat- ing all modules resulting in complex decoding. Consequently, it is quite difficult for non-experts to use/develop ASR systems for new applications, especially for new languages.</p><p>End-to-end ASR has the goal of simplifying the above module-based architecture into a single- network architecture within a deep learning frame- work, in order to address the above issues. There are two major types of end-to-end architectures for ASR: attention-based methods use an attention mechanism to perform alignment between acous- tic frames and recognized symbols, and connec- tionist temporal classification (CTC), uses Markov assumptions to efficiently solve sequential prob- lems by dynamic programming <ref type="bibr">(Chorowski et al., 2014;</ref><ref type="bibr">Graves and Jaitly, 2014</ref>).</p><p>The attention-based end-to-end method solves the ASR problem as a sequence mapping from speech feature sequences to text by using encoder- decoder architecture. The decoder network uses an attention mechanism to find an alignment be- tween each element of the output sequence and the hidden states generated by the acoustic en- coder network for each frame of acoustic input <ref type="bibr">(Chorowski et al., 2014</ref><ref type="bibr">(Chorowski et al., , 2015</ref><ref type="bibr">Chan et al., 2015;</ref><ref type="bibr">Lu et al., 2016)</ref>. This basic temporal attention mechanism is too flexible in the sense that it allows extremely non-sequential alignments. This may be fine for applications such as machine translation where input and output word order are different ( <ref type="bibr">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b6">Wu et al., 2016)</ref>. How- ever, in speech recognition, the feature inputs and corresponding letter outputs generally proceed in the same order. Another problem is that the input and output sequences in ASR can have very dif- ferent lengths, and these vary greatly from case to case, depending on the speaking rate and writing system, making it more difficult to track the align- ment.</p><p>However, an advantage is that the attention mechanism does not require any conditional in- dependence assumptions, and could address all the problems cited above. Although the align- ment problems of attention-based mechanisms have been partially addressed in <ref type="bibr">(Chorowski et al., 2014;</ref><ref type="bibr">Chorowski and Jaitly, 2016</ref>) using various mechanisms, here we propose more rigorous con- straints by using CTC-based alignment to guide the decoding.</p><p>CTC permits an efficient computation of a strictly monotonic alignment using dynamic pro- gramming ( <ref type="bibr">Graves et al., 2006;</ref><ref type="bibr">Graves and Jaitly, 2014</ref>) although it requires language models and graph-based decoding ( <ref type="bibr">Miao et al., 2015)</ref> except in the case of huge training data ( <ref type="bibr">Amodei et al., 2015;</ref><ref type="bibr" target="#b4">Soltau et al., 2016)</ref>. We propose to take ad- vantage of the constrained CTC alignment in a hy- brid CTC/attention based system during decoding. The proposed method adopts a CTC/attention hy- brid architecture, which was originally designed to regularize an attention-based encoder network by additionally using a CTC during training ( <ref type="bibr">Kim et al., 2017</ref>). The proposed method extends the ar- chitecture to perform one-pass/rescoring joint de- coding, where hypotheses of attention-based ASR are boosted by scores obtained by using CTC out- puts. This greatly reduces irregular alignments without any heuristic search techniques.</p><p>The proposed method is applied to Japanese and Mandarin ASR tasks, which require extra linguistic resources including morphological an- alyzer ( <ref type="bibr">Kudo et al., 2004</ref>) or word segmentation ( <ref type="bibr" target="#b7">Xue et al., 2003</ref>) in addition to pronunciation dic- tionary to provide accurate lexicon and language models in conventional DNN/HMM ASR. Sur- prisingly, the method achieved performance com- parable to, and in some cases superior to, several state-of-the-art DNN/HMM ASR systems, with- out using the above linguistic resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">From DNN/HMM to end-to-end ASR</head><p>This section briefly provides a formulation of con- ventional DNN/HMM ASR and CTC or attention based end-to-end ASR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conventional DNN/HMM ASR</head><p>ASR deals with a sequence mapping from T - length speech feature sequence</p><formula xml:id="formula_0">X = {x t ∈ R D |t = 1, · · · , T } to N -length word sequence W = {w n ∈ V|n = 1, · · · , N }. x t is a D dimensional speech feature vector (e.g., log</formula><p>Mel filterbanks) at frame t and w n is a word at posi- tion n in vocabulary V. ASR is mathematically formulated with the Bayes decision theory, where the most probable word sequencê W is estimated among all possible word sequences V * as follows:</p><formula xml:id="formula_1">ˆ W = arg max W ∈V * p(W |X).<label>(1)</label></formula><p>The posterior distribution p(W |X) is factorized into the following three distributions by using the Bayes theorem and introducing HMM state se- quence S = {s t ∈ {1, · · · , J}|t = 1, · · · , T }:</p><formula xml:id="formula_2">Eq. (1) ≈ arg max W S p(X|S)p(S|W )p(W ).</formula><p>The three factors, p(X|S), p(S|W ), and p(W ), are acoustic, lexicon, and language models, re- spectively. These are further factorized by using a probabilistic chain rule and conditional indepen- dence assumption as follows:</p><formula xml:id="formula_3">     p(X|S) ≈ t p(st|xt) p(st) , p(S|W )≈ t p(s t |s t−1 , W ), p(W ) ≈ n p(w n |w n−1 , . . . , w n−m−1 ),</formula><p>where the acoustic model is replaced with the product of framewise posterior distributions p(s t |x t ) computed by powerful DNN classi- fiers by using so-called pseudo likelihood trick <ref type="bibr">(Bourlard and Morgan, 1994)</ref>. p(s t |s t−1 , W ) is represented by an HMM state transition given W , and the conversion from W to HMM states is de- terministically performed by using a pronuncia- tion dictionary through a phoneme representation. p(w n |w n−1 , . . . , w n−m−1 ) is obtained based on an (m − 1)th-order Markov assumption as a m- gram model. These conditional independence assumptions are often regarded as too strong assumption, lead- ing to model mis-specification. Also, to train the framewise posterior p(s t |x t ), we have to provide a framewise state alignment s t as a target, which is often provided by a GMM/HMM system. Thus, conventional DNN/HMM systems make the ASR problem formulated with Eq. (1) feasible by us- ing factorization and conditional independence as- sumptions, at the cost of the problems discussed in Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Connectionist Temporal Classification (CTC)</head><p>The CTC formulation also follows from Bayes de- cision theory (Eq. <ref type="formula" target="#formula_1">(1)</ref>). Note that the CTC formu- lation uses L-length letter sequence C = {c l ∈ U|l = 1, · · · , L} with a set of distinct letters U.</p><p>Similarly to Section 2.1, by introducing frame- wise letter sequence with an additional "blank" ( &lt; b &gt;) symbol Z = {z t ∈ U ∪ &lt; b &gt;|t = 1, · · · , T }, and by using the probabilistic chain rule and conditional independence assumption, the posterior distribution p(C|X) is factorized as fol- lows:</p><formula xml:id="formula_4">p(C|X) ≈ Z t p(z t |z t−1 , C)p(z t |X) pctc(C|X) p(C) p(Z)<label>(2)</label></formula><p>As a result, CTC has three distribution com- ponents similar to the DNN/HMM case, i.e., framewise posterior distribution p(z t |X), tran- sition probability p(z t |z t−1 , C) 1 , and prior dis- tributions of letter and hidden-state sequences, p(C) and p(Z), respectively. We also define the CTC objective function p ctc (C|X) used in the later formulation. The framewise posterior distribution p(z t |X) is conditioned on all in- puts X, and it is quite natural to be modeled by using bidirectional long short-term memory (BLSTM): p(z t |X) = Softmax(Lin(h t )) and h t = BLSTM(X). Softmax(·) is a sofmax activa- tion function, and Lin(·) is a linear layer to convert hidden vector h t to a (|U| + 1) dimensional vector (+1 means a blank symbol introduced in CTC). Although Eq. (2) has to deal with a summa- tion over all possible Z, it is efficiently computed by using dynamic programming (Viterbi/forward- backward algorithm) thanks to the Markov prop- erty. In summary, although CTC and DNN/HMM systems are similar to each other due to condi- tional independence assumptions, CTC does not require pronunciation dictionaries and omits an GMM/HMM construction step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Attention mechanism</head><p>Compared with hybrid and CTC approaches, the attention-based approach does not make any con- ditional independence assumptions, and directly estimates the posterior p(C|X) based on a prob- abilistic chain rule, as follows:</p><formula xml:id="formula_5">p(C|X) = l p(c l |c 1 , · · · , c l−1 , X) patt(C|X) , (3) where p att (C|X) is an attention-based objective function. p(c l |c 1 , · · · , c l−1 , X) is obtained by p(c l |c 1 , · · · , c l−1 , X) = Decoder(r l , q l−1 , c l−1 ) h t = Encoder(X) (4) a lt = Attention({a l−1 } t , q l−1 , h t )<label>(5)</label></formula><formula xml:id="formula_6">r l = t a lt h t .<label>(6)</label></formula><p>Eq. (4) converts input feature vectors X into a framewise hidden vector h t in an encoder net- work based on BLSTM, i.e., Encoder(X) BLSTM(X). Attention(·) in Eq. <ref type="formula" target="#formula_5">(5)</ref> is based on a content-based attention mechanism with convo- lutional features, as described in ( <ref type="bibr">Chorowski et al., 2015</ref>) (see Appendix A). a lt is an attention weight, and represents a soft alignment of hidden vector h t for each output c l based on the weighted summa- tion of hidden vectors to form letter-wise hidden vector r l in Eq. (6). A decoder network is another recurrent network conditioned on previous output c l−1 and hidden vector q l−1 , similar to RNNLM, in addition to letter-wise hidden vector r l . We use Decoder(·) Softmax(Lin(LSTM(·))).</p><p>Attention-based ASR does not explicitly sep- arate each module, and potentially handles the all issues pointed out in Section 1. It implic- itly combines acoustic models, lexicon, and lan- guage models as encoder, attention, and decoder networks, which can be jointly trained as a single deep neural network.</p><p>Compared with DNN/HMM and CTC, which are based on a transition form from t − 1 to t due to the Markov assumption, the attention mecha- nism does not maintain this constraint, and often provides irregular alignments. A major focus of this paper is to address this problem by using joint CTC/attention decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Joint CTC/attention decoding</head><p>This section explains a hybrid CTC/attention net- work, which potentially utilizes both benefits of CTC and attention in ASR. <ref type="bibr">Kim et al. (2017)</ref> uses a CTC objective function as an auxiliary task to train the attention model en- coder within the multitask learning (MTL) frame- work, and this paper also uses the same archi- tecture. <ref type="figure">Figure 1</ref> illustrates the overall architec- ture of the framework, where the same BLSTM is shared with CTC and attention encoder networks, respectively). Unlike the sole attention model, the forward-backward algorithm of CTC can enforce monotonic alignment between speech and label sequences during training. That is, rather than solely depending on data-driven attention meth- ods to estimate the desired alignments in long se- quences, the forward-backward algorithm in CTC helps to speed up the process of estimating the de- sired alignment. The objective to be maximized is a logarithmic linear combination of the CTC and attention objectives, i.e., p ctc (C|X) in Eq. (2) and p att (C|X) in Eq. <ref type="formula">(3)</ref>:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hybrid CTC/attention architecture</head><formula xml:id="formula_7">L MTL = λ log p ctc (C|X) + (1 − λ) log p att (C|X),<label>(7)</label></formula><p>with a tunable parameter λ : 0 ≤ λ ≤ 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoding strategies</head><p>The inference step of our joint CTC/attention- based end-to-end speech recognition is performed</p><formula xml:id="formula_8">ㅡ ㅡ ㅡ z 2 … sos eos c 1 q 0 r 0 r 1 r L H h 2 h 4 q 1 q L h T x 1 x 2 x 3 x 4 x 5 x 6 x T Shared Encoder CTC Attention Decoder x 7 x 8 z 4 h 6 h 8 … c 2 r 2 q 2 … c 1 c 2 … c 1 c 2 …</formula><p>Figure 1: Joint CTC/attention based end-to-end framework: the shared encoder is trained by both CTC and attention model objectives simultane- ously. The shared encoder transforms our input sequence</p><formula xml:id="formula_9">{x t · · · x T } into high level features H = {h t · · · h T }</formula><p>, and the attention decoder generates the letter sequence</p><formula xml:id="formula_10">{c 1 · · · c L }.</formula><p>by label synchronous decoding with a beam search similar to conventional attention-based ASR. However, we take the CTC probabilities into account to find a hypothesis that is better aligned to the input speech, as shown in <ref type="figure">Figure 1</ref>. Here- after, we describe the general attention-based de- coding and conventional techniques to mitigate the alignment problem. Then, we propose joint de- coding methods with a hybrid CTC/attention ar- chitecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Attention-based decoding in general</head><p>End-to-end speech recognition inference is gener- ally defined as a problem to find the most probable letter sequencê C given the speech input X, i.e.</p><formula xml:id="formula_11">ˆ C = arg max C∈U * log p(C|X).<label>(8)</label></formula><p>In attention-based ASR, p(C|X) is computed by Eq. <ref type="formula">(3)</ref>, andˆCandˆ andˆC is found by a beam search tech- nique.</p><p>Let Ω l be a set of partial hypotheses of the length l. At the beginning of the beam search, Ω 0 contains only one hypothesis with the start- ing symbol &lt;sos&gt; and the hypothesis score α(&lt;sos&gt;, X) is set to 0. For l = 1 to L max , each partial hypothesis in Ω l−1 is expanded by append- ing possible single letters, and the new hypothe- ses are stored in Ω l , where L max is the maximum length of the hypotheses to be searched. The score of each new hypothesis is computed in the log do- main as</p><formula xml:id="formula_12">α(h, X) = α(g, X) + log p(c|g, X), (9)</formula><p>where g is a partial hypothesis in Ω l−1 , c is a letter appended to g, and h is the new hypothesis such that h = g · c. If c is a special symbol that repre- sents the end of a sequence, &lt;eos&gt;, h is added tôtô Ω but not Ω l , wherê Ω denotes a set of complete hypotheses. Finally, ˆ C is obtained byˆC</p><formula xml:id="formula_13">byˆ byˆC = arg max h∈ˆΩh∈ˆ h∈ˆΩ α(h, X).<label>(10)</label></formula><p>In the beam search process, Ω l is allowed to hold only a limited number of hypotheses with higher scores to improve the search efficiency.</p><p>Attention-based ASR, however, may be prone to include deletion and insertion errors because of its flexible alignment property, which can at- tend to any portion of the encoder state sequence to predict the next label, as discussed in Section 2.3. Since attention is generated by the decoder network, it may prematurely predict the end-of- sequence label, even when it has not attended to all of the encoder frames, making the hypothesis too short. On the other hand, it may predict the next label with a high probability by attending to the same portions as those attended to before. In this case, the hypothesis becomes very long and includes repetitions of the same letter sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Conventional decoding techniques</head><p>To alleviate the alignment problem, a length penalty term is commonly used to control the hy- pothesis length to be selected ( <ref type="bibr">Chorowski et al., 2015;</ref><ref type="bibr">Bahdanau et al., 2016)</ref>. With the length penalty, the decoding objective in Eq. <ref type="formula" target="#formula_11">(8)</ref> is changed tô</p><formula xml:id="formula_13">byˆ byˆC = arg max h∈ˆΩh∈ˆ h∈ˆΩ α(h, X).<label>(10)</label></formula><p>where |C| is the length of the sequence C, and γ is a tunable parameter. However, it is actually diffi- cult to completely exclude hypotheses that are too long or too short even if γ is carefully tuned. It is also effective to control the hypothesis length by the minimum and maximum lengths to some extent, where the minimum and maximum are se- lected as fixed ratios to the length of the input speech. However, since there are exceptionally long or short transcripts compared to the input speech, it is difficult to balance saving such excep- tional transcripts and preventing hypotheses with irrelevant lengths. Another approach is the coverage term re- cently proposed in <ref type="bibr">(Chorowski and Jaitly, 2016)</ref>, which is incorporated in the decoding objective in Eq. (11) asˆC asˆ asˆC = arg max</p><formula xml:id="formula_15">C∈U * {log p(C|X) + γ|C| +η · coverage(C|X)} , (12)</formula><p>where the coverage term is computed by</p><formula xml:id="formula_16">coverage(C|X) = T t=1 L l=1 a lt &gt; τ</formula><p>. <ref type="formula" target="#formula_1">(13)</ref> η and τ are tunable parameters. The coverage term represents the number of frames that have received a cumulative attention greater than τ . Accord- ingly, it increases when paying close attention to some frames for the first time, but does not in- crease when paying attention again to the same frames. This property is effective for avoiding looping of the same label sequence within a hy- pothesis. However, it is still difficult to obtain a common parameter setting for γ, η, τ , and the op- tional min/max lengths so that they are appropriate for any speech data from different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Joint decoding</head><p>Our joint CTC/attention approach combines the CTC and attention-based sequence probabilities in the inference step, as well as the training step. Suppose p ctc (C|X) in Eq. (2) and p att (C|X) in Eq. <ref type="formula">(3)</ref> are the sequence probabilities given by CTC and the attention model. The decoding ob- jective is defined similarly to Eq. (7) asˆC asˆ asˆC = arg max</p><formula xml:id="formula_17">C∈U * {λ log p ctc (C|X) +(1 − λ) log p att (C|X)} . (14)</formula><p>The CTC probability enforces a monotonic align- ment that does not allow large jumps or looping of the same frames. Accordingly, it is possible to choose a hypothesis with a better alignment and exclude irrelevant hypotheses without relying on the coverage term, length penalty, or min/max lengths.</p><p>In the beam search process, the decoder needs to compute a score for each partial hypothesis us- ing Eq. (9). However, it is nontrivial to combine the CTC and attention-based scores in the beam search, because the attention decoder performs it output-label-synchronously while CTC performs it frame-synchronously. To incorporate the CTC probabilities in the hypothesis score, we propose two methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rescoring</head><p>The first method is a two-pass approach, in which the first pass obtains a set of complete hypotheses using the beam search, where only the attention- based sequence probabilities are considered. The second pass rescores the complete hypotheses us- ing the CTC and attention probabilities, where the CTC probabilities are obtained by the forward al- gorithm for CTC ( <ref type="bibr">Graves et al., 2006</ref>). The rescor- ing pass obtains the final result according tô</p><formula xml:id="formula_18">tô C = arg max h∈ˆΩh∈ˆ h∈ˆΩ {λα ctc (h, X) + (1 − λ)α att (h, X)} ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_19">α ctc (h, X) log p ctc (h|X) α att (h, X) log p att (h|X) .<label>(16)</label></formula><p>One-pass decoding</p><p>The second method is one-pass decoding, in which we compute the probability of each partial hypoth- esis using CTC and an attention model. Here, we utilize the CTC prefix probability <ref type="bibr">(Graves, 2008)</ref> defined as the cumulative probability of all label sequences that have the partial hypothesis h as their prefix:</p><formula xml:id="formula_20">p ctc (h, . . . |X) = ν∈(U∪{&lt;eos&gt;}) + p ctc (h · ν|X),</formula><p>and we define the CTC score as</p><formula xml:id="formula_21">α ctc (h, X) log p ctc (h, . . . |X),<label>(17)</label></formula><p>where ν represents all possible label sequences ex- cept the empty string. The CTC score cannot be obtained recursively as in Eq. <ref type="formula">(9)</ref>, but it can be computed efficiently by keeping the forward prob- abilities over the input frames for each partial hy- pothesis. Then it is combined with α att (h, X). The beam search algorithm for one-pass decod- ing is shown in Algorithm 1. Ω l andˆΩandˆ andˆΩ are ini- tialized in lines 2 and 3 of the algorithm, which are implemented as queues that accept partial hy- potheses of the length l and complete hypothe- ses, respectively. In lines 4-25, each partial hy- pothesis g in Ω l−1 is extended by each label c Algorithm 1 Joint CTC/attention one-pass decod- ing 1: procedure ONEPASSBEAMSEARCH(X,Lmax) 2:</p><p>Ω0 ← {&lt;sos&gt;} 3:</p><p>ˆ Ω ← ∅ 4:</p><p>for l = 1 . . . Lmax do 5:</p><formula xml:id="formula_22">Ω l ← ∅ 6:</formula><p>while</p><note type="other">Ω l−1 = ∅ do 7: g ← HEAD(Ω l−1 ) 8: DEQUEUE(Ω l−1 ) 9: for each c ∈ U ∪ {&lt;eos&gt;} do 10: h ← g · c 11: α(h,X) ← λαctc(h,X)+(1−λ)αatt(h,X) 12: if c = &lt;eos&gt; then 13: ENQUEUE( ˆ Ω, h) 14: else 15: ENQUEUE(Ω l , h) 16: if |Ω l | &gt; beamWidth then 17: REMOVEWORST(Ω l ) 18: end if 19: end if 20: end for 21: end while 22: if ENDDETECT( ˆ Ω, l) = true then 23: break exit for loop 24: end if 25: end for 26:</note><p>return arg max h∈ˆΩh∈ˆ h∈ˆΩ α(h, X) 27: end procedure in the label set U. Each extended hypothesis h is scored in line 11, where CTC and attention- based scores are obtained by α ctc () and α att (). Af- ter that, if c = &lt;eos&gt;, the hypothesis h is as- sumed to be complete and stored inˆΩinˆ inˆΩ in line 13. If c = &lt;eos&gt;, h is stored in Ω l in line 15, where the number of hypotheses in Ω l is checked in line 16. If the number exceeds the beam width, the hy- pothesis with the worst score in Ω l is removed by REMOVEWORST() in line 17.</p><p>In line 11, the CTC and attention model scores are computed for each partial hypothesis. The at- tention score is easily obtained in the same man- ner as Eq. <ref type="formula">(9)</ref>, whereas the CTC score requires a modified forward algorithm that computes it label-synchronously. The algorithm to compute the CTC score is summarized in Appendix B. By considering the attention and CTC scores during the beam search, partial hypotheses with irregu- lar alignments can be excluded, and the number of search errors is reduced.</p><p>We can optionally apply an end detection tech- nique to reduce the computation by stopping the beam search before l reaches L max . Function ENDDETECT( ˆ Ω, l) in line 22 returns true if there is little chance of finding complete hypothe- ses with higher scores as l increases in the future.</p><p>In our implementation, the function returns true if </p><formula xml:id="formula_23">M −1 m=0 max h∈ˆΩh∈ˆ h∈ˆΩ:|h|=l−m α(h,X)−max h ∈ ˆ Ω α(h , X) &lt; D end =M,<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We used Japanese and Mandarin Chinese ASR benchmarks to show the effectiveness of the pro- posed joint CTC/attention decoding approach. The main reason for choosing these two languages is that those ideogram languages have relatively shorter lengths for letter sequences than those in alphabet languages, which reduces computational complexities greatly, and makes it easy to handle context information in a decoder network. Our preliminary investigation shows that Japanese and Mandarin Chinese end-to-end ASR can be eas- ily scaled up, and shows state-of-the-art perfor- mance without using various tricks developed in English tasks. Also, we would like to emphasize that the system did not use language-specific pro- cessing (e.g., morphological analyzer, Pinyin dic- tionary), and simply used all appeared characters in their transcriptions including Japanese syllable and Kanji, Chinese, Arabic number, and alphabet characters, as they are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Corpus of Spontaneous Japanese (CSJ)</head><p>We demonstrated ASR experiments by using the Corpus of Spontaneous Japanese (CSJ) ( <ref type="bibr">Maekawa et al., 2000</ref>). CSJ is a standard Japanese ASR task based on a collection of monologue speech data including academic lectures and simulated presen- tations. It has a total of 581 hours of training data and three types of evaluation data, where each evaluation task consists of 10 lectures (totally 5 hours). As input features, we used 40 mel-scale filterbank coefficients, with their first and second order temporal derivatives to obtain a total of 120- dimensional feature vector per frame. The encoder was a 4-layer BLSTM with 320 cells in each layer and direction, and linear projection layer is fol- lowed by each BLSTM layer. The 2nd and 3rd bottom layers of the encoder read every second hidden state in the network below, reducing the utterance length by the factor of 4. We used the content-based attention mechanism ( <ref type="bibr">Chorowski et al., 2015)</ref>, where the 10 centered convolution filters of width 100 were used to extract the con- volutional features. The decoder network was a 1-layer LSTM with 320 cells. The AdaDelta algo- rithm <ref type="bibr" target="#b8">(Zeiler, 2012</ref>) with gradient clipping (Pas- canu et al., 2012) was used for the optimization. D end and M in Eq (18) were set as log 1e −10 and 3, respectively. The hybrid CTC/attention ASR was implemented by using the Chainer deep learn- ing toolkit ( <ref type="bibr" target="#b5">Tokui et al., 2015)</ref>. <ref type="table">Table 1</ref> first compares the character error rate (CER) for conventional attention and MTL based end-to-end ASR without the joint decoding. λ in Eq. <ref type="formula" target="#formula_7">(7)</ref> was set to 0.1. When decoding, we man- ually set the minimum and maximum lengths of output sequences by 0.025 and 0.15 times input sequence lengths, respectively. The length penalty γ in Eq. (11) was set to 0.1. Multitask learning (MTL) significantly outperformed attention-based ASR in the all evaluation tasks, which confirms the effectiveness of a hybrid CTC/attention archi- tecture. Table 1 also shows that joint decoding, described in Section 3.2, further improved the per- formance without setting any search parameters (maximum and minimum lengths, length penalty), but only setting a weight parameter λ = 0.1 in Eq. (15) similar to the MTL case. <ref type="figure" target="#fig_0">Figure 2</ref> also compares the dependency of λ on the CER for the CSJ evaluation tasks, and showing that λ was not so sensitive to the performance if we set λ around the value we used at MTL (i.e., 0.1).</p><p>We also compare the performance of the proposed MTL-large, which has a larger net- work (5-layer encoder network), with the con- ventional state-of-the-art techniques obtained by using linguistic resources. The state-of-the-art CERs of GMM discriminative training and DNN- sMBR/HMM systems are obtained from the Kaldi recipe (Moriya et al., 2015) and a system based on syllable-based CTC with MAP decoding ( <ref type="bibr">Kanda et al., 2016)</ref>. The Kaldi recipe systems use aca- demic lectures (236h) for AM training and all training-data transcriptions for LM training. Un- like the proposed method, these methods use lin- guistic resources including a morphological an- alyzer, pronunciation dictionary, and language model. Note that since the amount of training <ref type="table">Table 1</ref>: Character error rate (CER) for conventional attention and hybrid CTC/attention end-to-end ASR. Corpus of Spontaneous Japanese speech recognition (CSJ) task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Hour <ref type="table" target="#tab_1">Task1 Task2 Task3  Attention  581</ref> 11.4 7.9 9.0 MTL 581 10.5 7.6 8.3 MTL + joint decoding (rescoring) 581 10.1 7.1 7.8 MTL + joint decoding (one pass) 581 10.0 7.1 7.6 MTL-large + joint decoding (rescoring) 581 8.4 6.2 6.9 MTL-large + joint decoding (one pass) 581 8.4 6.1 6.9 GMM-discr. ( <ref type="bibr">Moriya et al., 2015)</ref> 236 for AM, 581 for LM 11.2 9.2 12.1 DNN/HMM ( <ref type="bibr">Moriya et al., 2015)</ref> 236 for AM, 581 for LM 9.0 7.2 9.6 CTC-syllable ( <ref type="bibr">Kanda et al., 2016)</ref> 581 9.4 7. data and experimental configurations of the pro- posed and reference methods are different, it is difficult to compare the performance listed in the table directly. However, since the CERs of the proposed method are superior to those of the best reference results, we can state that the proposed method achieves the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mandarin telephone speech</head><p>We demonstrated ASR experiments on HKUST Mandarin Chinese conversational telephone speech recognition (MTS) ( <ref type="bibr">Liu et al., 2006</ref>). It has 5 hours recording for evaluation, and we extracted 5 hours from training data as a development set, and used the rest (167 hours) as a training set. All experimental conditions were same as those in Section 4.1 except that we used the λ = 0.5 in training and decoding instead of 0.1 based on our preliminary investi- gation and 80 mel-scale filterbank coefficients with pitch features as suggested in ( <ref type="bibr">Miao et al., 2016)</ref>. In decoding, we also added a result of the coverage-term based decoding <ref type="bibr">(Chorowski and Jaitly, 2016)</ref>, as discussed in Section 3.2 (η = 1.5, τ = 0.5, γ = −0.6 for attention model and η = 1.0, τ = 0.5, γ = −0.1 for MTL), since it was difficult to eliminate the irregular alignments during decoding by only tuning the maximum and minimum lengths and length penalty (we set the minimum and maximum lengths of output sequences by 0.0 and 0.1 times input sequence lengths, respectively and set γ = 0.6 in <ref type="table">Table 2</ref>). <ref type="table">Table 2</ref> shows the effectiveness of MTL and joint decoding over the attention-based approach, especially showing the significant improvement of the joint CTC/attention decoding. Similar to the CSJ experiments in Section 4.1, we did not use the length-penalty term or the coverage term in joint decoding. This is an advantage of joint de- coding over conventional approaches that require many tuning parameters. We also generated more training data by linearly scaling the audio lengths by factors of 0.9 and 1.1 (speed perturb.). The fi- nal model achieved 29.9% without using linguistic resources, which defeats moderate state-of-the-art systems including CTC-based methods 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Decoding speed</head><p>We evaluated the speed of the joint decoding meth- ods described in Section 3.2.3. ASR decoding was performed with different beam widths of 1, 3, 5, 10, and 20, and the processing time and CER were measured using a computer with Intel(R) Xeon(R) processors, E5-2690 v3, 2.6 GHz. Although the processors were multicore CPUs and the computer had GPUs, we ran the decoding program as a <ref type="table">Table 2</ref>: Character error rate (CER) for conventional attention and hybrid CTC/attention end-to-end ASR. HKUST Mandarin Chinese conversational telephone speech recognition (MTS) task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model dev eval</head><note type="other">Attention 40.3 37.8 MTL 38.7 36.6 Attention + coverage 39.4 37.6 MTL + coverage 36.9 35.3 MTL + joint decoding (rescoring) 35.9 34.2 MTL + joint decoding (one pass) 35.5 33.9 MTL-large (speed perturb.) + joint decoding (rescoring) 31.1 30.1 MTL-large (speed perturb.) + joint decoding (one pass) 31.0 29.9 DNN/HMM - 35.9 LSTM/HMM (speed perturb.) - 33.5 CTC with language model (Miao et al., 2016) - 34.8 TDNN/HMM, lattice-free MMI (speed perturb.) (Povey et al., 2016) - 28.2</note><p>single-threaded process on a CPU to investigate its basic computational cost.  <ref type="table" target="#tab_1">Table 3</ref> shows the relationships between the real-time factor (RTF) and the CER for the CSJ and HKUST tasks. We evaluated the rescoring and one-pass decoding methods when using the end detection in Eq. <ref type="bibr">(18)</ref>. In every beam width, we can see that the one-pass method runs faster with an equal or lower CER than the rescoring method. This result demonstrates that the one-pass decod- ing is effective for reducing search errors. Finally, we achieved 1xRT with one-pass decoding when using a beam width around 3 to 5, even though it was a single-threaded process on a CPU. However, the decoding process has not yet achieved real- time ASR since CTC and the attention mechanism need to access all of the frames of the input utter- ance even when predicting the first label. This is an essential problem of most end-to-end ASR ap- proaches and will be solved in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and discussion</head><p>This paper proposes end-to-end ASR by us- ing joint CTC/attention decoding, which outper- formed ordinary attention-based end-to-end ASR by solving the misalignment issues. The joint de- coding methods actually reduced most of the ir- regular alignments, which can be confirmed from the examples of recognition errors and alignment plots shown in Appendix C.</p><p>The proposed end-to-end ASR does not re- quire linguistic resources, such as morphological analyzer, pronunciation dictionary, and language model, which are essential components of conven- tional Japanese and Mandarin Chinese ASR sys- tems. Nevertheless, the method achieved com- parable/superior performance to the state-of-the- art conventional systems for the CSJ and MTS tasks. In addition, the proposed method does not require GMM/HMM construction for initial align- ments, DNN pre-training, lattice generation for se- quence discriminative training, complex search in decoding (e.g., FST decoder or lexical tree search based decoder). Thus, the method greatly simpli- fies the ASR building process, reducing code size and complexity.</p><p>Future work will apply this technique to the other languages including English, where we have to solve an issue of long sequence lengths, which requires heavy computation cost and makes it dif- ficult to train a decoder network. Actually, neu- ral machine translation handles this issue by us- ing a sub word unit (concatenating several letters to form a new sub word unit) ( <ref type="bibr" target="#b6">Wu et al., 2016)</ref>, which would be a promising direction for end-to- end ASR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Location-based attention mechanism</head><p>This section provides the equations of a location- based attention mechanism Attention(·) in Eq. (5).</p><formula xml:id="formula_24">a lt = Attention({a l−1 } t , q l−1 , h t ),</formula><p>where</p><formula xml:id="formula_25">{a l−1 } t = [a l−1,1 , · · · , a l−1,T ] .</formula><p>To obtain a lt , we use the following equations:</p><formula xml:id="formula_26">{f t } t = K * a l−1<label>(19)</label></formula><formula xml:id="formula_27">e lt = g tanh(G q q l−1 + G h h t + G f f t + b)<label>(20)</label></formula><formula xml:id="formula_28">a lt = exp(e tl ) t exp(e tl )<label>(21)</label></formula><p>K, G q , G h , G f are matrix parameters. b and g are vector parameters. * denotes convolution along in- put feature axis t with matrix K to produce feature {f t } t .</p><p>Algorithm 2 CTC hypothesis score 1: function αCTC(h, X) 2: g, c ← h split h into the last label c and the rest g 3:</p><p>if c = &lt;eos&gt; then 4:</p><p>return log{γ</p><formula xml:id="formula_29">(n) T (g) + γ (b) T (g)} 5:</formula><p>else</p><formula xml:id="formula_30">6: γ (n) 1 (h) ← p(z1 = c|X) if g = &lt;sos&gt; 0 otherwise 7: γ (b) 1 (h) ← 0 8: Ψ ← γ (n) 1 (h) 9:</formula><p>for t = 2 . . . T do 10:</p><formula xml:id="formula_31">Φ ← γ (b) t−1 (g) + 0 if last(g) = c γ (n) t−1 (g) otherwise 11: γ (n) t (h) ← γ (n) t−1 (h) + Φ p(zt = c|X) 12: γ (b) t (h) ← γ (b) t−1 (h) + γ (n) t−1 (h) p(zt = &lt;b&gt;|X) 13: Ψ ← Ψ + Φ · p(zt = c|X) 14:</formula><p>end for 15:</p><p>return log(Ψ) 16:</p><p>end if 17: end function B CTC-based hypothesis score</p><p>The CTC score α ctc (h, X) in Eq. <ref type="formula" target="#formula_1">(17)</ref> is computed as shown in Algorithm 2. Let γ </p><formula xml:id="formula_32">γ (b) t (&lt;sos&gt;) = t τ =1 γ (b) τ −1 (&lt;sos&gt;)p(z τ = &lt;b&gt;|X),<label>(22)</label></formula><p>where we assume that γ (b) 0 (&lt;sos&gt;) = 1 and &lt;b&gt; is a blank symbol. Note that the time index t and input length T may differ from those of the input utterance X owing to the subsampling technique for the encoder ( <ref type="bibr" target="#b3">Povey et al., 2016;</ref><ref type="bibr">Chan et al., 2015)</ref>.</p><p>In Algorithm 2, the hypothesis h is first split into the last label c and the rest g in line 2. If c is &lt;eos&gt;, it returns the logarithm of the forward probability assuming that h is a complete hypothe- sis in line 4. The forward probability of h is given by</p><formula xml:id="formula_34">p ctc (h|X) = γ (n) T (g) + γ (b) T (g)<label>(24)</label></formula><p>according to the definition of γ (n) bilities γ (n) t (h) and γ (b) t (h), and the prefix proba- bility Ψ = p ctc (h, . . . |X) assuming that h is not a complete hypothesis. The initialization and re- cursion steps for those probabilities are described in lines <ref type="bibr">6-14.</ref> In this function, we assume that whenever we compute the probabilities γ t (g) have already been obtained through the beam search process because g is a prefix of h such that |g| &lt; |h|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Examples of irregular alignments</head><p>We list examples of irregular alignments caused by attention-based ASR. <ref type="figure" target="#fig_3">Figure 3</ref> shows an example of repetitions of word chunks. The first chunk of blue characters in attention-based ASR (MTL) is appeared again, and the whole second chunk part becomes insertion errors. <ref type="figure" target="#fig_5">Figure 4</ref> shows an ex- ample of deletion errors. The latter half of the sentence in attention-based ASR (MTL) is bro- ken, which causes deletion errors. The hybrid CTC/attention with both multitask learning and joint decoding avoids these issues. <ref type="figure" target="#fig_6">Figures 5 and 6</ref> show alignment plots corresponding to <ref type="figure" target="#fig_3">Figs. 3 and  4</ref>, respectively, where X-axis shows time frames and Y-axis shows the character sequence hypoth- esis. These visual plots also demonstrate that the proposed joint decoding approach can suppress ir- regular alignments.   <ref type="table">)  Reference  ま   た   え   飛   行   時   の   エ   コ   ー   ロ   ケ   ー   シ   ョ   ン   機   能   を   よ   り  詳   細   に   解   明   す   る   為   に   超   小   型   マ   イ   ク   ロ   ホ   ン   お   よ   び  生   体   ア   ン   プ   を   コ   ウ   モ   リ   に   搭   載   す   る   こ   と   を   考   え   て  お   り   ま   す   そ   う   す   る   こ   と   に   よ   っ   て</ref> MTL Scores: (#Correctness #Substitution #Deletion #Insertion) 30 0 47 0</p><formula xml:id="formula_35">ま た え 飛 行 時 の エ コ ー ロ ケ ー シ ョ ン 機 能 を よ り 詳 細 に 解 明 す る 為 ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ ・ に ・ ・ ・</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint decoding</head><p>Scores: (#Correctness #Substitution #Deletion #Insertion) 67 9 1 0  </p><formula xml:id="formula_36">ま た え 飛 行 時 の エ コ ー ロ ケ ー シ ョ ン 機 能 を よ り 詳 細 に 解 明 す る 為 に 長 国 型 マ イ ク ロ ホ ン お ・ い く 声 単 位 方 を コ ウ モ リ に 登 載 す る こ と を 考 え て お り ま す そ う す る こ と に よ っ て</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The effect of weight parameter λ in Eq. (14) on the CSJ evaluation tasks (The CERs were obtained by one-pass decoding).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>be the forward probabilities of the hypothesis h over the time frames 1 . . . t, where the superscripts (n) and (b) denote different cases in which all CTC paths end with a nonblank or blank sym- bol, respectively. Before starting the beam search, γ (n) t () and γ (b) t () are initialized for t = 1, . . . , T as γ (n) t (&lt;sos&gt;) = 0,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of insertion errors appeared in attention-based ASR with MTL and joint decoding.</figDesc><graphic url="image-184.png" coords="12,307.28,560.47,115.19,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of deletion errors appeared in attention-based ASR with MTL and joint decoding.</figDesc><graphic url="image-183.png" coords="12,422.47,317.31,115.19,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example of alignments including deletion errors in attention-based ASR with MTL and joint decoding (Utterance id: A01F0001 0844951 0854386).</figDesc><graphic url="image-185.png" coords="12,422.47,560.47,115.19,115.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>where D end and M are predetermined thresholds. This equation becomes true if complete hypothe- ses with smaller scores are generated M times consecutively. This technique is also available in attention-based decoding and rescoring methods described in Sections 3.2.1-3.2.3.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>RTF versus CER for the one-pass and 
rescoring methods. 

Beam Rescoring 
One pass 
Task 
width RTF CER RTF CER 
1 
0.66 10.9 0.66 10.7 
CSJ 
3 
1.11 10.3 1.02 10.1 
Task1 
5 
1.50 10.2 1.31 10.0 
10 
2.46 10.1 2.07 10.0 
20 
5.02 10.1 3.76 10.0 
1 
0.68 37.1 0.65 35.9 
HKUST 
3 
0.89 34.9 0.86 34.4 
Eval set 
5 
1.04 34.6 1.03 34.2 
10 
1.55 34.4 1.50 34.0 
20 
2.66 34.2 2.55 33.9 

</table></figure>

			<note place="foot" n="1"> Note that in the implementation, the transition value is not normalized (i.e., not a probabilistic value) (Graves and Jaitly, 2014; Miao et al., 2015), similar to the HMM state transition implementation (Povey et al., 2011)</note>

			<note place="foot" n="2"> Although the proposed method did not reach the performance obtained by a time delayed neural network (TDNN) with lattice-free sequence discriminative training (Povey et al., 2016), our recent work scored 28.0%, and outperformed the lattice-free MMI result with advanced network architectures.</note>

			<note place="foot">t () and γ (b) t (). If c is not &lt;eos&gt;, it computes the forward proba</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno type="arXiv">arXiv:1211.5063</idno>
		<title level="m">On the difficulty of training recurrent neural networks</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>ASRU</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Purely sequence-trained neural networks for asr based on lattice-free MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pegah</forename><surname>Ghahrmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09975</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems (LearningSys) in NIPS</title>
		<meeting>Workshop on Machine Learning Systems (LearningSys) in NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chinese word segmentation as character tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics and Chinese Language Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="48" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
