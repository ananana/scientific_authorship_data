<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Alignment without NULL Words</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Schulz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schulz@uva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aziz</forename><forename type="middle">W</forename><surname>Nl Wilker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aziz@uva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simaan@uva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ILLC University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Word Alignment without NULL Words</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="169" to="174"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In word alignment certain source words are only needed for fluency reasons and do not have a translation on the target side. Most word alignment models assume a target NULL word from which they generate these untranslatable source words. Hypothesising a target NULL word is not without problems, however. For example, because this NULL word has a position, it interferes with the distribution over alignment jumps. We present a word alignment model that accounts for untranslatable source words by generating them from preceding source words. It thereby removes the need for a target NULL word and only models alignments between word pairs that are actually observed in the data. Translation experiments on English paired with Czech, German, French and Japanese show that the model outperforms its traditional IBM counterparts in terms of BLEU score.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When the IBM models ( <ref type="bibr" target="#b0">Brown et al., 1993)</ref> were designed, some way of accounting for words that likely have no translation was needed. The mod- ellers back then decided to introduce a NULL word on the target (generating) side <ref type="bibr">1</ref> . All words on the source side without a proper target transla- tion would then be generated by that NULL word.</p><p>While this solution is technically valid, it ne- glects that those untranslatable words are required for source fluency. Moreover, the NULL word, although hypothetical in nature, does have a po- sition. It is well-known that this NULL posi-tion is problematic for distortion-based alignment models. Alignments to NULL demand a special treatment as they would otherwise induce very long jumps that one does not usually observe in distortion-based alignment models. Examples of this can be found in <ref type="bibr" target="#b13">Vogel et al. (1996)</ref>, who drop the NULL word entirely and thus force all source words to align lexically, and <ref type="bibr" target="#b9">Och and Ney (2003)</ref>, who choose a fixed NULL probability.</p><p>In the present work, we introduce a family of IBM-style alignment models that can express de- pendencies between translated and untranslated source words. The models do not use NULL words and instead allow untranslatable source words to be generated from translated words in their context. This is achieved by modelling source word collocations. From a technical point of view the model can be seen as a mixture of an alignment and a language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IBM models 1 and 2</head><p>Here, we quickly review the IBM alignment mod- els 1 and 2 ( <ref type="bibr" target="#b0">Brown et al., 1993)</ref>. We assume a ran- dom variable E over the English (target) vocabu- lary 2 , a variable F over the French (source) vocab- ulary and a variable A over alignment links <ref type="bibr">3</ref> . The IBM models assign probabilities to alignment con- figurations and source sentences given the target side. Under the assumption that all source words are conditionally independent given the alignment links, these probabilities factorise as</p><formula xml:id="formula_0">P (f m 1 , a m 1 |e l 0 ) = P (a m 1 ) m j=1 P (f j |e a j )<label>(1)</label></formula><p>where x k 1 is a vector of outcomes x 1 , . . . , x k and e a j denotes the English word that the French word in the j th position (f j ) is aligned to under a m 1 . In IBM model 1 P (a m 1 ) is uniform. In IBM model 2, all alignment links a j are assumed to be independent and follow a categorical distribu- tion. Here, we choose to parametrise this categori- cal based on the distance between the two words to be aligned, as has been done by <ref type="bibr" target="#b13">Vogel et al. (1996)</ref> and <ref type="bibr" target="#b6">Liang et al. (2006)</ref>. Thus, in our IBM model 2</p><formula xml:id="formula_1">P (a m 1 ) = m j=1 P (a j ) = m j=1 P i − jl m (2)</formula><p>where i is the position of the English word that a j links to and the values l and m stand for the target and source sentence lengths. Notice that there is a target position i = 0 for the NULL word. Align- ment to this NULL position often causes unusually long alignment jumps.</p><p>3 Removing the NULL word</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model description</head><p>Our model consists of an alignment model com- ponent (which is either IBM model 1 or 2 without NULL words) and a language model component. It also contains a random variable Z that indicates which component to use. If Z = 0 we use the alignment model, if Z = 1 we instead use the lan- guage model. We generate each z j conditional on f j−1 . By making the outcome z j depend on f j−1 , we allow the model to capture the tendency of in- dividual source words to be part of a collocation, i.e. to be followed by a closely related word. A similar strategy has been employed for topic mod- elling by <ref type="bibr" target="#b2">Griffiths et al. (2007)</ref>. When generating the source side, the model does the following for each source word f j :</p><p>1. Depending on the previous source word f j−1 , draw z j .</p><p>2. If z j = 1, generate f j from f j−1 and choose a j according to P (a j ). Otherwise, if z j = 0, generate f j from the target side and choose a j according to the probability that it has under the relevant alignment model without a target NULL word.</p><p>Our model thus induces a joint probability dis- tribution of the form <ref type="figure">Figure 1</ref>: A graphical representation of our model for S sentence pairs. We use V f /e to denote the source/target vocabulary sizes and D to denote the number of possible alignment link configurations. Furthermore, S m/l is the number of source/target words in the current sentence and f prv the source word preceding the one that we currently generate.</p><formula xml:id="formula_2">P (f m 1 , a m 1 , z m 1 |e l 1 ) (3) = P (a m 1 ) m j=1 P (z j |f j−1 )P (f j |e a j , f j−1 , z j ) f f prv a z θ f q e S l 1 θ e θ a α β γ s, r Sm Ve V f V f D S</formula><p>where it is crucial to note that there is no E 0 variable, standing for the NULL word, anymore. Therefore, jumps to a NULL position do not need to be modelled. Notice further that the formula- tion of our model is general enough to be readily extensible to an HMM alignment model ( <ref type="bibr" target="#b13">Vogel et al., 1996)</ref>. Depending on the value of z j , F j is distributed either according to an alignment (4) or a language model 4 (5).</p><formula xml:id="formula_3">P (f j |e a j , f j−1 , z j = 0) = P (f j |e a j ) (4) P (f j |e a j , f j−1 , z j = 1) = P (f j |f j−1 ) (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The full model</head><p>Our full model is a Bayesian model, meaning that we treat all model parameters as random variables that are drawn from prior distributions. A graphi- cal depiction of the model can be found in <ref type="figure">Figure  1</ref>. We impose Dirichlet priors on the translation (θ e ), language model (θ f ) and distortion parame- ters (θ a ). This has been done before and improved the standard IBM models. In order to be able to bias the model against us- ing the language model component (5) too often and instead make it prefer the alignment model component (4), we impose a Beta prior on the Bernoulli distributions over component choices.</p><p>In effect, the model will only explain a source word with the language model if there is a lot of evidence that this word cannot be translated from the target side. The full model can be summarised as follows:</p><formula xml:id="formula_4">F j |e, a j , z j = 0 ∼ Cat(θ e aj ) Θ e aj ∼ Dir(α) F j |f j−1 , z j = 1 ∼ Cat(θ f j−1 ) Θ f j−1 ∼ Dir(β) Z j |f j−1 ∼ Bernoulli(q) Q ∼ Beta(s, r) .</formula><p>For IBM model 1, A j is uniformly distributed whereas for model 2 we have</p><formula xml:id="formula_5">A ∼ Cat(θ a ) Θ a ∼ Dir(γ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference</head><p>We use a Gibbs sampler to perform inference of the alignment and choice variables. Since our pri- ors are conjugate to the model distributions, we integrate over the model parameters, giving us a collapsed sampler <ref type="bibr">5</ref> . The sampler alternates be- tween sampling alignment links A and component choices Z.</p><p>The predictive posterior probabilities for Z j = 0 and Z j = 1 are given in Equations (6) and <ref type="formula" target="#formula_7">(7)</ref> (up to proportionality). We use c(·) as a (con- ditional) count function that counts how often an outcome has been observed in a given context. We furthermore use V f to denote the French (source) vocabulary size. To ease notation, we also in- troduce the context set C −X j which contains the current values of all variables in our model except X j and the set H which simply contains all hyper- parameters.</p><formula xml:id="formula_6">P Z j = 0|C −Z j , H ∝<label>(6)</label></formula><formula xml:id="formula_7">(c(z = 0|f j−1 ) + s) P (a j ) c(f j |e a j , z = 0) + α c(e a j |z = 0) + αV f P Z j = 1|C −Z j , H ∝<label>(7)</label></formula><formula xml:id="formula_8">(c(z = 1|f j−1 ) + r) c(f j |f j−1 , z = 1) + β c(z = 1|f j−1 ) + βV f</formula><p>When Z j = 0, the predictive probability for align- ment link A j is proportional to Equation (8).</p><formula xml:id="formula_9">P a j |C −Z j ,−A j , Z j = 0, H ∝<label>(8)</label></formula><formula xml:id="formula_10">P (a j ) c(f j |e a j , z = 0) + α c(e a j |z = 0) + αV f 5</formula><p>Derivations of samplers similar to ours can be found in the appendices of <ref type="bibr" target="#b8">Mermer et al. (2013)</ref> and <ref type="bibr" target="#b2">Griffiths et al. (2007)</ref>. We omit the derivation here for space reasons.</p><p>When Z j = 1, it is simply proportional to P (a j ). In the case of IBM model 1, P (a j ) is a constant. For IBM model 2, we use</p><formula xml:id="formula_11">P (a j ) ∝ c i − jl m + γ .</formula><p>where l and m are the target and source sentence lengths. Notice that target positions start at 1 as we do not use a NULL word.</p><p>Notice that a na¨ıvena¨ıve implementation of our sam- pler is unpractically slow. We therefore augment the sampler with an auxiliary variable <ref type="bibr" target="#b12">(Tanner and Wong, 1987)</ref> that uniformly chooses only one pos- sible new assignment per sampled link. The sam- pling complexity, which would normally be lin- ear in the size of the target sentence, thus becomes constant. In practice this speed up the sampler by several orders of magnitude, making our aligner as fast as Giza++. Unfortunately, this strategy also slightly impairs the mobility of our sampler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoding</head><p>Our samples contain assignments of the A and Z variables. If for a word f j we have z j = 1, we treat the word as not aligned. We then use maxi- mum marginal decoding <ref type="bibr" target="#b4">(Johnson and Goldwater, 2009</ref>) over alignment links to generate final word alignments. This means that we align each source word to the target word it has been aligned to most often in the samples. If the word was unaligned in most samples, we leave it unaligned in the output alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head><p>We present translation experiments on English paired with German, French, Czech and Japanese, thereby covering four language families. We com- pare our model and the Bayesian IBM models 1 and 2 of <ref type="bibr" target="#b8">Mermer et al. (2013)</ref> against IBM model 2 as a baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments</head><p>Data We use the news commentary data from the WMT 2014 translation task 6 for German, French and Czech paired with English. We use newstest-2013 as development data and we use the newstest-2014 for testing. We use all available monolingual data from WMT 2014 for language modelling. All data are truecased and sentences (b) Symmetrised: alignments obtained in both directions independently and heuristically symmetrised (grow-diag-final-and). <ref type="table">Table 1</ref>: Translation results from and into English. Alignments in the top (1a) and bottom (1b) tables were obtained in the target-to-source direction and symmetrised, respectively. Differences are computed with respect to the directional IBM model 2 in its original parameterisation <ref type="bibr" target="#b0">(Brown et al., 1993)</ref>. The best Bayesian model in each column is boldfaced.</p><note type="other">Model En-De En-Fr En-Cs En-Ja De-En Fr-En Cs-En Ja-En Brown</note><p>with more than 100 words discarded as is stan- dardly done in SMT. The Japanese training data consist of 200.000 randomly extracted sentence pairs from the NTCIR-8 Patent Translation Task. The full data are used for language modelling. We use the NTCIR-7 dev sets for tuning and the NTCIR-9 test set for testing. <ref type="bibr">7</ref> Training The maximum likelihood IBM model 2 is initialized with model 1 parameter estimates and trained for 5 EM iterations. Following Mer- mer and Saraçlar (2011), we initialize the Gibbs samplers of all Bayesian models with the Viterbi alignment from IBM model 1. We run each sam- pler for 1000 iterations and take a sample after ev- ery 25 th iteration. We do not use burn-in. 8</p><p>Hyperparameters All Bayesian models are trained with α = 0.0001 and β = 0.0001 to induce sparse lexical distributions. We also set s = 1 and r = 0.1 when IBM1 is the align- ment component in our model. This has the ef- fect of biasing the model towards using the align- <ref type="bibr">7</ref> The Japanese data was provided to us by a colleague with the pre-processing steps already performed, with sentences shortened to at most 40 words. Our algorithm can handle sen- tences of any length and there is actually no need to restrict the sentence lengths.</p><p>8 Burn-in is simply a heuristic that is not guaranteed to improve the samples in any way. See http://users. stat.umn.edu/ ˜ geyer/mcmc/burn.html for fur- ther details. ment component. For the IBM2 version we even set r = 0.01 since IBM2 is a more trustworthy alignment model. For IBM2, we furthermore set γ = 1 to obtain a flat distortion prior.</p><p>Observe that experiments presented here use the same fixed hyperparameters for all language pairs. We tried to add another level to our model by imposing Gamma priors on the hyperparam- eters. The hyperparameters were then inferred using slice sampling after each Gibbs iteration. When run on the German-English and Czech- English data, this strategy increased the posterior probability of the states visited by our sampler but had no effect on BLEU. This may indicate that either the hand-chosen hyperparameters are ade- quate for the task or that the model generally per- forms well for a large range of hyperparameters.</p><p>Translation We train Moses systems ( <ref type="bibr" target="#b5">Koehn et al., 2007</ref>) with 5-gram language models with modified Kneser-Ney-smoothing using <ref type="bibr">KenLM (Heafield et al., 2013</ref>) and orientation-based lex- icalised reordering. We tune the systems with MERT <ref type="bibr" target="#b10">(Och, 2003)</ref> on the dev sets. We report the BLEU score ( <ref type="bibr" target="#b11">Papineni et al., 2002</ref>) for all models averaged over 5 MERT runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We report the translation results in <ref type="table">Tables (1a)</ref> and (1b). Results of the full Giza++ pipeline and fastAlign <ref type="bibr" target="#b1">(Dyer et al., 2013</ref>) are reported as a com-parison standard. All symmetrised results were obtained using the grow-diag-final-and heuristic.</p><p>Using IBM2 as an alignment component, our model mostly outperforms the standard IBM mod- els and their Bayesian variants. Importantly, the improvement that our model 2 achieves over its model 1 variant is much larger than the difference between the corresponding models of <ref type="bibr" target="#b8">Mermer et al. (2013)</ref>. This indicates that our model makes better use of the distortion distribution that is not altered by NULL alignments. We also observe that our model gains relatively little from symmetrisa- tion, likely because it is a very strong model al- ready. It is interesting that although our model 2 does not use fertility parameters or dependencies between alignment links, it often approaches the performance of Giza which does use these fea- tures. Moreover, it also approaches the perfor- mance of fastAlign which does not use fertility nor dependencies between alignment links, but has a stronger inductive bias with respect to distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and future work</head><p>We have presented an IBM-style word alignment model that does not need to hypothesise a NULL word as it explains untranslatable source words by grouping them with translated words. This also leads to a cleaner handling of distortion probabili- ties.</p><p>In our present work, we have only considered IBM models 1 and 2. As we have mentioned al- ready, our model can easily be extended with the HMM alignment model. We are currently explor- ing this possibility. Our models also allow sym- metrisation ( <ref type="bibr" target="#b6">Liang et al., 2006</ref>) of all translation and distortion parameters where before the NULL distortion parameters had to be fixed. We therefore plan to extend them towards model-based instead of heuristic alignment symmetrisation.</p><p>A limitation of our model is that it is only ca- pable of modelling left-to-right linear dependen- cies in the source language. In languages like Ger- man or English, however, where an adjective or determiner is selected by the following noun, this may not be appropriate to model selection biases amongst neighbouring words. An interesting ex- tension to our model is thus to add more structure to it such that it will be able to capture more com- plex source side dependencies.</p><p>Another concern is the inference in our model.</p><p>Using the auxiliary variable sampler, inference be- comes very fast but may sacrifice performance. This is why we are interested in improving the inference method, e.g. by using a more mobile sampler or by employing a variational Bayes algo- rithm. The software used in our experiments can be downloaded from https://github.com/ philschulz/Aligner.</p></div>
			<note place="foot" n="1"> The target side is often identified with English and the source side is usually taken to be French.</note>

			<note place="foot" n="2"> Crucially, this vocabulary includes a NULL word. 3 We denote realisations of random variables by the corresponding lower case letters.</note>

			<note place="foot" n="4"> We use a bigram LM to avoid conditioning Z on longer (n − 1)-grams.</note>

			<note place="foot" n="6"> http://statmt.org/wmt14/ translation-task.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Miloš Stanojevi´cStanojevi´c for pro-viding us with the preprocessed Japanese data. We would also like to thank our reviewers for their helpful feedback. This work was supported by the Netherlands Organization for Scientific Research (NWO) VICI Grant nr. 277-89-002.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of IBM model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Topics in semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="244" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable modified Kneser-Ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving nonparameteric Bayesian inference: Experiments on unsupervised word segmentation with adaptor grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL &apos;09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="317" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL &apos;06</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian word alignment for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cos¸kuncos¸kun</forename><surname>Mermer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Saraçlar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation using Bayesian word alignment and Gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cos¸kuncos¸kun</forename><surname>Mermer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Saraçlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech &amp; Language Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1090" to="1101" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<title level="m">Minimum error rate training in statistical machine translation. ACL &apos;03</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The calculation of posterior distributions by data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wing Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">398</biblScope>
			<biblScope unit="page" from="528" to="550" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HMM-based word alignment in statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Computational Linguistics</title>
		<meeting>the 16th Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="836" to="841" />
		</imprint>
	</monogr>
	<note>COLING &apos;96</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
