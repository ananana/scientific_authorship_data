<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Elhadad</surname></persName>
							<email>noemie.elhadad@columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia University</orgName>
								<orgName type="institution" key="instit2">Columbia University</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1375" to="1384"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A convex and feature-rich discriminative approach to dependency grammar inductionÉdouard induction´inductionÉdouard Grave Abstract In this paper, we introduce a new method for the problem of unsupervised dependency parsing. Most current approaches are based on generative models. Learning the parameters of such models relies on solving a non-convex optimization problem , thus making them sensitive to initial-ization. We propose a new convex formulation to the task of dependency grammar induction. Our approach is discriminative, allowing the use of different kinds of features. We describe an efficient optimization algorithm to learn the parameters of our model, based on the Frank-Wolfe algorithm. Our method can easily be generalized to other unsupervised learning problems. We evaluate our approach on ten languages belonging to four different families , showing that our method is competitive with other state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Grammar induction is an important problem in computational linguistics. Despite having recently received a lot of attention, it is still considered to be an unsolved problem. In this work, we are inter- ested in unsupervised dependency parsing. More precisely, our goal is to induce directed depen- dency trees, which capture binary syntactic rela- tions between the words of a sentence. Since our method is unsupervised, it does not have access to such syntactic structure and only take as in- put a corpus of words and their associated parts of speech.</p><p>Most recent approaches to unsupervised depen- dency parsing are based on probabilistic genera- tive models, such as the dependency model with valence introduced by <ref type="bibr" target="#b26">Klein and Manning (2004)</ref>. Learning the parameters of such models is often All languages have their own grammar done by maximizing the log-likelihood of unla- beled data, leading to a non-convex optimization problem. Thus, the performance of those methods rely heavily on the initialization, and practitioners have to find good heuristics to initialize their mod- els.</p><p>In this paper, we describe a different approach to the problem of dependency grammar induction, inspired by discriminative clustering. We pro- pose to use a feature-rich discriminative parser, and to learn the parameters of this parser us- ing a convex quadratic objective function. In particular, this approach also allows us to in- duce non-projective dependency structures. Fol- lowing the work of <ref type="bibr" target="#b35">Naseem et al. (2010)</ref>, we use language-independent rules between pairs of parts-of-speech to guide our parser. More pre- cisely, we make the following contributions:</p><p>• Our method is based on a feature-rich dis- criminative parser (section 3);</p><p>• Learning the parameters of our parser is achieved using a convex objective, and is thus not sensitive to initialization (section 4);</p><p>• Our method can produce non-projective de- pendency structures (section 3.2.2);</p><p>• We propose an efficient algorithm to opti- mize the objective, based on the Frank-Wolfe method (section 5);</p><p>• We evaluate our approach on the universal treebanks dataset, showing that it is competi- tive with the state-of-the-art (section 6).</p><p>A lot of research has been carried out in the last decade on dependency grammar induction. We review the dependency model with valence, on which most unsupervised dependency parsers are based, before presenting different extensions and learning algorithms. Finally, we review discrimi- native clustering, on which our method is based.</p><p>DMV. The dependency model with valence (DMV), introduced by <ref type="bibr" target="#b26">Klein and Manning (2004)</ref>, was the first method to outperform the baseline consisting in attaching each token to the next one.</p><p>The DMV is a generative probabilistic model of the dependency tree and parts-of-speech of a sen- tence. It generates the root first, and then recur- sively generates the tokens down the tree. The probability of generating a new dependent for a given token depends on the direction (left or right) and whether a dependent was already generated in that direction. Then, the part-of-speech of the new dependent is generated according to a multinomial distribution conditioned on the direction and the head's POS.</p><p>Extensions. Several extensions of the depen- dency model with valence have been proposed. Headden <ref type="bibr" target="#b19">III et al. (2009)</ref> proposed the lexicalized extended valence grammar (EVG), in which the probability of generating a POS also depends on the valence information. They rely on smooth- ing to tackle the increased number of parame- ters. Mareček andŽabokrtsk`yandˇandŽabokrtsk`andŽabokrtsk`y (2012) described an approach using a n-gram reducibility measure, which capture which words can be deleted from a sentence without making it syntactically incor- rect. <ref type="bibr" target="#b10">Cohen and Smith (2009)</ref> introduced a prior, based on the shared logistic normal distribution. This prior allowed to tie the grammar parameters corresponding to different POS belonging to the same coarse groups, such as all the POS corre- sponding to verbs. <ref type="bibr" target="#b1">Berg-Kirkpatrick and Klein (2010)</ref> proposed to tie the parameters of grammars for different languages using a prior based on a phylogenetic tree. <ref type="bibr" target="#b35">Naseem et al. (2010)</ref> proposed a set of rules between parts-of-speech, encoding syntactic universals, such as the fact that adjec- tives are often dependents of nouns. They used posterior regularization ( <ref type="bibr" target="#b14">Ganchev et al., 2010)</ref> to impose that a certain amount of the infered depen- dencies verifies one of these rules. Also using pos- terior regularization, <ref type="bibr" target="#b15">Gillenwater et al. (2011)</ref> im- posed a sparsity bias on the infered dependencies, enforcing a small number of unique dependency types. Finally, <ref type="bibr" target="#b3">Blunsom and Cohn (2010)</ref> refor- mulated dependency grammar induction using tree substitution grammars, while <ref type="bibr" target="#b2">Bisk and Hockenmaier (2013)</ref> proposed to use combinatory cate- gorial grammars.</p><p>Learning. Different algorithms have been pro- posed to improve the learning of the parameters of the dependency model with valence. <ref type="bibr" target="#b39">Smith and Eisner (2005)</ref> proposed to use constrastive es- timation to learn the parameters of a log-linear parametrization of the DMV, while <ref type="bibr" target="#b41">Spitkovsky et al. (2010b)</ref> showed that using Viterbi EM instead of classic EM leads to higher accuracy. Observing that learning from shorter sentences is easier (be- cause less ambiguous), <ref type="bibr" target="#b40">Spitkovsky et al. (2010a)</ref> presented different techniques to learn grammar from increasingly longer sentences. Gimpel and Smith (2012) introduced a model inspired by the IBM1 translation model for grammar induction, resulting in a concave log-likelihood function. They show that initializing the DMV with the output of their model leads to improved depen- dency accuracies. Discriminative clustering. Our unsupervised parser is inspired by discriminative clustering, in- troduced by <ref type="bibr" target="#b44">Xu et al. (2004)</ref>. Given a set of points, the objective of discriminative clustering is to as- sign labels to these points that can be easily pre- dicted using a discriminative classifier. <ref type="bibr" target="#b44">Xu et al. (2004)</ref> introduced a formulation using the hinge loss, <ref type="bibr" target="#b0">Bach and Harchaoui (2007)</ref> proposed to use the squared loss instead, while <ref type="bibr" target="#b23">Joulin et al. (2010)</ref> proposed a formulation based on the logistic loss.</p><p>Recently, a formulation based on discriminative clustering was proposed for the problem of distant supervision for relation extraction <ref type="bibr" target="#b18">(Grave, 2014)</ref> and for the problem of finding the names of char- acters in TV series based on the corresponding scripts ( <ref type="bibr" target="#b38">Ramanathan et al., 2014</ref>). Closest to our approach, extensions of discriminative clustering were used to align sequences of labels or text with videos ( <ref type="bibr" target="#b4">Bojanowski et al., 2014;</ref><ref type="bibr" target="#b5">Bojanowski et al., 2015</ref>) or to co-localize objects in videos ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we describe the parsing model used in our approach and briefly review the correspond- ing decoding algorithms. Following <ref type="bibr" target="#b32">McDonald et al. (2005b)</ref>, we propose to cast the problem of de- pendency parsing as a maximum weight spanning tree problem in directed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Edge-based factorization</head><p>Let us start by setting up some notations. An input sentence of length n is represented by an n−uplet x = (x 1 , ..., x n ). The dependency tree corresponding to that sentence is represented by a n × (n + 1) binary matrix y, such that y ij = 1 if and only if the head of the token i is the token j (and thus, the integer n + 1 represents the root of the tree).</p><p>In this paper, we follow a common approach by factoring the score of dependency tree as the sum of the scores of the edges forming that tree. We assume that each pair of tokens (i, j) is represented by a high-dimensional feature vec- tor f (x, i, j) ∈ R d . Then, the score s ij of the edge (i, j) is obtained using the linear model</p><formula xml:id="formula_0">s ij = w f (x, i, j),</formula><p>where w ∈ R d is a parameter vector. Thus the score s corresponding to the tree y is equal to</p><formula xml:id="formula_1">s = (i,j) s.t. y ij =1 s ij = (i,j) s.t. y ij =1 w f (x, i, j).</formula><p>Assuming that the parameter vector w is known, parsing a sentence reduces to finding the tree with the highest score, which is the maximum weight spanning tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Maximum spanning trees</head><p>Different sets of spanning trees have been consid- ered in the setting of supervised dependency pars- ing. We briefly review those sets, and describe the corresponding algorithms to compute the max- imum weight spanning tree over those sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Projective dependency trees</head><p>First, we consider the set of projective spanning trees. A dependency tree is said to be projective if the dependencies do not cross when drawn above the words in linear order. Similarly, this means that word and all its descendants form a contigu- ous substring of the sentence. Projective depen- dency trees are thus strongly related to context free grammars, and it is possible to obtain the maxi- mum weight spanning projective tree using a mod- ified version of the CKY algorithm <ref type="bibr" target="#b9">(Cocke and Schwartz, 1970;</ref><ref type="bibr" target="#b25">Kasami, 1965;</ref><ref type="bibr" target="#b45">Younger, 1967)</ref>. The complexity of this algorithm is O(n 5 ). This led <ref type="bibr" target="#b12">Eisner (1996)</ref> to propose an algorithm for pro- jective parsing which has a complexity of O(n 3 ). Similarly to CKY, the Eisner algorithm is based on dynamic programming, parsing a sentence in a bottom-up fashion. Finally, it should be noted that the dependency model with valence, on which most approaches to dependency grammar induc- tion are based, produces projective dependency trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Non-projective dependency trees</head><p>Second, we consider the set of non-projective spanning trees. Indeed, many languages, such as Czech or Dutch, have a significant number of non-projective edges. In the context of supervised dependency parsing, <ref type="bibr" target="#b32">McDonald et al. (2005b)</ref> shown that using non-projective trees improves the accuracy of dependency parsers for those lan- guages. The maximum weight spanning tree in a directed graph can be computed using the Chu- Liu/Edmonds algorithm ( <ref type="bibr" target="#b8">Chu and Liu, 1965;</ref><ref type="bibr" target="#b11">Edmonds, 1967)</ref>, which has a complexity of O(n 3 ). Later, <ref type="bibr" target="#b43">Tarjan (1977)</ref> proposed an improved ver- sion of this algorithm for dense graphs, whose complexity is O(n 2 ), the same as for undirected graphs using Prim's algorithm. Thus a second ad- vantage of using non-projective dependency trees is the fact that it leads to more efficient parsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning the parameter vector</head><p>In this section, we describe the loss function we use to learn the parameter vector w from unla- beled sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem formulation</head><p>From now on, y is a vector representing the de- pendency trees corresponding to the whole corpus. Thus, each index i corresponds to a potential de- pendency between two words of a given sentence.</p><p>He gave a seminar yesterday about unsupervised dependency parsing <ref type="figure">Figure 2</ref>: Example of a non-projective dependency tree in english.</p><p>Like before, y i = 1 if and only if there is a de- pendency between those two words, and y i = 0 otherwise. The set of dependencies that form valid trees is denoted by the set T .</p><p>Inspired by the discriminative clustering frame- work introduced by <ref type="bibr" target="#b44">Xu et al. (2004)</ref>, our goal is to jointly find the dependencies represented by the vector y and the parameter vector w which mini- mize the regularized empirical risk</p><formula xml:id="formula_2">min y∈T min w 1 n n i=1 (y i , w x i ) + λΩ(w),<label>(1)</label></formula><p>where is a loss function and Ω is a regularizer.</p><p>The intuition is that we want to find the depen- dency trees y that can be easily predicted by a dis- criminative parser, whose parameters are w.</p><p>Following <ref type="bibr" target="#b0">Bach and Harchaoui (2007)</ref>, we pro- pose to use the squared loss defined by</p><formula xml:id="formula_3">(y, ˆ y) = 1 2 (y − ˆ y) 2</formula><p>and to use the 2 -norm as a regularizer. In that case, we obtain the objective function:</p><formula xml:id="formula_4">min y∈T min w 1 2n y − Xw 2 2 + λ 2 w 2 2 .<label>(2)</label></formula><p>One of the main advantages of using the squared loss is the fact that the corresponding objective function is jointly convex in y and w. Indeed, the objective is the composition of an affine map- ping, defined by (y, w) → y − Xw, with a con- vex function, defined by u → u u. Thus, the objective function is convex (see section 3.2.2 of Boyd and Vandenberghe <ref type="formula" target="#formula_4">(2004)</ref>). The problem <ref type="formula" target="#formula_4">(2)</ref> is thus non-convex only because of the combinato- rial constraints on the binary vector y, namely that y should represents valid trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convex relaxation</head><p>The set T of vectors representing valid depen- dency trees is a finite set of binary vectors. We can thus take the convex hull of those points and denote it by Y: <ref type="table">Table 1</ref>: Set of universal rules used in our parser.</p><formula xml:id="formula_5">Y = conv(T ). VERB → VERB NOUN → NOUN VERB → NOUN NOUN → ADJ VERB → PRON NOUN → DET VERB → ADV NOUN → NUM VERB → ADP NOUN → CONJ ADJ → ADV ADP → NOUN</formula><p>By definition, this set is a convex polytope. We then propose to replace the combinatorial con- straints on the vector y by the fact that y should be in the convex polytope Y. We thus obtain a convex quadratic program, with linear constraints, as follows:</p><formula xml:id="formula_6">min y∈Y min w 1 2n y − Xw 2 2 + λ 2 w 2 2 .<label>(3)</label></formula><p>We will describe how to compute the optimal so- lution of this problem in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Rounding</head><p>Given a continuous solution y c ∈ Y of the relaxed problem, it is possible to obtain a solution of the integer problem by finding the tree y d ∈ T which is closest to y c , by solving the problem</p><formula xml:id="formula_7">min y d ∈T y d − y c 2 2 .</formula><p>The solution of the previous problem can easily be formulated is a minimum weight spanning tree problem. Indeed, by developping the previous expression, and using the fact that for all trees</p><formula xml:id="formula_8">y d ∈ T , y d y d = n,</formula><p>where n is the number of tokens, the previous problem is equivalent to:</p><formula xml:id="formula_9">min y d ∈T −y d y c ,</formula><p>whose solution is obtained using the minimum weight spanning tree algorithm. It should be noted that the rounding solution is not necessarily the optimal solution of the integer problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Prior on y</head><p>We now describe how to guide our unsuper- vised parser, by using universal rules. Following <ref type="bibr" target="#b35">Naseem et al. (2010)</ref>, we want a certain percent- age of the infered dependencies to satisfy one of the twelve universal syntactic rules, listed in Ta- ble 1. Let S be the set of indices corresponding to word pairs that satisfy one of these rules. Then, imposing that a certain percentage c of dependen- cies satisfy one of those rules can be obtained by imposing the constraint:</p><formula xml:id="formula_10">1 n i∈S y i ≥ c.</formula><p>This linear constraint is equivalent to u y ≥ c, where the vector u is defined by</p><formula xml:id="formula_11">u i = 1/n if i ∈ S, 0 otherwise.</formula><p>Using Lagrangian duality, we can obtain the fol- lowing equivalent penalized problem:</p><formula xml:id="formula_12">min y∈Y min w 1 2n y − Xw 2 2 + λ 2 w 2 2 − µ u y.<label>(4)</label></formula><p>The penalized and constrained problems are equivalent, since for every c, there exists a µ such that the two problems have the same optimum.</p><p>From an optimization point of view, it is easier to deal with the penalized problem and we will thus use it in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Optimization</head><p>One could use a general purpose quadratic solver to compute the solution of the previous convex problem. However, this might be inefficient since Algorithm 1: Frank-Wolfe algorithm for t ∈ {1, ..., T } do Compute the gradient:</p><formula xml:id="formula_13">g t = f (z t )</formula><p>Solve the linear program:</p><formula xml:id="formula_14">s t = min s∈D s g t</formula><p>Take the Frank-Wolfe step: z t+1 = γ t s t + (1 − γ t )z t end it does not use the structure of the polytope and, in particular, the fact that one can easily minimize a linear function over the tree polytope using the minimum weight spanning tree algorithm. Instead we propose to use the Frank-Wolfe algorithm, that we now describe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Frank-Wolfe algorithm</head><p>The Frank-Wolfe algorithm <ref type="bibr" target="#b13">(Frank and Wolfe, 1956;</ref><ref type="bibr" target="#b22">Jaggi, 2013</ref>) is used to minimize a convex differentiable function f over a convex bounded set D. It is an iterative first-order optimization method. At each iteration t, the convex function f is approximated by a linear function defined by its gradient at the current point z t . Then it finds the point s t that minimizes that linear function, over the convex set D:</p><formula xml:id="formula_15">s t = min s s f (z t ) s.t. s ∈ D.</formula><p>The point z t+1 is then defined as the weighted av- erage between the solution s t and the current point z t : z t+1 = γ t s t + (1−γ t ) z t , where γ t is the step size (such as 2/(t + 2)). Compared to the gradi- ent descent algorithm, the Frank-Wolfe alogrithm does not take a step in the direction of the gradi- ent, but in the direction of the point that minimizes the linear approximation of the function f over the convex set D (see <ref type="figure" target="#fig_2">Fig 3)</ref>. In particular, this ensures that the points z t always stay inside the convex set, and there is thus no need for a projection step.</p><p>To summarize, in order to use the Frank-Wolfe algorithm, we need to compute the gradient of the objective function and to minimize a linear func- tion over our convex set. This is particularly ap- propriate to our problem, since we can easily min- imize a linear function over the tree polytopes (us- ing the minimum weight spanning tree algorithm), while projecting on those polytopes is more ex- pensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Optimization algorithm for our method.</head><p>for t ∈ {1, ..., T } do Compute the optimal w:</p><formula xml:id="formula_16">w t = argmin w 1 2n y t − Xw 2 2 + λ 2 w 2 2</formula><p>Compute the gradient w.r.t. y:</p><formula xml:id="formula_17">g t = 1 n (y t − Xw t ) − µ u</formula><p>Solve the linear program:</p><formula xml:id="formula_18">s t = min s∈Y s g t</formula><p>Take the Frank-Wolfe step:</p><formula xml:id="formula_19">y t+1 = γ t s t + (1 − γ t )y t end</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Application to our problem</head><p>We now describe how to use the Frank-Wolfe al- gorithm to optimize our objective function with re- spect to y. First, let us introduce the functions f and h defined by We will use the Frank-Wolfe algorithm to optimize the function h. Minimizing w.r.t w. First, we need to minimize the function f with respect to w, in order to com- pute the function h (and its gradient). One must note that this is an unconstrained quadratic pro- gram, whose solution can be obtained in closed form by solving the linear system: X X + λI w = X y.</p><formula xml:id="formula_20">f (w, y) = 1 2n y − Xw 2 2 + λ 2 w 2 2 − µ u y, h(y) = min w f (w, y).</formula><p>However, in case of a very large feature space, this system might be prohibitively expensive to solve exactly. We instead propose to approximately compute the optimal w using stochastic gradient descent.</p><p>Computing the gradient of h. Then, the gradi- ent of the function h at the point y is equal to h(y) = y f (w * , y), <ref type="table">Table 2</ref>: Features used in our parser to describe the dependency between tokens i and j, where i is the head, j the dependent and</p><formula xml:id="formula_21">POS i × d POS j × d POS i × POS j × d POS i × POS i−1 × POS j × d POS i × POS i+1 × POS j × d POS i × POS j × POS j−1 × d POS i × POS j × POS j+1 × d</formula><formula xml:id="formula_22">d = i − j.</formula><p>where w * is equal to</p><formula xml:id="formula_23">w * = argmin w f (w, y).</formula><p>Thus, in order to compute the gradient of h with respect to y, we start by computing the corre- sponding optimal value of w. Then, the gradient with respect to y is equal to The optimal value of a linear function over a bounded convex polytope is always attained on at least one vertex of that polytope. By definition of our polytope, those vertices correspond to span- ning trees. Thus, computing an optimal solution of this problem is obtained by finding a minimum weight spanning tree.</p><formula xml:id="formula_24">h(y) = 1 n (y − Xw * ) − µ u.</formula><p>Discussion. Similarly to the Expectation- Maximization algorithm, our optimization method is a two-steps iterative algorithm. In the first step, the optimal parameter vector w is estimated based on the previous dependency trees, while the second step consist in re-estimating the (relaxed) dependency trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we report the results of the experi- ments we have performed to evaluate our approach to grammar induction.  <ref type="table">Table 3</ref>: Directed dependency accuracy, on the universal treebanks with universal parts-of- speech, on sentences of length 10 or less. PR refers to posterior regularization, USR to universal rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Features</head><p>The features used in our unsupervised parser are based on the parts-of-speech of the head and the dependent of the corresponding dependency, and are given in <ref type="table">Table 2</ref>. Following <ref type="bibr" target="#b31">McDonald et al. (2005a)</ref>, we also include features capturing the context of the head or the dependent. These fea- tures are trigrams and are formed by the parts- of-speech of the two tokens of the dependency and one of the word appearing before/after the head/dependent. Finally, all the features are con- joined with the signed distance between the two words of the dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dataset</head><p>We use the universal treebanks, version 2.0, intro- duced by <ref type="bibr" target="#b33">McDonald et al. (2013)</ref>. This dataset contains dependency trees for ten languages be- longing to five different families: Spanish, French, Italian, Portuguese (Romanic family), English, German, Swedish (Germanic family), Korean, Japanese and Indonesian. The tokens of those treebanks are tagged using the universal part-of- speech tagset ( <ref type="bibr" target="#b37">Petrov et al., 2012</ref>). We focus on inducing dependency grammars using universal parts-of-speech, and will thus report results where all methods use (gold) universal POS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with baselines</head><p>We will compare our approach to three other un- supervised parsers. Our first baseline is the DMV model, introduced by <ref type="bibr" target="#b26">Klein and Manning (2004)</ref>. <ref type="table">Table 4</ref>: Computational times required to learn a grammar on the English treebank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DMV PR USR OUR 7 min 1 h 15 h 2 min</head><p>Our second baseline is the extended valence gram- mar model, with posterior sparsity constraints, as described by <ref type="bibr" target="#b15">Gillenwater et al. (2011)</ref>. Finally, our last baseline is the model with universal rules introduced by <ref type="bibr" target="#b35">Naseem et al. (2010)</ref>. It should be noted that these two baselines obtain perfor- mances that are near state-of-the-art. All methods are trained and tested on sentences of length 10 or less, after stripping punctuation.</p><p>Parameter selection. All the parameters were chosen using the English development set. Our method has two parameters, determined as: λ = 0.001 and µ = 0.1. We used T = 200 iterations in all the experiments.</p><p>Discussion. We report the results in <ref type="table">Table 3</ref>. First, we observe that our method performs bet- ter than the three baselines on seven out of ten languages. Overall, our approach outperforms the three baselines, with an absolute improvement of 13 points over the extended valence grammar with posterior sparsity and 8 points over the model with universal syntactic rules. We also note that the inter-language variance is lower for our method than the baselines (std of 4.6 for our method v.s. 8.3 for USR and 12.7 for PR). For the sake of completeness, we also compared those methods using the fine grained POS available in the univer- sal treebanks. Overall, our method obtains an ac- curacy of 68.4, while USR and PR achieve accura- cies of 67.3 and 58.5 respectively. Finally, we re- port computational times in <ref type="table">Table 4</ref>, showing that our approach is much faster than the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Non-projective grammar induction</head><p>In this section, we investigate non-projective grammar induction. With our approach, we only have to replace the Eisner algorithm by Chu- Liu/Edmonds. We report results in <ref type="table">Table 5</ref>. First, we observe that the non-projective results are slightly worse than projective one. This is not re- ally surprising since the amount of non-projective gold dependencies is very small on the considered data. Moreover, non-projective trees are much more ambiguous than projective ones, leading to  <ref type="table">Table 5</ref>: Comparison between projective and non- projective unsupervised dependency parsing using our method.</p><p>a harder problem. We still believe those results are interesting because the difference is small (less than 1.5 points), while non-projective parsing is computationaly more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Evaluation on longer sentences</head><p>We also evaluate our method on longer sentences (while still training on sentences of length 10 or less). Directed dependency accuracies are re- ported in <ref type="figure" target="#fig_5">Figure 4</ref>. On all sentences, our method achieve an overall accuracy of 55.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Feature ablation study</head><p>In this section, we study the importance of the different features used in our parser. We report directed accuracies when different groups of fea- tures are removed, one at a time, in <ref type="table" target="#tab_3">Table 6</ref>. First, we remove the distance information from the fea- tures (line DISTANCE). We observe that the per- formance of our parser is greatly affected by this ablation, especially for long sentences. Then, we remove the context features (line CONTEXT) and the unigram features (line UNIGRAM) from our model. We observe that the performance decreases slightly due to this ablations, but the differences are small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>In this paper, we introduced a new framework for the task of unsupervised dependency parsing. Our method is a based on a feature-rich discrimina- tive model, whose parameters are learned using a convex objective function. We demonstrated on  the universal treebanks that our approach leads to competitive results, while being computationaly very efficient. We now describe some directions we would like to explore as future work.</p><p>Richer feature set. In our experiments, we fo- cused on assessing the usefulness of our con- vex, discriminative approach, and thus considered only relatively simple features based on parts-of- speech. Inspired by supervised dependency pars- ing, we would like to explore the use of other fea- tures such as Brown clusters <ref type="bibr" target="#b7">(Brown et al., 1992)</ref> or distributed word representations ( <ref type="bibr" target="#b34">Mikolov et al., 2013)</ref>, in order to lexicalize our parser.</p><p>Higher-order parsing. So far, our model is lacking the notion of valency, that has proven very useful for grammar induction. In future work, we would thus like to replace our edge-based fac- torization by a higher-order one, in order to cap- ture siblings (and grandchilds) interactions. We would then have to use a higher-order parser, such as the ones described by <ref type="bibr" target="#b30">McDonald and Pereira (2006)</ref> and <ref type="bibr" target="#b27">Koo and Collins (2010)</ref>. Another po- tential approach would be to use the linear pro- gramming relaxed inference, described by <ref type="bibr" target="#b29">Martins et al. (2009)</ref>.</p><p>Transfer learning. In this paper, we used uni- versal syntactic rules, as described by <ref type="bibr" target="#b35">Naseem et al. (2010)</ref> to guide our parser. We would like to explore the use of weak supervision, such as the one considered in transfer learning ( <ref type="bibr" target="#b21">Hwa et al., 2005</ref>). For example, projected dependencies from a resource-rich language could be used as con- straints in our framework.</p><p>Code. The code for our method is distributed on the first author webpage. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of dependency tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Hsu et al. (2012) and Parikh et al. (2014) introduced spectral methods for un- supervised dependency and constituency parsing. Finally, Spitkovsky et al. (2013) introduced dif- ferent heuristics for avoiding local minima while Gormley and Eisner (2013) proposed a method to find the global optimum of non-convex problems, based on branch-and-bound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of a Frank-Wolfe step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The original problem is equivalent to min y∈Y min w f (w, y) = min y∈Y h(y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Minimizing a linear function over Y.</head><label></label><figDesc>We fi- naly need to compute the optimal solution of the following linear problem min s∈Y h(y) s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Directed dependency accuracies on longer sentences for our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Feature ablation study. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by National Science Foun-dation award 1344668 and National Institute of General Medical Sciences award R01 GM090187.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffrac: a discriminative and flexible framework for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Za¨ıdza¨ıd</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Phylogenetic grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An hdp model for inducing combinatory categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised induction of tree substitution grammars for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Weakly-supervised alignment of video with text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lagugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1505.06027" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Sinica</title>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Programming languages and their compilers: Preliminary notes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Optimum branchings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Edmonds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
		<respStmt>
			<orgName>Journal of Research of the National Bureau of Standards</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An algorithm for quadratic programming. Naval research logistics quarterly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marguerite</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Wolfe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Posterior sparsity in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Concavity and initialization for unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonconvex global optimization for latent-variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A convex relaxation for weakly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving unsupervised dependency parsing with richer contexts and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>William P Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identifiability and unmixing of latent parse trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bootstrapping parsers via syntactic projection across parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Kolak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting frank-wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient optimization for discriminative latent class models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient image and video co-localization with frankwolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An efficient recognition and syntax analysis algorithm for context-free languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadao</forename><surname>Kasami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting reducibility in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zdeněkzdeněkˇzdeněkžabokrtsk`zdeněkžabokrtsk`y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Polyhedral outer approximations with application to natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Ryan T Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Quirmbachbrundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Täckström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Using universal linguistic knowledge to guide grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spectral unsupervised parsing with additive tree metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>LREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Linking people with &quot;their&quot; names using coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Vignesh Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Guiding unsupervised grammar induction using contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI Workshop on Grammatical Inference Applications</title>
		<meeting>of IJCAI Workshop on Grammatical Inference Applications</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">From baby steps to leapfrog: How less is more in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Breaking out of local optima with count transforms and model recombination: A study in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Finding optimum branchings. Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert Endre Tarjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Maximum margin clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Neufeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryce</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Recognition and parsing of context-free languages in time n 3. Information and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Younger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
