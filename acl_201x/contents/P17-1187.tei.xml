<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Word Representation Learning with Sememes</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Niu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">Improved Word Representation Learning with Sememes</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2049" to="2058"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1187</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes. Since sememes are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases. In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks. The key idea is to utilize word sememes to capture exact meanings of a word within specific contexts accurately. More specifically, we follow the framework of Skip-gram and present three sememe-encoded models to learn representations of sememes, senses and words , where we apply the attention scheme to detect word senses in various contexts. We conduct experiments on two tasks including word similarity and word analogy, and our models significantly outperform baselines. The results indicate that WRL can benefit from sememes via the attention scheme, and also confirm our models being capable of correctly modeling sememe information.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sememes are defined as minimum semantic u- nits of word meanings, and there exists a lim- ited close set of sememes to compose the se- mantic meanings of an open set of concepts (i.e. word sense). However, sememes are not explicit * indicates equal contribution â€  Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn) for each word. Hence, people manually annotate word sememes and build linguistic common-sense knowledge bases. HowNet ( <ref type="bibr" target="#b7">Dong and Dong, 2003</ref>) is one of such knowledge bases, which annotates each concep- t in Chinese with one or more relevant sememes. Different from WordNet <ref type="bibr" target="#b15">(Miller, 1995)</ref>, the phi- losophy of HowNet emphasizes the significance of part and attribute represented by sememes. HowNet has been widely utilized in word similar- ity computation ( <ref type="bibr" target="#b12">Liu and Li, 2002</ref>) and sentiment analysis ( <ref type="bibr" target="#b23">Xianghua et al., 2013</ref>), and in section 3.2 we will give a detailed introduction to sememes, senses and words in HowNet.</p><p>In this paper, we aim to incorporate word se- memes into word representation learning (WRL) and learn improved word embeddings in a low- dimensional semantic space. WRL is a fundamen- tal and critical step in many NLP tasks such as lan- guage modeling ( <ref type="bibr" target="#b2">Bengio et al., 2003</ref>) and neural machine translation <ref type="bibr" target="#b21">(Sutskever et al., 2014)</ref>.</p><p>There have been a lot of researches for learn- ing word representations, among which word2vec ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>) achieves a nice balance be- tween effectiveness and efficiency. In word2vec, each word corresponds to one single embedding, ignoring the polysemy of most words. To address this issue, ( <ref type="bibr" target="#b9">Huang et al., 2012</ref>) introduces a multi- prototype model for WRL, conducting unsuper- vised word sense induction and embeddings ac- cording to context clusters. ( ) fur- ther utilizes the synset information in WordNet to instruct word sense representation learning.</p><p>From these previous studies, we conclude that word sense disambiguation are critical for WR- L, and we believe that the sememe annotation of word senses in HowNet can provide neces- sary semantic regularization for the both tasks. To explore its feasibility, we propose a novel Sememe-Encoded Word Representation Learning (SE-WRL) model, which detects word senses and learns representations simultaneously. More specifically, this framework regards each word sense as a combination of its sememes, and iter- atively performs word sense disambiguation ac- cording to their contexts and learn representation- s of sememes, senses and words by extending Skip-gram in <ref type="bibr">word2vec (Mikolov et al., 2013)</ref>. In this framework, an attention-based method is pro- posed to select appropriate word senses according to contexts automatically. To take full advantages of sememes, we propose three different learning and attention strategies for SE-WRL.</p><p>In experiments, we evaluate our framework on two tasks including word similarity and word anal- ogy, and further conduct case studies on sememe, sense and word representations. The evaluation results show that our models outperform other baselines significantly, especially on word analo- gy. This indicates that our models can build bet- ter knowledge representations with the help of se- meme information, and also implies the potential of our models on word sense disambiguation.</p><p>The key contributions of this work are conclud- ed as follows: (1) To the best of our knowledge, this is the first work to utilize sememes in HowNet to improve word representation learning. (2) We successfully apply the attention scheme to detect word senses and learn representations according to contexts with the favor of the sememe annotation in HowNet. (3) We conduct extensive experiments and verify the effectiveness of incorporating word sememes for improved WRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Representation</head><p>Recent years have witnessed the great thrive in word representation learning. It is simple and s- traightforward to represent words using one-hot representations, but it usually struggles with the data sparsity issue and the neglect of semantic re- lations between words.</p><p>To address these issues, <ref type="bibr" target="#b20">(Rumelhart et al., 1988)</ref> proposes the idea of distributed represen- tation which projects all words into a continuous low-dimensional semantic space, considering each word as a vector. Distributed word representation- s are powerful and have been widely utilized in many NLP tasks, including neural language mod- els ( <ref type="bibr" target="#b2">Bengio et al., 2003;</ref><ref type="bibr" target="#b14">Mikolov et al., 2010)</ref>, ma- chine translation <ref type="bibr" target="#b21">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, parsing <ref type="bibr" target="#b4">(Chen and Manning, 2014)</ref> and text classification ( <ref type="bibr" target="#b24">Zhang et al., 2015)</ref>. Word distributed representations are capable of encod- ing semantic meanings in vector space, serving as the fundamental and essential inputs of many NLP tasks.</p><p>There are large amounts of efforts devoted to learning better word representations. As the ex- ponential growth of text corpora, model efficien- cy becomes an important issue. ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>) proposes two models, CBOW and Skip- gram, achieving a good balance between effective- ness and efficiency. These models assume that the meanings of words can be well reflected by their contexts, and learn word representations by maxi- mizing the predictive probabilities between words and their contexts. ( <ref type="bibr" target="#b17">Pennington et al., 2014</ref>) fur- ther utilizes matrix factorization on word affinity matrix to learn word representations. However, these models merely arrange only one vector for each word, regardless of the fact that many word- s have multiple senses. ( <ref type="bibr" target="#b9">Huang et al., 2012;</ref><ref type="bibr" target="#b22">Tian et al., 2014</ref>) utilize multi-prototype vector model- s to learn word representations and build distinct vectors for each word sense. ( <ref type="bibr" target="#b16">Neelakantan et al., 2015)</ref> presents an extension to Skip-gram model for learning non-parametric multiple embeddings per word. ( <ref type="bibr" target="#b19">Rothe and SchÃ¼tze, 2015)</ref> also utilizes an Autoencoder to jointly learn word, sense and synset representations in the same semantic space. This paper, for the first time, jointly learns rep- resentations of sememes, senses and words. The sememe annotation in HowNet provides useful se- mantic regularization for WRL. Moreover, the u- nified representations incorporated with sememes also provide us more explicit explanations of both word and sense embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Sense Disambiguation and Representation Learning</head><p>Word sense disambiguation (WSD) aims to iden- tify word senses or meanings in a certain context computationally. There are mainly two approach- es for WSD, namely the supervised methods and the knowledge-based methods. Supervised meth- ods usually take the surrounding words or senses as features and use classifiers like SVM for word sense disambiguation ( <ref type="bibr" target="#b11">Lee et al., 2004</ref>), which are intensively limited to the time-consuming human annotation of training data. On contrary, knowledge-based methods utilize large external knowledge resources such as knowl- edge bases or dictionaries to suggest possible sens- es for a word. ( <ref type="bibr" target="#b1">Banerjee and Pedersen, 2002</ref>) ex- ploits the rich hierarchy of semantic relations in WordNet <ref type="bibr" target="#b15">(Miller, 1995)</ref> for an adapted dictionary- based WSD algorithm. <ref type="bibr" target="#b3">(Bordes et al., 2011</ref>) intro- duces synset information in WordNet to WR- L. ( ) considers synsets in Word- Net as different word senses, and jointly conducts word sense disambiguation and word / sense rep- resentation learning. ( <ref type="bibr" target="#b8">Guo et al., 2014</ref>) considers bilingual datasets to learn sense-specific word rep- resentations. Moreover, (Jauhar et al., 2015) pro- poses two approaches to learn sense-specific word representations that are grounded to ontologies.</p><p>( <ref type="bibr" target="#b18">Pilehvar and Collier, 2016</ref>) utilizes personalized PageRank to learn de-conflated semantic represen- tations of words.</p><p>In this paper, we follow the knowledge-based approach and automatically detect word senses ac- cording to the contexts with the favor of sememe information in HowNet. To the best of our knowl- edge, this is the first attempt to apply attention- based models to encode sememe information for word representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we present our framework Sememe-Encoded WRL (SE-WRL) that considers sememe information for word sense disambigua- tion and representation learning. Specifically, we learn our models on a large-scale text corpus with the semantic regularization of the sememe anno- tation in HowNet and obtain sememe, sense and word embeddings for evaluation tasks.</p><p>In the following sections, we first introduce HowNet and the structures of sememes, senses and words. Then we discuss the conventional WRL model Skip-gram that we utilize for the sememe- encoded framework. Finally, we propose three sememe-encoded models in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sememes, Senses and Words in HowNet</head><p>In this section, we first introduce the arrange- ment of sememes, senses and words in HowNet. HowNet annotates precise senses to each word, and for each sense, HowNet annotates the signif- icance of parts and attributes represented by se- memes. <ref type="figure" target="#fig_0">Fig. 1</ref> gives an example of sememes, senses and words in HowNet. The first layer represents the word "apple". The word "apple" actually has two main senses shown on the second layer: one is a sort of juicy fruit (apple), and another is a famous computer brand (Apple brand). The third and fol- lowing layers are those sememes explaining each sense. For instance, the first sense Apple brand in- dicates a computer brand, and thus has sememes computer, bring and SpeBrand.</p><p>From <ref type="figure" target="#fig_0">Fig. 1</ref> we can find that, sememes of many senses in HowNet are annotated with vari- ous relations, such as define and modifier, and for- m complicated hierarchical structures. In this pa- per, for simplicity, we only consider all annotat- ed sememes of each sense as a sememe set with- out considering their internal structure. HowNet assumes the limited annotated sememes can well represent senses and words in the real-world sce- nario, and thus sememes are expected to be useful for both WSD and WRL. We introduce the notions utilized in the follow- ing sections as follows. We define the overall se- meme, sense and word sets used in training as X, S and W respectively. For each w âˆˆ W , there are possible multiple senses s (w) i âˆˆ S (w) where S (w) represents the sense set of w. Each sense s</p><formula xml:id="formula_0">(w) i con- sists of several sememes x (s i ) j âˆˆ X (w)</formula><p>i . For each target word w in a sequential plain text, C(w) rep- resents its context word set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conventional Skip-gram Model</head><p>We directly utilize the widely-used model Skip- gram to implement our SE-WRL model, because Skip-gram has well balanced effectiveness as well as efficiency ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>). The standard skip-gram model assumes that word embeddings should relate to their context words. It aims at maximizing the predictive probability of contex- t words conditioned on the target word w. For- mally, we utilize a sliding window to select the context word set C(w). For a word sequence H = {w 1 , Â· Â· Â· , w n }, Skip-gram model intends to maximize:</p><formula xml:id="formula_1">L(H) = nâˆ’K i=K log Pr(w iâˆ’K , Â· Â· Â· , w i+K |w i ), (1)</formula><p>where K is the size of sliding window.</p><p>Pr(w iâˆ’K , Â· Â· Â· , w i+K |w i ) represents the predic- tive probability of context words conditioned on the target word w i , formalized by the following softmax function:</p><formula xml:id="formula_2">Pr(w iâˆ’K , Â· Â· Â· , w i+K |w i ) = wcâˆˆC(w i ) Pr(w c |w i ) = wcâˆˆC(w i ) exp(w c Â· w i ) w i âˆˆW exp(w c Â· w i ) ,<label>(2)</label></formula><p>in which w c and w i stand for embeddings of con- text word w c âˆˆ C(w i ) and target word w i respec- tively. We can also follow the strategies of hierar- chical softmax and negative sampling proposed in ( <ref type="bibr" target="#b13">Mikolov et al., 2013</ref>) to accelerate the calculation of softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SE-WRL Model</head><p>In this section, we introduce the SE-WRL model- s with three different strategies to utilize sememe information, including Simple Sememe Aggrega- tion Model (SSA), Sememe Attention over Con- text Model (SAC) and Sememe Attention over Target Model (SAT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Simple Sememe Aggregation Model</head><p>The Simple Sememe Aggregation Model (SSA) is a straightforward idea based on Skip-gram mod- el. For each word, SSA considers all sememes in all senses of the word together, and represents the target word using the average of all its sememe embeddings. Formally, we have:</p><formula xml:id="formula_3">w = 1 m s (w) i âˆˆS (w) x (s i ) j âˆˆX (w) i x (s i ) j ,<label>(3)</label></formula><p>which means the word embedding of w is com- posed by the average of all its sememe embed- dings. Here, m stands for the overall number of sememes belonging to w.</p><p>This model simply follows the assumption that, the semantic meaning of a word is composed of the semantic units, i.e., sememes. As compared to the conventional Skip-gram model, since sememes are shared by multiple words, this model can uti- lize sememe information to encode latent semantic correlations between words. In this case, similar words that share the same sememes may finally obtain similar representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Sememe Attention over Context Model</head><p>The SSA Model replaces the target word embed- ding with the aggregated sememe embeddings to encode sememe information into word representa- tion learning. However, each word in SSA model still has only one single representation in differ- ent contexts, which cannot deal with polysemy of most words. It is intuitive that we should construc- t distinct embeddings for a target word according to specific contexts, with the favor of word sense annotation in HowNet.</p><p>To address this issue, we come up with the Se- meme Attention over Context Model (SAC). SAC utilizes the attention scheme to automatically se- lect appropriate senses for context words accord- ing to the target word. That is, SAC conducts word sense disambiguation for context words to learn better representations of target words. The struc- ture of the SAC model is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. More specifically, we utilize the original word embedding for target word w, but use sememe embeddings to represent context word w c instead of original context word embeddings. Suppose a word typically demonstrates some specific senses in one sentence. Here we employ the target word embedding as an attention to select the most ap- propriate senses to make up context word embed- dings. We formalize the context word embedding w c as follows:</p><formula xml:id="formula_4">w c = |S (wc) | j=1 att(s (wc) j ) Â· s (wc) j ,<label>(4)</label></formula><p>where s ) represents the attention score of the j-th sense with respect to the target word w, defined as follows:</p><formula xml:id="formula_5">att(s (wc) j ) = exp(w Â· Ë† s (wc) j ) |S (wc) | k=1 exp(w Â· Ë† s (wc) k ) .<label>(5)</label></formula><p>Note that, when calculating attention, we use the average of sememe embeddings to represent each sense s</p><formula xml:id="formula_6">(wc) j : Ë† s (wc) j = 1 |X (wc) j | |X (wc) j | k=1 x (s j ) k .<label>(6)</label></formula><p>The attention strategy assumes that the more relevant a context word sense embedding is to the target word w, the more this sense should be con- sidered when building context word embeddings. With the favor of attention scheme, we can repre- sent each context word as a particular distribution over its sense. This can be regarded as soft WSD. As shown in experiments, it will help learn better word representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Sememe Attention over Target Model</head><p>The Sememe Attention over Context Model can flexibly select appropriate senses and sememes for context words according to the target word. The process can also be applied to select appropriate senses for the target word, by taking context words as attention. Hence, we propose the Sememe At- tention over Target Model (SAT) as shown in <ref type="figure" target="#fig_3">Fig.  3</ref>. Different from SAC model, SAT learns the o- riginal word embeddings for context words, but sememe embeddings for target words. We apply context words as attention over multiple senses of the target word w to build the embedding of w, formalized as follows:</p><formula xml:id="formula_7">w = |S (w) | j=1 att(s (w) j ) Â· s (w) j ,<label>(7)</label></formula><p>where s (w) j stands for the j-th sense embedding of w, and the context-based attention is defined as follows:</p><formula xml:id="formula_8">att(s (w) j ) = exp(w c Â· Ë† s (w) j ) |S (w) | k=1 exp(w c Â· Ë† s (w) k ) ,<label>(8)</label></formula><p>where, similar to Eq. <ref type="formula" target="#formula_6">(6)</ref>, we also use the average of sememe embeddings to represent each sense</p><formula xml:id="formula_9">s (w)</formula><p>j . Here, w c is the context embedding, consist- ing of a constrained window of word embeddings in C(w i ). We have:</p><formula xml:id="formula_10">w c = 1 2K k=i+K k=iâˆ’K w k , k = i.<label>(9)</label></formula><p>Note that, since in experiment we find the sense s- election of the target word only relies on more lim- ited context words for calculating attention, hence we select a smaller K as compared to K. Recall that, SAC only uses one target word as attention to select senses of context words, but SAT uses several context words together as atten- tion to select appropriate senses of target words. Hence SAT is expected to conduct more reliable WSD and result in more accurate word represen- tations, which will be explored in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the effectiveness of our SE-WRL 1 models on two tasks including word similarity and word analogy, which are two classi- cal evaluation tasks mainly focusing on evaluating the quality of learned word representations. We also explore the potential of our models in word sense disambiguation with case study, showing the power of our attention-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use the web pages in Sogou-T 2 as the text cor- pus to learn WRL models. Sogou-T is provided by a Chinese commercial search engine, which con- tains 2.7 billion words in total.</p><p>We also utilize the sememe annotation in HowNet. The number of distinct sememes used in this paper is 1, 889. The average senses for each word are about 2.4, while the average sememes for each sense are about 1.6. Throughout the Sogou-T corpus, we find that 42.2% of words have multiple senses. This indicates the significance of WSD.</p><p>For evaluation, we choose wordsim-240 and wordsim-297 3 to evaluate the performance of word similarity computation. The two datasets both contain frequently-used Chinese word pairs with similarity scores annotated manually. We choose the Chinese Word Analogy dataset pro- posed by <ref type="bibr" target="#b6">(Chen et al., 2015)</ref> to evaluate the performance of word analogy inference, that is, w("king") âˆ’ w("man") w("queen") âˆ’ w("woman").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>We evaluate three SE-WRL models including S- SA, SAC and SAT on all tasks. As for baselines, we consider three conventional WRL models in- cluding Skip-gram, CBOW and GloVe. For Skip- gram and CBOW, we directly use the code re- leased by <ref type="bibr">Google (Mikolov et al., 2013</ref>). GloVe is proposed by <ref type="bibr" target="#b17">(Pennington et al., 2014</ref>), which seeks the advantages of the WRL models based on statistics and those based on prediction. More- over, we propose another model, Maximum Selec- tion over Target Model (MST), for further compar- ison inspired by . It represents the current word embeddings with only the most probable sense according to the contexts, instead of viewing a word as a particular distribution over all its senses similar to that of SAT.</p><p>For a fair comparison, we train these model- s with the same experimental settings and with their best parameters. As for the parameter set- tings, we set the context window size K = 8 as the upper bound, and during training, the window size is dynamically selected ranging from 1 to 8 randomly. We set the dimensions of word, sense and sememe embeddings to be the same 200. For learning rate Î±, its initial value is 0.025 and will descend through iterations. We set the number of negative samples to be 25. We also set a lower bound of word frequency as 50, and in the training set, those words less frequent than this bound will be filtered out. For SAT, we set K = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word Similarity</head><p>The task of word similarity aims to evaluate the quality of word representations by comparing the similarity ranks of word pairs computed by WR- L models with the ranks given by dataset. WR- L models typically compute word similarities ac- cording to their distances in the semantic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Evaluation Protocol</head><p>In experiments, we choose the cosine similarity between two word embeddings to rank word pairs. For evaluation, we compute the Spearman correla- tion between the ranks of models and the ranks of human judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Wordsim  <ref type="table">Table 1</ref>: Evaluation results of word similarity computation. <ref type="table">Table 1</ref> shows the results of these models for word similarity computation. From the results we can observe that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Experiment Results</head><p>(1) Our SAT model outperforms other model- s, including all baselines, on both two test sets. This indicates that, by utilizing sememe annota- tion properly, our model can better capture the se- mantic relations of words, and learn more accurate word embeddings.</p><p>(2) The SSA model represents a word with the average of its sememe embeddings. In general, S- SA model performs slightly better than baselines, which tentatively proves that sememe information is helpful. The reason is that words which share common sememe embeddings will benefit from each other. Especially, those words with lower fre- quency, which cannot be learned sufficiently us- ing conventional WRL models, in contrast, can   <ref type="table">Table 2</ref>: Evaluation results of word analogy inference.</p><p>obtain better word embeddings from SSA simply because their sememe embeddings can be trained sufficiently through other words.</p><p>3) The SAT model performs much better than SSA and SAC. This indicates that SAT can obtain more precise sense distribution of a word. The rea- son has been mentioned above that, different from SAC using only one target word as attention for WSD, SAT adopts richer contextual information as attention for WSD.</p><p>(4) SAT works better than MST, and we can conclude that a soft disambiguation over senses prevents inevitable errors when selecting only one most-probable sense. The result makes sense be- cause, for many words, their various senses are not always entirely different from each other, but share some common elements. In some contexts, a sin- gle sense may not convey the exact meaning of this word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Word Analogy</head><p>Word analogy inference is another widely-used task to evaluate the quality of WRL models <ref type="bibr" target="#b13">(Mikolov et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Evaluation Protocol</head><p>The dataset proposed by <ref type="bibr" target="#b6">(Chen et al., 2015</ref>) con- sists of 1, 124 analogies, which contains three analogy types: (1) capitals of countries (Cap- ital), 677 groups; (2) states/provinces of cities (City), 175 groups; (3) family words (Relation- ship), 272 groups. Given an analogy group of words (w 1 , w 2 , w 3 , w 4 ), WRL models usually get w 2 âˆ’w 1 +w 3 equal to w 4 . Hence for word analo- gy inference, we suppose w 4 is missing, and WRL models will rank all candidate words according to their scores as follows:</p><formula xml:id="formula_12">R(w) = cos(w 2 âˆ’ w 1 + w 3 , w),<label>(10)</label></formula><p>and select the top-ranked word as the answer.</p><p>For word analogy inference, we consider two evaluation metrics: (1) Accuracy. For each anal- ogy group, a WRL model selects the top-ranked word w = arg max w R(w), which is judged as positive if w = w 4 . The percentage of positive samples is regarded as the accuracy score for this WRL model. <ref type="formula" target="#formula_2">(2)</ref> Mean Rank. For each anal- ogy group, a WRL model will assign a rank for the gold standard word w 4 according to the scores computed by Eq. (10). We use the mean rank of all gold standard words as the evaluation metric. <ref type="table">Table 2</ref> shows the evaluation results of these mod- els for word analogy inference. From the table, we can observe that:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Experiment Results</head><p>(1) The SAT model performs best among al- l models, and the superiority is more significant than that on word similarity computation. This in- dicates that SAT will enhance the modeling of im- plicit relations between word embeddings in the semantic space. The reason is that sememes an- notated to word senses have encoded these word relations. For example, capital and Cuba are two sememes of the word "Havana", which pro- vide explicit semantic relations between the words "Cuba" and "Havana".</p><p>(2) The SAT model does well on both classes of Capital and City, because some words in these classes have low frequencies, while their sememes occur so many times that sememe embeddings can be learned sufficiently. With these sememe em- beddings, these low-frequent words can be learned more efficiently by SAT.</p><p>(3) It seems that CBOW works better than SAT on Relationship class. Whereas for the mean rank, CBOW gets the worst results, which indi- cates the performance of CBOW is unstable. On the contrary, although the accuracy of SAT is a bit lower than that of CBOW, SAT seldom gives an outrageous prediction. In most wrong cas-Word: Â°J("Apple brand/apple") sense1: Apple brand (computer, PatternValue, able, bring, SpeBrand) sense2: duct (fruit)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Â° Â° Â°J J J Æ’kJÂ¥{Â¡Â£Apple is always famous as the king of fruitsÂ¤</head><p>Apple brand: 0.28 apple: 0.72 Â° Â° Â°J J J &gt;MÃƒ{~Ã©Ã„Â£The Apple brand computer can not startup normallyÂ¤ Apple brand: 0.87 apple: 0.13</p><p>Word: *Ã‘("proliferate/metastasize") sense1: proliferate (disperse) sense2: metastasize (disperse, disease) "Å½Â¼oe* * *Ã‘ Ã‘ Ã‘ Â£Prevent epidemic from metastasizingÂ¤ proliferate: 0.  es, SAT predicts the word "grandfather" instead of "grandmother", which is not completely non- sense, because in HowNet the words "grandmoth- er", "grandfather", "grandma" and some other similar words share four common sememes while only one sememe of them are different. These sim- ilar sememes make the attention process less dis- criminative with each other. But for the wrong cas- es of CBOW, we find that many mistakes are about words with low frequencies, such as "stepdaugh- ter" which occurs merely for 358 times. Consider- ing sememes may relieve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case study</head><p>The above experiments verify the effectiveness of our models for WRL. Here we show some exam- ples of sememes, senses and words for case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Word Sense Disambiguation</head><p>To demonstrate the validity of Sememe Attention, we select three attention results in training set, as shown in <ref type="table" target="#tab_2">Table 3</ref>. In this table, the first rows of three examples are word-sense-sememe structures of each word. For instance, in the third example, the word has two senses, contingent and troops; contingent has one sememe community, while troops has one sememe army. The three exam- ples all indicate that our models can estimate ap- propriate distributions of senses for a word given a context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Effect of Context Words for Attention</head><p>We demonstrate the effect of context words for at- tention in <ref type="table">Table.</ref> 4. The word "Havana" consist- s of four sememes, among which two sememes capital and Cuba describe distinct attributes of the word from different aspects.  Here, we list three different context words "Cu- ba", "Russia" and "cigar". Given the context word "Cuba", both sememes get high weights, indicat- ing their contributions to the meaning of "Havana" in this context. The context word "Russia" is more relevant to the sememe capital. When the con- text word is "cigar", the sememe Cuba has more influence, because cigar is a famous specialty of Cuba. From these examples, we can conclude that our Sememe Attention can accurately capture the word meanings in complicated contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose a novel method to model sememe information for learning better word rep- resentations. Specifically, we utilize sememe in- formation to represent various senses of each word and propose Sememe Attention to select appropri- ate senses in contexts automatically. We evaluate our models on word similarity and word analogy, and results show the advantages of our Sememe- Encoded WRL models. We also analyze sever- al cases in WSD and WRL, which confirms our models are capable of selecting appropriate word senses with the favor of sememe attention.</p><p>We will explore the following research direc- tions in future: (1) The sememe information in HowNet is annotated with hierarchical structure and relations, which have not been considered in our framework. We will explore to utilize these annotations for better WRL. (2) We believe the idea of sememes is universal and could be well- functioned beyond languages. We will explore the effectiveness of sememe information for WRL in other languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of sememes, senses and words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sememe Attention over Context Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>wc) j stands for the j-th sense embedding of w c , and att(s (wc) j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sememe Attention over Target Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>06 metastasize: 0.94 Ã˜* * *Ã‘ Ã‘ Ã‘ Ã˜Ã‰Ã¬^Â£Treaty on the Non-Proliferation of Nuclear WeaponsÂ¤ proliferate: 0.68 metastasize: 0.32 Word: Ã¨ÃŽ("contingent/troops") sense1: contingent (community) sense2: troops (army) l|Ã¨ Ã¨ Ã¨ÃŽ ÃŽ ÃŽ ?\1Ã£Ã¬NmÂ£Eight contingents enter the second stage of team competitionÂ¤ contingent: 0.90 troops: 0.10 ÃºSÃ„Ã¨ Ã¨ Ã¨ÃŽ ÃŽ ÃŽ |"Ã¯Â£Construct the organization of public security's troops in grass-roots unitÂ¤ contingent: 0.15 troops: 0.85</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Examples of sememes, senses and words in context with attention.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Sememe weight for computing attention.</figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/thunlp/SE-WRL</note>

			<note place="foot" n="2"> https://www.sogou.com/labs/resource/ t.php 3 https://github.com/Leonard-Xu/CWE/ tree/master/data</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An adapted lesk algorithm for word sense disambiguation using wordnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CICLing</title>
		<meeting>CICLing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RÃ©jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint learning of character and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan-Bo</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hownet-a hybrid language and knowledge resource</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NLP-KE</title>
		<meeting>NLP-KE</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="820" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning sense-specific word embeddings by exploiting bilingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ontologically grounded multi-sense representation learning for semantic vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Sujay Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised word sense disambiguation with support vector machines and multiple knowledge sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Yoong Keok Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tee Kiah</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SENSEVAL-3</title>
		<meeting>SENSEVAL-3</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="137" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Word similarity computing based on how-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CLCLP</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="76" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>KarafiÃ¡t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">De-conflated semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoextend: Extending word embeddings to embeddings for synsets and lexemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>SchÃ¼tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-aspect sentiment analysis for chinese online social reviews based on topic modeling and hownet lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Xianghua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Yanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Zhiqiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="186" to="195" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
