<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speech Act Modeling of Written Asynchronous Conversations with Task-Specific Embeddings and Conditional Structured Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ALT Research Group</orgName>
								<orgName type="institution">Qatar Computing Research Institute -HBKU</orgName>
								<address>
									<country>Qatar Foundation</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enamul</forename><surname>Hoque</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ALT Research Group</orgName>
								<orgName type="institution">Qatar Computing Research Institute -HBKU</orgName>
								<address>
									<country>Qatar Foundation</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speech Act Modeling of Written Asynchronous Conversations with Task-Specific Embeddings and Conditional Structured Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1746" to="1756"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper addresses the problem of speech act recognition in written asyn-chronous conversations (e.g., fora, emails). We propose a class of conditional structured models defined over arbitrary graph structures to capture the conversational dependencies between sentences. Our models use sentence representations encoded by a long short term memory (LSTM) recurrent neural model. Empirical evaluation shows the effectiveness of our approach over existing ones: (i) LSTMs provide better task-specific representations, and (ii) the global joint model improves over local models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Asynchronous conversations, where participants communicate with each other at different times (e.g., fora, emails), have become very common for discussing events, issues, queries and life experi- ences. In doing so, participants interact with each other in complex ways, performing certain com- municative acts like asking questions, requesting information or suggesting something. These are called speech acts <ref type="bibr" target="#b0">(Austin, 1962)</ref>.</p><p>For example, consider the excerpt of a forum conversation from our corpus in <ref type="figure" target="#fig_0">Figure 1</ref>. The participant who posted the first comment C 1 , de- scribes his situation by the first two sentences and then asks a question in the third sentence. Other participants respond to the query by suggesting something or asking for clarification. In this pro- cess, the participants get into a conversation by taking turns, each of which consists of one or more speech acts. The two-part structures across posts like 'question-answer' and 'request-grant' are called adjacency pairs <ref type="bibr" target="#b29">(Schegloff, 1968)</ref>. Identification of speech acts is an important step towards deep conversation analysis in these media ( <ref type="bibr" target="#b1">Bangalore et al., 2006</ref>), and has been shown to be useful in many downstream applications including summarization ( <ref type="bibr" target="#b19">McKeown et al., 2007)</ref> and ques- tion answering <ref type="bibr" target="#b10">(Hong and Davison, 2009)</ref>.</p><p>Previous attempts to automatic (sentence-level) speech act recognition in asynchronous conversa- tion ( <ref type="bibr" target="#b27">Qadir and Riloff, 2011;</ref><ref type="bibr" target="#b12">Jeong et al., 2009;</ref><ref type="bibr" target="#b33">Tavafi et al., 2013;</ref><ref type="bibr" target="#b24">Oya and Carenini, 2014</ref>) suffer from at least one of the two major flaws. Firstly, they use bag-of-word (BOW) represen- tation (e.g., unigram, bigram) to encode lexical in- formation in a sentence. However, consider the suggestion sentences in the example. Arguably, a model needs to consider the structure (e.g., word order) and the compositionality of phrases to iden- tify the right speech act. Furthermore, BOW rep- resentation could be quite sparse and may not gen- eralize well when used in classification models.</p><p>Secondly, existing approaches mostly disregard conversational dependencies between sentences. For instance, consider the example again, where we tag the sentences with the human annotations ('Human') and with the predictions of a local ('Local') classifier that considers word order for sentence representation but classifies each sen- tence separately. Prediction errors are underlined and highlighted in red. Notice the first and second sentences of comment 4, which are tagged mistak- enly as statement and response, respectively, by our best local classifier. We hypothesize that some of the errors made by the local classifier could be corrected by employing a global joint model that performs a collective classification taking into account the conversational dependencies between sentences (e.g., adjacency relations).</p><p>However, unlike synchronous conversations (e.g., phone, meeting), modeling conversational dependencies between sentences in asynchronous conversation is challenging, especially in those where explicit thread structure (reply-to relations) is missing, which is also our case. The conver- sational flow often lacks sequential dependencies in its temporal order. For example, if we arrange the sentences as they arrive in the conversation, it becomes hard to capture any dependency between the act types because the two components of the adjacency pairs can be far apart in the sequence. This leaves us with one open research question: how to model the dependencies between sentences in a single comment and between sentences across different comments? In this paper, we attempt to address this question by designing and exper- imenting with conditional structured models over arbitrary graph structure of the conversation.</p><p>More concretely, we make the following contri- butions. Firstly, we propose to use Recurrent Neu- ral Network (RNN) with Long Short Term Mem- ory (LSTM) hidden layer to perform composition of phrases and to represent sentences using dis- tributed condensed vectors (i.e., embeddings). We experiment with both unidirectional and bidirec- tional RNNs. Secondly, we propose conditional structured models in the form of pairwise Con- ditional Random Field (Murphy, 2012) over ar- bitrary conversational structures. We experiment with different variations of this model to capture different types of interactions between sentences inside the comments and across the comments. These models use the LSTM encoded vectors as feature vectors for performing the classification task jointly. As a secondary contribution, we also present and release a forum dataset annotated with a standard speech act tagset.</p><p>We train our models on different settings us- ing synchronous and asynchronous corpora, and evaluate on two forum datasets. Our main find- ings are: (i) LSTM RNNs provide better repre- sentation than BOW; (ii) Bidirectional LSTMs, which encode a sentence using two vectors pro- vide better representation than the unidirectional ones; and (iii) Global joint models improve over local models given that it considers the right graph structure. The source code and the new dataset are available at http://alt.qcri. org/tools/speech-act/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>Let s n m denote the m-th sentence of comment n in a conversation. Our framework works in two steps as demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref>. First, we use a recur- rent neural network (RNN) to compose sentence representations semantically from their words and to represent them with distributed condensed vec- tors z n m , i.e., sentence embeddings ( <ref type="figure" target="#fig_1">Figure 2a</ref>). In the second step, a multivariate (graphical) model, which operates on the sentence embeddings, cap- tures conversational dependencies between sen- tences in the conversation <ref type="figure" target="#fig_1">(Figure 2b</ref>). In the fol- lowing, we describe the two steps in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence Representation</head><p>One of our main hypotheses is that a sentence rep- resentation method should consider the word or- der of the sentence. To this end, we use an LSTM RNN <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) to en- code a sentence into a vector by processing its words sequentially, at each time step combining the current input with the previous hidden state. <ref type="figure" target="#fig_5">Figure 4b</ref> demonstrates the process for three sen- tences. Each word in the vocabulary V is repre- sented by a D dimensional vector in a shared look- up table L ∈ R |V |×D . L is considered a model parameter to be learned. We can initialize L ran- domly or by pretrained word embedding vectors like word2vec ( <ref type="bibr" target="#b20">Mikolov et al., 2013a)</ref>.</p><p>Given an input sentence s = (w 1 , · · · , w T ), we first transform it into a feature sequence by map- ping each token w t ∈ s to an index in L. The look- up layer then creates an input vector x t ∈ R D for each token w t . The input vectors are then passed to the LSTM recurrent layer, which com- putes a compositional representation − → h t at every time step t by performing nonlinear transforma- tions of the current input x t and the output of the previous time step − → h t−1 . Specifically, the recur- rent layer in a LSTM RNN is constituted with hid- den units called memory blocks. A memory block is composed of four elements: (i) a memory cell c (a neuron) with a self-connection, (ii) an input gate i to control the flow of input signal into the neu- ron, (iii) an output gate o to control the effect of the neuron activation on other neurons, and (iv) a for- get gate f to allow the neuron to adaptively reset its current state through the self-connection. The following sequence of equations describe how the memory blocks are updated at every time step t:</p><formula xml:id="formula_0">it = sigh(Uiht−1 + Vixt + bi) (1) ft = sigh(U f ht−1 + V f xt + b f ) (2) ct = it tanh(Ucht−1 + Vcxt) + ft ct−1 (3) ot = sigh(Uoht−1 + Voxt + bo) (4) ht = ot tanh(ct)<label>(5)</label></formula><p>where U k and V k are the weight matrices between two consecutive hidden layers, and between the in- put and the hidden layers, respectively, which are associated with gate k (input, output, forget and cell); and and b k is the corresponding bias vector.</p><p>The symbols sigh and tanh denote hard sigmoid and hard tan, respectively, and the symbol de- notes a element-wise product of two vectors. LSTM by means of its specifically designed gates (as opposed to simple RNNs) is capable of capturing long range dependencies. We can in- terpret h t as an intermediate representation sum- marizing the past. The output of the last time step − → h T = z thus represents the sentence, which can be fed to the output layer of the neural network ( <ref type="figure" target="#fig_5">Fig.  4b</ref>) or to other models (e.g, a fully-connected CRF in <ref type="figure" target="#fig_1">Fig. 2b</ref>) for classification. The output layer of our LSTM-RNN uses a softmax for multi- class classification. Formally, the probability of k-th class for classification into K classes is</p><formula xml:id="formula_1">p(y = k|s, θ) = exp (w T k z) K k=1 exp (w T k z)<label>(6)</label></formula><p>where w are the output layer weights.</p><p>Bidirectionality The RNN described above en- codes information that it gets only from the past. However, information from the future could also be crucial for recognizing speech acts. This is specially true for longer sentences, where a uni- directional LSTM can be limited in encoding the necessary information into a single vector. Bidi- rectional RNNs <ref type="bibr" target="#b30">(Schuster and Paliwal, 1997</ref>) cap- ture dependencies from both directions, thus pro- vide two different views of the same sentence. This amounts to having a backward counterpart for each of the equations from 1 to 5. For classi- fication, we use the concatenated vector</p><formula xml:id="formula_2">[ − → h T , ← − h T ],</formula><p>where − → h T and ← − h T are the encoded vectors summa- rizing the past and the future, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional Structured Model</head><p>Given the vector representation of the sentences in an asynchronous conversation, we explore two dif- ferent approaches to learn classification functions. The first and the traditional approach is to learn a local classifier ignoring the structure in the out- put and to use it for predicting the label of each sentence separately. This is the approach we took above when we fed the output layer of the LSTM RNN with the sentence-level embeddings. How- ever, this approach does not model the conversa- tional dependency (e.g., adjacency relations be- tween question-answer and request-accept pairs).</p><p>The second approach, which we adopt in this paper, is to model the dependencies between the output variables (labels) while learning the clas- sification functions jointly by optimizing a global performance criterion. We represent each conver- sation by a graph G=(V, E). Each node i∈V is associated with an input vector z i = z n m , repre- senting the features of the sentence s n m , and an out- put variable y i ∈{1, 2, · · · , K}, representing the class label. Similarly, each edge (i, j)∈E is as- sociated with an input feature vector φ(z i , z j ), de- rived from the node-level features, and an output variable y i,j ∈{1, 2, · · · , L}, representing the state transitions for the pair of nodes. We define the fol- lowing conditional joint distribution:</p><formula xml:id="formula_3">p(y|v, w, z) = 1 Z(v, w, z) i∈V ψn(yi|z, v) (i,j)∈E ψe(yi,j|z, w)<label>(7)</label></formula><p>where ψ n and ψ e are node and the edge factors, and Z(.) is the global normalization constant that ensures a valid probability distribution. We use a log-linear representation for the factors:</p><formula xml:id="formula_4">ψn(yi|z, v) = exp(v T φ(yi, z)) (8) ψe(yi,j|z, w) = exp(w T φ(yi,j, z))<label>(9)</label></formula><p>where φ(.) is a feature vector derived from the in- puts and the labels. This model is essentially a pairwise conditional random field or PCRF (Mur- phy, 2012). The global normalization allows CRFs to surmount the so-called label bias problem <ref type="bibr" target="#b16">(Lafferty et al., 2001</ref>), allowing them to take long- range interactions into account. The log likelihood for one data point (z, y) (i.e., a conversation) is:</p><formula xml:id="formula_5">f (θ) = i∈V v T φ(yi, z) + (i,j)∈E w T φ(yi,j, z) − log Z(v, w, z)<label>(10)</label></formula><p>This objective is convex, so we can use gradient- based methods to find the global optimum. The gradients have the following form:</p><formula xml:id="formula_6">f (v) = i∈V φ(yi, z) − E[φ(yi, z)]<label>(11)</label></formula><formula xml:id="formula_7">f (w) = (i,j)∈E φ(yi,j, z) − E[φ(yi,j, z)] (12)</formula><p>where E[φ(.)] denote the expected feature vector.</p><p>Training and Inference Traditionally, CRFs have been trained using offline methods like limited-memory BFGS <ref type="bibr" target="#b23">(Murphy, 2012)</ref>. Online training of CRFs using stochastic gradient de- scent (SGD) was proposed by <ref type="bibr" target="#b35">Vishwanathan et al. (2006)</ref>. Since RNNs are trained with online meth- ods, to compare our two methods, we use SGD to train our CRFs. Algorithm 1 in the Appendix gives a pseudocode of the training procedure. We use Belief Propagation or BP <ref type="bibr" target="#b26">(Pearl, 1988</ref>) for inference in our graphical models. BP is guar- anteed to converge to an exact solution if the graph is a tree. However, exact inference is intractable for graphs with loops. Despite this, it has been ad- vocated by <ref type="bibr" target="#b26">Pearl (1988)</ref> to use BP in loopy graphs as an approximation; see also <ref type="bibr" target="#b23">(Murphy, 2012)</ref>, page 768. The algorithm is then called "loopy" BP, or LBP. Although LBP gives approximate so- lutions for general graphs, it often works well in practice ( <ref type="bibr" target="#b22">Murphy et al., 1999</ref>), outperforming other methods such as mean field <ref type="bibr" target="#b36">(Weiss, 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variations of Graph Structures</head><p>One of the main advantages of our pairwise CRF is that we can define this model over arbitrary graph structures, which allows us to capture conver- sational dependencies at various levels. We distinguish between two types of dependencies: (i) intra-comment, which defines how the labels of the sentences in a comment are connected; and (ii) across-comment, which defines how the labels of the sentences across comments are connected. <ref type="table">Table 1</ref> summarizes the connection types that we have explored in our models. Each configu- ration of intra-and across-connections yields a different pairwise CRF model. <ref type="figure" target="#fig_2">Figure 3</ref> shows four such CRFs with three comments -C 1 be- ing the first comment, and C i and C j being two other comments in the conversation.   shows the structure for NO-NO con- figuration, where there is no link between nodes of both intra-and across-comments. In this setting, the CRF model is equivalent to MaxEnt. <ref type="figure" target="#fig_2">Figure  3b</ref> shows the structure for LC-LC, where there are linear chain relations between nodes of both intra-and across-comments. The linear chain across comments refers to the structure, where the last sentence of each comment is connected to the first sentence of the comment that comes next in the temporal order (i.e., posting time). <ref type="figure" target="#fig_2">Fig- ures 3c</ref> shows the CRF for LC-LC 1 , where sen- tences inside a comment have linear chain connec- tions, and the last sentence of the first comment is connected to the first sentence of the other com- ments. Similarly, <ref type="figure" target="#fig_2">Figure 3d</ref> shows the graph struc- ture for LC-FC 1 configuration, where sentences inside comments have linear chain connections, and sentences of the first comment are fully con- nected with the sentences of the other comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpora</head><p>There exist large corpora of utterances annotated with speech acts in synchronous spoken domains, e.g., Switchboard-DAMSL or SWBD ( <ref type="bibr" target="#b14">Jurafsky et al., 1997</ref>) and Meeting Recorder Dialog Act or MRDA ( <ref type="bibr" target="#b6">Dhillon et al., 2004</ref>). However, such large corpus does not exist in asynchronous domains. Some prior work <ref type="bibr" target="#b5">(Cohen et al., 2004;</ref><ref type="bibr" target="#b28">Ravi and Kim, 2007;</ref><ref type="bibr" target="#b7">Feng et al., 2006;</ref><ref type="bibr" target="#b2">Bhatia et al., 2014)</ref> tackles the task at the comment level, and uses   <ref type="table">Table 3</ref>: Distribution of speech acts in our corpora.</p><p>task-specific tagsets. In contrast, in this work we are interested in identifying speech acts at the sen- tence level, and also using a standard tagset like the ones defined in SWBD and MRDA. More recent studies attempt to solve the task at the sentence level. <ref type="bibr" target="#b12">Jeong et al. (2009)</ref> first created a dataset of TripAdvisor (TA) forum conversations annotated with the standard 12 act types defined in MRDA. They also remapped the BC3 email cor- pus ( <ref type="bibr" target="#b34">Ulrich et al., 2008)</ref> according to this tagset. <ref type="table" target="#tab_14">Table 10</ref> in the Appendix presents the tags and their relative frequency in the two datasets. Subse- quent studies <ref type="bibr" target="#b13">(Joty et al., 2011;</ref><ref type="bibr" target="#b33">Tavafi et al., 2013;</ref><ref type="bibr" target="#b24">Oya and Carenini, 2014</ref>) use these datasets. We also use these datasets in our work. <ref type="table" target="#tab_2">Table 2</ref> shows some basic statistics about these datasets. On aver- age, BC3 conversations are longer than TA in both number of comments and number of sentences.</p><p>Since these datasets are relatively small in size, we group the 12 acts into 5 coarser classes to learn a reasonable classifier. 1 More specifically, all the question types are grouped into one gen- eral class Question, all response types into Re- sponse, and appreciation and polite mechanisms into Polite class. Also since deep neural models like LSTM RNNs require a lot of training data, we also utilize the MRDA meeting corpus. Ta- ble 3 shows the label distribution of the resultant datasets. Statement is the most dominant class, followed by Question, Polite and Suggestion.  Two native speakers of English annotated each conversation using a web-based annotation frame- work. They were asked to annotate each sentence with the most appropriate speech act tag from the list of 5 speech act types. Since this task is not always obvious, we gave them detailed annota- tion guidelines with real examples. We use Co- hens Kappa κ to measure the agreement between the annotators. <ref type="table" target="#tab_4">Table 4</ref> presents the distribution of the speech acts and their respective κ values.After Statement, Suggestion is the most frequent class, followed by Question and Polite. The κ varies from 0.43 (for Response) to 0.87 (for Question).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QC3 Conversational Corpus</head><p>Finally, in order to create a consolidated dataset, we collected the disagreements and employed a third annotator to resolve those cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head><p>In this section we present our experimental set- tings, results and analysis. We evaluate our mod- els on the two forum corpora QC3 and TA. For performance comparison, we use both accuracy and macro-averaged F 1 score. Accuracy gives the overall performance of a classifier but could be bi- ased to most populated ones. Macro-averaged F 1 weights equally every class and is not influenced by class imbalance. Statistical significance tests are done using an approximate randomization test based on the accuracy. <ref type="bibr">3</ref> We used SIGF V.2 <ref type="bibr" target="#b25">(Padó, 2006</ref>) with 10,000 iterations.   Because of the noise and informal nature of conversational texts, we performed a series of pre- processing steps. We normalize all characters to their lower-cased forms, truncate elongations to two characters, spell out every digit and URL. We further tokenized the texts using the CMU TweetNLP tool ( <ref type="bibr" target="#b8">Gimpel et al., 2011</ref>).</p><p>In the following, we first demonstrate the effec- tiveness of LSTM RNNs for learning representa- tions of sentences automatically to identify their speech acts. Then in subsection 4.2, we show the usefulness of pairwise CRFs for capturing conver- sational dependencies in speech act recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Effectiveness of LSTM RNNs</head><p>To show the effectiveness of LSTMs for learn- ing sentence representations, we split each of our asynchronous corpora randomly into 70% sen- tences for training, 10% for development, and 20% for testing. For MRDA, we use the same train-test-dev split as <ref type="bibr" target="#b12">Jeong et al. (2009)</ref>. <ref type="table" target="#tab_6">Table  5</ref> summarizes the resultant datasets.</p><p>We compare the performance of LSTMs with that of MaxEnt (ME) and Multi-layer Perceptron (MLP) with one hidden layer. <ref type="bibr">4</ref> Both ME and MLP were fed with the bag-of-word (BOW) represen- tations of the sentence, i.e., vectors containing bi- nary values indicating the presence or absence of a word in the training set vocabulary.</p><p>We train the models by optimizing the cross en- tropy using the gradient-based online learning al- gorithm ADAM ( <ref type="bibr" target="#b15">Kingma and Ba, 2014)</ref>. <ref type="bibr">5</ref> The learning rate and other parameters were set to the values as suggested by the authors. To avoid over- fitting, we use dropout ( <ref type="bibr" target="#b32">Srivastava et al., 2014</ref>) of hidden units and early stopping based on the loss on the development set. <ref type="bibr">6</ref> Maximum number of epochs was set to 25 for RNNs and 100 for ME and MLP. We experimented with {0.0, 0.2, 0.4} dropout rates, {16, 32, 64} minibatch sizes, and {100, 150, 200} hidden layer units in MLP and in LSTMs. The vocabulary (V ) in LSTMs was lim- ited to the most frequent P % (P ∈ {85, 90, 95}) words in the training corpus. We initialize the word vectors in the loop-up table L in one of two ways: (i) by sampling randomly from the small uniform distribution U(−0.05, 0.05), and (ii) by using pretrained 300 dimensional Google word embeddings from <ref type="bibr" target="#b21">Mikolov et al. (2013b)</ref>. The di- mension for random initialization was set to 128.</p><p>We experimented with four LSTM variations: (i) U-LSTM r , referring to unidirectional with ran- dom initialization; (ii) U-LSTM p , referring to uni- directional with pretrained initialization; (iii) B- LSTM r , referring to bidirectional with random initialization; and (iv) B-LSTM p , referring to bidi- rectional with pretrained initialization. <ref type="table" target="#tab_8">Table 6</ref> shows the results for different models for the data splits in <ref type="table" target="#tab_6">Table 5</ref>. The first two rows show the best results reported so far on the MRDA corpus from <ref type="bibr" target="#b12">(Jeong et al., 2009</ref>) for classifying into 12 act types. The first row shows the results of the model that uses n-grams and the second row shows the results using all the features in- cluding speaker, part-of-speech, and dependency structure. Our LSTM RNNs and their n-gram model therefore use the same word sequence in- formation. To compare our results with the state of the art, we ran our models on MRDA for both 5-class and 12-class classification tasks. The re- sults are shown at the right most part of <ref type="table" target="#tab_8">Table 6</ref>.</p><p>Notice that all of our LSTMs achieve state of the art results and B-LSTM p achieves even signifi- cantly better with 99% confidence level. This is re- markable since our LSTMs learn the sentence rep- resentation automatically from the word sequence and do not use any hand-engineered features. Now consider the asynchronous domains QC3 and TA, where we show the results of our models based on 5-fold cross validation, in addition to the random (20%) testset. The 5-fold setting allows us to get more general performance of the models on a particular corpus. The comparison between our LSTMs shows that: (i) pretrained Google vec- tors provide better initialization to LSTMs than the random ones; (ii) bidirectional LSTMs outperform their unidirectional counterparts. When we com- pare these results with those of our baselines, the results are disappointing; the ME and MLP us- ing BOW outperform LSTMs by a good margin. <ref type="table" target="#tab_2">SU 34 0 1  0  27  R  0  4 0  2  12  Q  0  0 64 0  13  P  0  0 1  35 6  ST 8  1 3  4  311</ref> (a) B-LSTMp SU R Q P ST SU <ref type="table" target="#tab_2">21 1 1  0  39  R  0  6 0  1  11  Q  0  0 63 0  14  P  0  0 1  32 9  ST 8  2 0  2</ref>  However, this is not surprising since deep neural networks like LSTMs have a lot of parameters, for which they require a lot of data to learn from. To validate our claim, we create another train- ing setting CAT by merging the training and de- velopment sets of the four corpora in <ref type="table" target="#tab_6">Table 5</ref> (see the Train and Dev. columns in the last row); the testset for each dataset however remains the same. <ref type="table" target="#tab_9">Table 7</ref> shows the results of the baselines and the B-LSTM p on the QC3 and TA testsets. In both datasets, B-LSTM p outperforms ME and MLP significantly. When we compare these results with those in <ref type="table" target="#tab_8">Table 6</ref>, we notice that B-LSTM p , by virtue of its distributed and condensed represen- tation, generalizes well across different domains. In contrast, ME and MLP, because of their BOW representation, suffer from data diversity of differ- ent domains. These results also confirm that B- LSTM p gives better sentence representation than BOW, when it is given enough data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SU R Q P ST</head><p>To analyze further the cases where B-LSTM p makes a difference, <ref type="figure" target="#fig_5">Figure 4</ref> shows the corre- sponding confusion matrices for B-LSTM p and MLP on the concatenated testsets of QC3 and TA. It is noticeable that B-LSTM p is less affected by class imbalance and it can detect more suggestions than MLP. This indicates that LSTM RNNs can model the grammar of the sentence when compos- ing the words into phrases sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of CRFs</head><p>To demonstrate the effectiveness of CRFs for cap- turing inter-sentence dependencies in an asyn- chronous conversation, we create another dataset setting called CON, in which the random splits are done at the conversation (as opposed to sen- tence) level for the asynchronous corpora. This is required because our CRF models perform joint learning and inference based on a full conversa- tion. As presented in    opment, respectively. 7 The testsets contain 5 and 20 conversations for QC3 and TA, respectively.</p><p>As baselines, we use three models: (i) ME b , a MaxEnt using BOW representation; (ii) B- LSTM p , which is now trained on the concatenated set of sentences from MRDA and CON training sets; and (iii) ME e , a MaxEnt using sentence em- beddings extracted from the B-LSTM p , i.e., the sentence embeddings are used as feature vectors.</p><p>We experiment with the CRF variants in <ref type="table">Table  1</ref>. The CRFs are trained on the CON training set using the sentence embeddings that are extracted by applying the B-LSTM p model, as was done with ME e . <ref type="table" target="#tab_12">Table 9</ref> shows our results. We notice that CRFs generally outperform MEs in accuracy. This indicates that there are conversational depen- dencies between the sentences in a conversation.</p><p>When we compare between CRF variants, we notice that the model that does not consider any link across comments perform the worst; see CRF (LC-NO). A simple linear chain connection be- tween sentences in their temporal order does not <ref type="bibr">7</ref> We use the concatenated sets as train and dev. sets.  improve much (CRF (LC-LC)), which indicates that the widely used linear chain CRF ( <ref type="bibr" target="#b16">Lafferty et al., 2001</ref>) is not the most appropriate model for capturing conversational dependencies in these conversations. The CRF (LC-LC 1 ) is one of the best performing models and perform significantly (with 99% confidence) better than B-LSTM p . 8 This model considers linear chain connections be- tween sentences inside comments and only to the first comment. Note that both QC3 and TA are forum sites, where participants in a conversation interact mostly with the person who posts the first comment asking for some information. This is in- teresting that our model can capture this aspect. Another interesting observation is that when we change the above model to consider relations with every sentence in the first comment (CRF (LC- FC 1 )), this degrades the performance. This could be due to the fact that the information seeking per- son first explains her situation, and then asks for the information. Others tend to respond to the re- quested information rather than to her situation. The CRF (FC-FC) also yields as good results as CRF (LC-LC 1 ). This could be attributed to the ro- bustness of the fully-connected CRF, which learns from all possible relations.</p><p>To see some real examples in which CRF by means of its global learning and inference makes a difference, let us consider the example in <ref type="figure" target="#fig_0">Figure  1</ref> again. We notice that the two sentences in com- ment C 4 were mistakenly identified as Statement and Response, respectively, by the B-LSTM p local model. However, by considering these two sen- tences together with others in the conversation, the global CRF (FC-FC) model could correct them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Three lines of research are related to our work: (i) semantic compositionality with LSTM RNNs, (ii) conditional structured models, and (iii) speech act recognition in asynchronous conversations. <ref type="bibr" target="#b17">Li et al. (2015)</ref> compare recurrent neural models with recursive (syntax-based) models for several NLP tasks and conclude that recurrent models perform on par with the recursive for most tasks (or even bet- ter). For example, recurrent models outperform recursive on sentence level sentiment classifica- tion. This finding motivated us to use recurrent models rather than recursive. The application of LSTM RNNs to speech act recognition is novel to the best of our knowledge. LSTM RNNs have also been applied to sequence tagging in opinion min- ing ( <ref type="bibr" target="#b11">Irsoy and Cardie, 2014;</ref><ref type="bibr" target="#b18">Liu et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM RNNs for composition</head><p>Conditional structured models There has been an explosion of interest in CRFs for solving struc- tured output problems in NLP; see <ref type="bibr" target="#b31">(Smith, 2011)</ref> for an overview. Linear chain (for sequence label- ing) and tree structured CRFs (for parsing) are the common ones in NLP. However, speech act recog- nition in asynchronous conversation posits a dif- ferent problem, where the challenge is to model ar- bitrary conversational structures. In this work we propose a general class of models based on pair- wise CRFs that work on arbitrary graph structures.</p><p>Speech act recognition in asynchronous conver- sation <ref type="bibr" target="#b12">Jeong et al. (2009)</ref> use semi-supervised boosting to tag the sentences in email and forum discussions with speech acts by adapting knowl- edge from spoken conversations. Other sentence- level approaches use supervised classifiers and se- quence taggers ( <ref type="bibr" target="#b27">Qadir and Riloff, 2011;</ref><ref type="bibr" target="#b33">Tavafi et al., 2013;</ref><ref type="bibr" target="#b24">Oya and Carenini, 2014)</ref>. <ref type="bibr" target="#b5">Cohen et al. (2004)</ref> first use the term email speech act for classifying emails based on their acts (deliver, meeting). Their classifiers do not capture any contextual dependencies between the acts. To model contextual dependencies, <ref type="bibr" target="#b4">Carvalho and Cohen (2005)</ref> use a collective classification approach with two different classifiers, one for content and one for context, in an iterative algo- rithm. Our approach is similar in spirit to their ap- proach with three crucial differences: (i) our CRFs are globally normalized to surmount the label bias problem, where their classifiers are normalized lo- cally; (ii) the graph structure of the conversation is given in their case, which is not the case with ours; and (iii) their approach works at the com- ment level, where we work at the sentence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have presented a two-step framework for speech act recognition in asynchronous conversa- tion. A LSTM RNN first composes sentences into vector representations by considering the word or- der. Then a pairwise CRF jointly models the inter- sentence dependencies in the conversation. We ex- perimented with different LSTM variants (uni-vs. bi-directional, random vs. pretrained initializa- tion), and different CRF variants depending on the underlying graph structure. We trained our models on many different settings using synchronous and asynchronous corpora and evaluated on two forum datasets, one of which is presented in this work.</p><p>Our results show that LSTM RNNs provide bet- ter representations but requires more data, and global joint models improve over local models given that it considers the right graph structure.</p><p>In the future, we would like to combine CRFs with LSTMs for doing the two steps jointly, so that the LSTMs can learn the embeddings using the global thread-level feedback. This would require the backpropagation algorithm to take error sig- nals from the loopy BP inference. We would also like to apply our models to conversations, where the graph structure is extractable using the meta data or other clues, e.g., the fragment quotation graphs for email threads   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example conversation with Human annotations and automatic predictions by a Local classifier and a Global classifier. The labels st, ques, sug, and pol refers to Statement, Question, Suggestion, and Polite speech acts, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our two-step framework for speech act recognition in asynchronous conversation: (a) a bidirectional LSTM encodes each sentence s n m into a condensed vector z n m and classifies them separately; (b) a fully-connected CRF that takes the encoded vectors as input and performs joint learning and inference.</figDesc><graphic url="image-1.png" coords="3,74.40,62.81,279.38,107.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CRFs over different graph structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3a</head><label></label><figDesc>Figure 3a shows the structure for NO-NO configuration, where there is no link between nodes of both intra-and across-comments. In this setting, the CRF model is equivalent to MaxEnt. Figure 3b shows the structure for LC-LC, where there are linear chain relations between nodes of both intra-and across-comments. The linear chain across comments refers to the structure, where the last sentence of each comment is connected to the first sentence of the comment that comes next in the temporal order (i.e., posting time). Figures 3c shows the CRF for LC-LC 1 , where sentences inside a comment have linear chain connections, and the last sentence of the first comment is connected to the first sentence of the other comments. Similarly, Figure 3d shows the graph structure for LC-FC 1 configuration, where sentences inside comments have linear chain connections, and sentences of the first comment are fully connected with the sentences of the other comments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Corpora</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Confusion matrices for (a) B-LSTM p and (b) MLP on the testsets of QC3 and TA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Fully connected with first comment only across LC 1 Linear chain with first comment only across Table 1: Connection types in CRF models.</head><label></label><figDesc></figDesc><table>Tag Connection type 

Applicable to 

NO No connection between nodes 
intra &amp; across 
LC 
Linear chain connection 
intra &amp; across 
FC 
Fully connected 
intra &amp; across 
FC 1 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>TA BC3 Total number of conv. 200 39 Avg. nb of comments per conv. 4.02 6.54 Avg. nb of sentences per conv. 18.56 34.15 Avg. nb of words per sentence 14.90 12.61</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Statistics about TA and BC3 corpora.</head><label>2</label><figDesc></figDesc><table>Tag Description TA 
BC3 
MRDA 
SU Suggestion 7.71% 
5.48% 
5.97% 
R 
Response 
2.4% 
3.75% 
15.63% 
Q 
Question 
14.71% 8.41% 
8.62% 
P 
Polite 
9.57% 
8.63% 
3.77% 
ST Statement 
65.62% 73.72% 66.00% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Since both TA and BC3 are quite small to make a general com- ment about model performance in asynchronous</figDesc><table>Speech Act Distribution 

κ 
Suggestion 
17.38% 
0.86 
Response 
5.24% 
0.43 
Question 
12.59% 
0.87 
Polite 
6.13% 
0.75 
Statement 
58.66% 
0.78 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>50 conversations from a popular community question answering site named Qatar Living 2 for our annotation. We used 3 conversa- tions for our pilot study and used the remaining 47 for the actual study. The resultant corpus on aver- age contains 13.32 comments and 33.28 sentences per conversation, and 19.78 words per sentence.</figDesc><table>Corpus statistics for QC3. 

conversation, we have created a new dataset called 
Qatar Computing Conversational Corpus or QC3. 
We selected </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Number of sentences in train, develop-
ment and test sets for different datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 8 , this setting contains 197 and 24 conversations for training and devel-</head><label>8</label><figDesc></figDesc><table>QC3 

TA 
MRDA 
Testset 
5 folds 
Testset 
5 folds 
5 classes 
12 classes 

Jeong et al. (ng) 
-
-
-
-
-
57.53 (83.30) 
Jeong et al. (All) 
-
-
-
-
-
59.04 (83.49) 

ME 
55.12 (75.64) 50.23 (71.37) 
61.4 (85.44) 
59.23 (84.85) 65.25 (83.95) 
57.79 (82.84) 
MLP 
61.30 (74.36) 54.57 (71.63) 68.17 (85.98) 62.41 (85.02) 68.12 (84.24) 
58.19 (83.24) 

U-LSTMr 
51.57 (73.55) 48.64 (65.94) 56.54 (83.24) 56.39 (83.83) 71.29 (85.38) 
58.72 (83.34) 
U-LSTMp 
49.41 (70.97) 50.26 (65.62) 63.12(83.78) 59.10 (83.13) 72.32 (85.19) 
59.05 (84.06) 

B-LSTMr 
50.75 (72.26) 48.41 (66.19) 58.88 (82.97) 56.23 (83.34) 71.69 (85.62) 
58.33 (83.49) 
B-LSTMp 
53.22 (71.61) 51.59 (68.50) 60.73 (82.97) 59.68 (84.07) 72.02 (85.33) 60.12 (84.46*) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Macro-averaged F 1 and raw accuracy (in parenthesis) for baselines and LSTM variants on the 
testset and 5-fold splits of different corpora. For MRDA, we use the same train-test-dev split as (Jeong 
et al., 2009). Accuracy significantly superior to state-of-the-art is marked with *. 

QC3 (Testset) 
TA (Testset) 

ME 
50.64 (71.15) 
72.49 (84.10) 
MLP 
58.60 (74.36) 
73.07 (86.29) 
B-LSTMp 66.40 (80.65*) 73.14 (87.01*) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 7 : Results on CAT dataset.</head><label>7</label><figDesc></figDesc><table>Train 
Dev 
Test 

QC3 
38 (1332) 
4 (111) 
5 (122) 
TA 
160 (2957) 20 (310) 20 (444) 
Total 197 (4289) 24 (421) 25 (566) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 :</head><label>8</label><figDesc>Setting for CON dataset. The numbers in- side parentheses indicate the number of sentences.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 9 :</head><label>9</label><figDesc>Results of CRFs on CON dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>Dialog act tags and their relative fre-
quencies in the BC3 and TA corpora. </table></figure>

			<note place="foot" n="1"> Some prior work (Tavafi et al., 2013; Oya and Carenini, 2014) also took the same approach.</note>

			<note place="foot" n="2"> http://www.qatarliving.com/ 3 Significance tests operate on individual instances rather than individual classes; thus not applicable for macro F1.</note>

			<note place="foot" n="4"> More hidden layers worsened the performance. 5 Other algorithms (SGD, Adagrad) gave similar results. 6 l1 and l2 regularization on weights did not work well.</note>

			<note place="foot" n="8"> Significance was computed on the concatenated testset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Aseel Ghazal for her effort in creating the QC3 corpus. This work is part of the Interac-tive sYstems for Answer Search (IYAS) project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Algorithm 1: Online learning algorithm for conditional random fields 1. Initialize the model parameters v and w; 2. repeat for each thread G = (V, E) do a. Compute node and edge factors ψ n (y i |z, v) and ψ e (y i,j |z, w); b. Infer node and edge marginals using sum-product loopy BP; c. Update:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">How to do things with words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Langshaw</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning the structure of taskdriven human-human dialogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><forename type="middle">Di</forename><surname>Fabbrizio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44nd Annual Meeting on Association for Computational Linguistics, ACL&apos;06</title>
		<meeting>the 44nd Annual Meeting on Association for Computational Linguistics, ACL&apos;06</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Summarizing online forum discussions-can dialog acts of individual messages help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Biyani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="2127" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Summarizing emails with conversational cohesion and subjectivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46nd Annual Meeting on Association for Computational Linguistics, ACL&apos;08</title>
		<meeting>the 46nd Annual Meeting on Association for Computational Linguistics, ACL&apos;08</meeting>
		<imprint>
			<publisher>OH. ACL</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="353" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the collective classification of email &quot;speech acts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vitor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="345" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to classify email into &quot;speech acts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vitor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="309" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Meeting Recorder Project: Dialog Act Labeling Guide. Technical report, ICSI Tech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajdip</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonali</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Carvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to detect conversation focus of threaded discussions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihie</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL &apos;06</title>
		<meeting>the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, HLT-NAACL &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for twitter: Annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A classification-based approach to question answering in discussion boards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd Annual International ACM SIGIR Conference on Research and Development on Information Retrieval</title>
		<meeting><address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised speech act recognition in emails and forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary Geunbae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Singapore. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1250" to="1259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised Modeling of Dialog Acts in Asynchronous Conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty second International Joint Conference on Artificial Intelligence, IJCAI&apos;11</title>
		<meeting>the twenty second International Joint Conference on Artificial Intelligence, IJCAI&apos;11<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Switchboard SWBD-DAMSL Shallow-DiscourseFunction Annotation Coders Manual, Draft 13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liz</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debra</forename><surname>Biasca</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>University of Colorado at Boulder &amp; +SRI International</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">When are tree structures necessary for deep learning of representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finegrained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using question-answer pairs in extractive summarization of email conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lokesh</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CICLing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="542" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Loopy belief propagation for approximate inference: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;99</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;99<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Machine Learning A Probabilistic Perspective</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extractive summarization and dialogue act modeling on email threads: An integrated probabilistic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuro</forename><surname>Oya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)<address><addrLine>Philadelphia, PA, U.S.A. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">User&apos;s guide to sigf: Significance testing by approximate randomisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classifying sentences as speech acts in message board posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="748" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Profiling Student Interactions in Threaded Discussions with Speech Act Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihie</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AI in Education Conference</title>
		<meeting>AI in Education Conference</meeting>
		<imprint>
			<publisher>AIED</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequencing in conversational openings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schegloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Anthropologist</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1075" to="1095" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Synthesis Lectures on Human Language Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-05" />
			<publisher>Morgan and Claypool</publisher>
		</imprint>
	</monogr>
	<note>Linguistic Structure Prediction</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dialogue act recognition in synchronous and asynchronous conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Tavafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)<address><addrLine>Metz, France. ACL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A publicly available annotated corpus for supervised email summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI&apos;08 EMAIL Workshop</title>
		<meeting><address><addrLine>Chicago, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Accelerated training of conditional random fields with stochastic gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">W</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the 23rd International Conference on Machine Learning, ICML &apos;06<address><addrLine>Pittsburgh, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="969" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Comparing the mean field method and belief propaga-tion for approximate inference in mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Mean Field Methods</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
