<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Learning of Semantic Role Labeling Using Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baidu</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">End-to-end Learning of Semantic Role Labeling Using Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1127" to="1137"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Semantic role labeling (SRL) is one of the basic natural language processing (NLP) problems. To this date, most of the successful SRL systems were built on top of some form of parsing results (Koomen et al., 2005; Palmer et al., 2010; Pradhan et al., 2013), where pre-defined feature templates over the syntactic structure are used. The attempts of building an end-to-end SRL learning system without using parsing were less successful (Collobert et al., 2011). In this work, we propose to use deep bi-directional recurrent network as an end-to-end system for SRL. We take only original text information as input feature , without using any syntactic knowledge. The proposed algorithm for semantic role labeling was mainly evaluated on CoNLL-2005 shared task and achieved F 1 score of 81.07. This result outperforms the previous state-of-the-art system from the combination of different parsing trees or models. We also obtained the same conclusion with F 1 = 81.27 on CoNLL-2012 shared task. As a result of simplicity, our model is also computationally efficient that the parsing speed is 6.7k tokens per second. Our analysis shows that our model is better at handling longer sentences than traditional models. And the latent variables of our model implicitly capture the syntactic structure of a sentence.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic role labeling (SRL) is a form of shal- low semantic parsing whose goal is to discover the predicate-argument structure of each predicate in a given input sentence. Given a sentence, for each target verb (predicate) all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic argu- ments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Man- ner, Cause, etc.. SRL is useful as an intermedi- ate step in a wide range of natural language pro- cessing (NLP) tasks, such as information extrac- tion ( <ref type="bibr" target="#b1">Bastianelli et al., 2013)</ref>, automatic document categorization ( <ref type="bibr" target="#b23">Persson et al., 2009</ref>) and question- answering ( <ref type="bibr" target="#b11">Dan and Lapata, 2007;</ref><ref type="bibr" target="#b29">Surdeanu et al., 2003;</ref><ref type="bibr" target="#b21">Moschitti et al., 2003)</ref>.</p><p>SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to per- form this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was de- cided( <ref type="bibr" target="#b22">Palmer et al., 2010)</ref>.</p><p>Syntactic information is considered to play an essential role in solving this problem <ref type="bibr" target="#b26">(Punyakanok et al., 2008a</ref>). The location of an argument on syn- tactic tree provides an intermediate tag for improv- ing the performance. However, building this syn- tactic tree also introduces the prediction risk in- evitably. The analysis in <ref type="bibr" target="#b24">(Pradhan et al., 2005)</ref> found that the major source of the incorrect pre- dictions was the syntactic parser. Combination of different syntactic parsers was proposed to address this problem, from both feature level and model level ( <ref type="bibr" target="#b30">Surdeanu et al., 2007;</ref><ref type="bibr" target="#b16">Koomen et al., 2005;</ref><ref type="bibr" target="#b24">Pradhan et al., 2005</ref>).</p><p>Besides, feature templates in this classification task strongly rely on the expert experience. They need iterative modification after analyzing how the system performs on development data. When the corpus and data distribution are changed, or when people move to another language, the feature tem- plates have to be re-designed.</p><p>To address the above issues, <ref type="bibr" target="#b10">(Collobert et al., 2011</ref>) proposed a unified neural network architec- ture using word embedding and convolution. They applied their architecture on four standard NLP tasks: Part-Of-Speech tagging (POS), chunking (CHUNK), Named Entity Recognition (NER) and Semantic Role Labeling (SRL). They were able to reach the previous state-of-the-art performance on all these tasks except for SRL. They had to resort to parsing features in order to make the system competitive with state-of-the-art performance.</p><p>In this work, we propose an end-to-end system using deep bi-directional long short-term memo- ry (DB-LSTM) model to address the above dif- ficulties. We take only original text as the in- put features, without any intermediate tag such as syntactic information. The input features are processed by the following 8 layers of LSTM bi- directionally. At the top locates the conditional random field (CRF) model for tag sequence pre- diction. We achieve the state-of-the-art perfor- mance of f-score F 1 = 81.07 on CoNLL-2005 shared task and F 1 = 81.27 on CoNLL-2012 shared task. At last, we find the traditional syn- tactic information can also be inferred from the learned representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>People solve SRL problems in two major ways. The first one follows the traditional spirit widely used in NLP basic problems. A linear classifier is employed with feature templates. Most efforts fo- cus on how to extract the feature templates that can best describe the text properties from train- ing corpus. One of the most important features is from syntactic parsing, although syntactic pars- ing is also considered as a difficult problem. Thus system combination appear to be the general solu- tion.</p><p>In the work of ( <ref type="bibr" target="#b24">Pradhan et al., 2005</ref>), the syn- tactic tags are produced by Charniak parser <ref type="bibr" target="#b7">(Charniak, 2000;</ref><ref type="bibr" target="#b6">Charniak and Johnson, 2005</ref>) and Collins parser <ref type="bibr" target="#b8">(Collins, 2003)</ref> respectively. Based on this, different systems are built to generate SRL tags. These SRL tags are used to extend the original feature templates, along with flat syntactic chunking results. At last another classifier learns the final SRL tag from the above results. In their analysis, the combination of three different syntac- tic view brings large improvement for the system.</p><p>Similarly, <ref type="bibr" target="#b16">Koomen et al. (Koomen et al., 2005</ref>) combined the system in another way. They built multiple classifiers and then all outputs are com- bined through an optimization problem. Surdeanu et al. fully discussed the combination strategy in ( <ref type="bibr" target="#b30">Surdeanu et al., 2007)</ref>.</p><p>Beyond the above traditional methods, the sec- ond way try to solve this problem without feature engineering. <ref type="bibr" target="#b10">Collobert et al. (Collobert et al., 2011</ref>) introduced a neural network model consists of word embedding layer, convolution layers and CRF layer. This pipeline addressed the data spar- sity by initializing the model with word embed- dings which is trained from large unlabeled text corpus. However, the convolution layer is not the best way to model long distance dependency since it only includes words within limited context. So they processed the whole sequence for each giv- en pair of argument and predicate. This results in the computational complexity of O(n p L 2 ), with L denoting the sequence length and n p the number of predicate, while the complexity of our model is linear (O(n p L)). Moreover, in order to catch up with the performance of traditional methods, they had to incorporate the syntactic features by using parse trees of Charniak parser <ref type="bibr" target="#b7">(Charniak, 2000</ref>) which still provides the major contribution.</p><p>At the inference stage, structural constraints of- ten lead to improved results ( <ref type="bibr" target="#b27">Punyakanok et al., 2008b</ref>). The constraints comes from annotation conventions of the task and other linguistic consid- erations. With dynamic programming, <ref type="bibr">(Täckström et al., 2015</ref>) enhance the inference efficiency fur- ther. But designation of the constraints depends much on the linguistic knowledge.</p><p>Nevertheless, the attempts of building end-to- end systems for NLP become popular in recen- t years. Inspired by the work in computer vi- sion, people hierarchically organized a window of words through convolution layers in deep form to account for the higher level of organization to solve the document classification task <ref type="bibr" target="#b15">(Kim, 2014;</ref><ref type="bibr" target="#b39">Zhang and LeCun, 2015)</ref>. Step further, people have also achieved success in directly mapping the sequence to sequence level target as the work in dependency parsing and machine translation ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches</head><p>In this paper, we propose an end-to-end system based on recurrent topology. Recurrent neural net- work (RNN) has natural advantage in modeling sequence problems. The past information is built up through the recurrent layer when model con- sumes the sequence word by word as shown in E- q. 1. x and y are the input and output of the recur- rent layer with (t) denoting the time step, w m f and w m i are the matrix from input or recurrent layer to hidden layer. σ is the activation function. With- out y (t−1) term, the rnn model returns to the feed forward form.</p><formula xml:id="formula_0">y (t) m = σ( f w m f x (t) f + i w m i y (t−1) i )<label>(1)</label></formula><p>However, people often met with two difficulties. First, information of the current word strongly de- pends on distant words, rather than its neighbor- hood. Second, gradient parameters may explode or vanish especially in processing long sequences <ref type="bibr" target="#b2">(Bengio et al., 1994)</ref>. Thus long short-term mem- ory (LSTM) <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997</ref>) was proposed to address the above difficulties.</p><p>In the following part, we will first give a brief introduction about the LSTM and then demon- strate how to build up a network based on LSTM to solve a typical sequence tagging problem: se- mantic role labeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Long Short-Term Memory (LSTM)</head><p>Long short-term memory (LSTM) <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b12">Graves et al., 2009</ref>) is an RNN architecture specifically designed to address the vanishing gradient and exploding gradient problems. The hidden neural units are replaced by a number of memory blocks. Each memory block contains several cells, whose activations are controlled by three multiplicative gates: the input gate, forget gate and output gate. With the above change, the original rnn model is improved to be:</p><formula xml:id="formula_1">y (t) m = σ(s (t) c,m ) · π (t) m (2) = σ(n (t) m ρ (t) m + φ (t) m s (t−1) c,m ) · π (t) m (3)</formula><p>Now y is the memory block output. n is equivalent to the original hidden value y in rnn model. ρ, φ and π are the input, forget and output gates value. s c,m is state value of cell c in block m and c is fixed to be 1 and omitted in common work. The compu- tation of three multiplicative gates comes from in- put value, recurrent value and cell state value with different activations σ respectively as shown in the following and <ref type="figure">Fig. 1</ref>:</p><formula xml:id="formula_2">n (t) m : σ n ( f w m f,n x (t) f + i w m i,n y (t−1) i ) (4) ρ (t) m : σ ρ ( f w m f,ρ x (t) f + i w m i,ρ y (t−1) i + w m ρ s (t−1) m ) φ (t) m : σ φ ( f w m f,φ x (t) f + i w m i,φ y (t−1) i + w m φ s (t−1) m ) π (t) m : σ π ( f w m f,π x (t) f + i w m i,π y (t−1) i + w m π s (t) m )</formula><p>Figure 1: LSTM memory block with a single cell.</p><p>( <ref type="bibr" target="#b12">Graves et al., 2009</ref>)</p><p>The effect of the gates is to allow the cells to store and access information over long periods of time. When the input gate is closed, the new com- ing input information will not affect the previous cell state. Forget gate is used to remove the histor- ical information stored in the cells. The rest of the network can access the stored value of a cell only when its output gate is open.</p><p>In language related problems, the structural knowledge can be extracted out by processing se- quences both forward and backward so that the complementary information from the past and the future can be integrated for inference. Thus bi- directional LSTM (B-LSTM) containing two hid- den layers were proposed <ref type="bibr" target="#b28">(Schuster and Paliwal, 1997)</ref>. Both hidden layers connect to the same in- put layer and output layer, processing the same se- quence in two directions respectively <ref type="bibr" target="#b0">(A. Graves, 2013)</ref>.</p><p>In this work, we utilize the bi-directional infor- mation in another way. First a standard LSTM processes the sequence in forward direction. The output of this LSTM layer is taken by the next LSTM layer as input, processed in reversed di- rection. These two standard LSTM layers com- pose a pair of LSTM. Then we stack LSTM layer- s pair after pair to obtain the deep LSTM model. We call this topology as deep bi-directional LSTM (DB-LSTM) network. Our experiments show that this architecture is critical to achieve good perfor- mance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pipeline</head><p>We process the sequence word by word. Two in- put features play an essential role in this pipeline: predicate (pred) and argument (argu), with argu- ment describing the word under processing. The output for this pair of words is their semantic role. If a sequence has n p predicates, we will process this sequence n p times.</p><p>We also introduce two other features, predicate context (ctx-p) and region mark (m r ). Since a s- ingle predicate word can not exactly describe the predicate information, especially when the same words appear more than one times in a sentence. With the expanded context, the ambiguity can be largely eliminated. Similarly, we use region mark m r = 1 to denote the argument position if it lo- cates in the predicate context region, or m r = 0 if not. These four simple features are all we need for our SRL system. In <ref type="table">Tab</ref> Because the large number of parameters asso- ciated with the argument words, similar to (Col- lobert et al., 2011), the pre-trained word represen- tations are employed to address the data sparsity issue. We used a large unlabeled text corpus to train a neural language model (NLM) ( <ref type="bibr" target="#b4">Bengio et al., 2006;</ref><ref type="bibr" target="#b3">Bengio et al., 2003)</ref> and then initial- ized the argument and predicate word representa- tions with parameters from the NLM representa- tions. There are various ways of obtaining good word representations ( <ref type="bibr" target="#b19">Mikolov et al., 2013;</ref><ref type="bibr" target="#b9">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b20">Mnih and Kavukcuoglu, 2013;</ref><ref type="bibr" target="#b38">Yu et al., 2014)</ref>. A systematic comparison of them on the task of SRL is beyond the scope of this work.</p><p>The above four features are concatenated to be the input representation at this time step for the following LSTM layers. As described in Sec. 3.1, we use DB-LSTM topology to learn the sequence knowledge and we build up to 8 layers of DB- LSTM in our work.</p><p>As in traditional methods, we employ CRF ( <ref type="bibr" target="#b17">Lafferty et al., 2001</ref>) on top of the network for the final prediction. It takes the representations provided by the last LSTM layer as input to model the strong dependance among adjacent tags. The complete model with 4 LSTM layers is il- lustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. At the bottom of the graph lo- cates the word sequence in Tab. 1. For a given time step (step 2 as an example), argument and predi- cate are specified with different color. We use the shadowed region to denote the predicate contex- t. The temporal expanded version of the model is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. L-H denotes the LSTM hidden layer.</p><p>We use the stochastic gradient descent (SGD) algorithm as the training technique for the whole pipeline ( <ref type="bibr" target="#b18">Lecun et al., 1998</ref>). For a given se- quence, we look up the embedding of each word and process this vector with the following LSTM layers for the high level representation. After hav- ing finished the whole sequence, we take the rep- resentations of all time steps as the input features for CRF to perform the sequence tagging task. The traditional viterbi decoding is used for inference. The gradient of the log-likelihood of the tag se- quence with respect to the input of the CRF is cal- culated and back-propagated to all the DB-LSTM layers to get the gradient of the parameters (Col- lobert et al., 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We mainly evaluated and analyzed our system on the commonly used CoNLL-2005 shared task da- ta set and the conclusions are also validated on CoNLL-2012 shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data set</head><p>CoNLL-2005 data set takes section 2-21 of Wall Street Journal (WSJ) data as training set, and sec- tion 24 as development set. The test set consist- s of section 23 of WSJ concatenated with 3 sec- tions from Brown corpus <ref type="bibr" target="#b5">(Carreras and M` arquez, 2005</ref>). CoNLL-2012 data set is extracted from OntoNotes v5.0 corpus. The description and sep- aration of train, development and test data set can be found in ( <ref type="bibr" target="#b25">Pradhan et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word embedding</head><p>We trained word embeddings with English Wikipedia (Ewk) corpus using NLM ( <ref type="bibr" target="#b4">Bengio et al., 2006</ref>). The corpus contains 995 million to- kens. We transformed all the words into their lowercase and the vocabulary size is 4.9 million. About 5% words in CoNLL 2005 data set can not be found in Ewk dictionary and are marked as &lt;unk&gt;. In all experiments, we use the same word embedding with dimension 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network topology</head><p>In this part, we will analyze the performance of two different networks, the CNN and LSTM net- work. Although at last we find CNN can not pro- vide the results as good as that from LSTM, the analysis still help us to gain a deep insight of this problem. In CNN, we add argument context as the fifth feature and the other four features are the same as that used in LSTM. In order to have good understanding of the contribution from each mod- eling decision, we started from a simple model and add more units step by step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Convolutional neural network</head><p>Using CNN to solve SRL problem has been intro- duced in <ref type="bibr" target="#b10">(Collobert et al., 2011</ref>). Since we only focus on the analysis of features, a simplified ver- sion is used here.</p><p>Our feature set consists of five parts as de- scribed above. The representation of argument and predicate can be obtained by looking up the Emb(Ewk) dictionary. And the representation of argument context and predicate context can be ob- tained by concatenating the embedding of each word in the context. For each of the above four parts, we add a hidden layer. Then all these four hidden layers together with region mark are pro- jected onto the next hidden layer. At last we use a CRF layer for prediction (See <ref type="figure" target="#fig_2">Fig. 4</ref>). With above set up, the computational complexity is O(n p L). The size of hidden layers connected to argu- ment or predicate is set to be h 1w = 32. The size of the other two hidden layers connected to con- text embedding is set to be h 1c = 128 since the corresponding inputs are larger. To simplify the parameter setting and results comparison, we use the same learning rate l = 1 × 10 −3 for each layer and keep this rate a constant during model train- ing. The second hidden layer dimension h 2 is also 128. All hidden layer activation function is tanh.</p><p>In Tab. 2, it is shown that longer argument and predicate context result in better performance, since longer context brings more information. We observe the same trends in other NLP experiments, such as NER, POS tagging. The difference is that we do not need to use the context length up to 11. This is because most of the useful information for NER and POS tagging is local respect the label position, while in SRL there exists long distance relationship. So in traditional methods for SRL, syntactic trees are often introduced to account for such relation. In order to see whether the improve- ment from CNN-2 to CNN-3 is due to longer con- text or larger model size, we tested a model CNN- 6 with same context length but more model param- eters. As we can see from the result of CoNLL- 2005 data set (Tab. 2), larger model does not im- prove the result.  Without using region mark (m r ) feature, the F 1 drops from the 53.07 of CNN-3 to the 37.50 of CNN-5. Since it is generally believed that words near the predicate are more likely to be related to the predicate.</p><p>SRL is a typical problem with long distance de- pendency, while the convolution operation can on- ly learn the knowledge from the limited neighbor- hood. This is why we have to introduce long con- text. However, the language information can not be expressed just by linearly expanding the con- text as what we did in CNN pipeline. In order to better summarize the sequence structure, we turn to LSTM network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">LSTM network</head><p>Here the feature set consists of four parts. Ar- gument and predicate are necessary parts in this problem. In recurrent model, argument context (ctx-a) is no longer needed and we only expand the predicate context. We also need the region mark defined in the same way as in CNN. The archi- tecture has been shown in <ref type="figure" target="#fig_0">Fig. 2</ref> and described in Sec. 3.2.</p><p>Since it is difficult to propagate the error from the top to the bottom layers, we use two learning rates. At the bottom, i.e. from embeddings to the first LSTM layer, we use l b = 1 × 10 −2 for mod- el depth d &lt;= 4 and l b = 2 × 10 −2 for d &gt; 4. For the other LSTM layers and CRF layer, we set learning rate l = l b × 10 −3 . We kept all learn- ing rates constant during training. The model size can be enlarged by increasing the number of LST- M layers (d) or the dimension of hidden layers (h). L2 weight decay in SGD is used for model regu- larization and we set its strength r 2 = 8 × 10 −4 :</p><formula xml:id="formula_3">w ← w − l · (g + r 2 · w)<label>(5)</label></formula><p>where w denotes the parameter, g the gradient of the log likelihood of the label with respect to the parameter. We started on CoNLL-2005 dataset from a small model with only one LSTM layer and h = 32. All word embeddings were randomly initial- ized. Predicate context length was 1. Region mark is not used. With this model, we obtained F 1 = 49.44 (Tab. 3), better than that of CNN with- out using argument context (41.24) or region mark <ref type="bibr">(37.50)</ref>. This result suggests that, the recurrent structure can extract sequential information more effectively than CNN.</p><p>By adding predicate context with length 5, F 1 is improved from 49.44 to 56.85 <ref type="bibr">(Tab. 3)</ref>. This is because we only recurrently process the argument word, so we still need predicate context for more detail. Further more, F 1 rises to 58.71 with re- gion mark feature. The reason is the same as we explained in CNN pipeline.</p><p>Next we change the random initialization of word representation to the pre-trained word rep- resentation from Emb(Ewk). This representation is fixed in the training process. F 1 rises to 65.11 (See Tab. 3).</p><p>So far, we have shown the effect from each part of features in LSTM network. The conclusion is consistent with what we found in CNN network. Besides, LSTM exhibits better abilities to learn the sequence structure. Next, we gradually increase the model size to further enhance the performance. <ref type="table" target="#tab_2">CoNLL-2005 data set  Ran 1  1  n  32  47.88  49.44  Ran 1  5  n  32  54.63  56.85  Ran 1  5  y  32  57.13  58.71  Ewk 1  5  y  32  64.48  65.11  Ewk 2  5  y  32  72.72  72.56  Ewk 4  5  y  32</ref> 75.08 We find that the critical improvement comes from increasing the depth of LSTM network. Af- ter adding a reversed LSTM layer, F 1 is improved from 65.11 to 72.56. And the F 1 of the system with d = 4, 6, 8 are 75.74, 78.02 and 78.28 re- spectively. With 6-layer network, we have outper- formed the CoNLL-2005 shared task winner sys- tem with F 1 = 77.92 ( <ref type="bibr" target="#b16">Koomen et al., 2005</ref>). Our experiment results also show that the further per- formance gain by increasing the depth from 6 to 8 is relative small.</p><formula xml:id="formula_4">Emb d ctx-p mr h F1(dev) F1</formula><note type="other">75.74 Ewk 6 5 y 32 76.94 78.02 Ewk 8 5 y 32 77.50 78.28 Ewk 8 5 y 64 77.69 79.46 Ewk 8 5 y 128 79.10 80.28 fine tuning Ewk 8 5 y 128 79.55 81.07 CoNLL-2012 data set Ewk 8 5 y 128 80.51 80.70 fine tuning Ewk 8 5 y 128 81.07 81.27</note><p>Another way to increase the model size is to in- crease the hidden layer dimension h. We gradually increase the dimension from 32 to 64, 128, and the corresponding results are listed in Tab. 3. The best F 1 we obtained is 80.28 with h = 128. We al- so show the result F 1 = 80.70 on CoNLL-2012 dataset in Tab. 3 with exactly the same setup.</p><p>In the above experiments, learning rate and weight decay rate are fixed for the sake of sim- plicity in comparing different models. To fur- ther improve the model, we perform a fine tuning step to adjust the parameters based on previous- ly trained model. This includes the relaxation of weight decay and decrease of learning rate. We set r 2 = 4 × 10 −4 and l b = 1 × 10 −2 , and obtain F 1 = 81.07 as the final result of CoNLL-2005 da- ta set and F 1 = 81.27 of CoNLL-2012 data set.   In Tab. 4, we compare the performance of oth- er works. On CoNLL-2005 shared task, merg- ing syntactic tree at feature level instead of model level exhibits the similar performance with F 1 = 77. <ref type="bibr">30 (Pradhan et al., 2005</ref>). After further investi- gation on model combination, Surdeanu et al. ob- tained a better system ( <ref type="bibr" target="#b30">Surdeanu et al., 2007</ref>). We also list the results from ( <ref type="bibr" target="#b34">Toutanova et al., 2008)</ref> and <ref type="bibr">(Täckström et al., 2015</ref>) of the joint model with additional considerations of standard linguis- tic assumptions. For convolution based methods <ref type="bibr" target="#b10">(Collobert et al., 2011</ref>), the best F 1 is 76.06, in which syntactic parser plays an essential role. The result without using parser drops down to 74.15. On Brown set, we observe the better performance from the work of ( <ref type="bibr" target="#b30">Surdeanu et al., 2007)</ref> and <ref type="bibr">(Täckström et al., 2015)</ref>. We hypothesize that DB- LSTM is a data-driven method that can not per- forms well on out-domain dataset.</p><p>On CoNLL-2012 data set, the traditional method gives F 1 = 75.53 <ref type="bibr" target="#b25">(Pradhan et al., 2013</ref>) and a dynamic programming algorithm for effi- cient constrained inference in SRL gives F 1 = 79. <ref type="bibr">4 (Täckström et al., 2015)</ref> , both of them also rely on syntax trees.</p><p>Since the input feature size is much smaller then the traditional sparse feature templates, the infer- ence stage is very efficient that the model can pro- cess 6.7k tokens per second on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>We analyze our results on CoNLL-2005 data set. First we list the details including the performance on each sub-classes in Tab. 5. The results of CoNLL-2005 shared task winner system ( <ref type="bibr" target="#b16">Koomen et al., 2005</ref>) are also shown for comparison. Their <ref type="bibr">Results (Koomen et.al.)</ref> Results <ref type="formula">(</ref>  <ref type="table">Table 5</ref>: F 1 on each sub sets and classes <ref type="bibr">(CoNLL2005)</ref>. (We remove the classes with low statistics.)</p><p>final system is the combination of the results of 5 parsing trees from two different parsers. They also reported the scores of each single system on development set and we list the best one of them (dev(s)).</p><p>We observe the improvement of F 1 on develop- ment set and test set are 2.20 and 3.15 respective- ly. For single system, the improvement is 4.79 on development set. We also notice that our model show improvement on both WSJ and Brown test set. The advantage of our model is even more sig- nificant when comparing with the previous effort of end-to-end training of SRL model <ref type="bibr" target="#b10">(Collobert et al., 2011</ref>). Without using linguistic features from parse tree, the F 1 of Collobert's model is 74.15, which is 6.92 lower than our model. In order to analyze the performance of our mod- el on the sentences with different lengths, we split the data into 6 bins according to the sentence length, with bin width being 10 words and the last bin includes sequences with L &gt; 50 because of in- sufficient data for longer sentences. <ref type="figure" target="#fig_4">Fig. 5</ref> shows F 1 scores at different sequence lengths on WSJ test data and Brown test data for our model and Koomen's model (baseline) ( <ref type="bibr" target="#b16">Koomen et al., 2005</ref>). In all curves, performance degrades with increased sentence length. However, the performance gain of our model over the baseline model is larger for longer sentences. Since we do not use any syntactic information as input feature, we are curious about whether this information can be extracted out from the system parameters. In LSTM, forget gates are used to control the use of historical information. We com- pute the average value v f g of forget gates of the 7 th LSTM layer at word position for a given sen- tence. We also introduce a variable named syntac- tic distance d s to represent the number of edges between argument word and predicate word in the dependency parsing tree. Four example sentences are shown in <ref type="figure" target="#fig_6">Fig. 6</ref>. For each figure, the bottom axis denotes an example sentence. At the top of each graph is the corresponding dependency tree built from gold dependency parsing tag. At the bottom, v f g and d s are shown in black and red line. Noticed that the higher forget gates values means "Remember" and smaller values "Forget". Smaller d s means that it is easy to make prediction that long history is unnecessary. On the contrary, large d s results in a difficult prediction that long historical information is needed. We also com- puted the average v f g over instances and found it monotonously increases with d s <ref type="figure" target="#fig_5">(Fig. 7)</ref>. The co- incidence of v f g and d s suggests that the model implicitly captures some syntactic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future work</head><p>We investigate a traditional NLP problem SRL with DB-LSTM network. With this model, we are able to bypass the traditional steps for extracting the intermediate NLP features such as POS and syntactic parsing and avoid human engineering the feature templates. The model is trained to predict the SRL tag directly from the original word se- quence with four simple features without any ex- plicit linguistic knowledge. Our model achieves F 1 score of 81.07 on CoNLL-2005 shared task and 81.27 on CoNLL-2012 shared task, both out- performing the previous systems based on parsing results and feature engineering, which heavily re- ly on the linguistic knowledge from expert. Fur- thermore, the simplified feature templates results in high inference efficiency with 6.7k tokens per second.</p><p>In our experiments, increasing the model depth is the major contribution to the final improvement. With deep model, we achieve strong ability of learning semantic rules without worrying about over-fitting even on such limited training set. It al- so outperforms the convolution method with large context length. Moreover, with more sophisti- catedly designed network and training technique based on LSTM, such as the attempt to integrate the parse tree concept into LSTM framework <ref type="bibr" target="#b33">(Tai et al., 2015)</ref>, we believe the better performance can be achieved.</p><p>We show in our analysis that for long sequences our model has even larger advantage over the tra- ditional models. On one hand, LSTM network is capable of capturing the long distance dependen- cy especially in its deep form. On the other hand, the traditional feature templates are only good at describing the properties in neighborhood and a small mistake in syntactic tree will results in large deviation in SRL tagging. Moreover, from the analysis of the internal states of the deep network, we see that the model implicitly learn to capture some syntactic structure similar to the dependen- cy parsing tree. It is encouraging to see that deep learning mod- els with end-to-end training can outperform tra- ditional models on tasks which are previously believed to heavily depend on syntactic parsing ( <ref type="bibr" target="#b16">Koomen et al., 2005;</ref><ref type="bibr" target="#b25">Pradhan et al., 2013)</ref>. How- ever, we recognize that semantic role labeling it- self is an intermediate step towards the language problems we really care about, such as question answering, information extraction etc. We believe that end-to-end training with some suitable deep structure yet to be invented might be proven to be effective to solving these problems. And we are seeing some recent active research exploring this possibility ( <ref type="bibr" target="#b36">Weston et al., 2014;</ref><ref type="bibr" target="#b37">Weston et al., 2015;</ref><ref type="bibr" target="#b13">Graves et al., 2014</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: DB-LSTM network.Shadow part denote the predicate context within length 1.</figDesc><graphic url="image-2.png" coords="4,318.19,330.80,196.44,149.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Temporal expanded DB-LSTM network. Bars denote that the connections are blocked by the closed gates. Shadow part denotes the predicate context.</figDesc><graphic url="image-3.png" coords="5,82.92,62.81,196.44,149.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CNN Pipeline. Shadow parts denote the argument context and predicate context respectively</figDesc><graphic url="image-4.png" coords="5,318.19,523.05,196.44,118.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: F 1 vs. sentence length (CoNLL-2005).</figDesc><graphic url="image-5.png" coords="8,77.46,521.70,207.35,144.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Averaged Forget gates value vs. Syntactic distance (CoNLL-2005). The last point includes instances with syntactic distance d s ≥ 6.</figDesc><graphic url="image-6.png" coords="8,307.28,197.10,218.27,85.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Forget gates value vs. Syntactic distance on four example sentences. Top: dependency parsing tree from gold tag. Green square word: predicate word. Bottom black solid lines: forget gates value at each time step. Bottom red empty square lines: gold syntactic distance between the current argument and predicate.</figDesc><graphic url="image-9.png" coords="9,100.97,177.18,196.43,113.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>F 1 of CNN method on development set and test set of CoNLL-2005 data set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>F 1 with LSTM method on development 
set and test set of CoNLL-2005 data set and 
CoNLL-2012 data set. Emb: the type of embed-
ding. d: the number of LSTM layers. ctx-p: pred-
icate context length. m r : region mark feature. h: 
hidden layer size. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Comparison with previous methods.</head><label>4</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Textual inference and meaning representation in human robot interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bastianelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Castellucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora</title>
		<meeting>the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="65" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sbastien</forename><surname>Sencal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2005 shared task: Semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lluís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)</title>
		<meeting>the Ninth Conference on Computational Natural Language Learning (CoNLL-2005)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="152" to="164" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A maximum-entropyinspired parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL 2000</title>
		<meeting>the 1st North American Chapter of the Association for Computational Linguistics Conference, NAACL 2000<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marchine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using semantic roles to improve question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLPCoNLL</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel connectionist system for unconstrained handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized inference with multiple semantic role labeling systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Koomen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Conference on Computational Natural Language Learning, CONLL &apos;05</title>
		<meeting>the 9th Conference on Computational Natural Language Learning, CONLL &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the 8th International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Open domain information extraction via automatic semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Morarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference&apos;03</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="397" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technology Series. Morgan and Claypool</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Text categorization using predicatecargument structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Persson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NODALIDA</title>
		<meeting>NODALIDA</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="142" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic role chunking combining complementary syntactic views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kadri</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Hacioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Conference on Computational Natural Language Learning, CONLL &apos;05</title>
		<meeting>the 9th Conference on Computational Natural Language Learning, CONLL &apos;05<address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<publisher>USA. Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="217" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The importance of syntactic parsing and inference in semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The importance of syntactic parsing and inference in semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using predicate-argument structures for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Aarseth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
	<note>ACL &apos;03</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combination strategies for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pere</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient inference and structured learning for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53st Annual Meeting on Association for Computational Linguistics, ACL &apos;15</title>
		<meeting>the 53st Annual Meeting on Association for Computational Linguistics, ACL &apos;15<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A global joint model for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="161" to="191" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7449</idno>
		<title level="m">Grammar as a foreign language</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>arX- iv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Factor-based compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01710</idno>
		<title level="m">Text understanding from scratch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
