<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>3-7-1 Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
							<email>{hassy,tsuruoka}@logos.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>3-7-1 Hongo, Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Joint Learning of Compositional and Non-Compositional Phrase Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="205" to="215"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel method for jointly learning compositional and non-compositional phrase embeddings by adaptively weighting both types of em-beddings using a compositionality scoring function. The scoring function is used to quantify the level of compositionality of each phrase, and the parameters of the function are jointly optimized with the objective for learning phrase embeddings. In experiments, we apply the adaptive joint learning method to the task of learning embeddings of transitive verb phrases, and show that the compositionality scores have strong correlation with human ratings for verb-object compositionality, substantially outperforming the previous state of the art. Moreover, our embeddings improve upon the previous best model on a transitive verb disambiguation task. We also show that a simple ensemble technique further improves the results for both tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representing words and phrases in a vector space has proven effective in a variety of language pro- cessing tasks <ref type="bibr" target="#b26">(Pham et al., 2015;</ref><ref type="bibr" target="#b31">Sutskever et al., 2014</ref>). In most of the previous work, phrase em- beddings are computed from word embeddings by using various kinds of composition functions. Such composed embeddings are called composi- tional embeddings. An alternative way of comput- ing phrase embeddings is to treat phrases as single units and assigning a unique embedding to each candidate phrase ( <ref type="bibr" target="#b18">Mikolov et al., 2013;</ref><ref type="bibr" target="#b35">Yazdani et al., 2015)</ref>. Such embeddings are called non- compositional embeddings.</p><p>Relying solely on non-compositional embed- dings has the obvious problem of data sparsity (i.e. rare or unknown phrase problems). At the same time, however, using compositional embeddings is not always the best option since some phrases are inherently non-compositional. For example, the phrase "bear fruits" means "to yield results" 1 but it is hard to infer its meaning by composing the meanings of "bear" and "fruit". Treating all phrases as compositional also has a negative ef- fect in learning the composition function because the words in those idiomatic phrases are not just uninformative but can serve as noisy samples in the training. These problems have motivated us to adaptively combine both types of embeddings.</p><p>Most of the existing methods for learning phrase embeddings can be divided into two ap- proaches. One approach is to learn compositional embeddings by regarding all phrases as composi- tional ( <ref type="bibr" target="#b26">Pham et al., 2015;</ref><ref type="bibr" target="#b30">Socher et al., 2012</ref>). The other approach is to learn both types of embed- dings separately and use the better ones <ref type="bibr" target="#b22">Muraoka et al., 2014</ref>).  show that non-compositional embed- dings are better suited for a phrase similarity task, whereas <ref type="bibr" target="#b22">Muraoka et al. (2014)</ref> report the opposite results on other tasks. These results suggest that we should not stick to either of the two types of embeddings unconditionally and could learn better phrase embeddings by considering the composi- tionality levels of the individual phrases in a more flexible fashion.</p><p>In this paper, we propose a method that jointly learns compositional and non-compositional em- beddings by adaptively weighting both types of phrase embeddings using a compositionality scor- ing function. The scoring function is used to quan- tify the level of compositionality of each phrase <ref type="figure">Figure 1</ref>: The overview of our method and ex- amples of the compositionality scores. Given a phrase p, our method first computes the composi- tionality score α(p) (Eq. <ref type="formula" target="#formula_2">(3)</ref>), and then computes the phrase embedding v(p) using the composi- tional and non-compositional embeddings, c(p) and n(p), respectively (Eq. <ref type="formula" target="#formula_1">(2)</ref>). and learned in conjunction with the target task for learning phrase embeddings. In experiments, we apply our method to the task of learning transitive verb phrase embeddings and demonstrate that it allows us to achieve state-of-the-art performance on standard datasets for compositionality detec- tion and verb disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>In this section, we describe our approach in the most general form, without specifying the func- tion to compute the compositional embeddings or the target task for optimizing the embeddings. <ref type="figure">Figure 1</ref> shows the overview of our proposed method. At each iteration of the training (i.e. gradient calculation) of a certain target task (e.g. language modeling or sentiment analysis), our method first computes a compositionality score for each phrase. Then the score is used to weight the compositional and non-compositional embed- dings of the phrase in order to compute the ex- pected embedding of the phrase which is to be used in the target task. Some examples of the com- positionality scores are also shown in the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Compositional Phrase Embeddings</head><p>The compositional embedding c(p) ∈ R d×1 of a phrase p = (w 1 , · · · , w L ) is formulated as</p><formula xml:id="formula_0">c(p) = f (v(w 1 ), · · · , v(w L )),<label>(1)</label></formula><p>where d is the dimensionality, L is the phrase length, v(·) ∈ R d×1 is a word embedding, and f (·) is a composition function. The function can be simple ones such as element-wise addi- tion or multiplication <ref type="bibr" target="#b20">(Mitchell and Lapata, 2008</ref>).</p><p>More complex ones such as recurrent neural net- works ( <ref type="bibr" target="#b31">Sutskever et al., 2014</ref>) are also commonly used. The word embeddings and the composi- tion function are jointly learned on a certain target task. Since compositional embeddings are built on word-level (i.e. unigram) information, they are less prone to the data sparseness problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Non-Compositional Phrase Embeddings</head><p>In contrast to the compositional embedding, the non-compositional embedding of a phrase n(p) ∈ R d×1 is independently parameterized, i.e., the phrase p is treated just like a single word. <ref type="bibr" target="#b18">Mikolov et al. (2013)</ref> show that non-compositional em- beddings are preferable when dealing with id- iomatic phrases. Some recent studies <ref type="bibr" target="#b22">Muraoka et al., 2014</ref>) have dis- cussed the (dis)advantages of using compositional or non-compositional embeddings. However, in most cases, a phrase is neither completely com- positional nor completely non-compositional. To the best of our knowledge, there is no method that allows us to jointly learn both types of phrase em- beddings by incorporating the levels of composi- tionality of the phrases as real-valued scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adaptive Joint Learning</head><p>To simultaneously consider both compositional and non-compositional aspects of each phrase, we compute a phrase embedding v(p) by adaptively weighting c(p) and n(p) as follows:</p><formula xml:id="formula_1">v(p) = α(p)c(p) + (1 − α(p))n(p),<label>(2)</label></formula><p>where α(·) is a scoring function that quantifies the compositionality levels, and outputs a real value ranging from 0 to 1. What we expect from the scoring function is that large scores indicate high levels of compositionality. In other words, when α(p) is close to 1, the compositional em- bedding is mainly considered, and vice versa. For example, we expect α(buy car) to be large and α(bear fruit) to be small as shown in <ref type="figure">Figure 1</ref>. We parameterize the scoring function α(p) as logistic regression:</p><formula xml:id="formula_2">α(p) = σ(W · φ(p)),<label>(3)</label></formula><p>where φ(p) ∈ R N ×1 is a feature vector of the phrase p, W ∈ R N ×1 is a weight vector, N is the number of features, and σ(·) is the logistic func- tion. The weight vector W is jointly optimized in conjunction with the objective J for the target task of learning phrase embeddings v(p).</p><p>Updating the model parameters Given the par- tial derivative δ p = ∂J ∂v(p) ∈ R d×1 for the target task, we can compute the partial derivative for up- dating W as follows:</p><formula xml:id="formula_3">δ α = α(p)(1 − α(p)){δ p · (c(p) − n(p))} (4) ∂J ∂W = δ α φ(p).<label>(5)</label></formula><p>If φ(p) is not constructed by static features but is computed by a feature learning model such as neu- ral networks, we can propagate the error term δ α into the feature learning model by the following equation:</p><formula xml:id="formula_4">∂J ∂φ(p) = δ α W .<label>(6)</label></formula><p>When we use only static features, as in this work, we can simply compute the partial derivatives of J with respect to c(p) and n(p) as follows:</p><formula xml:id="formula_5">∂J ∂c(p) = α(p)δ p (7) ∂J ∂n(p) = (1 − α(p))δ p .<label>(8)</label></formula><p>As mentioned above, Eq. <ref type="formula">(7)</ref> and <ref type="formula" target="#formula_5">(8)</ref> show that the non-compositional embeddings are mainly up- dated when α(p) is close to 0, and vice versa. The partial derivative ∂J ∂c(p) is used to update the model parameters in the composition function via the backpropagation algorithm. Any differentiable composition functions can be used in our method.</p><p>Expected behavior of our method The training of our method depends on the target task; that is, the model parameters are updated so as to mini- mize the cost function as described above. More concretely, α(p) for each phrase p is adaptively ad- justed so that the corresponding parameter updates contribute to minimizing the cost function. As a result, different phrases will have different α(p) values depending on their compositionality. If the size of the training data were almost infinitely large, α(p) for all phrases would become nearly zero, and the non-compositional embeddings n(p) are dominantly used (since that would allow the model to better fit the data). In reality, however, the amount of the training data is limited, and thus the compositional embeddings c(p) are effectively used to overcome the data sparseness problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Verb Phrase Embeddings</head><p>This section describes a particular instantiation of our approach presented in the previous section, fo- cusing on the task of learning the embeddings of transitive verb phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word and Phrase Prediction in Predicate-Argument Relations</head><p>Acquisition of selectional preference using em- beddings has been widely studied, where word and/or phrase embeddings are learned based on syntactic links ( <ref type="bibr" target="#b0">Bansal et al., 2014;</ref><ref type="bibr" target="#b7">Hashimoto and Tsuruoka, 2015;</ref><ref type="bibr" target="#b14">Levy and Goldberg, 2014</ref>; Van de Cruys, 2014). As with language modeling, these methods perform word (or phrase) prediction us- ing (syntactic) contexts.</p><p>In this work, we focus on verb-object rela- tionships and employ a phrase embedding learn- ing method presented in <ref type="bibr" target="#b7">Hashimoto and Tsuruoka (2015)</ref>. The task is a plausibility judgment task for predicate-argument tuples. They extracted Subject-Verb-Object (SVO) and SVO-Preposition- Noun (SVOPN) tuples using a probabilistic HPSG parser, <ref type="bibr">Enju (Miyao and Tsujii, 2008)</ref>, from the training corpora. Transitive verbs and preposi- tions are extracted as predicates with two argu- ments. For example, the extracted tuples include (S, V, O) = ("importer", "make", "payment") and (SVO, P, N) = ("importer make payment", "in", "currency"). The task is to discriminate between observed and unobserved tuples, such as the (S, V, O) tuple mentioned above and (S, V', O) = ("im- porter", "eat", "payment"), which is generated by replacing "make" with "eat". The (S, V', O) tuple is unlikely to be observed.</p><p>For each tuple (p, a 1 , a 2 ) observed in the train- ing data, a cost function is defined as follows:</p><formula xml:id="formula_6">− log σ(s(p, a 1 , a 2 ))− log σ(−s(p , a 1 , a 2 )) − log σ(−s(p, a 1 , a 2 )) − log σ(−s(p, a 1 , a 2 )),<label>(9)</label></formula><p>where s(·) is a plausibility scoring function, and p, a 1 and a 2 are a predicate and its arguments, re- spectively. Each of the three unobserved tuples (p , a 1 , a 2 ), (p, a 1 , a 2 ), and (p, a 1 , a 2 ) is gener- ated by replacing one of the entries with a random sample.</p><p>In their method, each predicate p is represented with a matrix M (p) ∈ R d×d and each argument a with an embedding v(a) ∈ R d×1 . The matri- ces and embeddings are learned by minimizing the cost function using AdaGrad ( <ref type="bibr" target="#b4">Duchi et al., 2011</ref>). The scoring function is parameterized as</p><formula xml:id="formula_7">s(p, a 1 , a 2 ) = v(a 1 ) · (M (p)v(a 2 )),<label>(10)</label></formula><p>and the VO and SVO embeddings are computed as</p><formula xml:id="formula_8">v(V O) = M (V )v(O) (11) v(SV O) = v(S) v(V O),<label>(12)</label></formula><p>as proposed by <ref type="bibr" target="#b10">Kartsaklis et al. (2012)</ref>. The op- erator denotes element-wise multiplication. In summary, the scores are computed as</p><formula xml:id="formula_9">s(V, S, O) = v(S) · v(V O)<label>(13)</label></formula><formula xml:id="formula_10">s(P, SV O, N ) = v(SV O) · (M (P )v(N )).<label>(14)</label></formula><p>With this method, the word and composed phrase embeddings are jointly learned based on co- occurrence statistics of predicate-argument struc- tures.</p><p>Using the learned embeddings, they achieved state-of-the-art accuracy on a transi- tive verb disambiguation task (Grefenstette and Sadrzadeh, 2011).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Applying the Adaptive Joint Learning</head><p>In this section, we apply our adaptive joint learn- ing method to the task described in Section 3.1. We here redefine the computation of</p><formula xml:id="formula_11">v(V O) by first replacing v(V O) in Eq. (11) with c(V O) as, c(V O) = M (V )v(O),<label>(15)</label></formula><p>and then assigning V O to p in Eq. <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref>:</p><formula xml:id="formula_12">v(V O) = α(V O)c(V O) + (1 − α(V O))n(V O),<label>(16)</label></formula><formula xml:id="formula_13">α(V O) = σ(W · φ(V O)).<label>(17)</label></formula><p>The v(V O) in Eq. (16) is used in Eq. <ref type="formula" target="#formula_0">(12)</ref> and <ref type="formula" target="#formula_0">(13)</ref>. We assume that the candidates of the phrases are given in advance. For the phrases not included in the candidates, we set v(V O) = c(V O). This is analogous to the way a human guesses the meaning of an idiomatic phrase she does not know. We should note that φ(V O) can be computed for phrases not included in the candidates, using par- tial features among the features described below.</p><p>If any features do not fire, φ(V O) becomes 0.5 according to the logistic function. For the feature vector φ(V O), we use the fol- lowing simple binary and real-valued features:</p><p>• indices of V, O, and VO</p><p>• frequency and Pointwise Mutual Information (PMI) values of VO.</p><p>More concretely, the first set of the features (in- dices of V, O, and VO) is the concatenation of traditional one-hot vectors. The second set of features, frequency and PMI <ref type="bibr" target="#b2">(Church and Hanks, 1990</ref>) features, have proven effective in detect- ing the compositionality of transitive verbs in <ref type="bibr" target="#b17">McCarthy et al. (2007)</ref> and <ref type="bibr" target="#b34">Venkatapathy and Joshi (2005)</ref>. Given the training corpus, the frequency feature for a VO pair is computed as</p><formula xml:id="formula_14">f req(V O) = log(count(V O)),<label>(18)</label></formula><p>where count(V O) counts how many times the VO pair appears in the training corpus, and the PMI feature is computed as</p><formula xml:id="formula_15">PMI(V O) = log count(V O)count( * ) count(V )count(O) ,<label>(19)</label></formula><p>where count(V ), count(O), and count( * ) are the counts of the verb V , the object O, and all VO pairs in the training corpus, respectively. We nor- malize the frequency and PMI features so that their maximum absolute value becomes 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Data</head><p>As the training data, we used two datasets, one small and one large: the British National Corpus (BNC) ( <ref type="bibr" target="#b13">Leech, 1992)</ref> and the English Wikipedia. More concretely, we used the publicly available data 2 preprocessed by <ref type="bibr" target="#b7">Hashimoto and Tsuruoka (2015)</ref>. The BNC data consists of 1.38 million SVO tuples and 0.93 million SVOPN tuples. The Wikipedia data consists of 23.6 million SVO tu- ples and 17.3 million SVOPN tuples. Follow- ing the provided code 3 , we used exactly the same train/development/test split (0.8/0.1/0.1) for train- ing the overall model. As the third training data, we also used the concatenation of the two data, which is hereafter referred to as BNC-Wikipedia. We applied our adaptive joint learning method to verb-object phrases observed more than K times in each corpus.</p><p>K was set to 10 for the BNC data and 100 for the Wikipedia and BNC-Wikipedia data. Consequently, the non-compositional embeddings were assigned to 17,817, 28,933, and 30,682 verb-object phrase types in the BNC, Wikipedia, and BNC-Wikipedia data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>The model parameters consist of d-dimensional word embeddings for nouns, non-compositional phrase embeddings, d×d-dimensional matrices for verbs and prepositions, and a weight vector W for α(V O</p><note type="other">). All the model parameters are jointly optimized. We initialized the embeddings and matrices with zero-mean gaussian random val- ues with a variance of 1 d and 1 d 2 , respectively, and W with zeros. Initializing W with zeros forces the initial value of each α(V O) to be 0.5 since we use the logistic function to compute α(V O). The optimization was performed via mini- batch AdaGrad (Duchi et al., 2011). We fixed d to 25 and the mini-batch size to 100. We set candidate values for the learn- ing rate ε to {0.01, 0.02, 0.03, 0.04, 0.05}. For the weight vector W , we employed L2- norm regularization and set the coefficient λ to {10 −3 , 10 −4 , 10 −5 , 10 −6</note><p>, 0}. For selecting the hyperparameters, each training process was stopped when the evaluation score on the devel- opment split decreased. Then the best perform- ing hyperparameters were selected for each train- ing dataset. Consequently, ε was set to 0.05 for all training datasets, and λ was set to 10 −6 , 10 −3 , and 10 −5 for the BNC, Wikipedia, and BNC- Wikipedia data, respectively. Once the training is finished, we can use the learned embeddings and the scoring function in downstream target tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation on the Compositionality</head><p>Detection Function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation Settings</head><p>Datasets First, we evaluated the learned com- positionality detection function on two datasets, VJ'05 4 and MC'07 5 , provided by Venkatapathy and Joshi (2005) and <ref type="bibr" target="#b17">McCarthy et al. (2007)</ref>, respectively. VJ'05 consists of 765 verb-object pairs with human ratings for the compositional- ity. MC'07 is a subset of VJ'05 and consists of 638 verb-object pairs. For example, the rating of "buy car" is 6, which is the highest score, indicat- ing the phrase is highly compositional. The rating of "bear fruit " is 1, which is the lowest score, in- dicating the phrase is highly non-compositional.</p><p>Method MC'07 VJ'05 Proposed method (Wikipedia) 0.508 0.514 Proposed method (BNC) 0.507 0.507 Proposed method (BNC-Wikipedia) 0.518 0.527 Proposed method (Ensemble) 0.550 0.552 <ref type="bibr" target="#b12">Kiela and Clark (2013)</ref> w/ WordNet n/a 0.461 <ref type="bibr" target="#b12">Kiela and Clark (2013)</ref> n/a 0.420 DSPROTO ( <ref type="bibr" target="#b17">McCarthy et al., 2007)</ref> 0.398 n/a PMI ( <ref type="bibr" target="#b17">McCarthy et al., 2007)</ref> 0.274 n/a Frequency ( <ref type="bibr" target="#b17">McCarthy et al., 2007)</ref> 0.141 n/a DSPROTO+ ( <ref type="bibr" target="#b17">McCarthy et al., 2007)</ref> 0.454 n/a Human agreement 0.702 0.716 <ref type="table">Table 1</ref>: Compositionality detection task.</p><p>Evaluation metric The evaluation was per- formed by calculating Spearman's rank correlation scores 6 between the averaged human ratings and the learned compositionality scores α(V O).</p><p>Ensemble technique We also produced the re- sult by employing an ensemble technique. More concretely, we used the averaged compositionality scores from the results of the BNC and Wikipedia data for the ensemble result. <ref type="table">Table 1</ref> shows our results and the state of the art. Our method outperforms the previous state of the art in all settings. The result denoted as Ensem- ble is the one that employs the ensemble tech- nique, and achieves the strongest correlation with the human-annotated datasets. Even without the ensemble technique, our method performs better than all of the previous methods. <ref type="bibr" target="#b12">Kiela and Clark (2013)</ref> used window-based co- occurrence vectors and improved their score us- ing WordNet hypernyms. By contrast, our method does not rely on such external resources, and only needs parsed corpora. We should note that <ref type="bibr" target="#b12">Kiela and Clark (2013)</ref> reported that their score did not improve when using parsed corpora. Our method also outperforms DSPROTO+, which used a small amount of the labeled data, while our method is fully unsupervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Result Overview</head><p>We calculated confidence intervals (P &lt; 0.05) using bootstrap resampling <ref type="bibr" target="#b24">(Noreen, 1989)</ref>. For example, for the results using the BNC-Wikipedia data, the intervals on MC'07 and VJ'05 are (0.455, 0.574) and (0.475, 0.579), respectively. These re- sults show that our method significantly outper- forms the previous state-of-the-art results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phrase</head><p>Gold standard (a) BNC (b) Wikipedia BNC-Wikipedia Ensemble ((a)+(b))×0.5     <ref type="figure" target="#fig_1">Figure 2</ref> shows how α(V O) changes for the seven phrases during the training on the BNC data. As shown in the figure, starting from 0.5, α(V O) for each phrase converges to its corresponding value. The differences in the trends indicate that our method can adaptively learn compositionality lev- els for the phrases. <ref type="table" target="#tab_1">Table 2</ref> shows the learned com- positionality scores for the three groups of the ex- amples along with the gold-standard scores given by the annotators. The group (A) is considered to be consistent with the gold-standard scores, the group (B) is not, and the group (C) shows exam- ples for which the difference between the compo- sitionality scores of our results is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Analysis of Compositionality Scores</head><p>Characteristics of light verbs The verbs "take", "make", and "have" are known as light verbs <ref type="bibr">7</ref> , and the scoring function tends to assign low scores to light verbs. In other words, our <ref type="bibr">7</ref> In Section 5.2.2 in Newton (2006), the term light verb is used to refer to verbs which can be used in combination with some other element where their contribution to the meaning of the whole construction is reduced in some way.   <ref type="table" target="#tab_3">Table 3</ref> shows the 10 highest and lowest average scores with the corre- sponding verbs. We see that relatively low scores are assigned to the light verbs as well as other verbs which often form idiomatic phrases. As shown in the group (B) in <ref type="table" target="#tab_1">Table 2</ref>, however, light verb phrases are not always non-compositional. Despite this, the learned function assigns low scores to compositional phrases formed by the light verbs. These results suggest that using a more flexible scoring function may further strengthen our method.</p><p>Context dependence Both our method and the two datasets, VJ'05 and MC'07, assume that the compositionality score can be computed for each phrase with no contextual information. However, in general, the compositionality level of a phrase depends on its contextual information. For ex- ample, the meaning of the idiomatic phrase "bear fruit" can be compositionaly interpreted as "to yield fruit" for a plant or tree. We manually in- spected the BNC data to check whether the phrase "bear fruit" is used as the compositional mean- ing or the idiomatic meaning ("to yield results"). As a result, we have found that most of the usage was its idiomatic meaning. In the model training, our method is affected by the majority usage and fits the evaluation datasets where the phrase "bear fruit" is regarded as highly non-compositional. In- corporating contextual information into the com- positionality scoring function is a promising direc- tion of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Effects of Ensemble</head><p>We used the two different corpora for construct- ing the training data, and our method achieves the state-of-the-art results in all settings. To inspect the results on VJ'05, we calculated the correlation score between the outputs from our results of the BNC and Wikipedia data. The correlation score is 0.674 and that is, the two different corpora lead to reasonably consistent results, which indicates the robustness of our method. However, the correla- tion score is still much lower than perfect correla- tion; in other words, there are disagreements be- tween the outputs learned with the corpora. The group (C) in <ref type="table" target="#tab_1">Table 2</ref> shows such two examples. In these cases, the ensemble technique is helpful in improving the results as shown in the examples. Another interesting observation in our results is that the result of the ensemble technique outper- forms that of the BNC-Wikipedia data as shown in <ref type="table">Table 1</ref>. This shows that separately using the train- ing corpora of different nature and then perform- ing the ensemble technique can yield better re- sults. By contrast, many of the previous studies on embedding-based methods combine different cor- pora into a single dataset, or use multiple corpora just separately and compare them <ref type="bibr" target="#b7">(Hashimoto and Tsuruoka, 2015;</ref><ref type="bibr" target="#b22">Muraoka et al., 2014;</ref><ref type="bibr" target="#b25">Pennington et al., 2014</ref>). It would be worth investigating whether the results in the previous work can be improved by ensemble techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation on the Phrase Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation Settings</head><p>Dataset Next, we evaluated the learned embed- dings on the transitive verb disambiguation dataset GS'11 8 provided by <ref type="bibr" target="#b6">Grefenstette and Sadrzadeh (2011)</ref>. GS'11 consists of 200 pairs of transitive verbs and each verb pair takes the same subject and object. For example, the transitive verb "run" is known as a polysemous word and this task re- quires one to identify the meanings of "run" and "operate" as similar to each other when taking "people" as their subject and "company" as their object. In the same setting, however, the meanings of "run" and "move" are not similar to each other. Each pair has multiple human ratings indicating how similar the phrases of the pair are.</p><p>Evaluation metric The evaluation was per- formed by calculating Spearman's rank correla- tion scores between the human ratings and the cosine similarity scores of v(SV O) in Eq. <ref type="figure" target="#fig_1">(12)</ref>. Following the previous studies, we used the gold- standard ratings in two ways: averaging the human ratings for each SVO tuple (GS'11a) and treating each human rating separately (GS'11b).</p><p>Ensemble technique We used the same ensem- ble technique described in Section 5.1. In this task we produced two ensemble results: Ensemble A and Ensemble B. The former used the averaged cosine similarity from the results of the BNC and Wikipedia data, and the latter further incorporated the result of the BNC-Wikipedia data.</p><p>Baselines We compared our adaptive joint learn- ing method with two baseline methods. One is the method in <ref type="bibr" target="#b7">Hashimoto and Tsuruoka (2015)</ref> and it is equivalent to fixing α(V O) to 1 in our method. The other is fixing α(V O) to 0.5 in our method, which serves as a baseline to evaluate how effec- tive the proposed adaptive weighting method is. <ref type="table" target="#tab_6">Table 4</ref> shows our results and the state of the art, and our method outperforms almost all of the pre- vious methods in both datasets. Again, the en- semble technique further improves the results, and overall, Ensemble B yields the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Result Overview</head><p>The scores in <ref type="bibr" target="#b7">Hashimoto and Tsuruoka (2015)</ref>, the baseline results with α(V O) = 1 in our method, have been the best to date. As shown in <ref type="table" target="#tab_6">Table 4</ref>, our method outperforms the base- line results with α(V O) = 0.5 as well as those     with α(V O) = 1. We see that our method im- proves the baseline scores by adaptively combin- ing compositional and non-compositional embed- dings. Along with the results in <ref type="table">Table 1</ref>, these re- sults show that our method allows us to improve the composition function by jointly learning non- compositional embeddings and the scoring func- tion for compositionality detection.</p><formula xml:id="formula_16">α(V O) = 1 α(V O) = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Analysis of the Learned Embeddings</head><p>We inspected the effects of adaptively weighting the compositional and non-compositional embed- dings. <ref type="table" target="#tab_5">Table 5</ref> shows the five closest neighbor phrases in terms of the cosine similarity for the three idiomatic phrases "take toll", "catch eye", and "bear fruit" as well as the two non-idiomatic phrases "make noise" and "buy car". The exam- ples trained with the Wikipedia data are shown for our method and the two baselines, i.e., α(V O) = 1 and α(V O) = 0.5. As shown in <ref type="table" target="#tab_1">Table 2</ref>, the compositionality levels of the first three phrases are low and their non-compositional embeddings are dominantly used to represent their meaning. One observation with α(V O) = 1 is that head words (i.e. verbs) are emphasized in the shown examples except "take toll" and "make noise". As with other embedding-based methods, the compo- sitional embeddings are highly affected by their component words. As a result, the phrases consist- ing of the same verb and the similar objects are of- ten listed as the closest neighbors. By contrast, our method flexibly allows us to adaptively omit the information about the component words. There- fore, our method puts more weight on capturing the idiomatic aspects of the example phrases by adaptively using the non-compositional embed- dings.</p><p>The results of α(V O) = 0.5 are similar to those with our proposed method, but we can see some differences. For example, the phrase list for "make noise" of our proposed method captures offensive meanings, whereas that of α(V O) = 0.5 is some- what ambiguous. As another example, the phrase lists for "buy car" show that our method better cap- tures the semantic similarity between the objects than α(V O) = 0.5. This is achieved by adaptively assigning a relatively large compositionality score (0.71) to the phrase to use the information about the object "car".</p><p>We should note that "make noise" is highly compositional but our method outputs α(make noise) = 0.33, and the phrase list of α(V O) = 1 is the most appropriate in this case. Improving the compositionality detection function should thus further improve the learned embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Learning embeddings of words and phrases has been widely studied, and the phrase embeddings have proven effective in many language process- ing tasks, such as machine translation <ref type="bibr" target="#b1">(Cho et al., 2014;</ref><ref type="bibr" target="#b31">Sutskever et al., 2014)</ref>, sentiment analysis and semantic textual similarity <ref type="bibr" target="#b32">(Tai et al., 2015</ref>). Most of the phrase embeddings are constructed by word-level information via various kinds of composition functions like long short-term mem- ory <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) recur- rent neural networks. Such composition functions should be powerful enough to efficiently encode information about all the words into the phrase embeddings. By simultaneously considering the compositionality of the phrases, our method would be helpful in saving the composition models from having to be powerful enough to perfectly encode the non-compositional phrases. As a first step to- wards this purpose, in this paper we have shown the effectiveness of our method on the task of learning verb phrase embeddings.</p><p>Many studies have focused on detecting the compositionality of a variety of phrases <ref type="bibr" target="#b15">(Lin, 1999)</ref>, including the ones on verb phrases <ref type="bibr" target="#b3">(Diab and Bhutada, 2009;</ref><ref type="bibr" target="#b16">McCarthy et al., 2003)</ref> and compound nouns <ref type="bibr" target="#b29">Reddy et al., 2011</ref>). Compared to statistical feature-based methods <ref type="bibr" target="#b17">(McCarthy et al., 2007;</ref><ref type="bibr" target="#b34">Venkatapathy and Joshi, 2005</ref>), recent methods use word and phrase embeddings <ref type="bibr" target="#b12">(Kiela and Clark, 2013;</ref><ref type="bibr" target="#b35">Yazdani et al., 2015)</ref>. The embedding-based meth- ods assume that word embeddings are given in advance and as a post-processing step, learn or simply employ composition functions to com- pute phrase embeddings. In other words, there is no distinction between compositional and non- compositional phrases. <ref type="bibr" target="#b35">Yazdani et al. (2015)</ref> fur- ther proposed to incorporate latent annotations (binary labels) for the compositionality of the phrases. However, binary judgments cannot con- sider numerical scores of the compositionality. By contrast, our method adaptively weights the com- positional and non-compositional embeddings us- ing the compositionality scoring function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>We have presented a method for adaptively learn- ing compositional and non-compositional phrase embeddings by jointly detecting compositionality levels of phrases. Our method achieves the state of the art on a compositionality detection task of verb-object pairs, and also improves upon the pre- vious state-of-the-art method on a transitive verb disambiguation task. In future work, we will ap- ply our method to other kinds of phrases and tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Trends of α(V O) during the training on the BNC data.</figDesc><graphic url="image-2.png" coords="6,72.00,212.68,232.45,174.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples of the compositionality scores. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The 10 highest and lowest average com-
positionality scores with the corresponding verbs 
on the BNC data. 

method can recognize that the light verbs are 
frequently used to form idiomatic (i.e. non-
compositional) phrases. To verify the assumption, 
we calculated the average compositionality score 
for each verb by averaging the compositionality 
scores paired with its candidate objects. Here we 
used 135 verbs which take more than 30 types of 
objects in the BNC data. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Examples of the closest neighbors in the learned embedding space. All of the results were 
obtained by using the Wikipedia data, and the values of α(V O) are the same as those in Table 2. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Transitive verb disambiguation task. The 
results for α(V O) = 1 are reported in Hashimoto 
and Tsuruoka (2015). 

</table></figure>

			<note place="foot" n="1"> The definition is found at http://idioms. thefreedictionary.com/bear+fruit.</note>

			<note place="foot" n="2"> http://www.logos.t.u-tokyo.ac.jp/ ˜ hassy/publications/cvsc2015/ 3 https://github.com/hassyGo/ SVOembedding</note>

			<note place="foot" n="4"> http://www.dianamccarthy.co.uk/ downloads/SVAJ2005compositionality_ rating.txt 5 http://www.dianamccarthy.co.uk/ downloads/emnlp2007data.txt</note>

			<note place="foot" n="6"> We used the Scipy 0.12.0 implementation in Python.</note>

			<note place="foot" n="8"> http://www.cs.ox.ac.uk/activities/ compdistmeaning/GS2011data.txt</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their help-ful comments and suggestions. This work was supported by CREST, JST.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tailoring Continuous Word Representations for Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="809" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Word Association Norms, Mutual Information and Lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="312" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Verb Noun Construction MWE Token Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravin</forename><surname>Bhutada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications</title>
		<meeting>the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Multiword Expression Data Set: Annotating Non-Compositionality and Conventionalization for English Noun Compounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghdad</forename><surname>Farahmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on Multiword Expressions</title>
		<meeting>the 11th Workshop on Multiword Expressions</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="29" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Experimental Support for a Categorical Compositional Distributional Model of Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1394" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Embeddings for Transitive Verb Disambiguation by Implicit Tensor Factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1544" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Unified Sentence Space for Categorical Distributional-Compositional Semantics: Theory and Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics</title>
		<meeting>the 24th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Resolving Lexical Ambiguity in Tensor Regression Models of Meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="212" to="217" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting Compositionality of Multi-Word Expressions using Nearest Neighbours in Vector Space Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1427" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">100 Million Words of English: the British National Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Leech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DependencyBased Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic Identification of Noncompositional Phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="317" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting a Continuum of Compositionality in Phrasal Verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment</title>
		<meeting>the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting Compositionality of VerbObject Combinations using Selectional Preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Venkatapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="369" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating Neural Word Representations in Tensor-Based Compositional Settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="708" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vector-based Models of Semantic Composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature Forest Models for Probabilistic HPSG Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="80" />
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding The Best Model Among Representative Compositional Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayasu</forename><surname>Muraoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazeto</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotaro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation</title>
		<meeting>the 28th Pacific Asia Conference on Language, Information, and Computation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Basic English Syntax with Exercises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Newton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Bölcsész Konzorcium</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Computer-Intensive Methods for Testing Hypotheses: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>WileyInterscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Jointly optimizing word representations for lexical and sentential tasks with the C-PHRASE model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Nghia The Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="971" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using Sentence Plausibility to Learn the Semantics of Transitive Verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Learning Semantics at the 2014 Conference on Neural Information Processing Systems</title>
		<meeting>Workshop on Learning Semantics at the 2014 Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An Exploration of Discourse-Based Sentence Spaces for Compositional Distributional Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics</title>
		<meeting>the First Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An Empirical Study on Compositionality in Compound Nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic Compositionality through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Neural Network Approach to Selectional Preference Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Measuring the Relative Compositionality of Verb-Noun (V-N) Collocations by Integrating Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Venkatapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="899" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Semantic Composition to Detect Non-compositionality of Multiword Expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghdad</forename><surname>Farahmand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1733" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
