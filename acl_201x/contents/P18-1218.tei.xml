<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document Similarity for Texts of Varying Lengths via Hidden Topics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarek</forename><surname>Sakakini</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">T. J. Watson Research Center</orgName>
								<orgName type="institution" key="instit2">IBM</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Document Similarity for Texts of Varying Lengths via Hidden Topics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2341" to="2351"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2341</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical , contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outper-forms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Measuring the similarity between documents is of key importance in several natural process- ing applications including information retrieval <ref type="bibr" target="#b27">(Salton and Buckley, 1988)</ref>, book recommenda- tion ( <ref type="bibr" target="#b11">Gopalan et al., 2014</ref>), news categorization ( <ref type="bibr" target="#b23">Ontrup and Ritter, 2002</ref>) and essay scoring <ref type="bibr" target="#b17">(Landauer, 2003)</ref>. A range of document similarity ap- proaches have been proposed and effectively used in recent applications including <ref type="bibr" target="#b16">(Lai et al., 2015;</ref><ref type="bibr" target="#b4">Bordes et al., 2015)</ref>. Central to the tasks discussed above is the assumption that the documents being compared are of comparable lengths.</p><p>Advances in language processing approaches to transform natural language understanding, such as text summarization and recommendation, have generated new requirements for comparing doc- uments. For instance, summarization techniques (extractive and abstractive) are capable of auto- matically generating textual summaries by con- verting a long document of several hundred words into a condensed text of only a few words while preserving the core meaning of the original text ( <ref type="bibr" target="#b13">Kedzie and McKeown, 2016)</ref>. Conceivably, a re- lated aspect of summarization is the task of bidi- rectional matching of a summary and a document or a set of documents, which is the focus of this study. The document similarity considered in this paper is between texts that have significant differ- ences not only in length, but also in the abstraction level (such as a definition of an abstract concept versus a detailed instance of that abstract concept).</p><p>As an illustration, consider the task of match- ing a Concept with a Project as shown in <ref type="table" target="#tab_0">Table 1</ref>. Here a Concept is a grade-level science curricu- lum item and represents the summary. A Project, listed in a collection of science projects, represents the document. Projects typically are long texts including an introduction, materials and proce- dures, whereas science concepts are much shorter in comparison having a title and a concise and ab- stract description. The concepts and projects are described in detail in Section 5.1. The matching task here is to automatically suggest a hands-on project for a given concept in the curriculum, such that the project can help reinforce a learner's basic understanding of the concept. Conversely, given a science project, one may need to identify the con- cept it covers by matching it to a listed concept in the curriculum. This would be conceivable in the context of an intelligent tutoring system. Challenges to the matching task mentioned above include: 1) The mismatch in the relative lengths of the documents being compared -a long piece of text (henceforth termed document) and a short piece of text (termed summary) -gives rise to the vocabulary mismatch problem, where the document and the summary do not share a ma- jority of terms. 2) The context mismatch prob- lem arising because a document provides a reason- able amount of text to infer the contextual mean- ing of a term, but a summary only provides a lim- ited context, which may or may not involve the same terms considered in the document. These challenges render existing approaches to compar- ing documents-for instance, those that rely on document representations (e.g., <ref type="bibr">Doc2Vec (Le and Mikolov, 2014)</ref>)-inadequate, because the predom- inance of non-topic words in the document intro- duces noise to its representation while the sum- mary is relatively noise-free, rendering Doc2Vec inadequate for comparing them.</p><p>Our approach to the matching problem is to al- low a multi-view generalization of the document, where multiple hidden topics are used to estab- lish a common ground to capture as much infor- mation of the document and the summary as pos- sible and use this to score the relevance of the pair. We empirically validate our approach on two tasks -that of project-concept matching in grade- level science and that of scientific paper-summary matching -using both custom-made and publicly available datasets. The main contributions of this paper are: 1. We propose an embedding-based hidden topic model to extract topics and measure their impor- tance in long documents. 2. We present a novel geometric approach to com- pare documents with differing modality (a long document to a short summary) and validate its per- formance relative to strong baselines. 3. We explore the use of domain-specific word embeddings for the matching task and show the explicit benefit of incorporating domain knowl- edge in the algorithm. 4. We make available the first dataset 1 on project- concept matching in the science domain to help further research in this area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Document similarity approaches quantify the de- gree of relatedness between two pieces of texts of comparable lengths and thus enable matching between documents. Traditionally, statistical ap- proaches (e.g., <ref type="bibr" target="#b20">(Metzler et al., 2007)</ref>) and vector- space-based methods (including the robust Latent Semantic Analysis (LSA) <ref type="bibr" target="#b10">(Dumais, 2004)</ref>) have been used for text similarity. More recently, neural network-based methods have been used for doc- ument representation and these include average word embeddings ( <ref type="bibr" target="#b21">Mikolov et al., 2013</ref>), Doc2Vec ( <ref type="bibr" target="#b18">Le and Mikolov, 2014</ref>), Skip-Thought vectors ( <ref type="bibr" target="#b14">Kiros et al., 2015)</ref>, recursive neural network- based methods <ref type="bibr" target="#b31">(Socher et al., 2014</ref>), LSTM archi- tectures ( <ref type="bibr" target="#b32">Tai et al., 2015)</ref>, and convolutional neural networks ( <ref type="bibr" target="#b2">Blunsom et al., 2014)</ref>.</p><p>Considering works that avoid using an ex- plicit document representation for comparing documents, the state-of-the-art method is Word Mover's Distance (WMD), which relies on pre- trained word embeddings ( <ref type="bibr" target="#b15">Kusner et al., 2015)</ref>. Given these embeddings, the WMD defines the distance between two documents as the best trans- port cost of moving all words from one document to another within the space of word embeddings. The advantages of WMD are that it is hyper- parameter free and achieves high retrieval accu- racy on document classification tasks with docu- ments of comparable lengths. However, it is com- putationally expensive for long documents <ref type="bibr" target="#b15">(Kusner et al., 2015)</ref>.</p><p>Clearly, what is lacking in prior literature is a study of document similarity approaches that match documents with widely different sizes. It is this gap in literature that we expect to fill by way of this study. Latent Variable Models. Latent variable mod- els including count-based and probabilistic models have been studied in many previous works. Count- based models such as Latent Semantic Indexing (LSI) compare two documents based on their com- bined vocabulary <ref type="bibr" target="#b9">(Deerwester et al., 1990</ref>). When (a) word geometry of general embedding (b) word geometry of science domain embeddings <ref type="figure">Figure 1</ref>: Two key words "forces" and "matters" are shown in red and blue respectively. Red words represent different senses of "forces", and blue words carry senses of "matters". "forces" mainly refers to "army" and "matters" refers to "issues" in general embedding of (a), whereas "forces" shows its sense of "gravity" and "matters" shows the sense of "solids" in science-domain embedding of (b) documents have highly mismatched vocabularies such as those that we study, relevant documents might be classified as irrelevant. Our model is built upon word-embeddings which is more robust to such a vocabulary mismatch. Probabilistic models such as Latent Dirichlet Analysis (LDA) define topics as distributions over words ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>). In our model, topics are low-dimensional real-valued vectors (more details in Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Domain Knowledge</head><p>Domain information pertaining to specific areas of knowledge is made available in texts by the use of words with domain-specific meanings or senses. Consequently, domain knowledge has been shown to be critical in many NLP applications such as in- formation extraction and multi-document summa- rization (Cheung and Penn, 2013a), spoken lan- guage understanding <ref type="bibr" target="#b5">(Chen et al., 2015</ref>), aspect extraction ( ) and summarization (Cheung and Penn, 2013b).</p><p>As will be described later, our distance metric for comparing a document and a summary relies on word embeddings. We show in this work, that embeddings trained on a science-domain corpus lead to better performance than embeddings on the general corpus (WikiCorpus). Towards this, we extract a science-domain sub-corpus from the WikiCorpus, and the corpus extraction will be de- tailed in Section 5.</p><p>To motivate the domain-specific behavior of polysemous words, we will qualitatively explore how domain-specific embeddings differ from the general embeddings on two polysemous science terms: forces and matters. Considering the fact that the meaning of a word is dictated by its neigh- bors, for each set of word embeddings, we plot the neighbors of these two terms in <ref type="figure">Figure 1</ref> on to 2 di- mensions using Locally Linear Embedding (LLE), which preserves word distances <ref type="bibr" target="#b26">(Roweis and Saul, 2000</ref>). We then analyze the sense of the focus terms-here, forces and matters.</p><p>From <ref type="figure">Figure 1</ref>(a), we see that for the word forces, its general embedding is close to army, soldiers, allies indicating that it is related with violence and power in a general domain. Shift- ing our attention to <ref type="figure">Figure 1</ref>(b), we see that for the same term, its science embedding is closer to torque, gravity, acceleration implying that its science sense is more about physical interactions. Likewise, for the word matters, its general embed- ding is surrounded by affairs and issues, whereas, its science embedding is closer to particles and material, prompting that it represents substances. Thus, we conclude that domain specific embed- dings (here, science), is capable of incorporat- ing domain knowledge into word representations. We use this observation in our document-summary matching system to which we turn next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>Our model that performs the matching between document and summary is depicted in <ref type="figure" target="#fig_0">Figure 2</ref>. It is composed of three modules that perform prepro- cessing, document topic generation, and relevance measurement between a document and a summary. Each of these modules is discussed below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preprocessing</head><p>The preprocessing module tokenizes texts and re- moves stop words and prepositions. This step al- lows our system to focus on the content words without impacting the meaning of original texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Topic Generation from Documents</head><p>We assume that a document (a long text) is a structured collection of words, with the 'structure' brought about by the composition of topics. In some sense, this 'structure' is represented as a set of hidden topics. Thus, we assume that a docu- ment is generated from certain hidden "topics", analogous to the modeling assumption in LDA. However, unlike in LDA, the "topics" here are nei- ther specific words nor the distribution over words, but are are essentially a set of vectors. In turn, this means that words (represented as vectors) con- stituting the document structure can be generated from the hidden topic vectors.</p><p>Introducing some notation, the word vectors in a document are {w 1 , . . . , w n }, and the hidden topic vectors of the document are {h 1 , . . . , h K }, where w i , h k 2 R d , d = 300 in our experiments.</p><p>Linear operations using word embeddings have been empirically shown to approximate their compositional properties (e.g. the embedding of a phrase is nearly the sum of the embed- dings of its component words) ( <ref type="bibr" target="#b21">Mikolov et al., 2013)</ref>. This motivates the linear reconstruction of the words from the document's hidden topics while minimizing the reconstruction error. We stack the K topic vectors as a topic matrix</p><formula xml:id="formula_0">H = [h 1 , . . . , h K ](K &lt; d).</formula><p>We define the reconstructed word vector˜wvector˜ vector˜w i for the word w i as the optimal linear approximation given by topic vectors:</p><formula xml:id="formula_1">˜ w i = H ˜ ↵ i , where˜↵</formula><p>where˜ where˜↵ i = argmin</p><formula xml:id="formula_2">↵ i 2R K kw i H↵ i k 2 2 .<label>(1)</label></formula><p>The reconstruction error E for the whole docu- ment is the sum of each word's reconstruction er- ror and is given by:</p><formula xml:id="formula_3">E = n P i=1 kw i ˜ w i k 2 2</formula><p>. This being a function of the topic vectors, our goal is to find the optimal H ⇤ so as to minimize the error E:</p><formula xml:id="formula_4">H ⇤ = argmin H2R d⇥K E(H) = argmin H2R d⇥K n X i=1 min ↵ i kw i H↵ i k 2 2 ,<label>(2)</label></formula><p>where k·k is the Frobenius norm of a matrix. Without loss of generality, we require the topic vectors {h i } K i=1 to be orthonormal, i.e., h T i h j = 1 (i=j) . As we can see, the optimization problem (2) describes an optimal linear space spanned by the topic vectors, so the norm and the linear de- pendency of the vectors do not matter. With the orthonormal constraints, we simplify the form of the reconstructed vector˜wvector˜ vector˜w i as:</p><formula xml:id="formula_5">˜ w i = HH T w i .<label>(3)</label></formula><p>We stack word vectors in the document as a matrix W = [w 1 , . . . , w n ]. The equivalent formulation to problem (2) is:</p><formula xml:id="formula_6">min H kW HH T Wk 2 2 s.t. H T H = I,<label>(4)</label></formula><p>where I is an identity matrix. The problem can be solved by Singular Value Decomposition (SVD), using which, the matrix W can be decomposed as W = U⌃V T , where U T U = I,V T V = I, and ⌃ is a diagonal ma- trix where the diagonal elements are arranged in a decreasing order of absolute values. We show in the supplementary material that the first K vec- tors in the matrix U are exactly the solution to</p><formula xml:id="formula_7">H ⇤ = [h ⇤ 1 , . . . , h ⇤ K ]. We find optimal topic vectors H ⇤ = [h ⇤ 1 , . . . , h ⇤ K ]</formula><p>by solving problem (4). We note that these topic vectors are not equally important, and we say that one topic is more important than another if it can reconstruct words with smaller error. Define E k as the reconstruc- tion error when we only use topic vector h ⇤ k to reconstruct the document:</p><formula xml:id="formula_8">E k = kW h ⇤ k h ⇤ k T Wk 2 2 .<label>(5)</label></formula><p>Now define i k as the importance of topic h ⇤ k , which measures the topic's ability to reconstruct the words in a document:</p><formula xml:id="formula_9">i k = kh ⇤ k T Wk 2 2 (6)</formula><p>We show in the supplementary material that the higher the importance i k is, the smaller the recon- struction error E k is. Now we normalize i k as ¯ i k so that the importance does not scale with the norm of the word matrix W , and so that the importances of the K topics sum to 1. Thus,</p><formula xml:id="formula_10">¯ i k = i k /( K X j=1 i j ).<label>(7)</label></formula><p>The number of topics K is a hyperparameter in our model. A small K may not cover key ideas of the document, whereas a large K may keep trivial and noisy information. Empirically we find that K = 15 captures most important information from the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Topic Mapping to Summaries</head><p>We have extracted K topic vectors {h ⇤ k } K k=1 from the document matrix W, whose importance is re- flected by { ¯ i k } K k=1 . In this module, we measure the relevance of a document-summary pair. To- wards this, a summary that matches the document should also be closely related with the "topics" of that document. Suppose the vectors of the words in a summary are stacked as a d ⇥ m ma- trix S = [s 1 , . . . , s m ], where s j is the vector of the j-th word in a summary. Similar to the recon- struction of the document, the summary can also be reconstructed from the documents' topic vec- tors as shown in Eq. (3). Let˜sLet˜Let˜s k j be the reconstruc- tion of the summary word s j given by one topic</p><formula xml:id="formula_11">h ⇤ k : ˜ s k j = h ⇤ k h ⇤ k T s j . Let r(h ⇤ k , s j )</formula><p>be the relevance between a topic vector h ⇤ k and summary word s j . It is defined as the cosine similarity betweeñ s k j and s j :</p><formula xml:id="formula_12">r(h ⇤ k , s j ) = s j T ˜ s k j /(ks j k 2 · k˜sk˜s k j k 2 ).<label>(8)</label></formula><p>Furthermore, let r(h ⇤ k , S) be the relevance be- tween a topic vector and the summary, defined to be the average similarity between the topic vector and the summary words:</p><formula xml:id="formula_13">r(h ⇤ k , S) = 1 m m X j=1 r(h ⇤ k , s j ).<label>(9)</label></formula><p>The relevance between a topic vector and a sum- mary is a real value between 0 and 1. As we have shown, the topics extracted from a document are not equally important. Natu- rally, a summary relevant to more important top- ics is more likely to better match the document. Therefore, we define r(W, S) as the relevance be- tween the document W and the summary S, and r(W, S) is the sum of topic-summary relevance weighted by the importance of the topic:</p><formula xml:id="formula_14">r(W, S) = K X k=1 ¯ i k · r(h ⇤ k , S),<label>(10)</label></formula><p>where ¯ i k is the importance of topic h ⇤ k as defined in (7). The higher r(W, S) is, the better the sum- mary matches the document.</p><p>We provide a visual representation of the doc- uments as shown in <ref type="figure" target="#fig_1">Figure 3</ref> to illustrate the no- tion of hidden topics. The two documents are from science projects: a genetics project, Pedigree Analysis: A Family Tree of Traits (ScienceBud- dies, 2017a), and a weather project, How Do the Seasons Change in Each Hemisphere (Science- Buddies, 2017b). We project all embeddings to a three-dimensional space for ease of visualization.</p><p>As seen in <ref type="figure" target="#fig_1">Figure 3</ref>, the hidden topics recon- struct the words in their respective documents to the extent possible. This means that the words of a document lie roughly on the plane formed by their corresponding topic vectors. We also notice that the summary words (heredity and weather respec- tively for the two projects under consideration) lie very close to the plane formed by the hidden topics of the relevant project while remaining away from the plane of the irrelevant project. This shows that the words in the summary (and hence the summary itself) can also be reconstructed from the hidden topics of documents that match the summary (and are hence 'relevant' to the summary). <ref type="figure" target="#fig_1">Figure 3</ref> visually explains the geometric relations between the summaries, the hidden topics and the docu- ments. It also validates the representation power of the extracted hidden topic vectors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate our document- summary matching approach on two specific ap- plications where texts of different sizes are com- pared. One application is that of concept-project matching useful in science education and the other is that of summary-research paper matching.</p><p>Word Embeddings. Two sets of 300- dimension word embeddings were used in our ex- periments. They were trained by the Continu- ous Bag-of-Words (CBOW) model in word2vec ( <ref type="bibr" target="#b21">Mikolov et al., 2013</ref>) but on different corpora. One training corpus is the full English WikiCor- pus of size 9 GB (Al- <ref type="bibr" target="#b0">Rfou et al., 2013</ref>). The second consists of science articles extracted from the WikiCorpus. To extract these science articles, we manually selected the science categories in Wikipedia and considered all subcategories within a depth of 3 from these manually selected root categories. We then extracted all articles in the aforementioned science categories resulting in a science corpus of size 2.4 GB. The word vectors used for documents and summaries are both from the pretrained word2vec embeddings. Baselines We include two state-of-the-art methods of measuring document similarity for comparison using their implementations available in gensim <ref type="bibr">( ˇ Rehůřek and Sojka, 2010)</ref>.</p><p>(1) Word movers' distance (WMD) <ref type="bibr" target="#b15">(Kusner et al., 2015)</ref>. WMD quantifies the distance be- tween a pair of documents based on word em- beddings as introduced previously (c.f. Related Work). We take the negative of their distance as a measure of document similarity (here between a document and a summary). (2) Doc2Vec ( <ref type="bibr" target="#b18">Le and Mikolov, 2014</ref>). Document representations have been trained with neural net- works. We used two versions of doc2vec: one trained on the full English Wikicorpus and a sec- ond trained on the science corpus, same as the cor- pora used for word embedding training. We used the cosine similarity between two text vectors to measure their relevance.</p><p>For a given document-summary pair, we com- pare the scores obtained using the above two meth- ods with that obtained using our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Concept-Project matching</head><p>Science projects are valuable resources for learn- ers to instigate knowledge creation via experimen- tation and observation. The need for matching a science concept with a science project arises when learners intending to delve deeper into certain con- cepts search for projects that match a given con- cept. Additionally, they may want to identify the concepts with which a set of projects are related.</p><p>We note that in this task, science concepts are highly concise summaries of the core ideas in projects, whereas projects are detailed instructions of the experimental procedures, including an intro- duction, materials and a description of the proce- dure, as shown in <ref type="table" target="#tab_0">Table 1</ref>. Our matching method provides a way to bridge the gap between abstract concepts and detailed projects. The format of the concepts and the projects is discussed below. Concepts. For the purpose of this study we use the concepts available in the Next Generation Sci- ence Standards (NGSS) <ref type="bibr" target="#b22">(NGSS, 2017)</ref>. Each con- cept is accompanied by a short description. For example, one concept in life science is Heredity: Inheritance and Variation of Traits. Its descrip- tion is All cells contain genetic information in the form of DNA molecules. Genes are regions in the DNA that contain the instructions that code for the formation of proteins. Typical lengths of concepts are around 50 words. Projects. The website Science Buddies (Science- Buddies, 2017c) provides a list of projects from a variety of science and engineering disciplines such <ref type="table">Table 2</ref>: Classification results for the Concept-Project Matching task. All performance differences were statistically significant at p = 0.01. method topic science topic wiki wmd science wmd wiki doc2vec science doc2vec wiki precision 0.758 ± 0.012 0.750 ± 0.009 0.643 ± 0.070 0.568 ± 0.055 0.615 ± 0.055 0.661 ± 0.084 recall 0.885 ± 0.071 0.842 ± 0.010 0.735 ± 0.119 0.661 ± 0.119 0.843 ± 0.066 0.737 ± 0.149 fscore 0.818 ± 0.028 0.791 ± 0.007 0.679 ± 0.022 0.595 ± 0.020 0.695 ± 0.019 0.681 ± 0.032</p><p>as physical sciences, life sciences and social sci- ences. A typical project consists of an abstract, an introduction, a description of the experiment and the associated procedures. A project typically has more than 1000 words.</p><p>Dataset. We prepared a representative dataset 537 pairs of projects and concepts involving 53 unique concepts from NGSS and 230 unique projects from Science Buddies. Engineering undergrad- uate students annotated each pair with the deci- sion whether it was a good match or not and re- ceived research credit. As a result, each concept- project pair received at least three annotations, and upon consolidation, we considered a concept- project pair to be a good match when a majority of the annotators agreed. Otherwise it was not considered a good match. The ratio between good matches and bad matches in the collected data was 44 : 56. Classification Evaluation. Annotations from stu- dents provided the ground truth labels for the clas- sification task. We randomly split the dataset into tuning and test instances with a ratio of 1 : 9. A threshold score was tuned on the tuning data, and concept-project pairs with scores higher than this threshold were classified as a good matches dur- ing testing. We performed 10-fold cross valida- tion, and report the average precision, recall, F1 score and their standard deviation in <ref type="table">Table 2</ref>.</p><p>Our topic-based metric is denoted as "topic", and the general-domain and science-domain em- beddings are denoted as "wiki" and "science" respectively. We show the performance of our method against the two baselines while vary- ing the underlying embeddings, thus resulting in 6 different combinations.</p><p>For example, "topic science" refers to our method with science embeddings. From the table (column 1) we notice the following: 1) Our method significantly outper- forms the two baselines by a wide margin (⇡10%) in both the general domain setting as well as the domain-specific setting. 2) Using science domain- specific word embeddings instead of the general word embeddings results in the best performance across all algorithms. This performance was ob- served despite the word embeddings being trained on a significantly smaller corpus compared to the general domain corpus.</p><p>Besides the classification metrics, we also eval- uated the directed matching from concepts to projects with ranking metrics. Ranking Evaluation Our collected dataset re- sulted in having a many-to-many matching be- tween concepts and projects. This is because the same concept was found to be a good match for multiple projects and the same project was found to match many concepts. The previously described classification task evaluated the bidirec- tional concept-project matching. Next we eval- uated the directed matching from concepts to projects, to see how relevant these top ranking projects are to a given input concepts. Here we use precision@k <ref type="bibr" target="#b24">(Radlinski and Craswell, 2010)</ref> as the evaluation metric, considering the percent- age of relevant ones among top-ranking projects.</p><p>For this part, we only considered the methods using science domain embeddings as they have shown superior performance in the classificaiton task. For each concept, we check the precision@k of matched projects and place it in one of k+1 bins accordingly. For example, for k=3, if only two of the three top projects are a correct match, the con- cept is placed in the bin corresponding to 2/3. In <ref type="figure" target="#fig_2">Figure 4</ref>, we show the percentage of concepts that fall into each bin for the three different algorithms for k=1,3,6.</p><p>We observe that recommendations using the hidden topic approach fall more in the high value bin compared to others, performing consistently better than two strong baselines. The advan- tage becomes more obvious at precision@6. It is worth mentioning that wmd science falls behind doc2vec science in the classification task while it outperforms in the ranking task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Text Summarization</head><p>The task of matching summaries and documents is commonly seen in real life. For example, we use an event summary "Google's AlphaGo beats Korean player Lee Sedol in Go" to search for rel- evant news, or use the summary of a scientific pa- per to look for related research publications. Such matching constitutes an ideal task to evaluate our matching method between texts of different sizes.</p><p>Dataset. We use a dataset from the CL-SciSumm Shared Task ( <ref type="bibr" target="#b12">Jaidka et al., 2016</ref>). The dataset con- sists of 730 ACL Computational Linguistics re- search papers covering 50 categories in total. Each category consists of a reference paper (RP) and around 10 citing papers (CP) that contain citations to the RP. A human-generated summary for the RP is provided and we use the 10 CP as being relevant to the summary. The matching task here is be- tween the summary and all CPs in each category. Evaluation. For each paper, we keep all of its content except the sections of experiments and ac- knowledgement (these sections were omitted be- cause often their content is often less related to the topic of the summary). The typical summary length is about 100 words, while a paper has more than 2000 words. For each topic, we rank all 730 papers in terms of their relevance generated by our method and baselines using both sets of embed- dings. For evaluation, we use the information re- trieval measure of precision@k, which considers the number of relevant matches in the top-k match- ings ( <ref type="bibr" target="#b19">Manning et al., 2008</ref>). For each combination of the text similarity approaches and embeddings, we show precision@k for different k's in <ref type="figure">Figure 5</ref>. We observe that our method with science embed- ding achieves the best performance compared to the baselines, once again showing not only the benefits of our method but also that of incorpo- rating domain knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Analysis of Results. From the results of the two tasks we observe that our method outperforms two <ref type="figure">Figure 5</ref>: Summary-Article Matching strong baselines. The reason for WMD's poor performance could be that the many uninforma- tive words (those unrelated to the central topic) make WMD overestimate the distance between the document-summary pair. As for doc2vec, its sin- gle vector representation may not be able to cap- ture all the key topics of a document. A project could contain multifaceted information, e.g., a project to study how climate change affects grain production is related to both environmental sci- ence and agricultural science.</p><p>Effect of Topic Number. The number of hid- den topics K is a hyperparameter in our setting. We empirically evaluate the effect of topic number in the task of concept-project mapping. <ref type="figure">Figure 6</ref> shows the F1 scores and the standard deviations at different K. As we can see, optimal K is 18. When K is too small, hidden topics are too few to capture key information in projects. Thus we can see that the increase of topic number from 3 to 6 brings a big improvement to the performance. Topic numbers larger than the optimal value de- grade the performance since more topics incorpo- rate noisy information. We note that the perfor- <ref type="figure">Figure 6</ref>: F1 score on concept-project matching with different topic numbers K mance changes are mild when the number of top- ics are in the range of <ref type="bibr">[18,</ref><ref type="bibr">31]</ref>. Since topics are weighted by their importance, the effect of noisy information from extra hidden topics is mitigated. Interpretation of Hidden Topics. We consider the summary-paper matching as an example with around 10 papers per category. We extracted the hidden topics from each paper, reconstructed words with these topics as shown in Eq. <ref type="formula" target="#formula_5">(3)</ref>, and selected the words which had the smallest recon- struction errors. These words are thus closely re- lated to the hidden topics, and we call them topic words to serve as an interpretation of the hid- den topics. We visualize the cloud of such topic words on the set of papers about word sense dis- ambiguation as shown in <ref type="figure">Figure 7</ref>. We see that the words selected based on the hidden topics cover key ideas such as disambiguation, represent, clas- sification and sentence. This qualitatively vali- dates the representation power of hidden topics. More examples are available in the supplementary material.</p><p>We interpret this to mean that proposed idea of multiple hidden topics captures the key informa- tion of a document. The extracted "hidden top- ics" represent the essence of documents, suggest- ing the appropriateness of our relevance metric to measure the similarity between texts of different sizes. Even though our focus in this study was the science domain we point out that the results are more generally valid since we made no domain- specific assumptions. Varying Sensitivity to Domain. As shown in the results, the science-domain embeddings improved the classification of concept-project matching for <ref type="figure">Figure 7</ref>: Topic words from papers on word sense disambiguation the topic-based method by 2% in F1-score, WMD by 8% and doc2vec by 1%, thus underscoring the importance of domain-specific word embeddings.</p><p>Doc2vec is less sensitive to the domain, because it provides document-level representation. Even if some words cannot be disambiguated due to the lack of domain knowledge, other words in the same document can provide complementary infor- mation so that the document embedding does not deviate too much from its true meaning.</p><p>Our method, also a word embedding method, is not as sensitive to domain as WMD. It is robust to the polysemous words with domain-sensitive semantics, since hidden topics are extracted in the document level. Broader contexts beyond just words provide complementary information for word sense disambiguation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We propose a novel approach to matching docu- ments and summaries. The challenge we address is to bridge the gap between detailed long texts and its abstraction with hidden topics. We incorpo- rate domain knowledge into the matching system to gain further performance improvement. Our approach has beaten two strong baselines in two downstream applications, concept-project match- ing and summary-research paper matching.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The system for document-summary matching</figDesc><graphic url="image-3.png" coords="4,138.67,62.81,317.48,76.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Words mode and genes from the document on genetics and words storm and atmospheric from document on weather are represented by pink and blue points respectively. Linear space of hidden topics in genetics form the pink plane, where summary word heredity (the red point) roughly lies. Topic vectors of the document on weather form the blue plane, and the summary word weather (the darkblue point) lies almost on the same plane.</figDesc><graphic url="image-4.png" coords="6,93.60,62.81,172.34,160.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ranking Performance of All Methods</figDesc><graphic url="image-8.png" coords="8,307.56,212.87,217.69,169.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : A Sample Concept-Project Matching</head><label>1</label><figDesc>Concept Heredity: Inheritance and Variation of Traits All cells contain genetic information in the form of DNA molecules. Genes are regions in the DNA that contain the instructions that code for the formation of proteins. Project Pedigree Analysis: A Family Tree of Traits Do you have the same hair color or eye color as your mother? When we look at members of a family it is easy to see that some physical characteristics or traits are shared. To start this project, you should draw a pedigree showing the different members of your family. Ideally you should include multiple people from at least three generations.</figDesc><table></table></figure>

			<note place="foot" n="1"> Our code and data are available at: https: //github.com/HongyuGong/DocumentSimilarity-via-Hidden-Topics.git</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by IBM-ILLINOIS Cen-ter for Cognitive Computing Systems Research (C3SR)-a research collaboration as part of the IBM AI Horizons Network. We thank ACL anonymous reviewers for their constructive sug-gestions. We thank Mathew Monfort for helping deploy annotation tasks, and thank Jong Yoon Lee for the dataset collection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Matrix factorization with knowledge graph propagation for unsupervised spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander I</forename><surname>Rudnicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="483" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting domain knowledge in aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meichun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malu</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riddhiman</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1655" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Probabilistic domain modelling with contextualized distributional semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><forename type="middle">Chi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards robust abstractive multi-document summarization: A caseframe analysis of centrality and domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackie</forename><forename type="middle">Chi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1233" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Indexing by latent semantic analysis. Journal of the American society for information science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">391</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Latent semantic analysis. Annual review of information science and technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="188" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Content-based recommendations with poisson factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Prem K Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3176" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of the cl-scisumm 2016 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kokil</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muthu</forename><surname>Kumar Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sajal</forename><surname>Rustagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries</title>
		<meeting>Joint Workshop on Bibliometric-enhanced Information Retrieval and NLP for Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extractive and abstractive event summarization over streaming web text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Kedzie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4002" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">333</biblScope>
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic essay assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas K Landauer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="295" to="308" />
		</imprint>
	</monogr>
	<note>Assessment in education: Principles, policy &amp; practice</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Similarity measures for short segments of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngss</forename></persName>
		</author>
		<ptr target="https://www.nextgenscience.org" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2017" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hyperbolic selforganizing maps for semantic navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Ontrup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Comparing the sensitivity of information retrieval metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 33rd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="667" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Nonlinear dimensionality reduction by locally linear embedding. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence K</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Termweighting approaches in automatic text retrieval. Information processing &amp; management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Available at: https</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sciencebuddies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Available at: https</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sciencebuddies</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sciencebuddies</surname></persName>
		</author>
		<ptr target="http://www.sciencebuddies.org" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2017" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
