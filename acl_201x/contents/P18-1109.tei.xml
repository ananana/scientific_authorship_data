<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gaussian Mixture Latent Vector Grammars</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gaussian Mixture Latent Vector Grammars</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1181" to="1189"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1181</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce Latent Vector Grammars (LVeGs), a new framework that extends latent variable grammars such that each non-terminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the non-terminal. We show that previous models such as latent variable grammars and com-positional vector grammars can be interpreted as special cases of LVeGs. We then present Gaussian Mixture LVeGs (GM-LVeGs), a new special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals. A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the inside-outside algorithm, which enables efficient inference and learning. We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies. Our code is available at https://github.com/zhaoyanpeng/lveg.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In constituency parsing, refining coarse syntactic categories of treebank grammars <ref type="bibr" target="#b0">(Charniak, 1996)</ref> into fine-grained subtypes has been proven effec- tive in improving parsing results. Previous ap- proaches to refining syntactic categories use tree annotations <ref type="bibr" target="#b5">(Johnson, 1998)</ref>, lexicalization <ref type="bibr" target="#b1">(Charniak, 2000;</ref><ref type="bibr" target="#b2">Collins, 2003)</ref>, or linguistically mo- tivated category splitting ( <ref type="bibr" target="#b7">Klein and Manning, 2003)</ref>. <ref type="bibr" target="#b9">Matsuzaki et al. (2005)</ref> introduce latent variable grammars, in which each syntactic cate- gory (represented by a nonterminal) is split into a fixed number of subtypes and a discrete latent variable is used to indicate the subtype of the nonterminal when it appears in a specific parse tree. Since the latent variables are not observ- able in treebanks, the grammar is learned using expectation-maximization. <ref type="bibr" target="#b11">Petrov et al. (2006)</ref> present a split-merge approach to learning la- tent variable grammars, which hierarchically splits each nonterminal and merges ineffective splits. <ref type="bibr" target="#b14">Petrov and Klein (2008b)</ref> further allow a nonter- minal to have different splits in different produc- tion rules, which results in a more compact gram- mar.</p><p>Recently, neural approaches become very pop- ular in natural language processing (NLP). An im- portant technique in neural approaches to NLP is to represent discrete symbols such as words and syntactic categories with continuous vectors or embeddings. Since the distances between such vector representations often reflect the sim- ilarity between the corresponding symbols, this technique facilitates more informed smoothing in learning functions of symbols (e.g., the probability of a production rule). In addition, what a symbol represents may subtly depend on its context, and a continuous vector representation has the potential of representing each instance of the symbol in a more precise manner. For constituency parsing, recursive neural networks <ref type="bibr" target="#b17">(Socher et al., 2011)</ref> and their extensions such as compositional vector grammars <ref type="bibr" target="#b16">(Socher et al., 2013)</ref> can be seen as rep- resenting nonterminals in a context-free grammar with continuous vectors. However, exact inference in these models is intractable.</p><p>In this paper, we introduce latent vector gram- mars (LVeGs), a novel framework of grammars with fine-grained nonterminal subtypes. A LVeG associates each nonterminal with a continuous vector space that represents the set of (infinitely many) subtypes of the nonterminal. For each in-stance of a nonterminal that appears in a parse tree, its subtype is represented by a latent vector. For each production rule over nonterminals, a non- negative continuous function specifies the weight of any fine-grained production rule over subtypes of the nonterminals. Compared with latent vari- able grammars which assume a small fixed num- ber of subtypes for each nonterminal, LVeGs as- sume an unlimited number of subtypes and are potentially more expressive. By having weight functions of varying smoothness for different pro- duction rules, LVeGs can also control the level of subtype granularity for different productions, which has been shown to improve the parsing ac- curacy ( <ref type="bibr" target="#b14">Petrov and Klein, 2008b</ref>). In addition, similarity between subtypes of a nonterminal can be naturally modeled by the distance between the corresponding vectors, so by using continuous and smooth weight functions we can ensure that simi- lar subtypes will have similar syntactic behaviors.</p><p>We further present Gaussian Mixture LVeGs (GM-LVeGs), a special case of LVeGs that uses mixtures of Gaussian distributions as the weight functions of fine-grained production rules. A ma- jor advantage of GM-LVeGs is that the partition function and the expectations of fine-grained pro- duction rules can be computed using an extension of the inside-outside algorithm. This makes it pos- sible to efficiently compute the gradients during discriminative learning of GM-LVeGs. We evalu- ate GM-LVeGs on part-of-speech tagging and con- stituency parsing on a variety of languages and corpora and show that GM-LVeGs achieve com- petitive results.</p><p>It shall be noted that many modern state-of- the-art constituency parsers predict how likely a constituent is based on not only local information (such as the production rules used in composing the constituent), but also contextual information of the constituent. For example, the neural CRF parser <ref type="bibr" target="#b3">(Durrett and Klein, 2015)</ref> looks at the words before and after the constituent; and RNNG <ref type="bibr" target="#b4">(Dyer et al., 2016</ref>) looks at the constituents that are al- ready predicted (in the stack) and the words that are not processed (in the buffer). In this paper, however, we choose to focus on the basic frame- work and algorithms of LVeGs and leave the in- corporation of contextual information for future work. We believe that by laying a solid foundation for LVeGs, our work can pave the way for many interesting extensions of LVeGs in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Latent Vector Grammars</head><p>A latent vector grammar (LVeG) considers sub- types of nonterminals as continuous vectors and associates each nonterminal with a latent vector space representing the set of its subtypes. For each production rule, the LVeG defines a weight func- tion over the subtypes of the nonterminal involved in the production rule. In this way, it models the space of refinements of the production rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Definition</head><p>A latent vector grammar is defined as a 5-tuple G = (N, S, Σ, R, W ), where N is a finite set of nonterminal symbols, S ∈ N is the start sym- bol, Σ is a finite set of terminal symbols such that N ∩Σ = ∅, R is a set production rules of the form X γ where X ∈ N and γ ∈ (N ∪ Σ) * , W is a set of rule weight functions indexed by production rules in R (to be defined below). In the following discussion, we consider R in the Chomsky normal form (CNF) for clarity of presentation. However, it is straightforward to extend our formulation to the general case.</p><p>Unless otherwise specified, we always use cap- ital letters A, B, C, . . . for nonterminal symbols and use bold lowercase letters a, b, c, . . . for their subtypes. Note that subtypes are represented by continuous vectors. For a production rule of the form A BC, its weight function is W ABC (a, b, c). For a production rule of the form A w where w ∈ Σ, its weight func- tion is W Aw (a). The weight functions should be non-negative, continuous and smooth, and hence fine-grained production rules of similar sub- types of a nonterminal would have similar weight assignments. Rule weights can be normalized such that</p><formula xml:id="formula_0">B,C b,c W ABC (a, b, c)dbdc = 1</formula><p>, which leads to a probabilistic context-free gram- mar (PCFG). Whether the weights are normalized or not leads to different model classes and accord- ingly different estimation methods. However, the two model classes are proven equivalent by <ref type="bibr" target="#b15">Smith and Johnson (2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relation to Other Models</head><p>Latent variable grammars (LVGs) ( <ref type="bibr" target="#b9">Matsuzaki et al., 2005;</ref><ref type="bibr" target="#b11">Petrov et al., 2006</ref>) associate each nonterminal with a discrete latent vari- able, which is used to indicate the subtype of the nonterminal when it appears in a parse tree.</p><p>Through nonterminal-splitting and the expectation-maximization algorithm, fine-grained production rules can be automatically induced from a treebank. We show that LVGs can be seen as a special case of LVeGs. Specifically, we can use one-hot vec- tors in LVeGs to represent latent variables in LVGs and define weight functions in LVeGs accordingly. Consider a production rule r : A BC. In a LVG, each nonterminal is split into a number of subtypes. Suppose A, B, and C are split into n A , n B , and n C subtypes respectively. a x is the x-th subtype of A, b y is the y-th subtype of B, and c z is the z-th subtype of C. a x b y c z is a fine- grained production rule of A BC, where x = 1, . . . , n A , y = 1, . . . , n B , and z = 1, . . . , n C . The probabilities of all the fine-grained produc- tion rules can be represented by a rank-3 tensor Θ ABC ∈ R n A ×n B ×n C . To cast the LVG as a LVeG, we require that the latent vectors in the LVeG must be one-hot vectors. We achieve this by defining weight functions that output zero if any of the input vectors is not one-hot. Specifically, we define the weight function of the production rule A BC as:</p><formula xml:id="formula_1">W r (a, b, c) = x,y,z Θ ABC cba × (δ(a − a x ) × δ(b − b y ) × δ(c − c z )) ,<label>(1)</label></formula><p>where δ(·) is the Dirac delta function, a x ∈ R n A , b y ∈ R n B , c z ∈ R n C are one-hot vectors (which are zero everywhere with the exception of a single 1 at the x-th index of a x , the y-th index of b y , and the z-th index of c z ) and Θ ABC is multiplied sequentially by c, b, and a. Compared with LVGs, LVeGs have the follow- ing advantages. While a LVG contains a finite, typically small number of subtypes for each non- terminal, a LVeG uses a continuous space to rep- resent an infinite number of subtypes. When equipped with weight functions of sufficient com- plexity, LVeGs can represent more fine-grained syntactic categories and production rules than LVGs. By controlling the complexity and smooth- ness of the weight functions, a LVeG is also ca- pable of representing any level of subtype gran- ularity. Importantly, this allows us to change the level of subtype granularity for the same nonter- minal in different production rules, which is sim- ilar to multi-scale grammars <ref type="bibr" target="#b14">(Petrov and Klein, 2008b</ref>). In addition, with a continuous space of subtypes in a LVeG, similarity between subtypes can be naturally modeled by their distance in the space and can be automatically learned from data. Consequently, with continuous and smooth weight functions, fine-grained production rules over simi- lar subtypes would have similar weights in LVeGs, eliminating the need for the extra smoothing steps that are necessary in training LVGs.</p><p>Compositional vector grammars (CVGs) ( <ref type="bibr" target="#b16">Socher et al., 2013)</ref>, an extension of recursive neural networks (RNNs) <ref type="bibr" target="#b17">(Socher et al., 2011</ref>), can also be seen as a special case of LVeGs. For a production rule r : A BC, a CVG can be interpreted as specifying its weight function W r (a, b, c) in the following way. First, a neural network f indexed by B and C is used to compute a parent vector p = f BC (b, c). Next, the score of the parent vector is computed using a base PCFG and a vector v BC :</p><formula xml:id="formula_2">s(p) = v T BC p + log P (A BC) ,<label>(2)</label></formula><p>where P (A BC) is the rule probability from the base PCFG. Then, the weight function of the production rule A BC is defined as:</p><formula xml:id="formula_3">W r (a, b, c) = exp (s(p)) × δ(a − p) .<label>(3)</label></formula><p>This form of weight functions in CVGs leads to point estimation of latent vectors in a parse tree, i.e., for each nonterminal in a given parse tree, only one subtype in the whole subtype space would lead to a non-zero weight of the parse. In addition, different parse trees of the same sub- string typically lead to different point estimations of the subtype vector at the root nonterminal. Con- sequently, CVGs cannot use dynamic program- ming for inference and hence have to resort to greedy search or beam search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gaussian Mixture LVeGs</head><p>A major challenge in applying LVeGs to parsing is that it is impossible to enumerate the infinite num- ber of subtypes. Previous work such as CVGs re- sorts to point estimation and greedy search. In this section we present Gaussian Mixture LVeGs (GM- LVeGs), which use mixtures of Gaussian distribu- tions as the weight functions in LVeGs. Because Gaussian mixtures have the nice property of being closed under product, summation, and marginal- ization, we can compute the partition function and the expectations of fine-grained production rules using dynamic programming. This in turn makes efficient learning and parsing possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation</head><p>In a GM-LVeG, the weight function of a produc- tion rule r is defined as a Gaussian mixture con- taining K r mixture components:</p><formula xml:id="formula_4">W r (r) = Kr k=1 ρ r,k N (r|µ r,k , Σ r,k ) ,<label>(4)</label></formula><p>where r is the concatenation of the latent vectors of the nonterminals in r, which denotes a fine- grained production rule of r. ρ r,k &gt; 0 is the k-th mixture weight (the mixture weights do not nec- essarily sum up to 1), N (r|µ r,k , Σ r,k ) is the k-th Gaussian distribution parameterized by mean µ r,k and covariance matrix Σ r,k , and K r is the num- ber of mixture components, which can be differ- ent for different production rules. Below we write N (r|µ r,k , Σ r,k ) as N r,k (r) for brevity. Given a production rule of the form A BC, the GM-</p><formula xml:id="formula_5">LVeG expects r = [a; b; c] and a, b, c ∈ R d ,</formula><p>where d is the dimension of the vectors a, b, c. We use the same dimension for all the subtype vectors. For the sake of computational efficiency, we use diagonal or spherical Gaussian distributions, whose covariance matrices are diagonal, so that the inverse of covariance matrices in Equation 15- 16 can be computed in linear time. A spherical Gaussian has a diagonal covariance matrix where all the diagonal elements are equal, so it has fewer free parameters than a diagonal Gaussian and re- sults in faster learning and parsing. We empiri- cally find that spherical Gaussians lead to slightly better balance between the efficiency and the pars- ing accuracy than diagonal Gaussians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parsing</head><p>The goal of parsing is to find the most probable parse tree T * with unrefined nonterminals for a sentence w of n words w 1:n = w 1 . . . w n . This is formally defined as:</p><formula xml:id="formula_6">T * = argmax T ∈G(w) P (T |w) ,<label>(5)</label></formula><p>where G(w) denotes the set of parse trees with unrefined nonterminals for w. In a PCFG, T * can be found using dynamic programming such as the CYK algorithm. However, parsing becomes in- tractable with LVeGs, and even with LVGs, the special case of LVeGs. A common practice in parsing with LVGs is to use max-rule parsing ( <ref type="bibr" target="#b11">Petrov et al., 2006;</ref><ref type="bibr" target="#b12">Petrov and Klein, 2007)</ref>. The basic idea of max-rule parsing is to decompose the posteriors over parses into the posteriors over production rules approx- imately. This requires calculating the expected counts of unrefined production rules in parsing the input sentence. Since Gaussian mixtures are closed under product, summation, and marginal- ization, in GM-LVeGs the expected counts can be calculated using the inside-outside algorithm in the following way. Given a sentence w 1:n , we first calculate the inside score s <ref type="table" target="#tab_0">Table 1</ref>, we calculate the score s(A BC, i, k, j)</p><note type="other">A I (a, i, j) and out- side score s A O (a, i, j) for a nonterminal A over a span w i:j using Equation 6 and Equation 7 in Ta- ble 1 respectively. Note that both s A I (a, i, j) and s A O (a, i, j) are mixtures of Gaussian distributions of the subtype vector a. Next, using Equation 8 in</note><formula xml:id="formula_7">(1 ≤ i ≤ k &lt; j ≤ n),</formula><note type="other">where A BC, i, k, j represents a production rule A BC with nonter- minals A, B, and C spanning words w i:j , w i,k , and w k+1:j respectively in the sentence w 1:n . Then the expected count (or posterior) of A BC, i, k, j is calculated as:</note><formula xml:id="formula_8">q(A BC, i, k, j) = s(A BC, i, k, j) s I (S, 1, n) ,<label>(9)</label></formula><p>where s I (S, 1, n) is the inside score for the start symbol S spanning the whole sentence w 1:n . Af- ter calculating all the expected counts, we can use the MAX-RULE-PRODUCT algorithm <ref type="bibr" target="#b12">(Petrov and Klein, 2007</ref>) for parsing, which returns a parse with the highest probability that all the production rules are correct. Its objective function is given by</p><formula xml:id="formula_9">T * q = argmax T ∈G(w) e∈T q(e) ,<label>(10)</label></formula><p>where e ranges over all the 4-tuples A BC, i, k, j in the parse tree T . This objective function can be efficiently solved by dynamic programming such as the CYK algorithm.</p><p>Although the time complexity of the inside- outside algorithm with GM-LVeGs is polynomial in the sentence length and the nonterminal num- ber, in practice the algorithm is still slow because the number of Gaussian components in the inside and outside scores increases dramatically with the recursion depth. To speed up the computation, we prune Gaussian components in the inside and out- side scores using the following technique. Sup- pose we have a minimum pruning threshold k min  <ref type="figure">(a, i, j)</ref> is the inside score of a nonterminal A over a span wi:j in the sentence w1:n, where 1 ≤ i &lt; j ≤ n. Equation 7: s A O (a, i, j) is the outside score of a nonterminal A over a span wi:j in the sentence w1:n, where 1 ≤ i ≤ j ≤ n. Equation 8: s(A BC, i, k, j) is the score of a production rule A BC with nonterminals A, B, and C spanning words wi:j, w i,k , and w k+1:j respectively in the sentence w1:n, where 1 ≤ i ≤ k &lt; j ≤ n. and a maximum pruning threshold k max . Given an inside or outside score with k c Gaussian com- ponents, if k c ≤ k min , then we do not prune any Gaussian component; otherwise, we compute k allow = min{k min + floor(k ϑ c ), k max } (0 ≤ ϑ ≤ 1 is a constant) and keep only k allow components with the largest mixture weights.</p><formula xml:id="formula_10">s A I (a, i, j) = ABC∈R k=i,··· ,j−1 W ABC (a, b, c) × s B I (b, i, k) × s C I (c, k + 1, j) dbdc .(6) s A O (a, i, j) = BCA∈R k=1,··· ,i−1 W BCA (b, c, a) × s B O (b, k, j) × s C I (c, k, i − 1) dbdc + BAC∈R k=j+1,··· ,n W BAC (b, a, c) × s B O (b, i, k) × s C I (c, j + 1, k) dbdc .(7) s(A BC, i, k, j) = W ABC (a, b, c) × s A O (a, i, j) × s B I (b, i, k) × s C I (c, k + 1, j) dadbdc .(8)</formula><p>In addition to component pruning, we also em- ploy two constituent pruning techniques to reduce the search space during parsing. The first tech- nique is used by <ref type="bibr" target="#b11">Petrov et al. (2006)</ref>. Before parsing a sentence with a GM-LVeG, we run the inside-outside algorithm with the treebank gram- mar and calculate the posterior probability of ev- ery nonterminal spanning every substring. Then a nonterminal would be pruned from a span if its posterior probability is below a pre-specified threshold p min . When parsing with GM-LVeGs, we only consider the unpruned nonterminals for each span.</p><p>The second constituent pruning technique is similar to the one used by <ref type="bibr" target="#b16">Socher et al. (2013)</ref>. Note that for a strong constituency parser such as the Berkeley parser ( <ref type="bibr" target="#b12">Petrov and Klein, 2007)</ref>, the constituents in the top 200 best parses of a sen- tence can cover almost all the constituents in the gold parse tree. So we first use an existing con- stituency parser to run k-best parsing with k = 200 on the input sentence. Then we parse with a GM-LVeG and only consider the constituents that appear in the top 200 parses. Note that this method is different from the re-ranking technique because it may produce a parse different from the top 200 parses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>Given a training dataset D = {(T i , w i ) | i = 1, . . . , m} containing m samples, where T i is the gold parse tree with unrefined nonterminals for the sentence w i , the objective of discriminative learn- ing is to minimize the negative log conditional likelihood:</p><formula xml:id="formula_11">L(Θ) = − log m i=1 P (T i |w i ; Θ) ,<label>(11)</label></formula><p>where Θ represents the set of parameters of the GM-LVeG. We optimize the objective function using the Adam ( <ref type="bibr" target="#b6">Kingma and Ba, 2014</ref>) optimization algo- rithm. The derivative with respect to Θ r , the pa- rameters of the weight function W r (r) of an un- refined production rule r, is calculated as follows (the derivation is in the supplementary material):</p><formula xml:id="formula_12">∂L(Θ) ∂Θ r = m i=1 ∂W r (r) ∂Θ r<label>(12)</label></formula><formula xml:id="formula_13">× E P (t|w i ) [f r (t)] − E P (t|T i ) [f r (t)] W r (r) dr ,</formula><p>where t indicates a parse tree with nonterminal subtypes, and f r (t) is the number of occurrences of the unrefined rule r in the unrefined parse tree that is obtained by replacing all the subtypes in t with the corresponding nonterminals. The two ex- pectations in Equation 12 can be efficiently com- puted using the inside-outside algorithm. Because the second expectation is conditioned on the parse tree T i , in Equation 6 and Equation 7 we can skip all the summations and assign the values of B, C, and k according to T i .</p><p>In GM-LVeGs, Θ r is the set of parameters in a Gaussian mixture:</p><formula xml:id="formula_14">Θ r = {(ρ r,k , µ r,k , Σ r,k )|k = 1, . . . , K r } . (13)</formula><p>According to Equation 12, we need to take the derivatives of W r (r) respect to ρ r,k , µ r,k , and Σ r,k respectively:</p><formula xml:id="formula_15">∂W r (r)/∂ρ r,k = N r,k (r) ,<label>(14)</label></formula><p>∂W r (r)/∂µ r,k = ρ r,k N r,k (r)Σ −1 r,k (r − µ r,k ) ,(15)</p><formula xml:id="formula_16">∂W r (r)/∂Σ r,k = ρ r,k N r,k (r)Σ −1 r,k 1 2 − I (16) + (r − µ r,k )(r − µ r,k ) T Σ −1 r,k .</formula><p>Substituting Equation 14-16 into Equation 12, we have the full gradient formulations of all the pa- rameters. In spite of the integral in Equation 12, we can derive a closed-form solution for the gradi- ent of each parameter, which is shown in the sup- plementary material.</p><p>In order to keep each mixture weight ρ r,k posi- tive, we do not directly optimize ρ r,k ; instead, we set ρ r,k = exp(θ ρ r,k ) and optimize θ ρ r,k by gradi- ent descent. We use a similar trick to keep each covariance matrix Σ r,k positive definite.</p><p>Since we use the inside-outside algorithm de- scribed in Section 3.2 to calculate the two ex- pectations in Equation 12, we face the same ef- ficiency problem that we encounter in parsing. To speed up the computation,we again use both com- ponent pruning and constituent pruning introduced in Section 3.2.</p><p>Because gradient descent is often sensitive to the initial values of the parameters, we employ the following informed initialization method. Mixture weights are initialized using the treebank gram- mar. Suppose in the treebank grammar P (r) is the probability of a production rule r. We initial- ize the mixture weights in the weight function W r by ρ r,k = α · P (r) where α &gt; 1 is a constant. We initialize all the covariance matrices to iden- tity matrices and initialize each mean with a value uniformly sampled from [−0.05, 0.05].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We evaluate the GM-LVeG on part-of-speech (POS) tagging and constituency parsing and com- pare it against its special cases such as LVGs and CVGs. It shall be noted that in this paper we focus on the basic framework of LVeGs and aim to show its potential advantage over previous special cases. It is therefore not our goal to compete with the latest state-of-the-art approaches to tagging and parsing. In particular, we currently do not incor- porate contextual information of words and con- stituents during tagging and parsing, while such information is critical in achieving state-of-the-art accuracy. We will discuss future improvements of LVeGs in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Parsing. We use the Wall Street Journal corpus from the Penn English Treebank (WSJ) <ref type="bibr" target="#b8">(Marcus et al., 1994)</ref>. Following the standard data splitting, we use sections 2 to 21 for training, section 23 for testing, and section 22 for development. We pre- process the treebank using a right-branching bina- rization procedure to obtain an unannotated X-bar grammar, so that there are only binary and unary production rules. To deal with the problem of un- known words in testing, we adopt the unknown word features used in the Berkeley parser and set the unknown word threshold to 1. Specifically, any word occurring less than two times is replaced by one of the 60 unknown word categories. Tagging. (1) We use Wall Street Journal corpus from the Penn English Treebank (WSJ) <ref type="bibr" target="#b8">(Marcus et al., 1994)</ref>. Following the standard data split- ting, we use sections 0 to 18 for training, sections 22 to 24 for testing, and sections 19 to 21 for de- velopment. (2) The Universal Dependencies tree- bank 1.4 (UD) ( <ref type="bibr" target="#b10">Nivre et al., 2016)</ref>, in which En- glish, French, German, Russian, Spanish, Indone- sian, Finnish, and Italian treebanks are used. We use the original data splitting of these corpora for training and testing. For both WSJ and UD En- glish treebanks, we deal with unknown words in the same way as we do in parsing. For the rest of the data, we use only one unknown word category and the unknown word threshold is also set to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">POS Tagging</head><p>POS tagging is the task of labeling each word in a sentence with the most probable part-of-speech tag. Here we focus on POS tagging with Hidden Markov Models (HMMs). Because HMMs are equivalent to probabilistic regular grammars, we can extend HMMs with both LVGs and LVeGs. Specifically, the hidden states in HMMs can be seen as nonterminals in regular grammars and therefore can be associated with latent variables or latent vectors.</p><p>We implement two training methods for LVGs. The first (LVG-G) is generative training us- ing expectation-maximization that maximizes the joint probability of the sentence and the tags. The second (LVG-D) is discriminative training using gradient descent that maximizes the conditional probability of the tags given the sentence. In both cases, each nonterminal is split into a fixed num- ber of subtypes. In our experiments we test 1, 2, 4, 8, and 16 subtypes of each nonterminal. Due to the limited space, we only report experimental results of LVG with 16 subtypes for each nonter- minal. Full experimental results can be found in the supplementary material.</p><p>We experiment with two different GM-LVeGs: GM-LVeG-D with diagonal Gaussians and GM- LVeG-S with spherical Gaussians. In both cases, we fix the number of Gaussian components K r to 4 and the dimension of the latent vectors d to 3. We do not use any pruning techniques in learning and inference because we find that our algorithm is fast enough with the current setting of K r and d. We train the GM-LVeGs for 20 epoches and se- lect the models with the best token accuracy on the development data for the final testing.</p><p>We report both token accuracy and sentence ac- curacy of POS tagging in <ref type="table" target="#tab_2">Table 2</ref>. It can be seen that, on all the testing data, GM-LVeGs consis- tently surpass LVGs in terms of both token ac- curacy and sentence accuracy. GM-LVeG-D is slightly better than GM-LVeG-S in sentence ac- curacy, producing the best sentence accuracy on 5 of the 9 testing datasets. GM-LVeG-S performs slightly better than GM-LVeG-D in token accuracy on 5 of the 9 datasets. Overall, there is not sig- nificant difference between GM-LVeG-D and GM- LVeG-S. However, GM-LVeG-S admits more effi- cient learning than GM-LVeG-D in practice since it has fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parsing</head><p>For efficiency, we train GM-LVeGs only on sen- tences with no more than 50 words (totally 39115 sentences). Since we have found that spherical Gaussians are better than diagonal Gaussians con- sidering both model performance and learning ef- ficiency, here we use spherical Gaussians in the weight functions. The dimension of latent vectors d is set to 3, and all the Gaussian mixtures have K r = 4 components. We use α = 8 in initializing mixture weights. We train the GM-LVeG for 15 epoches and select the model with the highest F1 score on the development data for the final testing. We use component pruning in both learning and parsing, with k max = 50 and ϑ = 0.35 in both learning and parsing, k min = 40 in learning and k min = 20 in parsing. During learning we use the first constituent pruning technique with the prun- ing threshold p min = 1e − 5, and during parsing we use the second constituent pruning technique based on the Berkeley parser which produced 133 parses on average for each testing sentence. As can be seen, we use weaker pruning during train- ing than during testing. This is because in training stronger pruning (even if accurate) results in worse estimation of the first expectation in Equation 12, which makes gradient computation less accurate.  <ref type="table" target="#tab_3">Table 3</ref>. It can be seen that GM-LVeG-S pro- duces the best F1 scores on both the development data and the testing data. It surpasses the Berkeley parser by 0.92% in F1 score on the testing data. Its exact match score on the testing data is only slightly lower than that of LVG-D-16.</p><p>We further investigate the influence of the la- tent vector dimension and the Gaussian compo- nent number on the efficiency and the parsing ac- curacy . We experiment on a small dataset (statis- tics of this dataset are in the supplemental mate- rial). We first fix the component number to 4 and experiment with the dimension 2, 3, 4, 5, 6, 7, 8, 9. Then we fix the dimension to 3 and experiment with the component number 2, 3, 4, 5, 6, 7, 8, 9. F1 scores on the development data are shown in the first row in <ref type="figure" target="#fig_3">Figure 1</ref>. Average time consumed per epoch in learning is shown in the second row in <ref type="figure" target="#fig_3">Figure 1</ref>. When K r = 4, the best dimension is 5; when d = 3, the best Gaussian component num- ber is 3. A higher dimension or a larger Gaussian component number hurts the model performance and requires much more time for learning. Thus  <ref type="table" target="#tab_0">English  French  German  Russian  Spanish  Indonesian  Finnish  Italian   T  S  T  S  T  S  T  S  T  S  T  S  T  S  T  S  T  S   LVG-D-16</ref>    our choice of K r = 4 and d = 3 in GM-LVeGs for parsing is a good balance between the efficiency and the parsing accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>It shall be noted that in this paper we choose to focus on the basic framework and algorithms of LVeGs, and therefore we leave a few important extensions for future work. One extension is to incorporate contextual information of words and constituents. which is a crucial technique that can be found in most state-of-the-art approaches to parsing or POS tagging. One possible way to uti- lize contextual information in LVeGs is to allow the words in the context of an anchored produc- tion rule to influence the rule's weight function. For example, we may learn neural networks to pre- dict the parameters of the Gaussian mixture weight functions in a GM-LVeG from the pre-trained em- beddings of the words in the context.</p><p>In GM-LVeGs, we currently use the same num- ber of Gaussian components for all the weight functions. A more desirable way would be au- tomatically determining the number of Gaussian components for each production rule based on the ideal refinement granularity of the rule, e.g., we may need more Gaussian components for NP DT NN than for NP DT JJ, since the latter is rarely used. There are a few possible ways to learn the component numbers such as greedy addition and removal, the split-merge method, and sparsity priors over mixture weights.</p><p>An interesting extension beyond LVeGs is to have a single continuous space for subtypes of all the nonterminals. Ideally, subtypes of the same nonterminal or similar nonterminals are close to each other. The benefit is that similarity between nonterminals can now be modeled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present Latent Vector Grammars (LVeGs) that associate each nonterminal with a latent continu- ous vector space representing the set of subtypes of the nonterminal. For each production rule, a LVeG defines a continuous weight function over the subtypes of the nonterminals involved in the rule. We show that LVeGs can subsume latent vari- able grammars and compositional vector gram- mars as special cases. We then propose Gaus- sian mixture LVeGs (GM-LVeGs). which formu- late weight functions of production rules by mix- tures of Gaussian distributions. The partition func- tion and the expectations of fine-grained produc- tion rules in GM-LVeGs can be efficiently com- puted using dynamic programming, which makes learning and inference with GM-LVeGs feasible.</p><p>We empirically show that GM-LVeGs can achieve competitive accuracies on POS tagging and con- stituency parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>We compare LVeGs with CVGs and several variants of LVGs: (1) LVG-G-16 and LVG-D-16, which are LVGs with 16 subtypes for each nonter- minal with discriminative and generative training respectively (accuracies obtained from Petrov and Klein (2008a)); (2) Multi-scale grammars (Petrov and Klein, 2008b), trained without using the span features in order for a fair comparison; (3) Berkeley parser (Petrov and Klein, 2007) (accura- cies obtained from Petrov and Klein (2008b) be- cause Petrov and Klein (2007) do not report exact match scores). The experimental results are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: F1 score and average time (min) consumed per epoch in learning. Left: # of Gaussian components fixed to 4 with different dimensions; Right: dimension of Gaussians fixed to 3 with different # of Gaussian components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Equation 6: s A 
I </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Token accuracy (T) and sentence accuracy (S) for POS tagging on the testing data. 

Model 

dev (all) 
test ≤ 40 
test (all) 

F1 
F1 
EX 
F1 
EX 

LVG-G-16 
88.70 35.80 
LVG-D-16 
89.30 39.40 
Multi-Scale 
89.70 39.60 89.20 37.20 
Berkeley Parser 
90.60 39.10 90.10 37.10 
CVG (SU-RNN) 
91.20 
91.10 
90.40 

GM-LVeG-S 
91.24 
91.38 41.51 91.02 39.24 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Parsing accuracy on the testing data of WSJ. EX indicates the exact match score.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Natu-ral Science Foundation of China <ref type="formula">(61503248)</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tree-bank grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th National Conference on Artificial Intelligence</title>
		<meeting>the 30th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1031" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A maximum-entropy-inspired parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 1st Meeting of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural CRF parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="302" to="312" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PCFG models of linguistic tree representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="613" to="632" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st annual meeting on Association for Computational Linguistics</title>
		<meeting>the 41st annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The penn treebank: annotating predicate argument structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Britta</forename><surname>Schasberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on Human Language Technology</title>
		<meeting>the workshop on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="114" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic CFG with latent annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Universal dependencies v1: A multilingual treebank collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Language Resources and Evaluation</title>
		<meeting>the 10th International Conference on Language Resources and Evaluation<address><addrLine>Natalia Silveira, Reut Tsarfaty, and Daniel Zeman</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1659" to="1666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2007 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative loglinear grammars with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse multi-scale grammars for discriminative latent variable parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="867" to="876" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weighted and probabilistic context-free grammars are equally expressive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="491" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
