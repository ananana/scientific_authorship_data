<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jigg: A Framework for an Easy Natural Language Processing Pipeline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
							<email>noji@is.naist.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>8916-5</postCode>
									<settlement>Takayama, Ikoma</settlement>
									<region>Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
							<email>yusuke@nii.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<addrLine>2-1-2 Hitotsubashi, Chiyoda-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jigg: A Framework for an Easy Natural Language Processing Pipeline</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-System Demonstrations</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-System Demonstrations <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="103" to="108"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present Jigg, a Scala (or JVM-based) NLP annotation pipeline framework , which is easy to use and is exten-sible. Jigg supports a very simple interface similar to Stanford CoreNLP, the most successful NLP pipeline toolkit, but has more flexibility to adapt to new types of annotation. On this framework, system developers can easily integrate their downstream system into a NLP pipeline from a raw text by just preparing a wrapper of it.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A common natural language processing system works as a component in a pipeline. For example, a syntactic parser typically requires that an input sentence is correctly tokenized or assigned part- of-speech (POS) tags. The syntactic trees given by the parser may be required in further downstream tasks such as coreference resolution and semantic role labelling. While this pipeline-based approach has been quite successful due to its modularity, it suffers from several drawbacks from a viewpoint of software use and development:</p><p>• For a user, building a pipeline connecting ex- isting tools and aggregating the outputs are painful, since often each system outputs the results in a different format;</p><p>• For researchers or tool developers of down- stream tasks, supporting the full pipeline from an input text in their software is boring and time consuming. For example, two famous dependency parsing sys- tems, <ref type="bibr">MaltParser (Nivre et al., 2006</ref>) and MST- Parser ( <ref type="bibr" target="#b4">McDonald et al., 2005</ref>), both assume that an input sentence is already tokenized and as- signed POS tags, and encoded in a specific format, such as the CoNLL format.  <ref type="figure">Figure 1</ref>: In a pipeline, annotations are performed on a Scala XML object. A pipeline is built by choosing annotator tools at each step, e.g., the bold or dotted lines in the figure. Each component is implemented as a wrapper, which manipulates the XML object. If we prepare a new wrapper of some component, one can integrate it in a pipeline (e.g., the POS tagger in the dotted lines).</p><p>In this paper, we present Jigg, which aims to make it easy to incorporate an existing or new tool (component) in an NLP pipeline. <ref type="figure">Figure 1</ref> describes the overview. Using Jigg, a user can easily construct a pipeline by choosing a tool at each step on a command-line interface. Jigg is written in Scala, and can easily be extended with JVM languages including Java. A new tool can be incorporated into this framework by writing a wrapper of that to follow the common API of Jigg (Scala XML object), which requires typically sev- eral dozes of lines of code.</p><p>The software design of Jigg is highly inspired by the success of Stanford CoreNLP ( <ref type="bibr" target="#b3">Manning et al., 2014)</ref>, which is now the most widely used NLP toolkit supporting pipeline processing from raw texts. One characteristic of Stanford CoreNLP is Stanford CoreNLP is basically a collection of NLP tools developed by the Stanford NLP group, e.g., Stanford POS tagger ( <ref type="bibr" target="#b9">Toutanova et al., 2003)</ref> and Stanford parser <ref type="bibr" target="#b8">(Socher et al., 2013)</ref>. Jigg, on the other hand, is an integration framework of vari- ous NLP tools developed by various groups. This means that adding a new component in Jigg is easier than Stanford CoreNLP. Also as indicated in <ref type="figure">Figure 1</ref>, Jigg provides a wrapper to Stanford CoreNLP itself, so a user can enjoy combination of Stanford CoreNLP and other tools, e.g., Berke- ley parser <ref type="bibr" target="#b6">(Petrov and Klein, 2007)</ref> (see Section 2). This difference essentially comes from the un- derlying object annotated on each step, which is CoreMap object in Stanford CoreNLP, and Scala XML object in Jigg, which gives more flexibility as we describe later (Section 5). Before that, in the following, we first describes the concrete us- age (Section 2), the core software design (Section 3), and a way to add a new component (Section 4).</p><p>The code is open-source under the Apache Li- cense Version 2.0. Followings are the pointers to the related websites:</p><p>• Github: https://github.com/mynlp/jigg</p><p>• Maven:</p><formula xml:id="formula_0">http://mvnrepository.com/ artifact/com.github.mynlp/jigg</formula><p>Jigg is also available from Maven, so it can eas- ily be incorporated into another JVM project. See REAME on the project Github for this usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic Usages</head><p>As an example, let us consider the scenario to run the Berkeley parser on a raw text. This parser is state-of-the-art but it requires that the input is cor- $ cat sample.txt This is a cat. That is a dog. $ echo sample.txt | java -cp " * " \ jigg.pipeline.Pipeline\ -annotators "corenlp <ref type="bibr">[tokenize,ssplit]</ref>,berkeleyparser"\ -berkeleyparser.grFileName ./eng_sm6.gr &gt; sample.xml  rectly tokenized and splitted on sentences. <ref type="figure" target="#fig_0">Fig- ure 2</ref> shows a concrete command-line to build a pipeline, on which tokenization and sentence split- ting are performed using the components in Stan- ford CoreNLP. This pipeline corresponds to the bold lines in <ref type="figure">Figure 1</ref>. jigg.pipeline.Pipeline is the path to the main class. −annotators argu- ment is essential, and specifies which components (tools) one wishes to apply. In the command-line, corenlp <ref type="bibr">[tokenize, ssplit]</ref> is an abbreviation of two components, corenlp[tokenize] (tokenization) and corenlp[ssplit] (sentence splitting by CoreNLP). <ref type="bibr">1</ref> The last argument −berkeleyparser.grFileName is necessary and specifies the path to the parser model (learned grammar).</p><p>XML output In the current implementation, the output format of annotations is always XML. <ref type="figure" target="#fig_1">Fig- ure 3</ref> shows the output for this example. In this output, parse element specifies a (constituent) parse tree with a collection of spans, each of which consists of a root symbol (e.g., S) and child nodes (ids). This format is intended to be eas- ily processed with a computer, and differs in sev- eral points from the outputs of Stanford CoreNLP, which we describe more in Section 5.</p><p>import jigg.pipeline.Pipeline import scala.xml.Node import java.util.Properties object ScalaExample { def main(args: Array <ref type="bibr">[String]</ref>): Unit = { val props = new Properties() props.setProperty("annotators", "corenlp[tokenize,ssplit],berkeleyparser") props.setProperty("berkeleyparser.grFileName", "eng_sm6.gr") val pipeline = new Pipeline(props) val annotation: Node = pipeline.annotate( "This is a cat. That is a dog") // Find all sentence elements recursively, // and get the first one. val firstSentence = (annotation \\ "sentence") <ref type="formula">(0)</ref> // All tokens on the sentence val tokens = firstSentence \\ "token"</p><p>println("POS tags on the first sentence: " + (tokens map (_ \@ "pos") mkString " ")) // Output "DT VBZ DT NN ." } } <ref type="figure" target="#fig_2">Figure 4</ref>: A programmatic usage from Scala.</p><p>Properties As in Stanford CoreNLP, these argu- ments can be customized through a Java properties file. For example, the following properties file cus- tomizes the behavior of corenlp besides the parser: This file can be used as follows:</p><formula xml:id="formula_1">$</formula><p>jigg.pipeline.Pipeline -props sample.properties Each annotator-specific argument has the form annotator name.key. In the case of corenlp, all keys of the arguments prefixed with that are di- rectly transferred to the CoreNLP object, so the all arguments defined in Stanford CoreNLP can be used to customize the behavior. The setting above yields tokenization on white spaces, and sentence splitting on new lines only (i.e., the input text is assumed to be properly preprocessed beforehand).</p><p>Programmatic usage Jigg can also be used as a Scala library, which can be called on JVM lan- guages. <ref type="figure" target="#fig_2">Figure 4</ref> shows an example on a Scala code. The annotate method of Pipeline object performs annotations on the given input, and re- turns the annotated XML object (Node class). The example also shows how we can manipulate the Scala XML object, which can be searched with methods similar to XPath, e.g., \\. \@ key returns the attribute value for the key if exists. <ref type="figure">Figure 5</ref> shows that Jigg can also be used via a Java code.</p><p>Another example Jigg is a growing project, and the supported tools are now increasing. Histori- Properties props = new Properties(); props.setProperty("annotators", "corenlp[tokenize,ssplit],berkeleyparser"); props.setProperty("berkeleyparser.grFileName", "eng_sm6.gr"); Pipeline pipeline = new Pipeline(props); Node annotation = pipeline.annotate( "This is a cat. That is a dog");</p><p>// Though the search methods such as \\ cannot be // used on Java, we provide utilities to support // Java programming. List&lt;Node&gt; sentences = jigg.util.XMLUtil.findAllSub( annotation, "sentence"); Node firstSentence = sentences.get(0); List&lt;Node&gt; tokens = jigg.util.XMLUtil.findAllSub( firstSentence, "token"); System.out.print("POS tags on the first sentence: "); for (Node token: tokens) { String pos = XMLUtil.find(token, "@pos").toString(); System.out.print(pos + " "); } <ref type="figure">Figure 5</ref>: Jigg also supports Java programming.</p><p>cally, Jigg has been started as a pipeline frame- work focusing on Japanese language processing. Jigg thus supports many Japanese processing tools such as MeCab ( <ref type="bibr" target="#b2">Kudo et al., 2004</ref>), a famous mor- phological analyzer, as well as a Japanese CCG parser based on the Japanese CCGBank ( <ref type="bibr" target="#b10">Uematsu et al., 2013</ref>). For English, currently the core tool is Stanford CoreNLP. Here we present an inter- esting application to integrate Berkeley parser into the full pipeline of Stanford CoreNLP:</p><formula xml:id="formula_2">-annotators "corenlp[tokenize,ssplit],berkeleyparser, corenlp[lemma,ner,dcoref]"</formula><p>where dcoref is a coreference resolution system relying on constituent parse trees ( <ref type="bibr" target="#b7">Recasens et al., 2013)</ref>. This performs annotation of corefer- ence resolution based on the parse trees given by the Berkeley parser instead of the Stanford parser.</p><p>Using Jigg, a user can enjoy these combinations of existing tools quite intuitively. Also if a user has her own (higher-performance) system on the pipeline, one can replace the existing component with that in a minimal effort, by writing a wrapper of that tool in JVM languages (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design</head><p>We now describe the internal mechanisms of Jigg, which comprise of two steps: the first is a check for correctness of the given pipeline, and the sec- ond is annotations on a raw text with the con- structed pipeline. We describe the second anno- tation step first (Section 3.1), and then discuss the first pipeline check phase (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Annotation on Scala XML</head><p>As shown in <ref type="figure">Figure 1</ref>, each annotator (e.g., the to- kenizer in Stanford CoreNLP) communicates with the Scala XML object. Basically, each annotator only adds new elements or attributes into the re- ceived XML. <ref type="bibr">2</ref> For example, the Berkeley parser receives an XML, on which each sentence element is annotated with tokens elements lacking pos at- tribute on each token. Then, the parser (i.e., the wrapper of the parser) adds the predicted syntactic tree and POS tags on each sentence XML (see <ref type="figure" target="#fig_1">Fig- ure 3)</ref>. Scala XML (Node object) is an immutable data structure, but it is implemented as an im- mutable tree, so a modification can be performed efficiently (in terms of memory and speed).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Requirement-based Pipeline Check</head><p>On this process, the essential point for the pipeline to correctly work is to guarantee that all the re- quired annotations for an annotator are provided at each step. For example, the berkeleyparser anno- tator assumes each sentence element in the XML has the following structure:</p><p>&lt;sentence id="..."&gt; sentence text &lt;tokens&gt; &lt;token form="..." id="..."/&gt; &lt;token form="..." id="..."/&gt; ... &lt;/tokens&gt; &lt;/sentence&gt; where form means the surface form of a token. How do we guarantee that the XML given to berkeleyparser satisfies this form?</p><p>Currently, Jigg manages these dependen- cies between annotators using the concept of Requirement, which we also borrowed from Stan- ford CoreNLP. Each annotator has a field called requires, which specifies the type of necessary an- notations that must be given before running it. In berkeleyparser it is defined as follows:</p><formula xml:id="formula_3">override def requires:Set[Requirement] = Set(Tokenize, Ssplit)</formula><p>where Ssplit is an object (of Requirement type), which guarantees that sentences element (a col- lection of sentence elements) exists on the current annotation, while Tokenize guarantees that each sentence element has tokens element (a collec- tion of token elements), and each token has four attributes: id, form, characterOffsetBegin, and characterOffsetEnd.</p><p>Each annotator also has requirementsSatisfied field, which declares which Requirements will be satisfied (annotated). In the above requirements, Ssplit is given by corenlp[ssplit] while Tokenize is given by corenlp <ref type="bibr">[tokenize]</ref>. In berkeleyparser, it is POS and Parse; POS guarantees that each token element has pos attribute. Before running annota- tion, Jigg checks whether the constructed pipeline correctly works by checking that all elements in requires for each annotator are satisfied by (in- cluded in) the requirementsSatisfied elements of the previous annotators. For example, if we run the pipeline with −annotators berkeleyparser ar- gument, the program fails with an error message suggesting missing Requirements.</p><p>Note that currently Requirement is something just like a contract on the structure of annotated XML, and it is the author's responsibility to im- plement each annotator to output the correct XML structure. Currently the correspondence between each Requirement and the satisfied XML structure is managed with a documentation on the wiki of the project Github. We are seeking a more sophis- ticated (safe) mechanism to guarantee these corre- spondences in a code; one possible solution might be to define the skeletal XML structure for each Requirement, and test in each annotator whether the annotated object follows the defined structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adding New Annotator</head><p>Here we describe how to implement a new annota- tor and integrate it into the Jigg pipeline. We also discuss a way to distribute a new system in Jigg.</p><p>Implementing new annotator We focus on im- plementation of Berkeley parser as an example to get intuition into what we should do. Annotator is the base trait 3 of all annotator classes, which de- fines the following basic methods:</p><p>• def annotate(annotation : Node) : Node</p><formula xml:id="formula_4">• def requires : Set[Requirement]</formula><p>• def requirementsSatisfied : Set <ref type="bibr">[Requirement]</ref> We have already seen the roles of requires and requirementsSatisfied in Section 3.2. Note that in many cases including the Berkeley parser, an- notation is performed on each sentence indepen- dently. For this type of annotation, we provide a useful trait SentenceAnnotator, which replaces the method to be implemented from annotate to newSentenceAnnotation, which has the same sig- nature as annotate. package jigg.pipeline import ... // By supporting a constructor with signature // (String, Properties), the annotator can be // instantiated dynamically using reflection. class BerkeleyParserAnnotator( override val name: String, override val props: Properties   shows an excerpt of essential parts in BerkeleyParserAnnotator. It creates a parser object in the constructor, and then in each newSentenceAnnotation, it first extracts a se- quence of (yet annotated) tokens (1), gets a tree object from the parser (2), converts the tree into Scala XML object (3), and returns the updated sentence XML object (4). This workflow to en- code to and decode from the API-specific objects is typical when implementing new annotators.</p><p>Calling with reflection The class in <ref type="figure" target="#fig_4">Fig- ure 6</ref> has a constructor with the signature (String, Properties), which allows us to instanti- ate the class dynamically using reflection. To do this, a user has to add a new property prefixed with customAnnotatorClass (the same as Stanford CoreNLP). In the case above, the property customAnnotatorClass.berkeleyparser : jigg.pipeline.BerkeleyParser</p><p>Another advantage of this trait is that annotations are auto- matically performed in parallel if the code is thread-safe. One can also prohibit this behavior by overriding nThreads vari- able by 1 in the annotator class. makes it possible to load the implemented annota- tor with the name berkeleyparser.</p><p>Distributing new annotators An ultimate goal of Jigg is that the developers of a new tool in a pipeline distribute their system along with the wrapper (Jigg annotator) when releasing the soft- ware. If the system is JVM-based, the most stable way to integrate it is releasing the annotator (along with the software) into Maven repositories. Then, a user can build an extended Jigg by adding the dependency to it. For example, now the annotator for the MST parser is implemented, but is not in- cluded in Jigg, as it is a relatively old system. One way to extend Jigg with this tool is to prepare an- other project, on which its build.sbt may contain the following lines: 5 libraryDependencies ++= Seq( "com.github.mynlp" % "jigg" % "VVV", "com.github.mynlp" % "jigg-mstparser" % "0.1-SNAPSHOT")</p><p>Jigg itself focuses more on the central NLP tools for wider users, but one can obtain the customized Jigg in this way.</p><p>Tools beyond JVM So far we have only dealt with JVM softwares such as Stanford CoreNLP, but Jigg can also wraps the softwares written in other languages such as C++ and python. In fact, many existing tools for Japanese are implemented in C or C++, and Jigg provides wrappers for those softwares. One problem of these languages is that installation is sometimes hard due to complex de- pendencies to other libraries. We thus put a pri- ority on supporting the tool written in JVM lan- guages in particular on Maven first, which can be safely incorporated in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparison to Stanford CoreNLP</head><p>As we have seen so far, Jigg follows the software design of Stanford CoreNLP in many respects. Fi- nally, in this section, we highlight the important differences between two approaches.</p><p>Annotated objects Conceptually this is the most crucial difference as we mentioned in Sec- tion 1. In Stanford CoreNLP, each annotator ma- nipulates an object called CoreMap. A clear ad- vantage of this data structure is that one can take out a typed data structure, such as a well imple- mented Sentence or Graph object, which is easy to use. In Jigg's XML, on the other hand, one ac- cesses the fields through literals (e.g., \@ pos to get the POS attribute of a token). This may sug- gests Jigg needs more careful implementation for each annotator. However, we note that the prob- lem can be alleviated by adding a simple unit test, which we argue is important as well in other plat- forms.</p><p>The main advantage of using Scala XML as a primary object is its flexibility for adapting to new types of annotations. It is just an XML object, so there is no restriction on the allowed structure. This is not the case in Stanford CoreNLP, where each element in CoreMap must be a proper data structure defined in the library, which means that the annotation that goes beyond the assumption of Stanford CoreNLP is difficult to support. Even if we define a new data structure in CoreMap, an- other problem occurs when outputting the annota- tion into other formats such as XML. In Stanford CoreNLP, this output component is hard-coded in the outputter class, which is difficult to extend. This is the problem that we encountered when we explored an extension to Stanford CoreNLP for Japanese processing pipeline as our initial attempt. Historically in Japanese NLP, the basic analyzing unit is called bunsetsu, which is a kind of chunk; a syntactic tree is often represented as a dependency tree on bunsetsu. Jigg is preferable to handle these new data structures, which go beyond the assump- tion on typical NLP focusing primarily on English, and we believe this flexibility make Jigg suitable for an integration framework, which has no restric- tions on the applicable softwares and languages.</p><p>Output format Another small improvement is that our XML output format <ref type="figure" target="#fig_1">(Figure 3</ref>) is (we be- lieve) more machine-friendly. For example, in Stanford CoreNLP, the parse element is just a Lisp-style tree like (S (NP (DT This)) ((VBZ is) (NP (DT a) (NN cat))) (. .)), which is parsable el- ements in Jigg. For some attribute names we em- ploy different names, e.g., surface form is called form in Jigg instead of word in Stanford CoreNLP. We decide these names basically following the naming convention found in Universal Dependen- cies <ref type="bibr">6</ref> , which we expect becomes the standard in fu- ture NLP. Finally, now we implement each wrap- per so that each id attribute is unique across the XML, which is not the case in Stanford CoreNLP. This makes search of elements more easier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A command-line usage to run the Berkeley parser on sentences tokenized and splitted by Stanford CoreNLP.</figDesc><graphic url="image-1.png" coords="2,307.56,175.79,217.69,112.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The output of the command in Figure 2 (sample.xml).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>) extends SentenceAnnotator { // Instantiate a parser by reading the gramar file. val parser: CoarseToFineMaxRuleParser = ... override def newSentenceAnnotation(sentence: Node): Node = { val tokens: Node = (sentence \ "tokens").head val tokenSeq: Seq[Node] = tokens \ "token" // (1) Get a list of surface forms. val formSeq: Seq[String] = tokenSeq.map(_ \@ "form") // (2) Parse the sentence by calling the API. val binaryTree: Tree[String] = parser. getBestConstrainedParse(formSeq.asJava, null, null) val tree = TreeAnnotations.unAnnotateTree(binaryTree, true) // (3) Convert the output tree into annotation. val taggedTokens = addPOSToTokens(tree, tokens) val parse = treeToNode(tree, tokenSeq) // (4) Return a new sentence node with updated // child elements. XMLUtil.addOrOverrideChild( sentence, Seq(newTokens, parseNode)) } // Return the new tokens element on which each element has // pos attributes. def addPOSToTokens(tree: Tree[String], tokens: Node): Node = { ... } // Convert the Tree object in Berkeley parser into XML. def treeToNode( tree: Tree[String], tokenSeq: Seq[Node]): Node = { ... } override def requires = Set(Tokenize) override def requirementsSatisfied = Set(POS, Parse) }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Core parts in BekeleyParserAnnotator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6</head><label>6</label><figDesc>Figure 6 shows an excerpt of essential parts in BerkeleyParserAnnotator. It creates a parser object in the constructor, and then in each newSentenceAnnotation, it first extracts a sequence of (yet annotated) tokens (1), gets a tree object from the parser (2), converts the tree into Scala XML object (3), and returns the updated sentence XML object (4). This workflow to encode to and decode from the API-specific objects is typical when implementing new annotators.</figDesc></figure>

			<note place="foot" n="103"> its simplicity of API, which allows wider users to easily get linguistic annotations for a text. Following this strategy, Jigg is also quite simple to use; all the basic components are included into one jar file, so a user need not install the external dependencies. The basic usage of Jigg is command-line interface, and the behavior can be customized with a Java properties file. On the other hand, it focuses just on processing of a single document on a single machine, and does not provide the solution to more complex scenarios such as distributed processing or visualization, which UIMA and related projects (Ferrucci and Lally, 2004; Kano et al., 2011) may provide. The largest difference between Jigg and Stanford CoreNLP is the focused NLP components.</note>

			<note place="foot" n="1"> Precisely, the two commands have different meanings and the former abbreviated form is recommended. In the latter separated form, transformation between CoreMap object and Scala XML is performed at each step (twice), while it occurs once in the former one after ssplit.</note>

			<note place="foot" n="2"> One exception in the current implementation is ssplit in corenlp, which breaks the result of tokenize (one very long tokenized sentence) into several sentences.</note>

			<note place="foot" n="3"> Trait is similar to interface in Java. 4 This trait implements annotate to traverse all sentences and replace them using newSentenceAnnotation method.</note>

			<note place="foot" n="5"> To call a new annotator, a user have to give a class path to the annotator with the property. Note that the mappings for the built-in annotators such as berkeleyparser are preserved in the Jigg package, so they can be used without any settings.</note>

			<note place="foot" n="6"> http://universaldependencies.org/docs/ 6 Conclusion We presented Jigg, an open source framework for an easy natural language processing pipeline both for system developers and users. We hope that this platform facilitates distribution of a new high quality system on the pipeline to wider users.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by CREST, JST.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Uima: an architectural approach to unstructured information processing in the corporate research environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="327" to="348" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">U-compare: A modular nlp workflow construction and evaluation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananiadou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
		<idno>11:1-11:10</idno>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Applying conditional random fields to japanese morphological analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaoru</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<editor>Dekang Lin and Dekai Wu</editor>
		<imprint>
			<biblScope unit="page" from="230" to="237" />
			<date type="published" when="2004-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: System Demonstrations</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-EMNLP</title>
		<imprint>
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maltparser: a data-driven parser-generator for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-04" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The life and death of discourse entities: Identifying singleton mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL: HLT</title>
		<meeting><address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="627" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL: HLT</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Integrating multiple dependency corpora for inducing wide-coverage japanese ccg resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumire</forename><surname>Uematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Hanaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Mima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="1042" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
