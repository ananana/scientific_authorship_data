<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baidu</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="800" to="810"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>How can we enable computers to automatically answer questions like &quot;Who created the character Harry Potter&quot;? Carefully built knowledge bases provide rich sources of facts. However, it remains a challenge to answer factoid questions raised in natural language due to numerous expressions of one question. In particular, we focus on the most common questions-ones that can be answered with a single fact in the knowledge base. We propose CFO, a Conditional Focused neural-network-based approach to answering fac-toid questions with knowledge bases. Our approach first zooms in a question to find more probable candidate subject mentions , and infers the final answers with a unified conditional probabilistic framework. Powered by deep recurrent neural networks and neural embeddings, our proposed CFO achieves an accuracy of 75.7% on a dataset of 108k questions-the largest public one to date. It outperforms the current state of the art by an absolute margin of 11.8%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Community-driven question answering (QA) web- sites such as Quora, Yahoo-Answers, and An- swers.com are accumulating millions of users and hundreds of millions of questions. A large portion of the questions are about facts or trivia. It has been a long pursuit to enable machines to answer such questions automatically.</p><p>In recent years, several efforts have been made on utilizing open-domain knowledge bases to answer factoid questions.</p><p>A knowledge * Part of the work was done while at <ref type="bibr">Baidu.</ref> base (KB) consists of structured representation of facts in the form of subject-relation-object triples.</p><p>Lately, several large-scale general- purpose KBs have been constructed, including <ref type="bibr">YAGO (Suchanek et al., 2007)</ref>, Freebase <ref type="bibr" target="#b4">(Bollacker et al., 2008</ref>), NELL <ref type="bibr" target="#b10">(Carlson et al., 2010)</ref>, and DBpedia ( <ref type="bibr" target="#b23">Lehmann et al., 2014</ref>). Typically, structured queries with predefined semantics (e.g. SPARQL) can be issued to retrieve specified facts from such KBs. Thus, answering factoid questions will be straightforward once they are converted into the corresponding structured form. However, due to complexity of language, converting natural language questions to structure forms remains an open challenge.</p><p>Among all sorts of questions, there is one cat- egory that only requires a single fact (triple) in KB as the supporting evidence. As a typical ex- ample, the question "Who created the charac- ter Harry Potter" can be answered with the sin- gle fact <ref type="bibr">(HarryPotter, CharacterCreatedBy, J.K.Rowling)</ref>. In this work, we refer to such questions as single-fact questions. Previously, it has been observed that single-fact questions con- stitute the majority of factoid questions in commu- nity QA sites <ref type="bibr" target="#b14">(Fader et al., 2013)</ref>. Despite the sim- plicity, automatically answering such questions re- mains far from solved -the latest best result on a dataset of 108k single-fact questions is only 63.9% in terms of accuracy ( .</p><p>To find the answer to a single-fact question, it suffices to identify the subject entity and relation (implicitly) mentioned by the question, and then forms a corresponding structured query. The prob- lem can be formulated into a probabilistic form. Given a single-fact question q, finding the subject- relation pairˆspairˆ pairˆs, ˆ r from the KB K which maximizes the conditional probability p(s, r|q), i.e. </p><p>Based on the formulation (1), the central problem is to estimate the conditional distribution p(s, r|q). It is very challenging because of a) the vast amount of facts -a large-scale KB such as Free- base contains billions of triples, b) the huge vari- ety of language -there are multiple aliases for an entity, and numerous ways to compose a question, c) the severe sparsity of supervision -most com- binations of s, r, q are not expressed in training data. Faced with these challenges, existing meth- ods have exploited to incorporate prior knowledge into semantic parsers, to design models and rep- resentations with better generalization property, to utilize large-margin ranking objective to estimate the model parameters, and to prune the search space during inference. Noticeably, models based on neural networks and distributed representations have largely contributed to the recent progress (see section 2). In this paper, we propose CFO, a novel method to answer single-fact questions with large-scale knowledge bases. The contributions of this paper are,</p><p>• we employ a fully probabilistic treatment of the problem with a novel conditional param- eterization using neural networks, • we propose the focused pruning method to re- duce the search space during inference, and • we investigate two variations to improve the generalization of representations for millions of entities under highly sparse supervision.</p><p>In experiments, CFO achieves 75.7% in terms of top-1 accuracy on the largest dataset to date, out- performing the current best record by an absolute margin of 11.8%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The research of KB supported QA has evolved from earlier domain-specific QA ( <ref type="bibr" target="#b42">Zelle and Mooney, 1996;</ref><ref type="bibr" target="#b34">Tang and Mooney, 2001;</ref>) to open-domain QA based on large- scale KBs. An important line of research has been trying to tackle the problem by semantic parsing, which directly parses natural language questions into structured queries <ref type="bibr" target="#b24">(Liang et al., 2011;</ref><ref type="bibr" target="#b9">Cai and Yates, 2013;</ref><ref type="bibr" target="#b21">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b38">Yao and Van Durme, 2014</ref>). Recent progresses in- clude designing KB specific logical representation and parsing grammar <ref type="bibr" target="#b3">(Berant et al., 2013</ref>), using distant supervision <ref type="bibr" target="#b3">(Berant et al., 2013)</ref>, utilizing paraphrase information <ref type="bibr" target="#b14">(Fader et al., 2013;</ref><ref type="bibr" target="#b1">Berant and Liang, 2014</ref>), requiring little question- answer pairs ( <ref type="bibr" target="#b31">Reddy et al., 2014)</ref>, and exploit- ing ideas from agenda-based parsing <ref type="bibr" target="#b2">(Berant and Liang, 2015)</ref>.</p><p>In contrast, another line of research tackles the problem by deep learning powered similarity matching. The core idea is to learn semantic repre- sentations of both the question and the knowledge from observed data, such that the correct support- ing evidence will be the nearest neighbor of the question in the learned vector space. Thus, a main difference among several approaches lies in the neural networks proposed to represent questions and KB elements. While ( <ref type="bibr" target="#b7">Bordes et al., 2014b;</ref><ref type="bibr" target="#b6">Bordes et al., 2014a;</ref><ref type="bibr" target="#b37">Yang et al., 2014</ref>) use relatively shallow embedding mod- els to represent the question and knowledge, <ref type="bibr" target="#b39">(Yih et al., 2014;</ref>) employ a convolu- tional neural network (CNN) to produce the repre- sentation. In the latter case, both the question and the relation are treated as a sequence of letter-tri- gram patterns, and fed into two parameter shared CNNs to get their embeddings. What's more, in- stead of measuring the similarity between a ques- tion and an evidence triple with a single model as in , <ref type="bibr" target="#b39">(Yih et al., 2014;</ref>) adopt a multi-stage approach. In each stage, one element of the triple is compared with the question to produce a partial similarity score by a dedicated model. Then, these partial scores are combined to generate the overall measurement.</p><p>Our proposed method is closely related to the second line of research, since neural models are employed to learn semantic representations. As in ( <ref type="bibr" target="#b39">Yih et al., 2014</ref>), we focus on single-fact questions. However, we propose to use recurrent neural networks (RNN) to produce the question representation. More importantly, our method follows a probabilistic formulation, and our parameterization relies on factors other than similarity measurement.</p><p>Besides KB-based QA, our work is also loosely related to work using deep learning systems in QA tasks with free text evidences. For example, <ref type="bibr" target="#b19">(Iyyer et al., 2014</ref>) focuses questions from the quiz bowl competition with recursive neural network. New architectures including memory networks , dynamic memory networks ( <ref type="bibr" target="#b20">Kumar et al., 2015)</ref>, and more ( <ref type="bibr" target="#b27">Peng et al., 2015;</ref><ref type="bibr" target="#b22">Lee et al., 2015</ref>) have been explored under the bAbI syn-thetic QA task ( <ref type="bibr" target="#b36">Weston et al., 2016)</ref>. In addition, ( <ref type="bibr" target="#b17">Hermann et al., 2015</ref>) seeks to answer Cloze style questions based on news articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview</head><p>In this section, we formally formulate the problem of single-fact question answering with knowledge bases. A knowledge base K contains three com- ponents: a set of entities E, a set of relations R, and a set of facts F = {{s, r, o} ⊆ E × R × E, where s, o ∈ E are the subject and object enti- ties, and r ∈ R is a binary relation. E(r), E(s) are the vector representations of a relation and an en- tity, respectively. s → r indicates that there exists some entity o such that s, r, o ∈ F. For single- fact questions, a common assumption is that the answer entity o and some triple s i , r k , o ∈ F reside in the given knowledge base. The goal of our model is to find such subject s i and relation r k mentioned or implied in the question. Once found, a structured query (e.g. in SPARQL) can be con- structed to retrieve the result entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conditional Factoid Factorization</head><p>Given a question q, the joint conditional probabil- ity of subject-relation pairs p(s, r|q) can be used to retrieve the answer using the exact inference defined by Eq. (1). However, since there can be millions of entities and thousands of relations in a knowledge base, it is less effective to model p(s, r|q) directly. Instead, we propose a condi- tional factoid factorization,</p><formula xml:id="formula_1">p(s, r|q) = p(r|q) · p(s|q, r)<label>(2)</label></formula><p>and utilize two neural networks to parameter- ize each component, p(r|q) and p(s|q, r), respec- tively. Hence, our proposed method contains two phases: inferring the implied relation r from the question q, and inferring the mentioned subject en- tity s given the relation r and the question q.</p><p>There is an alternative factorization p(s, r|q) = p(s|q) · p(r|s, q). However, it is rather challenging to estimate p(s|q) directly due to the vast amount of entities (&gt; 10 6 ) in a KB. In comparison, our proposed factorization takes advantage of the rel- atively limited number of relations (on the order of thousands). What's more, by exploiting addi- tional information from the candidate relation r, it's more feasible to model p(s|q, r) than p(s|q), leading to more robust estimation.</p><p>A key difference from prior multi-step approach is that our method do not assume any indepen- dence between the target subject and relation given a question, as does in the prior method <ref type="bibr" target="#b39">(Yih et al., 2014)</ref>. It proves effective in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference via Focused Pruning</head><p>As defined by the Eq. (1), a solution needs to con- sider all available subject-relation pairs in the KB as candidates. With a large-scale KB, the number of candidates can be notoriously large, resulting in a extremely noisy candidate pool. We propose a method to prune the candidate space. The pruning is equivalent to a function that takes a KB K and a question q as input, and outputs a much limited set C of candidate subject-relation pairs.</p><formula xml:id="formula_2">H(K, q) → C (3)</formula><p>C s and C r are used to represent the subject and re- lation candidates, respectively. The fundamental intuition for pruning is that the subject entity must be mentioned by some textual substring (subject mention) in the question. Thus, the candidate space can be restricted to entities whose name/alias matches an n-gram of the ques- tion, as in ( <ref type="bibr" target="#b39">Yih et al., 2014;</ref>. We refer to this straight-forward method as N-Gram pruning. By considering all n- grams, this approach usually achieves a high recall rate. However, the candidate pool is still noisy due to many non-subject-mention n-grams.</p><p>Our key idea is to reduce the noise by guiding the pruning method's attention to more probable parts of a question. An observation is that cer- tain parts of a sentence are more likely to be the subject mention than others. For example, "Harry Potter" in "Who created the character Harry Pot- ter" is more likely than "the character", "charac- ter Harry", etc. Specifically, our method employs a deep network to identify such focus segments in a question. This way, the candidate pool can be not only more compact, but also significantly less noisy.</p><p>Finally, combing the ideas of Eq. <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula">(3)</ref>, we propose an approximate solution to the problem defined by Eq.</p><formula xml:id="formula_3">(1) ˆ s, ˆ r ≈ arg max s,r∈C p(s|q, r)p(r|q)<label>(4)</label></formula><p>this work. Then, we describe the model parame- terization of p(r|q) and p(s|q, r), and the focused pruning method in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Review: Gated Recurrent Units</head><p>In this work we employ GRU ( <ref type="bibr" target="#b11">Cho et al., 2014</ref>) as the RNN structure. At time step t, a GRU com- putes its hidden state h t using the following com- pound functions</p><formula xml:id="formula_4">z = sigmoid (W xz x t + W hz h t−1 + b z ) (5) r = sigmoid (W xr x t + W hr h t−1 + b r ) (6) ˜ h = tanh (W xh x t + r ⊗ W hh h t−1 + b h ) (7) h t = z ⊗ h t−1 + (1 − z) ⊗ ˜ h (8)</formula><p>where W {·} , and b {·} are all trainable parameters.</p><p>To better capture the context information on both sides, two GRUs with opposite directions can be combined to form a bidirectional GRU (BiGRU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Parameterization</head><p>Relation network In this work, the probability of relations given a question, p(r|q), is modeled by the following network</p><formula xml:id="formula_5">p θr (r|q) = exp v(r, q) r exp v(r , q)<label>(9)</label></formula><p>where the relation scoring function v(r, q) mea- sures the similarity between the question and the relation v(r, q) = f (q) E(r)</p><p>E(r) is the trainable embedding of the relation (randomly initialized in this work) and f (q) com- putes the semantic question embedding. Specifi- cally, the question q is represented as a sequence of tokens (potentially with unknown ones). Then, the question embedding model f consists of a word embedding layer to transform tokens into distributed representations, a two-layer BiGRU to capture the question semantics, and a linear layer to project the final hidden states of the BiGRU into the same vector space as E(r).</p><p>Subject network As introduced in section 3, the factor p(s|q, r) models the fitness of a subject s appearing in the question q, given the main topic is about the relation r. Thus, two forces a) the raw context expressed by q, and b) the candidate topic described by r, jointly impact the fitness of the subject s. For simplicity, we use two additive terms to model the joint effect</p><formula xml:id="formula_7">p θs (s|q, r) = exp u(s, r, q) s exp u(s , r, q)<label>(11)</label></formula><p>where u(s, r, q) is the subject scoring function,</p><formula xml:id="formula_8">u(s, r, q) = g(q) E(s) + αh(r, s)<label>(12)</label></formula><p>g <ref type="formula">(q)</ref> is another semantic question embedding, E(s) is a vector representation of a subject, h(r, s) is the subject-relation score, and α is the weight parameter used to trade off the two sources. Firstly, the context score g(q) E(s) models the intrinsic plausibility that the subject s appears in the question q using vector space similarity. As g(q) E(s) has the same form as equation <ref type="formula" target="#formula_0">(10)</ref>, we let g adpot the same model structure as f . However, initializing E(s) randomly and training it with supervised signal, just like training E(r), is insufficient in practice -while a large-scale KB has millions of subjects, only thousands of question-triple pairs are available for training. To alleviate the problem, we seek two potential solu- tions: a) pretrained embeddings, and b) type vec- tor representation.</p><p>The pretrained embedding approach utilizes un- supervised method to train entity embedings. In particular, we employ the TransE ( <ref type="bibr" target="#b5">Bordes et al., 2013)</ref>, which trains the embedings of entities and relations by enforcing E(s) + E(r) = E(o) for every observed triple (s, r, o) ∈ K. As there exists other improved variants ( <ref type="bibr" target="#b16">Gu et al., 2015)</ref>, TransE scales the best when KB size grows.</p><p>Alternatively, type vector is a fixed (not train- able) vector representation of entities using type information. Since each entity in the KB has one or more predefined types, we can encode the en- tity as a vector (bag) of types. Each dimension of a type vector is either 1 or 0, indicating whether the entity is associated with a specific type or not. Thus, the dimensionality of a type vector is equal to the number of types in KB. Under this setting, with E(s) being a binary vector, let g(q) be a con- tinuous vector with arbitrary value range can be problematic. Therefore, when type vector is used as E(s), we add a sigmoid layer upon the final lin- ear projection of g, squashing each element of g(q) to the range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>Compared to the first solution, type vector is fully based on the type profile of an entity, and requires no training. As a benefit, considerably  <ref type="figure">Figure 1</ref>: Overall structure of the subject network. Sigmoid layer is added only when type vector is used as E(s).</p><p>fewer parameters are needed. Also, given the type information is discriminative enough, using type vector will lead to easier generalization. However, containing only type information can be very re- strictive.</p><p>In addition to the context score, we use the subject-relation score h(r, s) to capture the com- patibility that s and r show up together. Intuitively, for an entity to appear in a topic characterized by a relation, a necessary condition will be that the entity has the relation connected to it. Inspired by this structural regularity, in the simplest manner, we instantiate the idea with an indicator function,</p><formula xml:id="formula_9">h(r, s) = 1(s → r)<label>(13)</label></formula><p>As there exists other more sophisticated statistical parameterizations, the proposed approach is able to capture the core idea of the structural regularity without any parameter. Finally, putting two scores together, <ref type="figure">Fig.1</ref> summarizes the overall structure of the subject network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Focused Pruning</head><p>As discussed in section 3.2, N-Gram pruning is still subject to large amount of noise in inference due to many non-subject-mention n-grams. Moti- vated by this problem, we propose to reduce such noise by focusing on more probable candidates us- ing a special-purpose sequence labeling network. Basically, a sequence labeling model is trained to tag some consecutive tokens as the subject men- tion. Following this idea, during inference, only the most probable n-gram predicted by the model will be retained, and then used as the subject men- tion to generate the candidate pool C. Hence, we refer to this method as focused pruning. Formally, let W(q) be all the n-grams of the question q, p(w|q) be the probability that the n-gram w is the subject mention of q, the focused pruning function H s is defined asˆw asˆ asˆw = arg max</p><formula xml:id="formula_10">w∈W(q) p κ (w|q) C = {(s, r) : M(s, ˆ w), s → r}<label>(14)</label></formula><p>where M(s, ˆ w) represents some predefined match between the subject s and the predicted subject mentionˆwmentionˆ mentionˆw. Intuitively, this pruning method re- sembles the human behavior of first identifying the subject mention with the help of context, and then using it as the key word to search the KB.</p><p>To illustrate the effectiveness of this idea, we parameterize p κ (w|q) with a general-purpose neu- ral labeling model, which consists of a word em- bedding layer, two layers of BiGRU, and a linear- chain conditional random field (CRF). Thus, given a question q of length T , the score of a sequence label configuration y ∈ R T is</p><formula xml:id="formula_11">s(y, q) = T t=1 H(q) t,yt + T t=2 A y t−1 ,yt</formula><p>where H(q) is the hidden output of the top-layer BiGRU, A is the transition matrix possesed by the CRF, and <ref type="bibr">[·]</ref> i,j indicates the matrix element on row i collum j.</p><p>Finally, the match function M(s, ˆ w) is simply defined as either strict match between an alias of s andˆwandˆ andˆw, or approximate match provided by the Freebase entity suggest API 1 . Note that more elaborative match function can further boost the performance, but we leave it for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Parameter Estimation</head><p>In this section, we discuss the parameter estima- tion for the neural models presented in section 4.</p><p>With standard parameterization, the focused la- beling model p κ (w|q) can be directly trained by maximum likelihood estimation (MLE) and back- propagation. Thus, we omit the discussion here, and refer readers to ( <ref type="bibr" target="#b18">Huang et al., 2015</ref>) for de- tails. Also, we leave the problem of how to obtain the training data to section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Decomposable Log-Likelihood</head><p>To estimate the parameters of p θr (r|q) and p θs (s|r, q), MLE can be utilized to maximize the empirical (log-)likelihood of subject-relation pairs given the associated question. Following this idea, let {s (i) , r (i) , q (i) } N i=1 be the training dataset, the MLE solution takes the form</p><formula xml:id="formula_12">θ MLE = arg max θr,θs N i=1</formula><p>log p θr (r (i) |q (i) )</p><p>+ log p θs (s (i) |r (i) , q (i) )</p><p>Note that there is no shared parameter between p θs (s|q, r) and p θr (r|q). 2 Therefore, the same so- lution can be reached by separately optimizing the two log terms, i.e.</p><formula xml:id="formula_14">θ MLE r = arg max θr N i=1 log p θr (r (i) |q (i) ) θ MLE s = arg max θs N i=1 log p θs (s (i) |r (i) , q (i) )<label>(16)</label></formula><p>It is important to point out that the decomposabil- ity does not always hold. For example, when the parametric form of h(s, r) depends on the embed- ding of r, the two terms will be coupled and joint optimization must be performed. From this per- spective, the simple form of h(s, r) also eases the training by inducing the decomposability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Approximation with Negative Samples</head><p>As the two problems defined by equation (16) take the standard form of classification, theoretically, cross entropy can used as the training objective. However, computing the partition function is often intractable, especially for p θs (s|r, q), since there can be millions of entities in the KB. Faced with this problem, classic solutions include contrastive estimation <ref type="bibr" target="#b32">(Smith and Eisner, 2005</ref>), importance sampling approximation ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref>, and hinge loss with negative samples <ref type="bibr" target="#b12">(Collobert and Weston, 2008)</ref>. In this work, we utilize the hinge loss with nega- tive samples as the training objective. Specifically, the loss function w.r.t θ r has the form</p><formula xml:id="formula_15">L(θ r ) = N i=1 Mr j=1 max 0, γ r − v(r (i) , q (i) ) + v(r (j) , q (i) )<label>(17)</label></formula><p>where r (j) is one of the M r negative samples (i.e. s (i) → r (j) ) randomly sampled from R, and γ r is the predefined margin. Similarly, the loss function w.r.t θ s takes the form</p><formula xml:id="formula_16">L(θ s ) = N i=1 Ms j=1 max 0, γ s − u(s (i) , r (i) , q (i) ) + u(s (j) , r (i) , q (i) )<label>(18)</label></formula><p>Despite the negative sample based approximation, there is another practical difficulty when type vec- tor is used as the subject representation. Specifi- cally, computing the value of u(s (j) , r (i) , q (i) ) re- quires to query the KB for all types of each nega- tive sample s (j) . So, when M s is large, the train- ing can be extremely slow due to the limited band- width of KB query. Consequently, under the set- ting of type vector, we instead resort to the follow- ing type-wise binary cross-entropy loss˜L</p><formula xml:id="formula_17">loss˜ loss˜L(θ s ) = − N i=1 K k=1 E(s (i) ) k log g(q (i) ) k + 1 − E(s (i) ) k log 1 − g(q (i) ) k (19)</formula><p>where K is the total number of types, g(q) k and E(s (i) ) k are the k-th element of g(q) and E(s (i) ) respectively. Intuitively, with sigmoid squashed output, g(q) can be regarded as K binary classi- fiers, one for each type. Hence, g(q) k reprents the predicted probability that the subject is associated with the k-th type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we conduct experiments to evaluate the proposed system empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dataset and Knowledge Base</head><p>We train and evaluate our method on the SIMPLE- QUESTIONS dataset <ref type="bibr">3</ref>    <ref type="bibr" target="#b9">(Cai and Yates, 2013)</ref>. However, these datasets are quite restricted in sample size -the former includes 5,810 samples (train + test) and the latter includes 917 ones. They are fewer than the number of relations in Freebase.</p><p>To train the focused labeling model, the infor- mation about whether a word is part of the sub- ject mention is needed. We obtain such informa- tion by reverse linking from the ground-truth sub- ject to its mention in the question. Given a ques- tion q corresponding to subject s, we match the name and aliases of s to all n-grams that can be generated from q. Once a match is found, we la- bel the matched n-gram as the subject mention. In the case of multiple matches, only the longest matched n-gram is used as the correct one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation and Baselines</head><p>For evaluation, we consider the same metric in- troduced in , which takes the prediction as correct if both the subject and rela- tion are correctly retrieved. Based on this met- ric, we compare CFO with a few baseline systems, which include both the Memory Network QA sys- tem ( , and systems with al- ternative components and parameterizations from existing work ( <ref type="bibr" target="#b39">Yih et al., 2014;</ref>). We did not compare with alternative subject net- works because the only existing method ( <ref type="bibr" target="#b39">Yih et al., 2014</ref>) relies on unique textual name of each entity, which does not generally hold in knowledge bases (except in REVERB). Alternative approaches for pruning method, relation network, and entity rep- resentation are described below.</p><p>Pruning methods We consider two baseline methods previously used to prune the search space. The first baseline is the N-Gram pruning method introduced in Section 3, as it has been suc- cessfully used in previous work <ref type="bibr" target="#b39">(Yih et al., 2014;</ref>. Basically, it establishes the candidate pool by retaining subject-relation pairs whose subject can be linked to one of the n-grams generated from the question. The second one is N- Gram+, a revised version of the N-Gram pruning with additional heuristics ( . In- stead of considering all n-grams that can be linked to entities in KB, heuristics related to overlapping n-grams, stop words, interrogative pronouns, and so on are exploited to further shrink the n-gram pool. Accordingly, the search space is restricted to subject-relation pairs whose subject can be linked to one of the remaining n-grams after applying the heuristic filtering.</p><p>Relation scoring network We compare our pro- posed method with two previously used models. The first baseline is the embedding average model (Embed-AVG) used in ( <ref type="bibr" target="#b6">Bordes et al., 2014a;</ref><ref type="bibr" target="#b7">Bordes et al., 2014b;</ref>. Basically, it takes the element-wise average of the word em- beddings of the question to be the question rep- resentation. The second one is the letter-tri-gram CNN (LTG-CNN) used in ( <ref type="bibr" target="#b39">Yih et al., 2014;</ref>, where the question and relation are separately embedded into the vector space by two parameter shared LTG-CNNs. <ref type="bibr">4</ref> In addition, ( <ref type="bibr" target="#b39">Yih et al., 2014;</ref>) observed better per- formance of the LTG-CNN when substituting the subject mention with a special symbol. Naturally, this can be combined with the proposed focused labeling, since the latter is able to identify the po- tential subject mention in the question. So, we train another LTG-CNN with symbolized ques- tions, which is denoted as LTG-CNN+. Note that this model is only tested when the focused labeling pruning is used.</p><p>Entity representation In section 4.2, we de- scribe two possible ways to improve the vector representation of the subject, TransE pretrained embedding and type vectors. To evaluate their ef- fectiveness, we also include this variation in the experiment, and compare their performance with randomly initialized entity embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiment Setting</head><p>During training, all word embeddings are initial- ized using the pretrained GloVe ( <ref type="bibr" target="#b28">Pennington et al., 2014)</ref>, and then fine tuned in subsequent train- ing. The word embedding dimension is set to 300, and the BiGRU hidden size 256. For pre- training the entity embeddings using TransE (see section 4.2), only triples included in FB5M are used. All other parameters are randomly ini- tialized uniformly from [−0.08, 0.08], following <ref type="bibr" target="#b15">(Graves, 2013)</ref>. Both hinge loss margins γ s and γ r are set to 0.1. Negative sampling sizes M s and M r are both 1024.</p><p>For optimization, parameters are trained using mini-batch AdaGrad <ref type="bibr" target="#b13">(Duchi et al., 2011</ref>) with Mo- mentum ( <ref type="bibr" target="#b30">Pham et al., 2015</ref> tuned to be 0.001 for question embedding with type vector, 0.03 for LTG-CNN methods, and 0.02 for rest of the models. Momentum rate is set to 0.9 for all models, and the mini-batch size is 256. In addition, vertical dropout ( <ref type="bibr" target="#b29">Pham et al., 2014;</ref><ref type="bibr" target="#b41">Zaremba et al., 2014</ref>) is used to regularize all Bi- GRUs in our experiment. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results</head><p>Trained on 75,910 questions, our proposed model and baseline methods are evaluated on the testing set with 21,687 questions. <ref type="table">Table 1</ref> presents the ac- curacy of those methods. We evaluated all combi- nations of pruning methods, relation networks and entity representation schemes, as well as the result from memory network, as described in Section 6.1. CFO (focused pruning + BiGRU + type vec- tor) achieves the best performance, outperforming all other methods by substantial margins. By inspecting vertically within each cell in Ta- ble 1, for the same pruning methods and entity rep- resentation scheme, BiGRU based relation scor- ing network boosts the accuracy by 3.5 % to 4.8% compared to the second best alternative. This ev- idence suggests the superiority of RNN in captur- ing semantics of question utterances. Surprisingly, it turns out that Embed-AVG achieves better per- formance than the more complex LTG-CNN.</p><p>By inspecting <ref type="table">Table 1</ref> horizontally, type vec- tor based representation constantly leads to bet- ter performance, especially when N-Gram pruning is used. It suggests that under sparse supervision, training high-quality distributed knowledge repre-sentations remains a challenging problem. That said, pretraining entity embeddings with TransE indeed gives better performance compared to ran- dom initialization, indicating the future potential of unsupervised methods in improving continuous knowledge representation.</p><p>In addition, all systems using our proposed fo- cused pruning method outperform their counter- parts with alternative pruning methods. Without using ensembles, CFO is already better than the memory network ensembles by 11.8%. It sub- stantiates the general effectiveness of the focused pruning with subject labeling method regardless of other sub-modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Effectiveness of Pruning</head><p>According to the results in section 6.4, the focused pruning plays a critical role in achieving the best performance. To get a deeper understanding of its effectiveness, we analyze how the pruning meth- ods affect the accuracy of the system. Due to space limit, we focus on systems with BiGRU as the re- lation scoring function and type vector as the en- tity representation. <ref type="table">Table 2</ref> summarizes the recall -the percent- age of pruned subject-relation candidates contain- ing the answer -and the resulting accuracy. The single-subject case refers to the scenario that there is only one candidate entity in C s (possi- bly with multiple relations), and the multi-subject case means there are multiple entities in C s . As the table shows, focused pruning achieves com- parable recall rate to N-Gram pruning. <ref type="bibr">6</ref> Given the state-of-the-art performance of sequence la- beling systems, this result should not be surpris- ing. Thus, the difference in performances entirely comes from their resulting accuracy. Notice that there exists a huge accuracy gap between the two cases. Essentially, in the single-candidate case, the system only need to identify the relation based on the more robust model p θr (r|q). In contrast, under the multi-candidate case, the system also relies on p θs (s|q, r), which has significantly more parame- ters to estimate, and thus is less robust. Conse- quently, by only focusing on the most probable sub-string, the proposed focused pruning produces much more single-candidate situations, leading to a better overall accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pruning method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pruning recall</head><p>Inference accuracy within the recalled Overall accuracy Single-subject case Multi-subject case N-Gram 94.8% 18 / 21 = 85.7% 12051 / 20533 = 58.7% 55.7% N-Gram+ 92.9% 126 / 138 = 91.3% 13460 / 20017 = 67.2% 62.6% Focused pruning 94.9% 9925 / 10705 = 92.7% 6482 / 9876 = 65.6% 75.7% <ref type="table">Table 2</ref>: Comparison of different space pruning methods. N-Gram+ uses additional heuristics. Single-and multi-subject refers to the number of distinct subjects in candidates. The proposed focused pruning achieves best scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Additional Analysis</head><p>In the aforementioned experiments, we have kept the focused labeling model and the subject scoring network fixed. To further understand the impor- tance and sensitivity of this specific model design, we investigate some variants of these two models.</p><p>Alternative focus with CRF RNN-CRF based models have achieved the state-of-the-art perfor- mance on various sequence labeling tasks <ref type="bibr" target="#b18">(Huang et al., 2015;</ref>. However, the la- beling task we consider here is relatively unso- phisticated in the sense that there are only two categories of labels -part of subject string (SUB) or not (O). Thus, it's worth investigating whether RNN (BiGRU in our case) is still a critical com- ponent when the task gets simple. Hence, we es- tablish a CRF baseline which uses traditional fea- tures as input. Specifically, the model is trained with Stanford CRF-NER toolkit 7 on the same reversely linked labeling data (section 6.1). For evaluation, we directly compare the sentence level accuracy of these two models on the test portion of the labeling data. A sentence labeling is con- sidered correct only when all tokens are correctly labeled. 8 It turns out the RNN-CRF achieves an accuracy of 95.5% while the accuracy of feature based CRF is only 91.2%. Based on the result, we conclude that BiGRU plays a crucial role in our focused pruning module.</p><p>Subject scoring with average embedding As discussed in section 4.2, the subject network g is chosen to be the same as f , mainly relying on a two-layer BiGRU to produce the semantic ques- tion embeding. Although it is a natural choice, it remains unclear whether the final performance is sensitive to this design. Motivated by this ques- tion, we substitute the BiGRU with an Embed- AVG model, and evalute the system performance.</p><p>7 http://nlp.stanford.edu/software/ CRF-NER.shtml 8 As F -1 score is usually used as the metric for sequence labeling, sentence level accuracy is more informative here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Network Subject Network</head><p>Embed  <ref type="table">Table 3</ref>: System performance with different subject network structures.</p><p>For this experiment, we always use focused prun- ing and type vector, but vary the structure of the relation scoring network to allow high-order inter- action across models. The result is summarized in <ref type="table">Table 3</ref>. Insepcting the table horizontally, when BiGRU is employed as the subject network, the accuracy is consistently higher regardless of re- lation network structures. However, the margin is quite narrow, especially compared to the effect of varying the relation network structure the same way. We suspect this difference reflects the fact that modeling p(s|r, q) is intrinsically more chal- lenging than modeling p(r|q). It also suggests that learning smooth entity representations with good discriminative power remains an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose CFO, a novel approach to single-fact question answering. We employ a conditional factoid factorization by inferring the target relation first and then the target subject as- sociated with the candidate relations. To resolve the representation for millions of entities, we pro- posed type-vector scheme which requires no train- ing. Our focused pruning largely reduces the can- didate space without loss of recall rate, leading to significant improvement of overall accuracy. Compared with multiple baselines across three as- pects, our method achieves the state-of-the-art ac- curacy on a 108k question dataset, the largest pub- licly available one. Future work could be extend- ing the proposed method to handle more complex questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Free917</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>). Learning rates are</head><label></label><figDesc></figDesc><table>Pruning 
Method 
Relation 
Network 
Entity Representation 

Random Pretrain Type Vec 

Memory Network 
62.9 63.9  *  

N-Gram 
Embed-AVG 
39.4 
42.2 
50.9 
LTG-CNN 
32.8 
36.8 
45.6 
BiGRU 
43.7 
46.7 
55.7 

N-Gram+ 
Embed-AVG 
53.8 
57.0 
58.7 
LTG-CNN 
46.3 
50.9 
56.0 
BiGRU 
58.3 
61.6 
62.6 

Focused 
Pruning 

Embed-AVG 
71.4 
71.7 
72.1 
LTG-CNN 
67.6 
67.9 
68.6 
LTG-CNN+ 
70.2 
70.4 
71.1 
BiGRU 
75.2 
75.5 
75.7 

Table 1: Accuracy on SIMPLEQUESTIONS testing set.  *  indi-
cates using ensembles. N-Gram+ uses additional heuristics. 
The proposed CFO (focused pruning + BiGRU + type vector) 
achieves the top accuracy. 

</table></figure>

			<note place="foot" n="4"> Proposed CFO In this section, we first review the gated recurrent unit (GRU), an RNN variant extensively used in</note>

			<note place="foot" n="1"> The approximate match is used only when there is no strict match. The suggest API takes a string as input, and returns no more than 20 potentially matched entities.</note>

			<note place="foot" n="2"> Word embeddings are not shared across models.</note>

			<note place="foot" n="3"> https://research.facebook.com/ researchers/1543934539189348</note>

			<note place="foot" n="4"> In Freebase, each predefined relation has a single humanrecognizable reference form, usually a sequence of words.</note>

			<note place="foot" n="5"> For more details, source code is available at http:// zihangdai.github.io/cfo for reference.</note>

			<note place="foot" n="6"> Less than 3% of the recalled candidates rely on approximate matching in the focused pruning.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imitation learning of agenda-based semantic parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="545" to="558" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="165" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="423" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Estevam R Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1608" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07285</idno>
		<title level="m">Ask me anything: Dynamic memory networks for natural language processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with onthe-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Reasoning in vector space: An exploratory study of question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moontae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolensky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06426</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Twisted recurrent network for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zefu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bay Area Machine Learning Symposium</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05508</idno>
		<title level="m">Towards neural network-based reasoning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théodore</forename><surname>Vu Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Louradour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="285" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On optimization algorithms for recurrent networks with long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bay Area Machine Learning Symposium</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing without question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="354" to="362" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using multiple clause constructors in inductive logic programming for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lappoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: ECML 2001</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="466" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations (ICLR2015)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint relational embeddings for knowledgebased question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Chul</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Chang</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="645" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Recurrent neural network regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1409.2329</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to parse database queries using inductive logic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
