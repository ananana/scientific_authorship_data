<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
							<email>sascha@cis.lmu.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information &amp; Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information &amp; Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Information &amp; Language Processing</orgName>
								<orgName type="institution">University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1793" to="1803"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present AutoExtend, a system to learn embeddings for synsets and lexemes. It is flexible in that it can take any word embed-dings as input and does not need an additional training corpus. The synset/lexeme embeddings obtained live in the same vector space as the word embeddings. A sparse tensor formalization guarantees efficiency and parallelizability. We use WordNet as a lexical resource, but Auto-Extend can be easily applied to other resources like Freebase. AutoExtend achieves state-of-the-art performance on word similarity and word sense disam-biguation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised methods for word embeddings (also called "distributed word representations") have become popular in natural language processing (NLP). These methods only need very large cor- pora as input to create sparse representations (e.g., based on local collocations) and project them into a lower dimensional dense vector space. Examples for word embeddings are SENNA <ref type="bibr" target="#b9">(Collobert and Weston, 2008)</ref>, the hierarchical log-bilinear model <ref type="bibr" target="#b25">(Mnih and Hinton, 2009</ref>), word2vec <ref type="bibr" target="#b22">(Mikolov et al., 2013c</ref>) and GloVe ( <ref type="bibr" target="#b28">Pennington et al., 2014</ref>). However, there are many other resources that are undoubtedly useful in NLP, including lexical re- sources like WordNet and Wiktionary and knowl- edge bases like Wikipedia and Freebase. We will simply call these resources in the rest of the pa- per. Our goal is to enrich these valuable resources with embeddings for those data types that are not words; e.g., we want to enrich WordNet with em- beddings for synsets and lexemes. A synset is a set of synonyms that are interchangeable in some con- text. A lexeme pairs a particular spelling or pro- nunciation with a particular meaning, i.e., a lex- eme is a conjunction of a word and a synset. Our premise is that many NLP applications will bene- fit if the non-word data types of resources -e.g., synsets in WordNet -are also available as embed- dings. For example, in machine translation, en- riching and improving translation dictionaries (cf. <ref type="bibr" target="#b21">Mikolov et al. (2013b)</ref>) would benefit from these embeddings because they would enable us to cre- ate an enriched dictionary for word senses. Gen- erally, our premise is that the arguments for the utility of embeddings for word forms should carry over to the utility of embeddings for other data types like synsets in WordNet.</p><p>The insight underlying the method we propose is that the constraints of a resource can be formal- ized as constraints on embeddings and then allow us to extend word embeddings to embeddings of other data types like synsets. For example, the hy- ponymy relation in WordNet can be formalized as such a constraint.</p><p>The advantage of our approach is that it de- couples embedding learning from the extension of embeddings to non-word data types in a resource. If somebody comes up with a better way of learn- ing embeddings, these embeddings become imme- diately usable for resources. And we do not rely on any specific properties of embeddings that make them usable in some resources, but not in others.</p><p>An alternative to our approach is to train embed- dings on annotated text, e.g., to train synset em- beddings on corpora annotated with synsets. How- ever, successful embedding learning generally re- quires very large corpora and sense labeling is too expensive to produce corpora of such a size.</p><p>Another alternative to our approach is to add up all word embedding vectors related to a particular node in a resource; e.g., to create the synset vector of lawsuit in WordNet, we can add the word vec- tors of the three words that are part of the synset (lawsuit, suit, case). We will call this approach naive and use it as a baseline (S naive in <ref type="table" target="#tab_4">Table 3</ref>).</p><p>We will focus on WordNet <ref type="bibr" target="#b11">(Fellbaum, 1998</ref>) in this paper, but our method -based on a formaliza- tion that exploits the constraints of a resource for extending embeddings from words to other data types -is broadly applicable to other resources in- cluding Wikipedia and Freebase.</p><p>A word in WordNet can be viewed as a compo- sition of several lexemes. Lexemes from different words together can form a synset. When a synset is given, it can be decomposed into its lexemes. And these lexemes then join to form words. These observations are the basis for the formalization of the constraints encoded in WordNet that will be presented in the next section: we view words as the sum of their lexemes and, analogously, synsets as the sum of their lexemes.</p><p>Another motivation for our formalization stems from the analogy calculus developed by <ref type="bibr" target="#b20">Mikolov et al. (2013a)</ref>, which can be viewed as a group theory formalization of word relations: we have a set of elements (our vectors) and an operation (addition) satisfying the properties of a mathemat- ical group, in particular, associativity and invert- ibility. For example, you can take the vector of king, subtract the vector of man and add the vec- tor of woman to get a vector near queen. In other words, you remove the properties of man and add the properties of woman. We can also see the vec- tor of king as the sum of the vector of man and the vector of a gender-neutral ruler. The next thing to notice is that this does not only work for words that combine several properties, but also for words that combine several senses. The vector of suit can be seen as the sum of a vector representing law- suit and a vector representing business suit. Auto- Extend is designed to take word vectors as input and unravel the word vectors to the vectors of their lexemes. The lexeme vectors will then give us the synset vectors.</p><p>The main contributions of this paper are: (i) We present AutoExtend, a flexible method that ex- tends word embeddings to embeddings of synsets and lexemes. AutoExtend is completely general in that it can be used for any set of embeddings and for any resource that imposes constraints of a cer- tain type on the relationship between words and other data types. (ii) We show that AutoExtend achieves state-of-the-art word similarity and word sense disambiguation (WSD) performance. (iii) We publish the AutoExtend code for extending word embeddings to other data types, the lexeme and synset embeddings and the software to repli- cate our WSD evaluation. This paper is structured as follows. Section 2 in- troduces the model, first as a general tensor formu- lation then as a matrix formulation making addi- tional assumptions. In Section 3, we describe data, experiments and evaluation. We analyze Auto- Extend in Section 4 and give a short summary on how to extend our method to other resources in Section 5. Section 6 discusses related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We are looking for a model that extends standard embeddings for words to embeddings for the other two data types in WordNet: synsets and lexemes. We want all three data types -words, lexemes, synsets -to live in the same embedding space.</p><p>The basic premise of our model is: (i) words are sums of their lexemes and (ii) synsets are sums of their lexemes. We refer to these two premises as synset constraints. For example, the embedding of the word bloom is a sum of the embeddings of its two lexemes bloom(organ) and bloom(period); and the embedding of the synset flower-bloom- blossom(organ) is a sum of the embeddings of its three lexemes flower(organ), bloom(organ) and blossom(organ).</p><p>The synset constraints can be argued to be the simplest possible relationship between the three WordNet data types. They can also be motivated by the way many embeddings are learned from corpora -for example, the counts in vector space models are additive, supporting the view of words as the sum of their senses. The same assumption is frequently made; for example, it underlies the group theory formalization of analogy discussed in Section 1.</p><p>We denote word vectors as w (i) ∈ R n , synset vectors as s (j) ∈ R n , and lexeme vectors as l (i,j) ∈ R n . l <ref type="bibr">(i,j)</ref> is that lexeme of word w (i) that is a mem- ber of synset s (j) . We set lexeme vectors l (i,j) that do not exist to zero. For example, the non-existing lexeme flower(truck) is set to zero. We can then formalize our premise that the two constraints (i) and (ii) hold as follows:</p><formula xml:id="formula_0">w (i) = j l (i,j)<label>(1)</label></formula><formula xml:id="formula_1">s (j) = i l (i,j)<label>(2)</label></formula><p>These two equations are underspecified. We there- fore introduce the matrix E (i,j) ∈ R n×n :</p><formula xml:id="formula_2">l (i,j) = E (i,j) w (i)<label>(3)</label></formula><p>We make the assumption that the dimensions in Eq. 3 are independent of each other, i.e., E (i,j) is a diagonal matrix. Our motivation for this as- sumption is: (i) This makes the computation tech- nically feasible by significantly reducing the num- ber of parameters and by supporting parallelism.</p><p>(ii) Treating word embeddings on a per-dimension basis is a frequent design choice (e.g., <ref type="bibr" target="#b15">Kalchbrenner et al. (2014)</ref>). Note that we allow E (i,j) &lt; 0 and in general the distribution weights for each di- mension (diagonal entries of E (i,j) ) will be differ- ent. Our assumption can be interpreted as word w (i) distributing its embedding activations to its lexemes on each dimension separately. Therefore, Eqs. 1-2 can be written as follows:</p><formula xml:id="formula_3">w (i) = j E (i,j) w (i)<label>(4)</label></formula><formula xml:id="formula_4">s (j) = i E (i,j) w (i)<label>(5)</label></formula><p>Note that from Eq. 4 it directly follows that:</p><formula xml:id="formula_5">j E (i,j) = I n ∀i<label>(6)</label></formula><p>with I n being the identity matrix. Let W be a |W | × n matrix where n is the di- mensionality of the embedding space, |W | is the number of words and each row w (i) is a word em- bedding; and let S be a |S| × n matrix where |S| is the number of synsets and each row s (j) is a synset embedding. W and S can be interpreted as linear maps and a mapping between them is given by the rank 4 tensor E ∈ R |S|×n×|W |×n . We can then write Eq. 5 as a tensor product:</p><formula xml:id="formula_6">S = E ⊗ W (7)</formula><p>while Eq. 6 states, that</p><formula xml:id="formula_7">j E i,d 1 j,d 2 = 1 ∀i, d 1 , d 2 (8)</formula><p>Additionally, there is no interaction between dif- ferent dimensions, so</p><formula xml:id="formula_8">E i,d 1 j,d 2 = 0 if d 1 = d 2 .</formula><p>In other words, we are creating the tensor by stacking the diagonal matrices E (i,j) over i and j. Another sparsity arises from the fact that many lexemes do not exist:</p><formula xml:id="formula_9">E i,d 1 j,d 2 = 0 if l (i,j) = 0; i.e., l (i,j) = 0</formula><p>only if word i has a lexeme that is a member of synset j. To summarize the sparsity:</p><formula xml:id="formula_10">E i,d 1 j,d 2 = 0 ⇐ d 1 = d 2 ∨ l (i,j) = 0 (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning</head><p>We adopt an autoencoding framework to learn em- beddings for lexemes and synsets. To this end, we view the tensor equation S = E ⊗ W as the en- coding part of the autoencoder: the synsets are the encoding of the words. We define a corresponding decoding part that decodes the synsets into words as follows:</p><formula xml:id="formula_11">s (j) = i l (i,j) , w (i) = j l (i,j)<label>(10)</label></formula><p>In analogy to E (i,j) , we introduce the diagonal ma- trix D <ref type="bibr">(j,i)</ref> :</p><formula xml:id="formula_12">l (i,j) = D (j,i) s (j)<label>(11)</label></formula><p>In this case, it is the synset that distributes itself to its lexemes. We can then rewrite Eq. 10 to:</p><formula xml:id="formula_13">s (j) = i D (j,i) s (j) , w (i) = j D (j,i) s (j) (12)</formula><p>and we also get the equivalent of Eq. 6 for D <ref type="bibr">(j,i)</ref> :</p><formula xml:id="formula_14">i D (j,i) = I n ∀j<label>(13)</label></formula><p>and in tensor notation:</p><formula xml:id="formula_15">W = D ⊗ S<label>(14)</label></formula><p>Normalization and sparseness properties for the decoding part are analogous to the encoding part:</p><formula xml:id="formula_16">i D j,d 2 i,d 1 = 1 ∀j, d 1 , d 2 (15) D j,d 2 i,d 1 = 0 ⇐ d 1 = d 2 ∨ l (i,j) = 0<label>(16)</label></formula><p>We can state the learning objective of the autoen- coder as follows:</p><formula xml:id="formula_17">argmin E,D D ⊗ E ⊗ W − W<label>(17)</label></formula><p>under the conditions Eq. 8, 9, 15 and 16. Now we have an autoencoder where input and output layers are the word embeddings and the hidden layer represents the synset vectors. A sim- plified version is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The tensors E and D have to be learned. They are rank 4 tensors of size ≈10 15 . However, we already discussed that they are very sparse, for two reasons: (i) We make the assumption that there is no interaction between dimensions. (ii) There are only few interactions between words and synsets (only when a lexeme exists). In practice, there are only ≈10 7 elements to learn, which is technically feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Matrix formalization</head><p>Based on the assumption that each dimension is fully independent from other dimensions, a sepa- rate autoencoder for each dimension can be cre- ated and trained in parallel. Let W ∈ R |W |×n be a matrix where each row is a word embedding and</p><formula xml:id="formula_18">w (d) = W ·,d the d-th column of W , i.</formula><p>e., a vector that holds the d-th dimension of each word vector. In the same way, s (d) = S ·,d holds the d-th di- mension of each synset vector and</p><formula xml:id="formula_19">E (d) = E ·,d ·,d ∈ R |S|×|W | .</formula><p>We can write S = E ⊗ W as:</p><formula xml:id="formula_20">s (d) = E (d) w (d) ∀d<label>(18)</label></formula><p>with</p><formula xml:id="formula_21">E (d) i,j = 0 if l (i,j) = 0.</formula><p>The decoding equation W = D ⊗ S takes this form:</p><formula xml:id="formula_22">w (d) = D (d) s (d) ∀d<label>(19)</label></formula><p>where</p><formula xml:id="formula_23">D (d) = D ·,d ·,d ∈ R |W |×|S| and D (d) j,i = 0 if l (i,j) = 0</formula><p>. So E and D are symmetric in terms of non-zero elements. The learning objective be- comes:</p><formula xml:id="formula_24">argmin E (d) ,D (d) D (d) E (d) w (d) − w (d) ∀d<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Lexeme embeddings</head><p>The hidden layer S of the autoencoder gives us synset embeddings. The lexeme embeddings are defined when transitioning from W to S, or more explicitly by:</p><formula xml:id="formula_25">l (i,j) = E (i,j) w (i)<label>(21)</label></formula><p>However, there is also a second lexeme embedding in AutoExtend when transitioning form S to W :</p><formula xml:id="formula_26">l (i,j) = D (j,i) s (j)<label>(22)</label></formula><p>Aligning these two representations seems natural, so we impose the following lexeme constraints: <ref type="table" target="#tab_0">noun  verb  adj adv  hypernymy 84,505 13,256  0  0  antonymy  2,154  1,093  4,024 712  similarity  0  0 21,434  0  verb group  0</ref> 1,744 0 0 This can also be expressed dimension-wise. The matrix formulation is given by:</p><formula xml:id="formula_27">argmin E (i,j) ,D (j,i) E (i,j) w (i) − D (j,i) s (j) ∀i, j<label>(23)</label></formula><formula xml:id="formula_28">argmin E (d) ,D (d) E (d) diag(w (d) ) − D (d) diag(s (d) ) T ∀d<label>(24)</label></formula><p>with diag(x) being a square matrix having x on the main diagonal and vector s (d) defined by Eq. 18. While we try to align the embeddings, there are still two different lexeme embeddings. In all experiments reported in Section 4 we will use the average of both embeddings and in Section 4 we will analyze the weighting in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">WN relations</head><p>Some WordNet synsets contain only a single word (lexeme). The autoencoder learns based on the synset constraints, i.e., lexemes being shared by different synsets (and also words); thus, it is dif- ficult to learn good embeddings for single-lexeme synsets. To remedy this problem, we impose the constraint that synsets related by WordNet (WN) relations should have similar embeddings. <ref type="table" target="#tab_0">Table 1</ref> shows relations we used. WN relations are entered in a new matrix R ∈ R r×|S| , where r is the number of WN relation tuples. For each relation tuple, i.e., row in R, we set the columns corresponding to the first and second synset to 1 and −1, respectively. The values of R are not updated during training. We use a squared error function and 0 as target value. This forces the system to find similar val- ues for related synsets. Formally, the WN relation constraints are:</p><formula xml:id="formula_29">argmin E (d) RE (d) w (d) ∀d<label>(25)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Implementation</head><p>Our training objective is minimization of the sum of synset constraints (Eq. 20), weighted by α, the lexeme constraints (Eq. 24), weighted by β, and the WN relation constraints (Eq. 25), weighted by</p><formula xml:id="formula_30">1 − α − β.</formula><p>The training objective cannot be solved analyt- ically because it is subject to constraints Eq. 8, Eq. 9, Eq. 15 and Eq. 16. We therefore use back- propagation. We do not use regularization since we found that all learned weights are in <ref type="bibr">[−2, 2]</ref>. AutoExtend is implemented in MATLAB. We run 1000 iterations of gradient descent. On an In- tel Xeon CPU E7-8857 v2 3.00GHz, one iteration on one dimension takes less than a minute because the gradient computation ignores zero entries in the matrix.</p><formula xml:id="formula_31">L/suit (textil) S/suit-of-clothes L/suit (textil) W/suit L/suit (law) L/suit (law) W/suit W/case L/case S/lawsuit L/case W/case W/lawsuit L/lawsuit L/lawsuit W/lawsuit</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Column normalization</head><p>Our model is based on the premise that a word is the sum of its lexemes (Eq. 1). From the defini- tion of E (i,j) , we derived that E ∈ R |S|×n×|W |×n is normalized over the first dimension (Eq. 8). So E (d) ∈ R |S|×|W | is also normalized over the first dimension. In other words, E (d) is a column nor- malized matrix. Another premise of the model is that a synset is the sum of its lexemes. Therefore, D (d) is also column normalized. A simple way to implement this is to start the computation with column normalized matrices and normalize them again after each iteration as long as the error func- tion still decreases. When the error function starts increasing, we stop normalizing the matrices and continue with a normal gradient descent. This re- spects that while E (d) and D (d) should be column normalized in theory, there are a lot of practical issues that prevent this, e.g., OOV words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data, experiments and evaluation</head><p>We downloaded 300-dimensional embeddings for 3,000,000 words and phrases trained on Google News, a corpus of ≈10 11 tokens, using word2vec CBOW ( <ref type="bibr" target="#b22">Mikolov et al., 2013c</ref>). Many words in the word2vec vocabulary are not in WordNet, e.g., inflected forms (cars) and proper nouns (Tony Blair). Conversely, many WordNet lemmas are not in the word2vec vocabulary, e.g., 42 (digits were converted to 0). This results in a number of empty synsets (see <ref type="table" target="#tab_2">Table 2</ref>). Note however that AutoExtend can produce embeddings for empty synsets because we use WN relation constraints in addition to synset and lexeme constraints.</p><p>We run AutoExtend on the word2vec vectors. As we do not know anything about a suitable weighting for the three different constraints, we set α = β = 0.33. Our main goal is to produce compatible embeddings for lexemes and synsets. Thus, we can compute nearest neighbors across all three data types as shown in <ref type="figure">Figure 2</ref>.</p><p>We evaluate the embeddings on WSD and on similarity performance. Our results depend di- rectly on the quality of the underlying word em- beddings, in our case word2vec embeddings. We would expect even better evaluation results as word representation learning methods improve. Using a new and improved set of underlying em- beddings is simple: it is a simple switch of the input file that contains the word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Sense Disambiguation</head><p>For WSD we use the shared tasks of Senseval- 2 ( <ref type="bibr" target="#b16">Kilgarriff, 2001)</ref>   nearest neighbors of W/suit S/suit (businessman), L/suit (businessman), L/accomodate, S/suit (be acceptable), L/suit (be accept- able), L/lawsuit, W/lawsuit, S/suit (playing card), L/suit (playing card), S/suit (petition), S/lawsuit, W/countersuit, W/complaint, W/counterclaim nearest neighbors of W/lawsuit Ng, 2010). Senseval-2 contains 139, Senseval-3 57 different words. They provide 8,611, respec- tively 8,022 training instances and 4,328, respec- tively 3,944 test instances. For the system, we use the same setting as in the original paper. Pre- processing consists of sentence splitting, tokeniza- tion, POS tagging and lemmatization; the classi- fier is a linear SVM. In our experiments <ref type="table" target="#tab_4">(Table 3)</ref>, we run IMS with each feature set by itself to as- sess the relative strengths of feature sets (lines 1- 7) and on feature set combinations to determine which combination is best for WSD (lines 8, 12- 15).</p><formula xml:id="formula_32">L/lawsuit, S/lawsuit, S/countersuit, L/countersuit, W/countersuit, W/suit, W/counterclaim, S/counterclaim (n), L/counterclaim (n), S/counterclaim (v),</formula><p>IMS implements three standard WSD feature sets: part of speech (POS), surrounding word and local collocation (lines 1-3).</p><p>Let w be an ambiguous word with k senses. The three feature sets on lines 5-7 are based on the AutoExtend embeddings s (j) , 1 ≤ j ≤ k, of the synsets of w and the centroid c of the sentence in which w occurs. The centroid is simply the sum of all word2vec vectors of the words in the sentence, excluding stop words.</p><p>The S-cosine feature set consists of the k cosines of centroid and synset vectors:</p><formula xml:id="formula_33">&lt; cos(c, s (1) ), cos(c, s (2) ), . . . , cos(c, s (k) ) &gt;</formula><p>The S-product feature set consists of the nk element-wise products of centroid and synset vec- tors:</p><formula xml:id="formula_34">&lt; c 1 s (1)</formula><p>1 , . . . , c n s <ref type="formula" target="#formula_0">(1)</ref> n , . . . , c 1 s</p><formula xml:id="formula_35">(k) 1 , . . . , c n s (k) n &gt;</formula><p>where c i (resp. s</p><formula xml:id="formula_36">(j)</formula><p>i ) is element i of c (resp. s <ref type="bibr">(j)</ref> ). The idea is that we let the SVM estimate how im- portant each dimension is for WSD instead of giv- ing all equal weight as in S-cosine. The S-raw feature set simply consists of the n(k + 1) elements of centroid and synset vectors: &lt; c 1 , . . . , c n , s <ref type="formula" target="#formula_0">(1)</ref> 1 , . . . , s <ref type="formula" target="#formula_0">(1)</ref> n , . . . , s</p><formula xml:id="formula_37">(k) 1 , . . . , s (k) n &gt;</formula><p>Our main goal is to determine if AutoExtend features improve WSD performance when added to standard WSD features. To make sure that improvements we get are not solely due to the power of word2vec, we also investigate a sim- ple word2vec baseline. For S-product, the Auto- Extend feature set that performs best in the exper- iment (cf. lines 6 and 14), we test the alternative word2vec-based S naive -product feature set. It has the same definition as S-product except that we replace the synset vectors s (j) with naive synset vectors z (j) , defined as the sum of the word2vec vectors of the words that are members of synset j.</p><p>Lines 1-7 in <ref type="table" target="#tab_4">Table 3</ref> show the performance of each feature set by itself. We see that the synset feature sets (lines 5-7) have a comparable perfor- mance to standard feature sets. S-product is the strongest of them. <ref type="bibr">Lines 8-16</ref> show the performance of different feature set combinations. MFS (line 8) is the most frequent sense baseline. Lines 9&amp;10 are the win- ners of Senseval. The standard configuration of IMS (line 11) uses the three feature sets on lines 1-3 (POS, surrounding word, local collocation) and achieves an accuracy of 65.2% on the English lexical sample task of Senseval-2 and 72.3% on Senseval-3. 1 Lines 12-16 add one additional fea- ture set to the IMS system on line 11; e.g., the sys- tem on line 14 uses POS, surrounding word, local collocation and S-product feature sets. The system on line 14 outperforms all previous systems, most of them significantly. While S-raw performs quite reasonably as a feature set alone, it hurts the per- formance when used as an additional feature set. As this is the feature set that contains the largest number of features (n(k + 1)), overfitting is the likely reason. Conversely, S-cosine only adds k features and therefore may suffer from underfit- ting. <ref type="bibr">†</ref> We do a grid search (step size .1) for optimal values of α and β, optimizing the average score of Senseval-2 and Senseval-3. The best performing feature set combination is S optimized -product with <ref type="bibr">Senseval-2 Senseval-3</ref> IMS feature sets 1 POS 53.6 58.0 † 2 surrounding word 57.6 65.3 † 3 local collocation 58.7 64.7 † 4 S naive -product 56.5 62.2 † 5 S-cosine 55.5 60.5 † 6 S-product 58.3 64.3 † 7 S-raw 56.8 63.   The main result of this experiment is that we achieve an improvement of more than 1% in WSD performance when using AutoExtend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synset and lexeme similarity</head><p>We use SCWS <ref type="figure" target="#fig_0">(Huang et al., 2012)</ref> for the similar- ity evaluation. SCWS provides not only isolated words and corresponding similarity scores, but also a context for each word. SCWS is based on WordNet, but the information as to which synset a word in context came from is not available. How- ever, the dataset is the closest we could find for sense similarity. Synset and lexeme embeddings are obtained by running AutoExtend. Based on the results of the WSD task, we set α = 0.2 and β = 0.5. Lexeme embeddings are the natural choice for this task as human subjects are provided with two words and a context for each and then have to assign a similarity score. But for complete- ness, we also run experiments for synsets.</p><p>For each word, we compute a context vector c by adding all word vectors of the context, ex- cluding the test word itself. Following <ref type="bibr" target="#b29">Reisinger and Mooney (2010)</ref>, we compute the lexeme (resp. synset) vector l either as the simple average of the lexeme (resp. synset) vectors l (ij) (method AvgSim, no dependence on c in this case) or as the average of the lexeme (resp. synset) vec- tors weighted by cosine similarity to c (method AvgSimC). <ref type="table" target="#tab_6">Table 4</ref> shows that AutoExtend lexeme embed- dings (line 7) perform better than previous work,  including <ref type="bibr" target="#b14">(Huang et al., 2012)</ref> and <ref type="bibr" target="#b33">(Tian et al., 2014</ref>). Lexeme embeddings perform better than synset embeddings (lines 7 vs. 6), presumably be- cause using a representation that is specific to the actual word being judged is more precise than us- ing a representation that also includes synonyms.</p><p>A simple baseline is to use the underlying word2vec embeddings directly (line 5). In this case, there is only one embedding, so there is no difference between AvgSim and AvgSimC. It is in- teresting that even if we do not take the context into account (method AvgSim) the lexeme embed- dings outperform the original word embeddings. As AvgSim simply adds up all lexemes of a word, this is equivalent to the constraint we proposed in the beginning of the paper (Eq. 1). Thus, replacing a word's embedding by the sum of the embeddings of its senses could generally improve the quality of embeddings (cf. <ref type="bibr" target="#b14">Huang et al. (2012)</ref> for a similar point). We will leave a deeper evaluation of this topic for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>We first look at the impact of the parameters α, β (Section 2.5) that control the weighting of synset constraints vs lexeme constraints vs WN relation constraints. We investigate the impact for three different tasks. WSD-alone: accuracy of IMS (average of Senseval-2 and Senseval-3) if only S- product is used as a feature set (line 6 in <ref type="table" target="#tab_4">Table 3</ref>). WSD-additional: accuracy of IMS (average of Senseval-2 and Senseval-3) if S-product is used together with the feature sets POS, surrounding word and local collocation (line 14 in <ref type="table" target="#tab_4">Table 3)</ref>. SCWS: Spearman correlation on SCWS (line 7 in <ref type="table" target="#tab_6">Table 4</ref>).</p><p>For WSD-alone <ref type="figure" target="#fig_3">(Figure 3, center)</ref>, the best per- forming weightings (red) all have high weights for WN relations and are therefore at the top of triangle. Thus, WN relations are very important for WSD-alone and adding more weight to the synset and lexeme constraints does not help. How- ever, all three constraints are important in WSD- additional: the red area is in the middle (corre- sponding to nonzero weights for all three con- straints) in the left panel of <ref type="figure" target="#fig_3">Figure 3</ref>. Apparently, strongly weighted lexeme and synset constraints enable learning of representations that in their in- teraction with standard WSD feature sets like lo- cal collocation increase WSD performance. For SCWS (right panel), we should not put too much weight on WN relations as they artificially bring related, but not similar lexemes together. So the maximum for this task is located in the lower part of the triangle.</p><p>The main result of this analysis is that Auto- Extend never achieves its maximum performance when using only one set of constraints. All three constraints are important -synset, lexeme and WN relation constraints -with different weights for different applications.</p><p>We also analyzed the impact of the four differ- ent WN relations (see <ref type="table" target="#tab_0">Table 1</ref>) on performance. In <ref type="table" target="#tab_4">Table 3 and Table 4</ref>, all four WN relations are used together. We found that any combination of three relation types performs worse than using all four together. A comparison of different relations must be done carefully as they differ in the POS they affect and in quantity (see <ref type="table" target="#tab_0">Table 1</ref>). In general, re- lation types with more relations outperformed re- lation types with fewer relations.</p><p>Finally, the relative weighting of l (i,j) and l <ref type="bibr">(i,j)</ref> when computing lexeme embeddings is also a pa- rameter that can be tuned. We use simple aver- aging (θ = 0.5) for all experiments reported in this paper. We found only small changes in per- formance for 0.2 ≤ θ ≤ 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Resources other than WordNet</head><p>AutoExtend is broadly applicable to lexical and knowledge resources that have certain properties. While we only run experiments with WordNet in this paper, we will briefly address other resources. For Freebase ( <ref type="bibr" target="#b4">Bollacker et al., 2008)</ref>, we could re- place the synsets with Freebase entities. Each en- tity has several aliases, e.g. Barack Obama, Presi- dent Obama, Obama. The role of words in Word- Net would correspond to these aliases in Freebase. This will give us the synset constraint, as well as the lexeme constraint of the system. Relations are given by Freebase types; e.g., we can add a con- straint that entity embeddings of the type "Presi- dent of the US" should be similar.</p><p>To explorer multilingual word embeddings we require the word embeddings of different lan- guages to live in the same vector space, which can easily be achieved by training a transforma- tion matrix L between two languages using known translations ( <ref type="bibr" target="#b21">Mikolov et al., 2013b</ref>). Let X be a matrix where each row is a word embedding in language 1 and Y a matrix where each row is a word embedding in language 2. For each row the words of X and Y are a translation of each other. We then want to minimize the following objective:</p><formula xml:id="formula_38">argmin L LX − Y<label>(26)</label></formula><p>We can use a gradient descent to solve this but a matrix inversion will run faster. The matrix L is given by:</p><formula xml:id="formula_39">L = (X T * X) −1 (X T * Y )<label>(27)</label></formula><p>The matrix L can be used to transform unknown embeddings into the new vector space, which en- ables us to use a multilingual WordNet like Ba- belNet ( <ref type="bibr" target="#b26">Navigli and Ponzetto, 2010</ref>) to compute synset embeddings. We can add cross-linguistic relationships to our model, e.g., aligning German and English synset embeddings of the same con- cept. This prior work needs a training step to learn embeddings. In contrast, we can "AutoExtend" any set of given word embeddings -without (re)training them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WSD-additional</head><p>There is only little work on taking existing word embeddings and producing embeddings in the same space. <ref type="bibr" target="#b17">Labutov and Lipson (2013)</ref> tuned existing word embeddings in supervised training, not to create new embeddings for senses or enti- ties, but to get better predictive performance on a task while not changing the space of embeddings.</p><p>Lexical resources have also been used to im- prove word embeddings. In the Relation Con- strained Model, <ref type="bibr" target="#b35">Yu and Dredze (2014)</ref> use word2vec to learn embeddings that are optimized to predict a related word in the resource, with good evaluation results.  used not only semantic, but also morphological and syn- tactic knowledge to compute more effective word embeddings.</p><p>Another interesting approach to create sense specific word embeddings uses bilingual resources ( <ref type="bibr" target="#b13">Guo et al., 2014</ref>). The downside of this approach is that parallel data is needed.</p><p>We used the SCWS dataset for the word similar- ity task, as it provides a context. Other frequently used datasets are <ref type="bibr">WordSim-353 (Finkelstein et al., 2001</ref>) or MEN ( <ref type="bibr" target="#b7">Bruni et al., 2014</ref>).</p><p>And while we use cosine to compute similar- ity between synsets, there are also a lot of simi- larity measures that only rely on a given resource, mostly WordNet. These measures are often func- tions that depend on the provided information like gloss or the topology like shortest-path. Examples include ( <ref type="bibr" target="#b34">Wu and Palmer, 1994)</ref> and <ref type="bibr" target="#b18">(Leacock and Chodorow, 1998)</ref>; <ref type="bibr" target="#b3">Blanchard et al. (2005)</ref> give a good overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented AutoExtend, a flexible method to learn synset and lexeme embeddings from word embeddings. It is completely general and can be used for any other set of embeddings and for any other resource that imposes constraints of a cer- tain type on the relationship between words and other data types. Our experimental results show that AutoExtend achieves state-of-the-art perfor- mance on word similarity and word sense disam- biguation. Along with this paper, we will pub- lish AutoExtend for extending word embeddings to other data types; the lexeme and synset em- beddings used in the experiments; and the code needed to replicate our WSD evaluation 2 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A small subgraph of WordNet. The circles are intended to show four different embedding dimensions. These dimensions are treated as independent. The synset constraints align the input and the output layer. The lexeme constraints align the second and fourth layers.</figDesc><graphic url="image-1.png" coords="5,72.00,62.81,453.54,144.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>L</head><label></label><figDesc>Figure 2: Five nearest word (W/), lexeme (L/) and synset (S/) neighbors for three items, ordered by cosine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>α</head><label></label><figDesc>= 0.2 and β = 0.5, with only a small improve- ment (line 16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of different weightings of the three constraints (WN relations:top, lexemes:left, synsets:right) on the three tasks WSD-additional, WSD-alone and SCWS. "x" indicates the maximum; "o" indicates a local minimum.</figDesc><graphic url="image-2.png" coords="9,72.00,82.65,453.55,112.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table># of WN relations by part-of-speech 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc># of items in WordNet and after intersection with word2vec vectors</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>WSD accuracy for different feature sets and systems. 
Best result (excluding line 16) in each column in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Spearman correlation (ρ × 100) on SCWS. Best re-
sult per column in bold. 

</table></figure>

			<note place="foot" n="1"> Zhong and Ng (2010) report accuracies of 65.3% / 72.6% for this configuration. † In Table 3 and Table 4, results significantly worse than the best (bold) result in each column are marked † for α = .05 and ‡ for α = .10 (one-tailed Z-test).</note>

			<note place="foot" n="2"> http://cistern.cis.lmu.de/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head> <ref type="bibr" target="#b31">Rumelhart et al. (1988)</ref> <p>introduced distributed word representations, usually called word embed-dings today. There has been a resurgence of work on them recently (e.g., <ref type="bibr">Bengio et al. (2003) Mnih and</ref><ref type="bibr" target="#b24">Hinton (2007)</ref>, , <ref type="bibr" target="#b20">Mikolov et al. (2013a)</ref>, <ref type="bibr" target="#b28">Pennington et al. (2014)</ref>). These models produce only a single embedding for each word. All of them can be used as input for AutoExtend.</p><p>There are several approaches to finding embed-dings for senses, variously called meaning, sense and multiple word embeddings. <ref type="bibr" target="#b32">Schütze (1998</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially funded by Deutsche Forschungsgemeinschaft (DFG SCHU 2246/2-2). We are grateful to Christiane Fellbaum for discus-sions leading up to this paper and to the anony-mous reviewers for their comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Knowledge-powered deep learning for word embedding</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML PKDD</title>
		<meeting>ECML PKDD</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A typology of ontology-based semantic measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounira</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henri</forename><surname>Briand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Kuntz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMOI-INTEROP</title>
		<meeting>EMOI-INTEROP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD</title>
		<meeting>ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning structured embeddings of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint learning of words and meaning representations for open-text semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning sense-specific word embeddings by exploiting bilingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Technical Papers</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">English lexical sample task description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SENSEVAL-2</title>
		<meeting>SENSEVAL-2</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Re-embedding words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The senseval-3 english lexical sample task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SENSEVAL-3</title>
		<meeting>SENSEVAL-3</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Babelnet: Building a very large multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-prototype vector-space models of word meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald J</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="213" to="220" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic word sense discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="123" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A probabilistic model for learning multi-prototype word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Coling</title>
		<meeting>Coling</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Technical Papers</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">It makes sense: A wide-coverage word sense disambiguation system for free text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL, System Demonstrations</title>
		<meeting>ACL, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
