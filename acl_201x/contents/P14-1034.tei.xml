<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anchors Regularized: Adding Robustness and Extensibility to Scalable Topic-Modeling Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland and National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">National Institutes of Health</orgName>
								<orgName type="institution" key="instit3">University of Maryland</orgName>
								<orgName type="institution" key="instit4">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Umiacs</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland and National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">National Institutes of Health</orgName>
								<orgName type="institution" key="instit3">University of Maryland</orgName>
								<orgName type="institution" key="instit4">University of Maryland</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
							<email>ynhu@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland and National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">National Institutes of Health</orgName>
								<orgName type="institution" key="instit3">University of Maryland</orgName>
								<orgName type="institution" key="instit4">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland and National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">National Institutes of Health</orgName>
								<orgName type="institution" key="instit3">University of Maryland</orgName>
								<orgName type="institution" key="instit4">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umiacs</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Maryland and National Library of Medicine</orgName>
								<orgName type="institution" key="instit2">National Institutes of Health</orgName>
								<orgName type="institution" key="instit3">University of Maryland</orgName>
								<orgName type="institution" key="instit4">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Anchors Regularized: Adding Robustness and Extensibility to Scalable Topic-Modeling Algorithms</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="359" to="369"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Spectral methods offer scalable alternatives to Markov chain Monte Carlo and expectation maximization. However, these new methods lack the rich priors associated with probabilistic models. We examine Arora et al.&apos;s anchor words algorithm for topic mod-eling and develop new, regularized algorithms that not only mathematically resemble Gaussian and Dirichlet priors but also improve the interpretability of topic models. Our new regularization approaches make these efficient algorithms more flexible; we also show that these methods can be combined with informed priors.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models are of practical and theoretical inter- est. Practically, they have been used to understand political perspective <ref type="bibr" target="#b27">(Paul and Girju, 2010)</ref>, im- prove machine translation <ref type="bibr" target="#b13">(Eidelman et al., 2012</ref>), reveal literary trends <ref type="bibr" target="#b19">(Jockers, 2013)</ref>, and under- stand scientific discourse ( <ref type="bibr" target="#b17">Hall et al., 2008)</ref>. The- oretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena <ref type="bibr" target="#b6">(Brody and Lapata, 2009)</ref>.</p><p>Modern topic models are formulated as a la- tent variable model. Like hidden Markov mod- els <ref type="bibr">(Rabiner, 1989, HMM)</ref>, each token comes from one of K unknown distributions. Unlike a HMM, topic models assume that each document is an ad- mixture of these hidden components called topics. Posterior inference discovers the hidden variables that best explain a dataset. Typical solutions use MCMC ( <ref type="bibr" target="#b16">Griffiths and Steyvers, 2004</ref>) or variational EM ( <ref type="bibr" target="#b5">Blei et al., 2003)</ref>, which can be viewed as local optimization: searching for the latent variables that maximize the data likelihood.</p><p>An exciting vein of new research provides provable polynomial-time alternatives. These ap- proaches provide solutions to hidden Markov mod- els ( <ref type="bibr" target="#b0">Anandkumar et al., 2012)</ref>, mixture mod- els ( <ref type="bibr" target="#b20">Kannan et al., 2005</ref>), and latent variable gram- mars ( <ref type="bibr" target="#b9">Cohen et al., 2013)</ref>. The key insight is not to directly optimize observation likelihood but to in- stead discover latent variables that can reconstruct statistics of the assumed generative model. Unlike search-based methods, which can be caught in lo- cal minima, these techniques are often guaranteed to find global optima.</p><p>These general techniques can be improved by making reasonable assumptions about the models. For example, <ref type="bibr" target="#b3">Arora et al. (2012b)</ref>'s approach for in- ference in topic models assume that each topic has a unique "anchor" word (thus, we call this approach anchor). This approach is fast and effective; be- cause it only uses word co-occurrence information, it can scale to much larger datasets than MCMC or EM alternatives. We review the anchor method in Section 2.</p><p>Despite their advantages, these techniques are not a panacea. They do not accommodate the rich priors that modelers have come to expect. Priors can improve performance ( <ref type="bibr" target="#b36">Wallach et al., 2009)</ref>, provide domain adaptation <ref type="bibr" target="#b10">(Daumé III, 2007;</ref><ref type="bibr" target="#b14">Finkel and Manning, 2009)</ref>, and guide mod- els to reflect users' needs ( <ref type="bibr" target="#b18">Hu et al., 2013)</ref>. In Section 3, we regularize the anchor method to trade-off the reconstruction fidelity with the penalty terms that mimic Gaussian and Dirichlet priors.</p><p>Another shortcoming is that these models have not been scrutinized using standard NLP evalua- tions. Because these approaches emerged from the theory community, anchor's evaluations, when present, typically use training reconstruction. In Section 4, we show that our regularized models can generalize to previously unseen data-as measured by held-out likelihood ( <ref type="bibr" target="#b5">Blei et al., 2003)</ref>-and are more interpretable <ref type="bibr" target="#b7">(Chang et al., 2009;</ref><ref type="bibr" target="#b25">Newman et al., 2010</ref>). We also show that our extension to the anchor method enables new applications: for K number of topics V vocabulary size M document frequency: minimum documents an an- chor word candidate must appear in</p><formula xml:id="formula_0">Q word co-occurrence matrix Qi,j = p(w1 = i, w2 = j) ¯ Q conditional distribution of Q ¯ Qi,j = p(w1 = j | w2 = i) ¯ Qi,· row i of ¯ Q A topic matrix, of size V × K A j,k = p(w = j | z = k) C anchor coefficient of size K × V C j,k = p(z = k | w = j) S</formula><p>set of anchor word indexes {s1, . . . sK } λ regularization weight <ref type="table">Table 1</ref>: Notation used. Matrices are in bold (Q, C), sets are in script S example, using an informed priors to discover con- cepts of interest. Having shown that regularization does improve performance, in Section 5 we explore why. We discuss the trade-off of training data reconstruction with sparsity and why regularized topics are more interpretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Anchor Words: Scalable Topic Models</head><p>In this section, we briefly review the anchor method and place it in the context of topic model inference. Once we have established the anchor objective function, in the next section we regularize the objective function.</p><p>Rethinking Data: Word Co-occurrence Infer- ence in topic models can be viewed as a black box: given a set of documents, discover the topics that best explain the data. The difference between an- chor and conventional inference is that while con- ventional methods take a collection of documents as input, anchor takes word co-occurrence statis- tics. Given a vocabulary of size V , we represent this joint distribution as Q i,j = p(w 1 = i, w 2 = j), each cell represents the probability of words appear- ing together in a document.</p><p>Like other topic modeling algorithms, the output of the anchor method is the topic word distribu- tions A with size V * K, where K is the total number of topics desired, a parameter of the al- gorithm. The k th column of A will be the topic distribution over all words for topic k, and A w,k is the probability of observing type w given topic k.</p><p>Anchors: Topic Representatives The anchor method ( <ref type="bibr" target="#b2">Arora et al., 2012a</ref>) is based on the sepa- rability assumption <ref type="bibr" target="#b11">(Donoho and Stodden, 2003)</ref>, which assumes that each topic contains at least one namesake "anchor word" that has non-zero proba- bility only in that topic. Intuitively, this means that each topic has unique, specific word that, when used, identifies that topic. For example, while "run", "base", "fly", and "shortstop" are associated with a topic about baseball, only "shortstop" is un- ambiguous, so it could serve as this topic's anchor word.</p><p>Let's assume that we knew what the anchor words were: a set S that indexes rows in Q. Now consider the conditional distribution of word i, the probability of the rest of the vocabulary given an observation of word i; we represent this as ¯ Q i,· , as we can construct this by normalizing the rows of Q. For an anchor word s a ∈ S, this will look like a topic; ¯ Q "shortstop",· will have high probability for words associated with baseball.</p><p>The key insight of the anchor algorithm is that the conditional distribution of polysemous non- anchor words can be reconstructed as a linear com- bination of the conditional distributions of anchor words. For example, ¯ Q "fly",· could be recon- structed by combining the anchor words "insecta", "boeing", and "shortshop". We represent the coeffi- cients of this reconstruction as a matrix C, where</p><formula xml:id="formula_1">C i,k = p(z = k | w = i). Thus, for any word i, ¯ Q i,· ≈ s k ∈S C i,k ¯ Q s k ,· .<label>(1)</label></formula><p>The coefficient matrix is not the usual output of a topic modeling algorithm. The usual output is the probability of a word given a topic. The coefficient matrix C is the probability of a topic given a word. We use Bayes rule to recover the topic distribution</p><formula xml:id="formula_2">p(w = i|z = k) ≡ A i,k ∝ p(z = k|w = i)p(w = i) = C i,k j ¯ Q i,j<label>(2)</label></formula><p>where p(w) is the normalizer of Q to obtain ¯ Q w,· . The geometric argument for finding the anchor words is one of the key contributions of <ref type="bibr" target="#b2">Arora et al. (2012a)</ref> and is beyond the scope of this paper. The algorithms in Section 3 use the anchor selec- tion subroutine unchanged. The difference in our approach is in how we discover the anchor coeffi- cients C.</p><p>From Anchors to Topics After we have the an- chor words, we need to find the coefficients that best reconstruct the data ¯ Q (Equation 1). Arora et al. (2012a) chose the C that minimizes the KL divergence between ¯ Q i,· and the reconstruction based on the anchor word's conditional word vec- tors</p><formula xml:id="formula_3">s k ∈S C i,k ¯ Q s k ,· , C i,· = argmin C i,· D KL   ¯ Q i,· || s k ∈S C i,k ¯ Q s k ,·   .<label>(</label></formula><p>3) The anchor method is fast, as it only de- pends on the size of the vocabulary once the co- occurrence statistics Q are obtained. However, it does not support rich priors for topic models, while MCMC ( <ref type="bibr" target="#b16">Griffiths and Steyvers, 2004</ref>) and varia- tional EM ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>) methods can. This prevents models from using priors to guide the models to discover particular themes ( <ref type="bibr" target="#b38">Zhai et al., 2012)</ref>, or to encourage sparsity in the models ( <ref type="bibr" target="#b37">Yao et al., 2009</ref>). In the rest of this paper, we correct this lacuna by adding regularization inspired by Bayesian priors to the anchor algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adding Regularization</head><p>In this section, we add regularizers to the anchor objective (Equation 3). In this section, we briefly review regularizers and then add two regularizers, inspired by Gaussian (L 2 , Section 3.1) and Dirich- let priors (Beta, Section 3.2), to the anchor objec- tive function (Equation 3).</p><p>Regularization terms are ubiquitous. They typ- ically appear as an additional term in an opti- mization problem. Instead of optimizing a func- tion just of the data x and parameters β, f (x, β), one optimizes an objective function that includes a regularizer that is only a function of parame- ters: f (w, β) + r(β). Regularizers are critical in staid methods like linear regression <ref type="bibr" target="#b26">(Ng, 2004)</ref>, in workhorse methods such as maximum entropy modeling <ref type="bibr" target="#b12">(Dudík et al., 2004</ref>), and also in emerging fields such as deep learning ( <ref type="bibr" target="#b35">Wager et al., 2013)</ref>.</p><p>In addition to being useful, regularization terms are appealing theoretically because they often corre- spond to probabilistic interpretations of parameters. For example, if we are seeking the MLE of a proba- bilistic model parameterized by β, p(x|β), adding a regularization term r(β) = L i=1 β 2 i corresponds to adding a Gaussian prior  <ref type="table">Table 2</ref>: The number of documents in the train, development, and test folds in our three datasets. and maximizing log probability of the posterior (ignoring constant terms) (Rennie, 2003).</p><formula xml:id="formula_4">f (β i ) = 1 √ 2πσ 2 exp − β 2 i 2σ 2<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">L 2 Regularization</head><p>The simplest form of regularization we can add is L 2 regularization. This is similar to assuming that probability of a word given a topic comes from a Gaussian distribution. While the distribution over topics is typically Dirichlet, Dirichlet distributions have been replaced by logistic normals in topic modeling applications <ref type="bibr" target="#b4">(Blei and Lafferty, 2005)</ref> and for probabilistic grammars of language <ref type="bibr" target="#b8">(Cohen and Smith, 2009)</ref>.</p><p>Augmenting the anchor objective with an L 2 penalty yields</p><formula xml:id="formula_5">C i,· =argmin C i,· D KL   ¯ Q i,· || s k ∈S C i,k ¯ Q s k ,·   + λC i,· − µ i,· 2 2 ,<label>(5)</label></formula><p>where regularization weight λ balances the impor- tance of a high-fidelity reconstruction against the regularization, which encourages the anchor coeffi- cients to be close to the vector µ. When the mean vector µ is zero, this encourages the topic coeffi- cients to be zero. In Section 4.3, we use a non-zero mean µ to encode an informed prior to encourage topics to discover specific concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Beta Regularization</head><p>The more common prior for topic models is a Dirichlet prior <ref type="bibr" target="#b24">(Minka, 2000</ref>). However, we cannot apply this directly because the optimization is done on a row-by-row basis of the anchor coefficient matrix C, optimizing C for a fixed word w for and all topics. If we want to model the probability of a word, it must be the probability of word w in a topic versus all other words. Modeling this dichotomy (one versus all others in a topic) is possible. The constructive definition of the Dirichlet distribution <ref type="bibr" target="#b33">(Sethuraman, 1994)</ref> states that if one has a V -dimensional multinomial</p><formula xml:id="formula_6">θ ∼ Dir(α 1 . . . α V ), then the marginal distribution of θ w follows θ w ∼ Beta(α w , i =w α i )</formula><p>. This is the tool we need to consider the distribution of a single word's probability.</p><p>This requires including the topic matrix as part of the objective function. The topic matrix is a lin- ear transformation of the coefficient matrix (Equa- tion 2). The objective for beta regularization be- comes</p><formula xml:id="formula_7">C i,· =argmin C i,· D KL   ¯ Q i,· || s k ∈S C i,k ¯ Q s k ,·   − λ s k ∈S log (Beta(A i,k ; a, b)),<label>(6)</label></formula><p>where λ again balances reconstruction against the regularization. To ensure the tractability of this algorithm, we enforce a convex regularization func- tion, which requires that a &gt; 1 and b &gt; 1. If we enforce a uniform prior-E Beta(a,b) [A i,k ] = 1 V - and that the mode of the distribution is also 1 V , 1 this gives us the following parametric form for a and b:</p><formula xml:id="formula_8">a = x V + 1, and b = (V − 1)x V + 1<label>(7)</label></formula><p>for real x greater than zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Initialization and Convergence</head><p>Equation 5 and Equation 6 are optimized using L- BFGS gradient optimization ( <ref type="bibr" target="#b15">Galassi et al., 2003)</ref>. We initialize C randomly from Dir(α) with α = 60 V ( <ref type="bibr" target="#b36">Wallach et al., 2009)</ref>. We update C after opti- mizing all V rows. The newly updated C replaces the old topic coefficients. We track how much the topic coefficients C change between two con- secutive iterations i and i + 1 and represent it as ∆C ≡ C i+1 − C i 2 . We stop optimization when ∆C ≤ δ. When δ = 0.1, the L 2 and unregularized anchor algorithm converges after a single iteration, while beta regularization typically converges after fewer than ten iterations <ref type="figure" target="#fig_4">(Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Regularization Improves Topic Models</head><p>In this section, we measure the performance of our proposed regularized anchor word algorithms. We will refer to specific algorithms in bold. For example, the original anchor algorithm is an- chor. Our L 2 regularized variant is anchor-L 2 , and our beta regularized variant is anchor-beta.</p><p>To provide conventional baselines, we also com- pare our methods against topic models from varia- tional inference ( <ref type="bibr">Blei et al., 2003, variational)</ref> and MCMC ( <ref type="bibr" target="#b16">Griffiths and Steyvers, 2004;</ref><ref type="bibr" target="#b23">McCallum, 2002</ref>, MCMC).</p><p>We apply these inference strategies on three di- verse corpora: scientific articles from the Neural Information Processing Society (NIPS), 2 Internet newsgroups postings (20NEWS), <ref type="bibr">3</ref> and New York Times editorials <ref type="bibr">(Sandhaus, 2008, NYT)</ref>. Statistics for the datasets are summarized in <ref type="table">Table 2</ref>. We split each dataset into a training fold (70%), develop- ment fold (15%), and a test fold (15%): the training data are used to fit models; the development set are used to select parameters (anchor threshold M , doc- ument prior α, regularization weight λ); and final results are reported on the test fold.</p><p>We use two evaluation measures, held-out likeli- hood ( <ref type="bibr">Blei et al., 2003, HL)</ref> and topic interpretabil- ity ( <ref type="bibr" target="#b7">Chang et al., 2009;</ref><ref type="bibr">Newman et al., 2010, TI)</ref>. Held-out likelihood measures how well the model can reconstruct held-out documents that the model has never seen before. This is the typical evaluation for probabilistic models. Topic interpretability is a more recent metric to capture how useful the topics can be to human users attempting to make sense of a large datasets.</p><p>Held-out likelihood cannot be computed with existing anchor algorithms, so we use the topic distributions learned from anchor as input to a ref- erence variational inference implementation ( <ref type="bibr" target="#b5">Blei et al., 2003</ref>) to compute HL. This requires an ad- ditional parameter, the Dirichlet prior α for the per-document distribution over topics. We select α using grid search on the development set.</p><p>To compute TI and evaluate topic coherence, we use normalized pairwise mutual informa- tion (NPMI) ( <ref type="bibr" target="#b22">Lau et al., 2014</ref>) over topics' twenty most probable words. Topic coherence is com- puted against the NPMI of a reference corpus. For coherence evaluations, we use both intrinsic and extrinsic text collections to compute NPMI. Intrin- sic coherence (TI-i) is computed on training and development data at development time and on train- ing and test data at test time. Extrinsic coherence (TI-e) is computed from English Wikipedia articles, with disjoint halves (1.1 million pages each) for distinct development and testing TI-e evaluation.   <ref type="figure">Figure 1</ref>: Grid search for document frequency M for our datasets with 20 topics (other configurations not shown) on development data. The performance on both HL and TI score indicate that the unregularized anchor algorithm is very sensitive to M . The M selected here is applied to subsequent models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topics q 20 40 60 80</head><p>Beta L2 q q q q q qq qqq q q q q q qq qq q q q q q q qq qqq q q q q q q qqq q q q q q q q qqq qq q q q q q qqqq q q q q q qqqqqq q q q q qq qqq q q q q q qqqq qq q q q q q qq qq q q q q q qqqqqq q q q q qq Figure 2: Selection of λ based on HL and TI scores on the development set. The value of λ = 0 is equivalent to the original anchor algorithm; regularized versions find better solutions as the regularization weight λ becomes non-zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Grid Search for Parameters on Development Set</head><p>Anchor Threshold A good anchor word must have a unique, specific context but also explain other words well. A word that appears only once will have a very specific cooccurence pattern but will explain other words' coocurrence poorly be- cause the observations are so sparse. As discussed in Section 2, the anchor method uses document frequency M as a threshold to only consider words with robust counts. Because all regularizations benefit equally from higher-quality anchor words, we use cross- validation to select the document frequency cut- off M using the unregularized anchor algorithm. <ref type="figure">Figure 1</ref> shows the performance of anchor with different M on our three datasets with 20 topics for our two measures HL and TI-i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regularization Weight</head><p>Once we select a cutoff M for each combination of dataset, number of top- ics K and a evaluation measure, we select a reg- ularization weight λ on the development set. <ref type="figure">Fig- ure 2</ref> shows that beta regularization framework im- proves topic interpretability TI-i on all datasets and improved the held-out likelihood HL on 20NEWS. The L 2 regularization also improves held-out like- lihood HL for the 20NEWS corpus <ref type="figure">(Figure 2</ref>).</p><p>In the interests of space, we do not show the figures for selecting M and λ using TI-e, which is similar to TI-i: anchor-beta improves TI-e score on all datasets, anchor-L 2 improves TI-e on 20NEWS and NIPS with 20 topics and NYT with 40 topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluating Regularization</head><p>With document frequency M and regularization weight λ selected from the development set, we compare the performance of those models on the test set. We also compare with standard implemen- tations of Latent Dirichlet Allocation: Blei's LDAC (variational) and Mallet (mcmc). We run 100 iter- ations for LDAC and 5000 iterations for Mallet.</p><p>Each result is averaged over three random runs and appears in <ref type="figure">Figure 3</ref>. The highly-tuned, widely- used implementations uniformly have better held- out likelihood than anchor-based methods, but the much faster anchor methods are often comparable. Within anchor-based methods, L 2 -regularization offers comparable held-out likelihood as unregular- ized anchor, while anchor-beta often has better interpretability. Because of the mismatch between the specialized vocabulary of NIPS and the general- purpose language of Wikipedia, TI-e has a high variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Informed Regularization</head><p>A frequent use of priors is to add information to a model. This is not possible with the existing an- chor method. An informed prior for topic models seeds a topic with words that describe a topic of in- terest. In a topic model, these seeds will serve as a "magnet", attracting similar words to the topic ( <ref type="bibr" target="#b38">Zhai et al., 2012)</ref>.</p><p>We can achieve a similar goal with anchor-L 2 . Instead of encouraging anchor coefficients to be zero in Equation 5, we can instead encourage word probabilities to close to an arbitrary mean µ i,k . This vector can reflect expert knowledge.</p><p>One example of a source of expert knowledge is Linguistic Inquiry and Word Count <ref type="bibr">(Pennebaker and Francis, 1999, LIWC)</ref>, a dictionary of key- words related to sixty-eight psychological concepts such as positive emotions, negative emotions, and death. For example, it associates "excessive, estate, money, cheap, expensive, living, profit, live, rich, income, poor, etc." for the concept materialism.</p><p>We associate each anchor word with its closest LIWC category based on the cooccurrence matrix Q. This is computed by greedily finding the an- chor word that has the highest cooccurrence score for any LIWC category: we define the score of a category to anchor word w s k as i Q s k ,i , where i ranges over words in this category; we compute the scores of all categories to all anchor words; then we find the highest score and assign the category to that anchor word; we greedily repeat this process until all anchor words have a category.</p><p>Given these associations, we create a goal mean µ i,k . If there are L i anchor words associated with LIWC word i, µ i,k = 1</p><formula xml:id="formula_9">L i</formula><p>if this keyword i is associ- ated with anchor word w s k and zero otherwise.</p><p>We apply anchor-L 2 with informed priors on NYT with twenty topics and compared the topics against the original topics from anchor. <ref type="table">Table 3</ref> shows that the topic with anchor word "soviet", when combined with LIWC, draws in the new words "bush" and "nuclear"; reflecting the threats of force during the cold war. For the topic with topic word "arms", when associated with the LIWC category with the terms "agree" and "agreement", draws in "clinton", who represented a more conciliatory foreign policy compared to his republican prede- cessors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Having shown that regularization can improve the anchor topic modeling algorithm, in this section we discuss why these regularizations can improve the model and the implications for practitioners.</p><p>Efficiency Efficiency is a function of the number of iterations and the cost of each iteration. Both anchor and anchor-L 2 require a single iteration, although the latter's iteration is slightly more ex- pensive. For beta, as described in Section 3.2, we update anchor coefficients C row by row, and then repeat the process over several iterations until it converges. However, it often converges within ten iterations <ref type="figure" target="#fig_4">(Figure 4</ref>) on all three datasets: this requires much fewer iterations than MCMC or vari- ational inference, and the iterations are less expen- sive. In addition, since we optimize each row C i,· independently, the algorithm can be easily paral- lelized.</p><p>Sensitivity to Document Frequency While the original anchor is sensitive to the document fre- quency M <ref type="figure">(Figure 1</ref>), adding regularization makes this less critical. Both anchor-L 2 and anchor-beta are less sensitive to M than anchor.</p><p>To highlight this, we compare the topics of an- chor and anchor-beta when M = 100. As <ref type="table">Table 4</ref> shows, the words "article", "write", "don" and "doe" appear in most of anchor's topics. While anchor-L 2 also has some bad topics, it still can find reasonable topics, demonstrating anchor-beta's greater robustness to suboptimal M . L 2 (Sometimes) Improves Generalization As <ref type="figure">Figure 2</ref> shows, anchor-L 2 sometimes improves held-out development likelihood for the smaller topic number <ref type="figure">Figure 3</ref>: Comparing anchor-beta and anchor-L 2 against the original anchor and the traditional vari- ational and MCMC on HL score and TI score. variational and mcmc provide the best held-out gener- alization. anchor-beta sometimes gives the best TI score and is consistently better than anchor. The specialized vocabulary of NIPS causes high variance for the extrinsic interpretability evaluation (TI-e).</p><note type="other">Algorithm q anchor anchor−beta anchor−L2 MCMC variational</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic Shared Words</head><p>Original (Top, green) vs. Informed L2 (Bottom, orange) soviet american make president soviet union war years gorbachev moscow russian force economic world europe politi- cal communist lead reform germany country military state service washington bush army unite chief troops officer nuclear time week district assembly board city county district member state york representative manhattan brooklyn queens election bronx council island local incumbent housing municipal people party group social republican year make years friend vote compromise million peace american force government israel peace political president state unite washington war military country minister leaders nation world palestinian israeli election offer justice aid deserve make bush years fair clinton hand arms arms bush congress force iraq make north nuclear president state washington weapon administration treaty missile defense war military korea reagan agree agreement american accept unite share clinton years trade administration america american country economic government make president state trade unite washington world market japan foreign china policy price political business economy congress year years clinton bush buy <ref type="table">Table 3</ref>: Examples of topic comparison between anchor and informed anchor-L 2 . A topic is labeled with the anchor word for that topic. The bold words are the informed prior from LIWC. With an informed prior, relevant words appear in the top words of a topic; this also draws in other related terms (red).</p><p>20NEWS corpus. However, the λ selected on devel- opment data does not always improve test set per- formance. This, in <ref type="figure">Figure 3</ref>, anchor-beta closely tracks anchor. Thus, L 2 regularization does not hurt generalization while imparting expressiveness and robustness to parameter settings.</p><p>Beta Improves Interpretability <ref type="figure">Figure 3</ref> shows that anchor-beta improves topic interpretability (TI) compared to unregularized anchor methods. In this section, we try to understand why.</p><p>We first compare the topics from the original anchor against anchor-beta to analyze the topics qualitatively. <ref type="table">Table 5</ref> shows that beta regulariza- tion promotes rarer words within a topic and de- motes common words. For example, in the topic about hockey with the anchor word game, "run" and "good"-ambiguous, polysemous words-in the unregularized topic are replaced by "playoff" anchor-beta frequently article write don <ref type="table">doe make time people good   file question  article write don doe make people time good  email file  debate  write article people make don doe god key gov- ernment time   people make god article write don</ref>   <ref type="table">Table 4</ref>: Topics from anchor and anchor-beta with M = 100 on 20NEWS with 20 topics. Each topic is identified with its associated anchor word. When M = 100, the topics of anchor suffer: the four colored words appear in almost every topic. anchor-beta, in contrast, is less sensitive to suboptimal M . and "trade" in the regularized topic. These words are less ambiguous and more likely to make sense to a consumer of topic models. <ref type="figure">Figure 5</ref> shows why this happens. Compared to the unregularized topics from anchor, the beta regularized topic steals from the rich and creates a more uniform distribution. Thus, highly frequent words do not as easily climb to the top of the distri- bution, and the topics reflect topical, relevant words rather than globally frequent terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>A topic model is a popular tool for quickly get- ting the gist of large corpora. However, running such an analysis on these large corpora entail a substantial computational cost. While techniques such as anchor algorithms offer faster solutions, it comes at the cost of the expressive priors common in Bayesian formulations.</p><p>This paper introduces two different regulariza- tions that offer users more interpretable models and the ability to inject prior knowledge without sacrificing the speed and generalizability of the underlying approach. However, one sacrifice that this approach does make is the beautiful theoretical guarantees of previous work. An important piece of future work is a theoretical understanding of generalizability in extensible, regularized models. Incorporating other regularizations could further improve performance or unlock new applications. Our regularizations do not explicitly encourage sparsity; applying other regularizations such as L 1 could encourage true sparsity <ref type="bibr" target="#b34">(Tibshirani, 1994)</ref>, and structured priors ( <ref type="bibr" target="#b1">Andrzejewski et al., 2009)</ref> could efficiently incorporate constraints on topic models.</p><p>These regularizations could improve spectral al- gorithms for latent variables models, improving the performance for other NLP tasks such as latent vari- able <ref type="bibr">PCFGs (Cohen et al., 2013</ref>) and HMMs <ref type="bibr" target="#b0">(Anandkumar et al., 2012)</ref>, combining the flexibility and robustness offered by priors with the speed and accuracy of new, scalable algorithms. Rank of word in topic (topic shown by anchor word) log p(word | topic) <ref type="figure">Figure 5</ref>: How beta regularization influences the topic distribution. Each topic is identified with its associated anchor word. Compared to the unregularized anchor method, anchor-beta steals probability mass from the "rich" and prefers a smoother distribution of probability mass. These words often tend to be unimportant, polysemous words common across topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Topic</head><p>Shared Words anchor (Top, green) vs. anchor-beta (Bottom, orange)</p><p>computer computer means science screen system phone university problem doe work windows internet software chip mac set fax technology information data quote mhz pro processor ship remote print devices complex cpu electrical transfer ray engineering serial reduce power power play period supply ground light battery engine car good make high problem work back turn control current small time circuit oil wire unit water heat hot ranger input total joe plug god god jesus christian bible faith church life christ belief religion hell word lord truth love people make things true doe sin christianity atheist peace heaven game game team player play win fan hockey season baseball red wings score division league goal leaf cup toronto run good playoff trade drive drive disk hard scsi controller card floppy ide mac bus speed monitor switch apple cable internal port meg problem work ram pin <ref type="table">Table 5</ref>: Comparing topics-labeled by their anchor word-from anchor and anchor-beta. With beta regularization, relevant words are promoted, while more general words are suppressed, improving topic coherence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convergence of anchor coefficient C for anchor-beta. ∆C is the difference of current C from the C at the previous iteration. C is converged within ten iterations for all three datasets.</figDesc></figure>

			<note place="foot" n="1"> For a, b &lt; 1, the expected value is still the uniform distribution but the mode lies at the boundaries of the simplex. This corresponds to a sparse Dirichlet distribution, which our optimization cannot at present model.</note>

			<note place="foot" n="2"> http://cs.nyu.edu/ ˜ roweis/data.html 3 http://qwone.com/ ˜ jason/20Newsgroups/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers, Hal Daumé III, Ke Wu, and Ke Zhai for their help-ful comments. This work was supported by NSF Grant IIS-1320538. Boyd-Graber is also supported by NSF Grant CCF-1018625. Any opinions, find-ings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A method of moments for mixture models and hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Learning Theory</title>
		<meeting>Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incorporating domain knowledge into topic modeling via Dirichlet forest priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1212.4777</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning topic models-going beyond svd. CoRR, abs/1204</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter</title>
		<meeting>the European Chapter<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Experiments with spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">When does non-negative matrix factorization give correct decomposition into parts? page</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Stodden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance guarantees for regularized maximum entropy density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslav</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Learning Theory</title>
		<meeting>Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Topic models for dynamic translation model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical bayesian domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Galassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Theiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Jungman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Rossi</surname></persName>
		</author>
		<title level="m">Gnu Scientific Library: Reference Manual. Network Theory Ltd</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Studying the history of ideas using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Emperical Methods in Natural Language Processing</title>
		<meeting>Emperical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interactive topic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brianna</forename><surname>Satinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">L</forename><surname>Jockers</surname></persName>
		</author>
		<title level="m">Macroanalysis: Digital Methods and Literary History. Topics in the Digital Humanities</title>
		<imprint>
			<publisher>University of Illinois Press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The spectral method for general mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravindran</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Salmasian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Learning Theory</title>
		<meeting>Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">20 newsgroups data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mallet: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://www.cs.umass.edu/mccallum/mallet" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Estimating a dirichlet distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Minka</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/en-us/um/people/minka/papers/dirichlet/" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature selection, l1 vs. l2 regularization, and rotational invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Machine Learning</title>
		<meeting>the International Conference of Machine Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A twodimensional topic-aspect model for discovering multi-faceted topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linguistic Inquiry and Word Count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lawrence Erlbaum</title>
		<imprint>
			<date type="published" when="1999-08" />
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On l2-norm regularization and the Gaussian prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Rennie</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Roweis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The New York Times annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A constructive definition of Dirichlet priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayaram</forename><surname>Sethuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="639" to="650" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout training as adaptive regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking LDA: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient methods for topic model inference on streaming document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mr. LDA: A flexible large scale topic modeling package using variational inference in mapreduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamad</forename><surname>Alkhouja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of World Wide Web Conference</title>
		<meeting>World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
