<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Neural Text Simplification Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Nisioi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Štajner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liviu</forename><forename type="middle">P</forename><surname>Dinu</surname></persName>
						</author>
						<title level="a" type="main">Exploring Neural Text Simplification Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="85" to="91"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2014</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present the first attempt at using sequence to sequence neural networks to model text simplification (TS). Unlike the previously proposed automated TS systems , our neural text simplification (NTS) systems are able to simultaneously perform lexical simplification and content reduction. An extensive human evaluation of the output has shown that NTS systems achieve almost perfect grammaticality and meaning preservation of output sentences and higher level of simplification than the state-of-the-art automated TS systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural sequence to sequence models have been successfully used in many applications <ref type="bibr">(Graves, 2012)</ref>, from speech and signal processing to text processing or dialogue systems ( <ref type="bibr" target="#b14">Serban et al., 2015)</ref>. Neural machine translation ( <ref type="bibr">Cho et al., 2014;</ref><ref type="bibr">Bahdanau et al., 2014</ref>) is a particular type of sequence to sequence model that recently at- tracted a lot of attention from industry ( <ref type="bibr" target="#b18">Wu et al., 2016</ref>) and academia, especially due to the capa- bility to obtain state-of-the-art results for various translation tasks ( <ref type="bibr">Bojar et al., 2016)</ref>. Unlike classi- cal statistical machine translation (SMT) systems <ref type="bibr" target="#b4">(Koehn, 2010)</ref>, neural networks are being trained end-to-end, without the need to have external de- coders, language models or phrase tables. The ar- chitectures are relatively simpler and more flexi- ble, making possible the use of character models <ref type="bibr" target="#b5">(Luong and Manning, 2016)</ref> or even training mul- tilingual systems in one go ( <ref type="bibr">Firat et al., 2016)</ref>.</p><p>Automated text simplification (ATS) systems are meant to transform original texts into differ- * * Both authors have contributed equally to this work ent (simpler) variants which would be understood by wider audiences and more successfully pro- cessed by various NLP tools. In the last sev- eral years, great attention has been given to ad- dressing ATS as a monolingual machine transla- tion problem translating from 'original' to 'sim- ple' sentences. So far, attempts were made at stan- dard phrase-based SMT (PBSMT) models <ref type="bibr" target="#b15">(Specia, 2010;</ref><ref type="bibr">ˇ Stajner et al., 2015)</ref>, PBSMT mod- els with added phrasal deletion rules <ref type="bibr">(Coster and Kauchak, 2011</ref>) or reranking of the n-best out- puts according to their dissimilarity to the output ( <ref type="bibr" target="#b19">Wubben et al., 2012</ref>), tree-based translation mod- els ( <ref type="bibr" target="#b22">Zhu et al., 2010;</ref><ref type="bibr" target="#b9">Paetzold and Specia, 2013)</ref>, and syntax-based MT with specially designed tun- ing function ( . Recently, lexi- cal simplification (LS) was addressed by unsuper- vised approaches leveraging word-embeddings, with reported good success <ref type="bibr">(Glavaš andŠtajnerandˇandŠtajner, 2015;</ref><ref type="bibr" target="#b10">Paetzold and Specia, 2016)</ref>.</p><p>To the best of our knowledge, our work is the first to address the applicability of neural sequence to sequence models for ATS. We make use of the recent advances in neural machine translation (NMT) and adapt the existing architectures for our specific task. We also perform an extensive human evaluation to directly compare our systems with the current state-of-the-art (supervised) MT-based and unsupervised lexical simplification systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Text Simplification (NTS)</head><p>We use the OpenNMT framework ( <ref type="bibr" target="#b3">Klein et al., 2017</ref>) to train and build our architecture with two LSTM layers <ref type="bibr" target="#b0">(Hochreiter and Schmidhuber, 1997)</ref>, hidden states of size 500 and 500 hidden units, and a 0.3 dropout probability ( <ref type="bibr" target="#b16">Srivastava et al., 2014</ref>). The vocabulary size is set to 50,000 and we train the model for 15 epochs with plain SGD optimizer, and after epoch 8 we halve the learning rate. At the end of each epoch we save the current state of the model and predict the perplex- ity values of the models on the development set. We employ early-stopping and select the model re- sulted from the epoch with the best perplexity to avoid over-fitting. The parameters are initialized over uniform distribution with support [-0.1, 0.1]. Additionally, for the decoder we employ global at- tention in combination with input feeding as de- scribed by <ref type="bibr" target="#b6">Luong et al. (2015)</ref>. The architecture 1 is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, with the input feeding ap- proach represented only for the last hidden state of the decoder. For the attention layer, we compute a context vector c t by using the information provided from the hidden states of the source sentence and by computing a weighted average with the alignment weights a t . The new hidden state is obtained us- ing a concatenation of the previous hidden state and the context vector:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Layer</head><formula xml:id="formula_0">˜ h t = tanh W [c t ; h t ]</formula><p>The global alignment weights a t are being com- puted with a softmax function over the general scoring method for attention:</p><formula xml:id="formula_1">a t (s) = exp h T t W as ¯ h s s exp h T t W as ¯ h s</formula><p>Input feeding is a process that sends the pre- vious hidden state obtained using the alignment method, to the input at the next step, presumably making the model keep track of anterior align- ment decisions. <ref type="bibr" target="#b6">Luong et al. (2015)</ref> showed this approach can increase the evaluation scores for neural machine translation, while in our case, for monolingual data, we believe it can be helpful to create better alignments. Our approach does not involve the use of character-based models <ref type="bibr" target="#b13">(Sennrich et al., 2015;</ref><ref type="bibr" target="#b5">Luong and Manning, 2016)</ref> to handle out of vocabulary words and entities. In- stead, we make use of alignment probabilities be- tween the predictions and the original sentences to retrieve the original words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word2vec Embeddings</head><p>Furthermore, we are interested to explore whether large scale pre-trained embeddings can improve text simplification models. <ref type="bibr" target="#b2">Kauchak (2013)</ref> indi- cates that combining normal data with simplified data can increase the performance of ATS systems. Therefore, we construct a secondary model (NTS- w2v) using a combination of pre-trained word2vec from Google News corpus ( <ref type="bibr" target="#b7">Mikolov et al., 2013a)</ref> of size 300 and locally trained embeddings of size 200. To ensure good representations of low- frequency words, we use word2vec <ref type="bibr">( ˇ Rehůřek and Sojka, 2010;</ref><ref type="bibr" target="#b8">Mikolov et al., 2013b</ref>) to train skip- gram with hierarchical softmax and we set a win- dow of 10 words.</p><p>Following <ref type="bibr">Garten et al. (2015)</ref> who showed that simple concatenation can improve the word rep- resentations, we construct two different sets of embeddings for the encoder and for the decoder. The former are constructed using the word2vec trained on the original English texts combined with Google News and the later (decoder) embed- dings are built from word2vec trained on the sim- plified version of the training data combined with Google News. To merge the local and global em- beddings, we concatenate the representations for each word in the vocabulary, thus obtaining a new representation of size 500. If a word is missing in the global embeddings, we replace it with a sam- ple from a Gaussian distribution with mean 0 and standard deviation of 0.9. The remaining param- eters are left unchanged from the previous model description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prediction Ranking</head><p>To ensure the best predictions and the best simpli- fied sentences at each step, we use beam search to sample multiple outputs from the two systems described previously <ref type="bibr">(NTS and NTS-w2v</ref>). Beam search works by generating the first k hypotheses at each step ordered by the log-likelihood of the target sentence given the input sentence. By de- fault, we use a beam size of 5 and take the first hy- pothesis, but we also observe that higher beam size and lower-ranked hypotheses can generate good simplification results. Therefore, we generate the first two candidate hypotheses for each beam size from 5 to 12. We then attempt to find the best beam size and hypothesis based on two metrics: the traditional MT-evaluation metric, BLEU <ref type="bibr" target="#b11">(Papineni et al., 2002;</ref><ref type="bibr">Bird et al., 2009</ref>) with NIST smoothing ( <ref type="bibr">Bird et al., 2009)</ref>, and SARI ( ), a recent text-simplification metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>To train our models, we use the publicly avail- able dataset provided by <ref type="bibr" target="#b1">Hwang et al. (2015)</ref> based on manual and automatic alignments be- tween standard English Wikipedia and Simple En- glish Wikipedia (EW-SEW). We discard the un- categorized matches, and use only good matches and partial matches which were above the 0.45 threshold ( <ref type="bibr" target="#b1">Hwang et al., 2015</ref>), totaling to 280K aligned sentences (around 150K full matches and 130K partial matches). It is one of the largest freely available resources for text simplification, and unlike the previously used EW-SEW cor- pus 2 <ref type="bibr" target="#b2">(Kauchak, 2013)</ref>, which only contains full matches (167K pairs), the newer dataset also con- tains partial matches. Therefore, it is not only larger, but it also allows for learning sentence shortening (dropping irrelevant parts) transforma- tions (see <ref type="table">Table 3</ref> We use the Stanford NER system ( <ref type="bibr">Finkel et al., 2005</ref>) to get an approximate number of locations, persons, organizations and miscellaneous entities in the corpus. A brief analysis of the vocabulary is rendered in <ref type="table">Table 1</ref>.</p><p>The dataset we use contains an abundant amount of named entities and consequently a large amount of low frequency words, but the majority of entities are not part of the model's 50,000 words vocabulary due to their small frequency. These words are replaced with 'UNK' symbols during training. At prediction time, we replace the un- known words with the highest probability score from the attention layer. We believe it is impor- tant to ensure that the models learn good word representations, either during the model training or through word2vec, in order to accurately create alignments between source and target sentences.</p><p>Given that in TS there is not only one best simplification, and that the quality of simplifi- cations in Simple English Wikipedia has been disputed before <ref type="bibr">(Amancio and Specia, 2014;</ref><ref type="bibr" target="#b20">Xu et al., 2015)</ref>, for tuning and testing we use the dataset previously released by , which contains 2000 sentences for tuning and 359 for testing, each with eight simplification variants obtained by eight Amazon Mechanical Turkers. <ref type="bibr">3</ref> The tune subset is also used as reference corpus in combination with BLEU and SARI to select the best beam size and hypothesis for prediction reranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>For the first 70 original sentences of the Xu et al.'s (2016) test set <ref type="bibr">4</ref> we perform three types of human evaluation to assess the output of our best sys- tems and three ATS systems of different architec- tures: (1) the PBSMT system with reranking of n-best outputs ( <ref type="bibr" target="#b19">Wubben et al., 2012)</ref>, which rep- resent the best PBSMT approach to ATS, trained and tuned over the same datasets as our systems; (2) the state-of-the-art SBMT system ( ) with modified tuning function (using SARI) and using PPDB paraphrase database ( <ref type="bibr">Ganitkevitch et al., 2013</ref>); <ref type="bibr">5</ref> and <ref type="formula">(3)</ref>    <ref type="bibr">Stajner, 2015)</ref>. <ref type="bibr">6</ref> We evaluate the output of all systems using three types of human evaluation.</p><p>Correctness and Number of Changes. First, we count the total number of changes made by each system (Total), counting the change of a whole phrase (e.g. "become defunct" → "was dissolved") as one change. Those changes that preserve the original meaning and grammatical- ity of the sentence (assessed by two native English speakers) and, at the same time, make the sentence easier to understand (assessed by two non-native fluent English speakers) are marked as Correct. In the case of content reduction, we instructed the annotators to count the deletion of each array of consecutive words as one change and consider the meaning unchanged if the main information of the sentence was retained and unchanged. The sen- tences for which the two annotators did not agree were given to a third annotator to obtain the ma- jority vote.</p><p>Grammaticality and Meaning Preservation. Second, three native English speakers rate the grammaticality (G) and meaning preservation (M) of each (whole) sentence with at least one change on a 1-5 Likert scale (1 -very bad; 5 -very good). The obtained inter-annotator agreement (quadratic Cohens kappa) was 0.78 for G and 0.63 for M.</p><p>Simplicity of sentences. Third, the three non- native fluent English speakers were shown origi- nal (reference) sentences and target (output) sen- tences, one pair at the time, and asked whether the target sentence is: +2 -much simpler; +1 -some- what simpler; 0 -equally difficult; -1 -somewhat more difficult; -2 -much more difficult, than the reference sentence. The obtained inter-annotator agreement (quadratic Cohens kappa) was 0.66.</p><p>While the correctness of changes takes into ac- count the influence of each individual change on grammaticality, meaning and simplicity of a sen- tence, the Scores (G and M) and Rank (S) take into account the mutual influence of all changes within a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>The results of the human evaluation <ref type="table" target="#tab_3">(Table 2)</ref> re- vealed that all NTS models achieve higher per- centage of correct changes and more simplified output than any of the state-of-the-art ATS systems with different architectures (PBSMT-R, SBMT, and LightLS). We also notice that the best models according to BLEU are obtained with hypothesis 1 and the maximum beam size for both models, while the SARI re-ranker prefers hypothesis 2 and beam size 5 for the first NTS and the maximum beam size for the custom word embeddings model.</p><p>The NTS with custom word2vec embeddings ranked with the text simplification specific met- ric (SARI) obtained the highest total number of changes among the neural systems, one of the highest percentage of correct changes, the second highest simplicity score, and solid grammaticality and meaning preservation scores. An example of the output of different systems is presented in Ta- ble 4 (Appendix A).</p><p>The use of different metrics for ranking the NTS predictions optimizes the output towards different evaluation objectives: SARI leads to the highest number of total changes, BLEU to the highest per- centage of correct changes, and the default beam scores to the best grammaticality (G) and meaning preservation (M). In addition, custom composed global and local word embeddings in combination with SARI metric improve the default translation system, given the joint scores for each evaluation criterion.</p><p>Here is important to note that for ATS sys- tems, the precision of the system (correctness of changes, grammaticality, meaning preservation, and simplicity of the output) is more important than the recall (the total number of changes made). The low recall would just leave the sentences sim- ilar to their originals thus not improving much the understanding or reading speed of the target users, or not improving much the NLP systems in which they are used as a pre-processing step. A low pre- cision, on the other hand, would make texts even more difficult to read and understand, and would worsen the performances of the NLP systems in which ATS is used as a pre-processing step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented a first attempt at modelling sentence simplification with a neural sequence to sequence model. Our extensive human evaluation showed that our NTS systems, if the output is ranked with the right metric, can significantly 7 outperform the best phrase-based and syntax-based MT ap- proaches, and unsupervised lexical ATS approach, <ref type="bibr">7</ref> Wilcoxon's signed rank test, p &lt; 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Changes</head><p>Scores  by grammaticality, meaning preservation and sim- plicity of the output sentences, the percentage of correct transformations, while at the same time achieving more than 1.5 changes per sentence, on average. Furthermore, we discovered that NTS systems are capable of correctly performing sig- nificant content reduction, thus being the only TS models proposed so far which can jointly perform lexical simplification and content reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix -Data Sample and System Output</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Match Transformation</head><p>Sentence pair Full syntactic simplification; reorder- ing of sentence constituents "During the 13th century, gingerbread was brought to Swe- den by German immigrants." and "German immigrants brought it to Sweden during the 13th century." Full lexical paraphrasing "During the 13th century, gingerbread was brought to Swe- den by German immigrants." and "German immigrants brought it to Sweden during the 13th century." Partial strong paraphrasing "Gingerbread foods vary, ranging from a soft, moist loaf cake to something close to a ginger biscuit." and "Ginger- bread is a word which describes different sweet food prod- ucts from soft cakes to a ginger biscuit." Partial adding explanations "Humidity is the amount of water vapor in the air." and "Humidity (adjective: humid) refers to water vapor in the air, but not to liquid droplets in fog, clouds, or rain." Partial sentence compression; dropping irrelevant information "Falaj irrigation is an ancient system dating back thousands of years and is used widely in Oman, the UAE, China, Iran and other countries." and "The ancient falaj system of irri- gation is still in use in some areas." <ref type="table">Table 3</ref>: Examples of full and partial matches from the EW-SEW dataset ( <ref type="bibr" target="#b1">Hwang et al., 2015</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the neural simplification model with global attention and input feeding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>ˇ</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Human evaluation results (the highest scores by each evaluation criterion are shown in bold).</figDesc><table></table></figure>

			<note place="foot" n="1"> The architecture configurations, data, and the pretrained models are released in https://github.com/ senisioi/NeuralTextSimplification</note>

			<note place="foot" n="2"> http://www.cs.pomona.edu/ ˜ dkauchak/ simplification/</note>

			<note place="foot" n="6"> For the LightLS system (Glavaš andŠtajnerandˇandŠtajner, 2015) we use the output of the original system provided by the authors.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been partially supported by a grant of the Romanian National Authority for Scientific Research and Innovation, CNCS/CCCDI UEFIS-CDI, project number PN-III-P2-2.1-53BG/2016, within PNCDI III, and by the SFB 884 on the Po-litical Economy of Reforms at the University of Mannheim (project C4), funded by the German Research Foundation (DFG). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aligning Sentences from Standard Wikipedia to Simple Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL&amp;HLT</title>
		<meeting>NAACL&amp;HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="211" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving text simplification language modeling using unsimplified text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1537" to="1546" />
		</imprint>
	</monogr>
	<note>Long Papers). ACL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">OpenNMT: Open-Source Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at International Conference on Learning Representations</title>
		<meeting>Workshop at International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text simplification as tree transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Brazilian Symposium in Information and Human Language Technology</title>
		<meeting>the 9th Brazilian Symposium in Information and Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised lexical simplification for non-native speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI</title>
		<meeting>the 30th AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks. ELRA<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04808</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Translating from complex to simplified sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international conference on Computational Processing of the Portuguese Language (PROPOR)</title>
		<meeting>the 9th international conference on Computational Processing of the Portuguese Language (PROPOR)<address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6001</biblScope>
			<biblScope unit="page" from="30" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Deeper Exploration of the Standard PBSMT Approach to Text Simplification and its Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Sanjaštajnersanjaˇsanjaštajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horacio</forename><surname>Bechara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL&amp;IJCNLP</title>
		<meeting>ACL&amp;IJCNLP</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="823" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sander Wubben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiel</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL): Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL): Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Problems in Current Text Simplification Research: New Data Can Help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="283" to="297" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Monolingual Tree-based Translation Model for Sentence Simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berndard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1353" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">System Output NTS-w2v default Perry Saturn (with terri) defeated Eddie Guerrero (with chyna) to win the WWF European Championship</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">NTS-w2v SARI Perry Saturn pinned Guerrero to win the WWF European Championship. NTS-w2v BLEU Perry Saturn pinned Guerrero after a diving drop drop. NTS default He (with terri) defeated Eddie Guerrero (with chyna) to win the WWF European Championship (8:10)</title>
		<imprint/>
	</monogr>
	<note>Saturn pinned Guerrero after a diving elbow drop</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nts</forename><surname>Bleu/Sari He Defeated</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Guerrero</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>with Chyna) to win the WWF European Championship (8:10); Saturn pinned Guerrero after a diving elbow drop</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">2015) Perry Saturn (with terri) defeated Eddie Guerrero (with chyna) to win the WWF European Championship</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lightls</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Saturn pinned Guerrero after a swimming shoulder fall</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Perry Saturn (with terri) beat Eddie Guerrero (with chyna) to win the WWF European League (8:10)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sbmt (xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Saturn pinned Guerrero after a diving elbow drop</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">2012) Perry Saturn with terri and Eddie Guerrero , chyna , to win the European Championship then-wwf 8:10); he pinned Guerrero after a diving elbow drop. Original Perry Saturn (with terri) defeated Eddie Guerrero</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pbsmt-R (</forename><surname>Wubben</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>with chyna) to win the WWF European Championship</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Saturn pinned Guerrero after a diving elbow drop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Table 4: Output examples, differences to the original sentence are shown in bold</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
