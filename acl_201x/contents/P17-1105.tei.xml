<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Abstract Syntax Networks for Code Generation and Semantic Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
						</author>
						<title level="a" type="main">Abstract Syntax Networks for Code Generation and Semantic Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1139" to="1149"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1105</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark HEARTHSTONE dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore , we perform competitively on the ATIS, JOBS, and GEO semantic parsing datasets with no task-specific engineering.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Tasks like semantic parsing and code generation are challenging in part because they are struc- tured (the output must be well-formed) but not synchronous (the output structure diverges from the input structure).</p><p>Sequence-to-sequence models have proven ef- fective for both tasks <ref type="bibr" target="#b10">(Dong and Lapata, 2016;</ref><ref type="bibr" target="#b19">Ling et al., 2016)</ref>, using encoder-decoder frame- works to exploit the sequential structure on both the input and output side. Yet these approaches do not account for much richer structural con- straints on outputs-including well-formedness, well-typedness, and executability. The well- formedness case is of particular interest, since it can readily be enforced by representing outputs as abstract syntax trees (ASTs) ( <ref type="bibr" target="#b0">Aho et al., 2006</ref>), an approach that can be seen as a much lighter weight * Equal contribution. show me the fare from ci0 to ci1 lambda $0 e ( exists $1 ( and ( from $1 ci0 ) ( to $1 ci1 ) ( = ( fare $1 ) $0 ) ) )</p><p>Figure 2: Example of a query and its logical form from the ATIS dataset. The ci0 and ci1 tokens are entity abstractions introduced in preprocess- ing <ref type="bibr" target="#b10">(Dong and Lapata, 2016</ref>).</p><p>version of CCG-based semantic parsing <ref type="bibr" target="#b30">(Zettlemoyer and Collins, 2005)</ref>. In this work, we introduce abstract syntax networks (ASNs), an extension of the standard encoder-decoder framework utilizing a modular decoder whose submodels are composed to na- tively generate ASTs in a top-down manner. The decoding process for any given input follows a dy-namically chosen mutual recursion between the modules, where the structure of the tree being produced mirrors the call graph of the recursion. We implement this process using a decoder model built of many submodels, each associated with a specific construct in the AST grammar and in- voked when that construct is needed in the out- put tree. As is common with neural approaches to structured prediction <ref type="bibr" target="#b8">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b27">Vinyals et al., 2015)</ref>, our decoder proceeds greed- ily and accesses not only a fixed encoding but also an attention-based representation of the in- put ( <ref type="bibr" target="#b4">Bahdanau et al., 2014)</ref>.</p><p>Our model significantly outperforms previous architectures for code generation and obtains com- petitive or state-of-the-art results on a suite of se- mantic parsing benchmarks. On the HEARTH- STONE dataset for code generation, we achieve a token BLEU score of 79.2 and an exact match ac- curacy of 22.7%, greatly improving over the pre- vious best results of 67.1 BLEU and 6.1% exact match ( <ref type="bibr" target="#b19">Ling et al., 2016)</ref>.</p><p>The flexibility of ASNs makes them readily ap- plicable to other tasks with minimal adaptation. We illustrate this point with a suite of seman- tic parsing experiments. On the JOBS dataset, we improve on previous state-of-the-art, achiev- ing 92.9% exact match accuracy as compared to the previous record of 90.7%. Likewise, we per- form competitively on the ATIS and GEO datasets, matching or exceeding the exact match reported by <ref type="bibr" target="#b10">Dong and Lapata (2016)</ref>, though not quite reaching the records held by the best previous se- mantic parsing approaches ( <ref type="bibr" target="#b28">Wang et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Encoder-decoder architectures, with and without attention, have been applied successfully both to sequence prediction tasks like machine translation and to tree prediction tasks like constituency pars- ing ( <ref type="bibr" target="#b9">Cross and Huang, 2016;</ref><ref type="bibr" target="#b11">Dyer et al., 2016;</ref><ref type="bibr" target="#b27">Vinyals et al., 2015</ref>). In the latter case, work has focused on making the task look like sequence-to- sequence prediction, either by flattening the output tree ( <ref type="bibr" target="#b27">Vinyals et al., 2015</ref>) or by representing it as a sequence of construction decisions <ref type="bibr" target="#b9">(Cross and Huang, 2016;</ref><ref type="bibr" target="#b11">Dyer et al., 2016)</ref>. Our work dif- fers from both in its use of a recursive top-down generation procedure. <ref type="bibr" target="#b10">Dong and Lapata (2016)</ref> introduced a sequence- to-sequence approach to semantic parsing, includ- ing a limited form of top-down recursion, but without the modularity or tight coupling between output grammar and model characteristic of our approach.</p><p>Neural (and probabilistic) modeling of code, in- cluding for prediction problems, has a longer his- tory. <ref type="bibr" target="#b1">Allamanis et al. (2015)</ref> and <ref type="bibr" target="#b20">Maddison and Tarlow (2014)</ref> proposed modeling code with a neural language model, generating concrete syn- tax trees in left-first depth-first order, focusing on metrics like perplexity and applications like code snippet retrieval. More recently, <ref type="bibr" target="#b25">Shin et al. (2017)</ref> attacked the same problem using a grammar-based variational autoencoder with top-down generation similar to ours instead. Meanwhile, a separate line of work has focused on the problem of program induction from input-output pairs ( <ref type="bibr" target="#b5">Balog et al., 2016;</ref><ref type="bibr" target="#b15">Liang et al., 2010;</ref><ref type="bibr" target="#b21">Menon et al., 2013</ref>).</p><p>The prediction framework most similar in spirit to ours is the doubly-recurrent decoder network in- troduced by <ref type="bibr" target="#b2">Alvarez-Melis and Jaakkola (2017)</ref>, which propagates information down the tree using a vertical LSTM and between siblings using a hor- izontal LSTM. Our model differs from theirs in using a separate module for each grammar con- struct and learning separate vertical updates for siblings when the AST labels require all siblings to be jointly present; we do, however, use a hori- zontal LSTM for nodes with variable numbers of children. The differences between our models re- flect not only design decisions, but also differences in data-since ASTs have labeled nodes and la- beled edges, they come with additional structure that our model exploits.</p><p>Apart from ours, the best results on the code- generation task associated with the HEARTH- STONE dataset are based on a sequence-to- sequence approach to the problem ( <ref type="bibr" target="#b19">Ling et al., 2016)</ref>. Abstract syntax networks greatly improve on those results.</p><p>Previously, <ref type="bibr" target="#b3">Andreas et al. (2016)</ref> introduced neural module networks (NMNs) for visual ques- tion answering, with modules corresponding to linguistic substructures within the input query. The primary purpose of the modules in NMNs is to compute deep features of images in the style of convolutional neural networks (CNN). These fea- tures are then fed into a final decision layer. In contrast to the modules we describe here, NMN modules do not make decisions about what to gen- erate or which modules to call next, nor do they  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Abstract Syntax Trees</head><p>Our model makes use of the Abstract Syntax Description Language (ASDL) framework ( <ref type="bibr" target="#b29">Wang et al., 1997)</ref>, which represents code fragments as trees with typed nodes. Primitive types correspond to atomic values, like integers or identifiers. Ac- cordingly, primitive nodes are annotated with a primitive type and a value of that type-for in- stance, in <ref type="figure" target="#fig_2">Figure 3a</ref>, the identifier node stor- ing "create minion" represents a function of the same name.</p><p>Composite types correspond to language con- structs, like expressions or statements. Each type has a collection of constructors, each of which specifies the particular language construct a node of that type represents. <ref type="figure" target="#fig_4">Figure 4</ref> shows con- structors for the statement (stmt) and expression (expr) types. The associated language constructs include function and class definitions, return state- ments, binary operations, and function calls.</p><p>Composite types enter syntax trees via compos- ite nodes, annotated with a composite type and a choice of constructor specifying how the node ex- pands. The root node in <ref type="figure" target="#fig_2">Figure 3a</ref>, for example, is  a composite node of type stmt that represents a class definition and therefore uses the ClassDef constructor. In <ref type="figure" target="#fig_2">Figure 3b</ref>, on the other hand, the root uses the Call constructor because it repre- sents a function call.</p><p>Children are specified by named and typed fields of the constructor, which have cardinalities of singular, optional, or sequential. By default, fields have singular cardinality, meaning they correspond to exactly one child. For instance, the ClassDef constructor has a singular name field of type identifier. Fields of optional cardinality are associ-ated with zero or one children, while fields of sequential cardinality are associated with zero or more children-these are designated us- ing ? and * suffixes in the grammar, respectively. Fields of sequential cardinality are often used to represent statement blocks, as in the body field of the ClassDef and FunctionDef construc- tors.</p><p>The grammars needed for semantic parsing can easily be given ASDL specifications as well, us- ing primitive types to represent variables, predi- cates, and atoms and composite types for standard logical building blocks like lambdas and counting (among others). <ref type="figure">Figure 2</ref> shows what the resulting λ-calculus trees look like. The ASDL grammars for both λ-calculus and Prolog-style logical forms are quite compact, as Figures 9 and 10 in the ap- pendix show.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Input Representation</head><p>We represent inputs as collections of named com- ponents, each of which consists of a sequence of tokens. In the case of semantic parsing, inputs have a single component containing the query sen- tence. In the case of HEARTHSTONE, the card's name and description are represented as sequences of characters and tokens, respectively, while cate- gorical attributes are represented as single-token sequences. For HEARTHSTONE, we restrict our input and output vocabularies to values that occur more than once in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Architecture</head><p>Our model uses an encoder-decoder architecture with hierarchical attention. The key idea behind our approach is to structure the decoder as a col- lection of mutually recursive modules. The mod- ules correspond to elements of the AST gram- mar and are composed together in a manner that mirrors the structure of the tree being generated. A vertical LSTM state is passed from module to module to propagate information during the de- coding process.</p><p>The encoder uses bidirectional LSTMs to em- bed each component and a feedforward network to combine them. Component-and token-level at- tention is applied over the input at each step of the decoding process.</p><p>We train our model using negative log likeli- hood as the loss function. The likelihood encom- passes terms for all generation decisions made by the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>Each component c of the input is encoded using a component-specific bidirectional LSTM. This re- sults in forward and backward token encodings ( − → h c , ← − h c ) that are later used by the attention mech- anism. To obtain an encoding of the input as a whole for decoder initialization, we concatenate the final forward and backward encodings of each component into a single vector and apply a linear projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoder Modules</head><p>The decoder decomposes into several classes of modules, one per construct in the grammar, which we discuss in turn. Throughout, we let v de- note the current vertical LSTM state, and use f to represent a generic feedforward neural network. LSTM updates with hidden state h and input x are notated as LSTM(h, x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Composite type modules Each composite type</head><p>T has a corresponding module whose role is to se- lect among the constructors C for that type. As <ref type="figure" target="#fig_6">Figure 5a</ref> exhibits, a composite type module re- ceives a vertical LSTM state v as input and ap- plies a feedforward network f T and a softmax out- put layer to choose a constructor:</p><formula xml:id="formula_0">p (C | T, v) = softmax (f T (v)) C .</formula><p>Control is then passed to the module associated with constructor C.</p><p>Constructor modules Each constructor C has a corresponding module whose role is to compute an intermediate vertical LSTM state v u,F for each of its fields F whenever C is chosen at a composite node u.</p><p>For each field F of the constructor, an embed- ding e F is concatenated with an attention-based context vector c and fed through a feedforward neural network f C to obtain a context-dependent field embedding:</p><formula xml:id="formula_1">˜ e F = f C (e F , c) .</formula><p>An intermediate vertical state for the field F at composite node u is then computed as   Constructor field modules Each field F of a constructor has a corresponding module whose role is to determine the number of children asso- ciated with that field and to propagate an updated vertical LSTM state to them. In the case of fields with singular cardinality, the decision and up- date are both vacuous, as exactly one child is al- ways generated. Hence these modules forward the field vertical LSTM state v u,F unchanged to the child w corresponding to F:</p><formula xml:id="formula_2">v u,F = LSTM v (v u , ˜ e F ) .</formula><formula xml:id="formula_3">v w = v u,F .<label>(1)</label></formula><p>Fields with optional cardinality can have either zero or one children; this choice is made using a feedforward network applied to the vertical LSTM state:</p><formula xml:id="formula_4">p(z F = 1 | v u,F ) = sigmoid (f gen F (v u,F )) .<label>(2)</label></formula><p>If a child is to be generated, then as in (1), the state is propagated forward without modification.</p><p>In the case of sequential fields, a horizon- tal LSTM is employed for both child decisions and state updates. We refer to <ref type="figure" target="#fig_6">Figure 5c</ref> for an illus- tration of the recurrent process. After being ini- tialized with a transformation of the vertical state, s F,0 = W F v u,F , the horizontal LSTM iteratively decides whether to generate another child by ap- plying a modified form of <ref type="formula" target="#formula_4">(2)</ref>:</p><formula xml:id="formula_5">p (z F,i = 1 | s F,i−1 , v u,F ) = sigmoid (f gen F (s F,i−1 , v u,F )) .</formula><p>If z F,i = 0, generation stops and the process ter- minates, as represented by the solid black circle in <ref type="figure" target="#fig_6">Figure 5c</ref>. Otherwise, the process continues as represented by the white circle in <ref type="figure" target="#fig_6">Figure 5c</ref>. In that case, the horizontal state s u,i−1 is combined with the vertical state v u,F and an attention-based context vector c F,i using a feedforward network f update F to obtain a joint context-dependent encod- ing of the field F and the position i:</p><formula xml:id="formula_6">˜ e F,i = f update F (v u,F , s u,i−1 , c F,i ).</formula><p>The result is used to perform a vertical LSTM up- date for the corresponding child w i :</p><formula xml:id="formula_7">v w i = LSTM v (v u,F , ˜ e F,i ).</formula><p>Finally, the horizontal LSTM state is updated us- ing the same field-position encoding, and the pro- cess continues:</p><formula xml:id="formula_8">s u,i = LSTM h (s u,i−1 , ˜ e F,i ).</formula><p>Primitive type modules Each primitive type T has a corresponding module whose role is to se- lect among the values y within the domain of that type. <ref type="figure" target="#fig_6">Figure 5d</ref> presents an example of the sim- plest form of this selection process, where the value y is obtained from a closed list via a soft- max layer applied to an incoming vertical LSTM state:</p><formula xml:id="formula_9">p (y | T, v) = softmax (f T (v))</formula><p>y . Some string-valued types are open class, how- ever. To deal with these, we allow generation both from a closed list of previously seen values, as in <ref type="figure" target="#fig_6">Figure 5d</ref>, and synthesis of new values. Synthesis is delegated to a character-level LSTM language model ( <ref type="bibr" target="#b6">Bengio et al., 2003)</ref>, and part of the role of the primitive module for open class types is to choose whether to synthesize a new value or not. During training, we allow the model to use the character LSTM only for unknown strings but in- clude the log probability of that binary decision in the loss in order to ensure the model learns when to generate from the character LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding Process</head><p>The decoding process proceeds through mutual re- cursion between the constituting modules, where the syntactic structure of the output tree mirrors the call graph of the generation procedure. At each step, the active decoder module either makes a generation decision, propagates state down the tree, or both.</p><p>To construct a composite node of a given type, the decoder calls the appropriate composite type module to obtain a constructor and its associated module. That module is then invoked to obtain updated vertical LSTM states for each of the con- structor's fields, and the corresponding constructor field modules are invoked to advance the process to those children.</p><p>This process continues downward, stopping at each primitive node, where a value is generated but no further recursion is carried out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Attention</head><p>Following standard practice for sequence-to- sequence models, we compute a raw bilinear at- tention score q raw t for each token t in the input us- ing the decoder's current state x and the token's encoding e t :</p><formula xml:id="formula_10">q raw t = e t Wx.</formula><p>The current state x can be either the vertical LSTM state in isolation or a concatentation of the vertical LSTM state and either a horizontal LSTM state or a character LSTM state (for string gener- ation). Each submodule that computes attention does so using a separate matrix W. </p><formula xml:id="formula_11">q t = q raw t + q comp c(t)</formula><p>, where c(t) denotes the component in which token t occurs. The attention weight vector a is then computed using a softmax:</p><formula xml:id="formula_12">a = softmax (q) .</formula><p>Given the weights, the attention-based context is given by:</p><formula xml:id="formula_13">c = t a t e t .</formula><p>Certain decision points that require attention have been highlighted in the description above; however, in our final implementation we made attention available to the decoder at all decision points.</p><p>Supervised Attention In the datasets we con- sider, partial or total copying of input tokens into primitive nodes is quite common. Rather than pro- viding an explicit copying mechanism ( <ref type="bibr" target="#b19">Ling et al., 2016)</ref>, we instead generate alignments where pos- sible to define a set of tokens on which the atten- tion at a given primitive node should be concen- trated. <ref type="bibr">2</ref> If no matches are found, the correspond- ing set of tokens is taken to be the whole input.</p><p>The attention supervision enters the loss through a term that encourages the final attention weights to be concentrated on the specified sub- set. Formally, if the matched subset of component- token pairs is S, the loss term associated with the supervision would be</p><formula xml:id="formula_14">log t exp (a t ) − log t∈S exp (a t ),<label>(3)</label></formula><p>where a t is the attention weight associated with to- ken t, and the sum in the first term ranges over all tokens in the input. The loss in (3) can be inter- preted as the negative log probability of attending to some token in S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental evaluation 4.1 Semantic parsing</head><p>Data We use three semantic parsing datasets: JOBS, GEO, and ATIS. All three consist of nat- ural language queries paired with a logical repre- sentation of their denotations. JOBS consists of 640 such pairs, with Prolog-style logical represen- tations, while GEO and ATIS consist of 880 and 5,410 such pairs, respectively, with λ-calculus log- ical forms. We use the same training-test split as <ref type="bibr" target="#b30">Zettlemoyer and Collins (2005)</ref> for JOBS and GEO, and the standard training-development-test split for ATIS. We use the preprocessed versions of these datasets made available by <ref type="bibr" target="#b10">Dong and Lapata (2016)</ref>, where text in the input has been low- ercased and stemmed using NLTK ( <ref type="bibr" target="#b7">Bird et al., 2009)</ref>, and matching entities appearing in the same input-output pair have been replaced by numbered abstract identifiers of the same type.</p><p>Evaluation We compute accuracies using tree exact match for evaluation. Following the pub- licly released code of <ref type="bibr" target="#b10">Dong and Lapata (2016)</ref>, we canonicalize the order of the children within con- junction and disjunction nodes to avoid spurious errors, but otherwise perform no transformations before comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Code generation</head><p>Data We use the HEARTHSTONE dataset intro- duced by <ref type="bibr" target="#b19">Ling et al. (2016)</ref>, which consists of 665 cards paired with their implementations in the open-source Hearthbreaker engine. 3 Our training- development-test split is identical to that of <ref type="bibr" target="#b19">Ling et al. (2016)</ref>, with split sizes of 533, 66, and 66, respectively.</p><p>Cards contain two kinds of components: tex- tual components that contain the card's name and a description of its function, and categorical ones that contain numerical attributes (attack, health, cost, and durability) or enumerated attributes (rar- ity, type, race, and class). The name of the card is represented as a sequence of characters, while its description consists of a sequence of tokens split on whitespace and punctuation. All categori- cal components are represented as single-token se- quences.</p><p>Evaluation For direct comparison to the results of <ref type="bibr" target="#b19">Ling et al. (2016)</ref>, we evaluate our predicted code based on exact match and token-level BLEU relative to the reference implementations from the library. We additionally compute node-based pre- cision, recall, and F1 scores for our predicted trees compared to the reference code ASTs. Formally, these scores are obtained by defining the intersec- tion of the predicted and gold trees as their largest common tree prefix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Settings</head><p>For each experiment, all feedforward and LSTM hidden dimensions are set to the same value. We select the dimension from {30, 40, 50, 60, 70} for the smaller JOBS and GEO datasets, or from {50, 75, 100, 125, 150} for the larger ATIS and HEARTHSTONE datasets. The dimensionality used for the inputs to the encoder is set to 100 in all cases. We apply dropout to the non-recurrent connections of the vertical and horizontal LSTMs, selecting the noise ratio from {0.2, 0.3, 0.4, 0.5}. All parameters are randomly initialized using Glo- rot initialization <ref type="bibr" target="#b12">(Glorot and Bengio, 2010)</ref>.</p><p>We perform 200 passes over the data for the JOBS and GEO experiments, or 400 passes for the ATIS and HEARTHSTONE experiments. Early stopping based on exact match is used for the se- mantic parsing experiments, where performance is evaluated on the training set for JOBS and GEO or on the development set for ATIS. Parameters for the HEARTHSTONE experiments are selected based on development BLEU scores. In order to promote generalization, ties are broken in all cases with a preference toward higher dropout ratios and lower dimensionalities, in that order.</p><p>Our system is implemented in Python using the DyNet neural network library ( <ref type="bibr">Neubig et al., 2017)</ref>. We use the Adam optimizer <ref type="bibr" target="#b13">(Kingma and Ba, 2014</ref>) with its default settings for optimiza- tion, with a batch size of 20 for the semantic pars- ing experiments, or a batch size of 10 for the HEARTHSTONE experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Our results on the semantic parsing datasets are presented in  a new state-of-the-art accuracy of 91.4% on the JOBS dataset, and this number improves to 92.9% when supervised attention is added. On the ATIS and GEO datasets, we respectively exceed and match the results of <ref type="bibr" target="#b10">Dong and Lapata (2016)</ref>. However, these fall short of the previous best re- sults of 91.3% and 90.4%, respectively, obtained by <ref type="bibr" target="#b28">Wang et al. (2014)</ref>. This difference may be par- tially attributable to the use of typing information or rich lexicons in most previous semantic pars- ing approaches <ref type="bibr" target="#b31">(Zettlemoyer and Collins, 2007;</ref><ref type="bibr" target="#b14">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b28">Wang et al., 2014;</ref><ref type="bibr" target="#b32">Zhao and Huang, 2015</ref>).</p><p>On the HEARTHSTONE dataset, we improve significantly over the initial results of <ref type="bibr" target="#b19">Ling et al. (2016)</ref> across all evaluation metrics, as shown in <ref type="table" target="#tab_2">Table 2</ref>. On the more stringent exact match metric, we improve from 6.1% to 18.2%, and on token- level BLEU, we improve from 67.1 to 77.6. When supervised attention is added, we obtain an ad- ditional increase of several points on each scale, achieving peak results of 22.7% accuracy and 79.2 BLEU. Figure 6: Cards with minimal descriptions exhibit a uniform structure that our system almost always predicts correctly, as in this instance. </p><formula xml:id="formula_15">return Minion( 1, 3, effects=[ Effect( SpellCast(), ActionTag( Give(ChangeAttack(1)), SelfSelector())) ])</formula><p>Figure 7: For many cards with moderately com- plex descriptions, the implementation follows a functional style that seems to suit our modeling strategy, usually leading to correct predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Error Analysis and Discussion</head><p>As the examples in <ref type="figure">Figures 6-8</ref> show, classes in the HEARTHSTONE dataset share a great deal of common structure. As a result, in the simplest cases, such as in <ref type="figure">Figure 6</ref>, generating the code is simply a matter of matching the overall structure and plugging in the correct values in the initializer and a few other places. In such cases, our sys- tem generally predicts the correct code, with the class MultiShot(SpellCard): def __init__(self): super().__init__( 'Multi-Shot', 4, CHARACTER_CLASS.HUNTER, CARD_RARITY.FREE) def use(self, player, game): super().use(player, game) targets = copy.copy( game.other_player.minions) for i in range(0, 2): target = game.random_choice(targets) targets.remove(target) target.damage( player.effective_spell_damage(3), self) def can_use(self, player, game):</p><p>return ( super().can_use(player, game) and (len(game.other_player.minions) &gt;= 2)) class MultiShot(SpellCard): def __init__(self): super().__init__( 'Multi-Shot', 4, CHARACTER_CLASS.HUNTER, CARD_RARITY.FREE) def use(self, player, game): super().use(player, game) minions = copy.copy( game.other_player.minions) for i in range(0, 3): minion = game.random_choice(minions) minions.remove(minion) def can_use(self, player, game):</p><p>return ( super().can_use(player, game) and len(game.other_player.minions) &gt;= 3) <ref type="figure">Figure 8</ref>: Cards with nontrivial logic expressed in an imperative style are the most challenging for our system. In this example, our prediction comes close to the gold code, but misses an important statement in addition to making a few other minor errors. (Left) gold code; (right) predicted code.</p><p>exception of instances in which strings are incor- rectly transduced. Introducing a dedicated copy- ing mechanism like the one used by <ref type="bibr" target="#b19">Ling et al. (2016)</ref> or more specialized machinery for string transduction may alleviate this latter problem.</p><p>The next simplest category of card-code pairs consists of those in which the card's logic is mostly implemented via nested function calls. <ref type="figure">Figure 7</ref> illustrates a typical case, in which the card's effect is triggered by a game event (a spell being cast) and both the trigger and the effect are described by arguments to an Effect construc- tor. Our system usually also performs well on in- stances like these, apart from idiosyncratic errors that can take the form of under-or overgeneration or simply substitution of incorrect predicates.</p><p>Cards whose code includes complex logic ex- pressed in an imperative style, as in <ref type="figure">Figure 8</ref>, pose the greatest challenge for our system. Factors like variable naming, nontrivial control flow, and in- terleaving of code predictable from the descrip- tion with code required due to the conventions of the library combine to make the code for these cards difficult to generate. In some instances (as in the figure), our system is nonetheless able to synthesize a close approximation. However, in the most complex cases, the predictions deviate sig- nificantly from the correct implementation.</p><p>In addition to the specific errors our system makes, some larger issues remain unresolved. Ex- isting evaluation metrics only approximate the actual metric of interest: functional equiva- lence. Modifications of BLEU, tree F1, and exact match that canonicalize the code-for example, by anonymizing all variables-may prove more meaningful. Direct evaluation of functional equiv- alence is of course impossible in general <ref type="bibr" target="#b26">(Sipser, 2006</ref>), and practically challenging even for the HEARTHSTONE dataset because it requires inte- grating with the game engine.</p><p>Existing work also does not attempt to enforce semantic coherence in the output. Long-distance semantic dependencies, between occurrences of a single variable for example, in particular are not modeled. Nor is well-typedness or executability. Overcoming these evaluation and modeling issues remains an important open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>ASNs provide a modular encoder-decoder archi- tecture that can readily accommodate a variety of tasks with structured output spaces. They are par- ticularly applicable in the presence of recursive decompositions, where they can provide a simple decoding process that closely parallels the inher- ent structure of the outputs. Our results demon- strate their promise for tree prediction tasks, and we believe their application to more general out- put structures is an interesting avenue for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example code for the "Dire Wolf Alpha" Hearthstone card.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Excerpt from the same AST, corresponding to the code snip- pet Aura(ChangeAttack(1),MinionSelector(Adjacent())).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Fragments from the abstract syntax tree corresponding to the example code in Figure 1. Blue boxes represent composite nodes, which expand via a constructor with a prescribed set of named children. Orange boxes represent primitive nodes, with their corresponding values written underneath. Solid black squares correspond to constructor fields with sequential cardinality, such as the body of a class definition (Figure 3a) or the arguments of a function call (Figure 3b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A simplified fragment of the Python ASDL grammar. 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5b illustrates the process, starting with a single vertical LSTM state and ending with one updated state per field.</head><label></label><figDesc>Figure 5b illustrates the process, starting with a single vertical LSTM state and ending with one updated state per field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The module classes constituting our decoder. For brevity, we omit the cardinality modules for singular and optional cardinalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>A separate attention score q comp c is computed for each component of the input, independent of its content: q comp c = w c x. The final token-level attention scores are the sums of the raw token-level scores and the corre- sponding component-level scores:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 . Our basic system achieves</head><label>1</label><figDesc></figDesc><table>ATIS 

GEO 
JOBS 
System 
Accuracy System 
Accuracy System 
Accuracy 
ZH15 
84.2 
ZH15 
88.9 
ZH15 
85.0 
ZC07 
84.6 
KCAZ13 
89.0 
PEK03 
88.0 
WKZ14 
91.3 
WKZ14 
90.4 
LJK13 
90.7 
DL16 
84.6 
DL16 
87.1 
DL16 
90.0 
ASN 
85.3 
ASN 
85.7 
ASN 
91.4 
+ SUPATT 
85.9 
+ SUPATT 
87.1 
+ SUPATT 
92.9 

Table 1: Accuracies for the semantic parsing tasks. ASN denotes our abstract syntax network framework. 
SUPATT refers to the supervised attention mentioned in Section 3.4. 

System 
Accuracy BLEU F1 
NEAREST 
3.0 
65.0 
65.7 
LPN 
6.1 
67.1 
-
ASN 
18.2 
77.6 
72.4 
+ SUPATT 
22.7 
79.2 
75.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results for the HEARTHSTONE task. SU-
PATT refers to the system with supervised atten-
tion mentioned in Section 3.4. LPN refers to the 
system of Ling et al. (2016). Our nearest neigh-
bor baseline NEAREST follows that of Ling et al. 
(2016), though it performs somewhat better; its 
nonzero exact match number stems from spurious 
repetition in the data. 

</table></figure>

			<note place="foot" n="1"> The full grammar can be found online on the documentation page for the Python ast module: https://docs.python.org/3/library/ast. html#abstract-grammar</note>

			<note place="foot" n="2"> Alignments are generated using an exact string match heuristic that also included some limited normalization, primarily splitting of special characters, undoing camel case, and lemmatization for the semantic parsing datasets.</note>

			<note place="foot" n="3"> Available online at https://github.com/ danielyule/hearthbreaker.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>MR is supported by an NSF Graduate Research Fellowship and a Fannie and John Hertz Founda-tion Google Fellowship. MS is supported by an NSF Graduate Research Fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head> <ref type="figure">Figure 10</ref><p>: The λ-calculus grammar used by our system.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<title level="m">Compilers: Principles, Techniques, and Tools</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>2Nd Edition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bimodal modelling of source code and natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="2123" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tree-structured decoding with doubly-recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Oral</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deepcoder: Learning to write programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<idno>CoRR abs/1611.01989</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=944919.944966" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D/D14/D14-1082.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno>CoRR abs/1601.01280</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1545" to="1556" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning programs: A hierarchical bayesian approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)<address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06-21" />
			<biblScope unit="page" from="639" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<idno type="doi">10.1162/COLIa00127</idno>
		<ptr target="https://doi.org/10.1162/COLIa00127" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Latent predictor networks for code generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured generative models of natural source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A machine learning framework for programming by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Tamuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Butler</forename><forename type="middle">W</forename><surname>Lampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="187" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaitanya Malaviya</title>
		<imprint>
			<date>Gaurav Kumar</date>
			<publisher>Paul Michel</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naomi</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saphra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
		<title level="m">Swabha Swayamdipta, and Pengcheng Yin. 2017. Dynet: The dynamic neural network toolkit</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards a theory of natural language interfaces to databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on Intelligent user interfaces</title>
		<meeting>the 8th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tree-structured variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to the Theory of Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Course Technology</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Morpho-syntactic lexical generalization for ccg semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrienne</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1284" to="1295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The zephyr abstract syntax description language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">L</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">S</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Domain-Specific Languages on Conference on Domain-Specific Languages (DSL), 1997. USENIX Association</title>
		<meeting>the Conference on Domain-Specific Languages on Conference on Domain-Specific Languages (DSL), 1997. USENIX Association<address><addrLine>Berkeley, CA, USA, DSL&apos;97</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="17" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI &apos;05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence</title>
		<meeting><address><addrLine>Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-07-26" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Online learning of relaxed ccg grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="678" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Type-driven incremental semantic parsing with polymorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-31" />
			<biblScope unit="page" from="1416" to="1421" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2015</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
