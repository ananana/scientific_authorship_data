<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<postCode>98052</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1321" to="1331"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel semantic parsing framework for question answering using a knowledge base. We define a query graph that resembles subgraphs of the knowledge base and can be directly mapped to a logical form. Semantic parsing is reduced to query graph generation, formulated as a staged search problem. Unlike traditional approaches, our method leverages the knowledge base in an early stage to prune the search space and thus simplifies the semantic matching problem. By applying an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate sequences, our system outper-forms previous methods substantially, and achieves an F 1 measure of 52.5% on the WEBQUESTIONS dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Organizing the world's facts and storing them in a structured database, large-scale knowledge bases (KB) like <ref type="bibr">DBPedia (Auer et al., 2007)</ref> and <ref type="bibr">Freebase (Bollacker et al., 2008)</ref> have become important resources for supporting open-domain question answering (QA). Most state-of-the-art approaches to KB-QA are based on semantic pars- ing, where a question (utterance) is mapped to its formal meaning representation (e.g., logical form) and then translated to a KB query. The answers to the question can then be retrieved simply by exe- cuting the query. The semantic parse also provides a deeper understanding of the question, which can be used to justify the answer to users, as well as to provide easily interpretable information to devel- opers for error analysis.</p><p>However, most traditional approaches for se- mantic parsing are largely decoupled from the knowledge base, and thus are faced with sev- eral challenges when adapted to applications like QA. For instance, a generic meaning represen- tation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB ( <ref type="bibr" target="#b15">Kwiatkowski et al., 2013)</ref>. Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vo- cabulary in the KB to relations described in the utterance remains a difficult problem <ref type="bibr" target="#b2">(Berant and Liang, 2014)</ref>.</p><p>Inspired by <ref type="bibr" target="#b24">(Yao and Van Durme, 2014</ref>; <ref type="bibr" target="#b1">Bao et al., 2014</ref>), we propose a semantic parsing frame- work that leverages the knowledge base more tightly when forming the parse for an input ques- tion. We first define a query graph that can be straightforwardly mapped to a logical form in λ- calculus and is semantically closely related to λ- DCS ( <ref type="bibr" target="#b16">Liang, 2013)</ref>. Semantic parsing is then re- duced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The representation power of the semantic parse is thus controlled by the set of le- gitimate actions applicable to each state. In partic- ular, we stage the actions into three main steps: locating the topic entity in the question, finding the main relationship between the answer and the topic entity, and expanding the query graph with additional constraints that describe properties the answer needs to have, or relationships between the answer and other entities in the question.</p><p>One key advantage of this staged design is that through grounding partially the utterance to some entities and predicates in the KB, we make the search far more efficient by focusing on the promising areas in the space that most likely lead to the correct query graph, before the full parse is determined. For example, after linking "Fam-ily Guy" in the question "Who first voiced Meg on Family Guy?" to FamilyGuy (the TV show) in the knowledge base, the procedure needs only to examine the predicates that can be applied to FamilyGuy instead of all the predicates in the KB. Resolving other entities also becomes easy, as given the context, it is clear that Meg refers to MegGriffin (the character in Family Guy). Our design divides this particular semantic pars- ing problem into several sub-problems, such as en- tity linking and relation matching. With this in- tegrated framework, best solutions to each sub- problem can be easily combined and help pro- duce the correct semantic parse. For instance, an advanced entity linking system that we em- ploy outputs candidate entities for each question with both high precision and recall. In addi- tion, by leveraging a recently developed semantic matching framework based on convolutional net- works, we present better relation matching models using continuous-space representations instead of pure lexical matching. Our semantic parsing ap- proach improves the state-of-the-art result on the WEBQUESTIONS dataset <ref type="bibr" target="#b3">(Berant et al., 2013</ref>) to 52.5% in F 1 , a 7.2% absolute gain compared to the best existing method.</p><p>The rest of this paper is structured as follows. Sec. 2 introduces the basic notion of the graph knowledge base and the design of our query graph. Sec. 3 presents our search-based approach for gen- erating the query graph. The experimental results are shown in Sec. 4, and the discussion of our ap- proach and the comparisons to related work are in Sec. 5. Finally, Sec. 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this work, we aim to learn a semantic parser that maps a natural language question to a logi- cal form query q, which can be executed against a knowledge base K to retrieve the answers. Our ap- proach takes a graphical view of both K and q, and reduces semantic parsing to mapping questions to query graphs. We describe the basic design below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge base</head><p>The knowledge base K considered in this work is a collection of subject-predicate-object triples (e 1 , p, e 2 ), where e 1 , e 2 ∈ E are the entities (e.g., FamilyGuy or MegGriffin) and p ∈ P is a binary predicate like character. A knowledge base in this form is often called a knowledge graph  because of its straightforward graphical represen- tation -each entity is a node and two related en- tities are linked by a directed edge labeled by the predicate, from the subject to the object entity. To compare our approach to existing methods, we use Freebase, which is a large database with more than 46 million topics and 2.6 billion facts. In Freebase's design, there is a special entity cate- gory called compound value type (CVT), which is not a real-world entity, but is used to collect mul- tiple fields of an event or a special relationship. <ref type="figure" target="#fig_1">Fig. 1</ref> shows a small subgraph of Freebase re- lated to the TV show Family Guy. Nodes are the entities, including some dates and special CVT en- tities <ref type="bibr">1</ref> . A directed edge describes the relation be- tween two entities, labeled by the predicate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query graph</head><p>Given the knowledge graph, executing a logical- form query is equivalent to finding a subgraph that can be mapped to the query and then resolving the binding of the variables. To capture this intuition, we describe a restricted subset of λ-calculus in a graph representation as our query graph.</p><p>Our query graph consists of four types of nodes: grounded entity (rounded rectangle), existential variable (circle), lambda variable (shaded circle), aggregation function (diamond). Grounded enti- ties are existing entities in the knowledge base K. Existential variables and lambda variables are un- grounded entities. In particular, we would like to retrieve all the entities that can map to the lambda variables in the end as the answers. Aggregation function is designed to operate on a specific entity, which typically captures some numerical proper- ties. Just like in the knowledge graph, related nodes in the query graph are connected by directed edges, labeled with predicates in K.</p><p>To demonstrate this design, <ref type="figure" target="#fig_2">Fig. 2</ref> shows one possible query graph for the question "Who first voiced Meg on Family Guy?" using Freebase. The two entities, MegGriffin and FamilyGuy are represented by two rounded rectangle nodes. The circle node y means that there should exist an entity describing some casting relations like the character, actor and the time she started the role 2 . The shaded circle node x is also called the answer node, and is used to map entities re- trieved by the query. The diamond node arg min constrains that the answer needs to be the ear- liest actor for this role. Equivalently, the logi- cal form query in λ-calculus without the aggrega- tion function is: λx.∃y.cast(FamilyGuy, y) ∧ actor(y, x) ∧ character(y, MegGriffin) Running this query graph against K as in <ref type="figure" target="#fig_1">Fig. 1</ref> will match both LaceyChabert and MilaKunis before applying the aggregation function, but only LaceyChabert is the correct answer as she started this role earlier (by checking the from property of the grounded CVT node).</p><p>Our query graph design is inspired by <ref type="bibr" target="#b18">(Reddy et al., 2014</ref>), but with some key differences. The nodes and edges in our query graph closely re- semble the exact entities and predicates from the knowledge base. As a result, the graph can be straightforwardly translated to a logical form query that is directly executable. In contrast, the query graph in ( <ref type="bibr" target="#b18">Reddy et al., 2014</ref>) is mapped from the CCG parse of the question, and needs fur- ther transformations before mapping to subgraphs 2 y should be grounded to a CVT entity in this case. of the target knowledge base to retrieve answers. Semantically, our query graph is more related to simple λ-DCS ( <ref type="bibr" target="#b3">Berant et al., 2013;</ref><ref type="bibr" target="#b16">Liang, 2013)</ref>, which is a syntactic simplification of λ-calculus when applied to graph databases. A query graph can be viewed as the tree-like graph pattern of a logical form in λ-DCS. For instance, the path from the answer node to an entity node can be described using a series of join operations in λ-DCS. Differ- ent paths of the tree graph are combined via the intersection operators.</p><formula xml:id="formula_0">f S e S p S c A e A p A a /A c A a /A c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Staged Query Graph Generation</head><p>We focus on generating query graphs with the fol- lowing properties. First, the tree graph consists of one entity node as the root, referred as the topic entity. Second, there exists only one lambda vari- able x as the answer node, with a directed path from the root to it, and has zero or more existential variables in-between. We call this path the core inferential chain of the graph, as it describes the main relationship between the answer and topic entity. Variables can only occur in this chain, and the chain only has variable nodes except the root. Finally, zero or more entity or aggregation nodes can be attached to each variable node, including the answer node. These branches are the addi- tional constraints that the answers need to satisfy. For example, in <ref type="figure" target="#fig_2">Fig. 2</ref>, FamilyGuy is the root and FamilyGuy → y → x is the core inferential chain. The branch y → MegGriffin specifies the character and y → arg min constrains that the answer needs to be the earliest actor for this role.</p><p>Given a question, we formalize the query graph generation process as a search problem, with staged states and actions.</p><p>Let S = {φ, S e , S p , S c } be the set of states, where each state could be an empty graph (φ), a single- node graph with the topic entity (S e ), a core in- ferential chain (S p ), or a more complex query graph with additional constraints (S c ). Let A = {A e , A p , A c , A a } be the set of actions. An ac- tion grows a given graph by adding some edges and nodes. In particular, A e picks an entity node; A p determines the core inferential chain; A c and A a add constraints and aggregation nodes, respec- tively. Given a state, the valid action set can be de- fined by the finite state diagram in <ref type="figure" target="#fig_3">Fig. 3</ref>. Notice that the order of possible actions is chosen for the convenience of implementation. In principle, we could choose a different order, such as matching the core inferential chain first and then resolving the topic entity linking. However, since we will consider multiple hypotheses during search, the order of the staged actions can simply be viewed as a different way to prune the search space or to bias the exploration order.</p><p>We define the reward function on the state space using a log-linear model. The reward basically estimates the likelihood that a query graph cor- rectly parses the question. Search is done using the best-first strategy with a priority queue, which is formally defined in Appendix A. In the follow- ing subsections, we use a running example of find- ing the semantic parse of question q ex = "Who first voiced Meg of Family Guy?" to describe the sequence of actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linking Topic Entity</head><p>Starting from the initial state s 0 , the valid actions are to create a single-node graph that corresponds to the topic entity found in the given question. For instance, possible topic entities in q ex can either be FamilyGuy or MegGriffin, shown in <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>We use an entity linking system that is designed for short and noisy text ( <ref type="bibr" target="#b22">Yang and Chang, 2015</ref>). For each entity e in the knowledge base, the sys- tem first prepares a surface-form lexicon that lists all possible ways that e can be mentioned in text. This lexicon is created using various data sources, such as names and aliases of the entities, the an- chor text in Web documents and the Wikipedia re- direct table. Given a question, it considers all the  Figure 5: Candidate core inferential chains start from the entity FamilyGuy.</p><p>consecutive word sequences that have occurred in the lexicon as possible mentions, paired with their possible entities. Each pair is then scored by a sta- tistical model based on its frequency counts in the surface-form lexicon. To tolerate potential mis- takes of the entity linking system, as well as ex- ploring more possible query graphs, up to 10 top- ranked entities are considered as the topic entity. The linking score will also be used as a feature for the reward function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identifying Core Inferential Chain</head><p>Given a state s that corresponds to a single-node graph with the topic entity e, valid actions to ex- tend this graph is to identify the core inferential chain; namely, the relationship between the topic entity and the answer. For example, <ref type="figure">Fig. 5</ref> shows three possible chains that expand the single-node graph in s 1 . Because the topic entity e is given, we only need to explore legitimate predicate se- quences that can start from e. Specifically, to re- strict the search space, we explore all paths of length 2 when the middle existential variable can be grounded to a CVT node and paths of length 1 if not. We also consider longer predicate sequences if the combinations are observed in training data <ref type="bibr">3</ref> .</p><p>Analogous to the entity linking problem, where the goal is to find the mapping of mentions to en- tities in K, identifying the core inferential chain is to map the natural utterance of the question to the correct predicate sequence. For question "Who first voiced Meg on [Family Guy]?" we need to measure the likelihood that each of the sequences in {cast-actor, writer-start, genre} correctly captures the relationship between Family Guy and Who. We reduce this problem to measur- ing semantic similarity using neural networks.  See text for the description of each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Deep Convolutional Neural Networks</head><p>To handle the huge variety of the semantically equivalent ways of stating the same question, as well as the mismatch of the natural language ut- terances and predicates in the knowledge base, we propose using Siamese neural networks <ref type="bibr" target="#b8">(Bromley et al., 1993</ref>) for identifying the core inferen- tial chain. For instance, one of our constructions maps the question to a pattern by replacing the en- tity mention with a generic symbol &lt;e&gt; and then compares it with a candidate chain, such as "who first voiced meg on &lt;e&gt;" vs. cast-actor. The model consists of two neural networks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similar- ity is then computed using some distance func- tion, such as cosine. This continuous-space rep- resentation approach has been proposed recently for semantic parsing and question answering (Bor- des et al., 2014a; <ref type="bibr" target="#b25">Yih et al., 2014</ref>) and has shown better results compared to lexical matching ap- proaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; <ref type="bibr" target="#b13">Gao et al., 2014</ref>) to this matching problem. The network architecture is illustrated in <ref type="figure">Fig. 6</ref>. The CNN model first applies a word hashing technique ( <ref type="bibr" target="#b14">Huang et al., 2013</ref>) that breaks a word into a vector of letter-trigrams (x t → f t in <ref type="figure">Fig. 6</ref>). For example, the bag of letter-trigrams of the word "who" are #-w-h, w-h-o, h-o-# after adding the word boundary symbol #. Then, it uses a convo- lutional layer to project the letter-trigram vectors of words within a context window of 3 words to a local contextual feature vector (f t → h t ), fol- lowed by a max pooling layer that extracts the most salient local features to form a fixed-length global feature vector (v). The global feature vector is then fed to feed-forward neural network layers to output the final non-linear semantic features (y), as the vector representation of either the pattern or the inferential chain.</p><p>Training the model needs positive pairs, such as a pattern like "who first voiced meg on &lt;e&gt;" and an inferential chain like cast-actor. These pairs can be extracted from the full semantic parses when provided in the training data. If the correct semantic parses are latent and only the pairs of questions and answers are available, such as the case in the WEBQUESTIONS dataset, we can still hypothesize possible inferential chains by traversing the paths in the knowledge base that connect the topic entity and the answer. Sec. 4.1 will illustrate this data generation process in detail.</p><p>Our model has two advantages over the embed- ding approach <ref type="figure" target="#fig_1">(Bordes et al., 2014a)</ref>. First, the word hashing layer helps control the dimensional- ity of the input space and can easily scale to large vocabulary. The letter-trigrams also capture some sub-word semantics (e.g., words with minor ty- pos have almost identical letter-trigram vectors), which makes it especially suitable for questions from real-world users, such as those issued to a search engine. Second, it uses a deeper archi- tecture with convolution and max-pooling layers, which has more representation power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Augmenting Constraints &amp; Aggregations</head><p>A graph with just the inferential chain forms the simplest legitimate query graph and can be exe- cuted against the knowledge base K to retrieve the answers; namely, all the entities that x can be grounded to. For instance, the graph in s 3 in <ref type="figure" target="#fig_7">Fig. 7</ref> will retrieve all the actors who have been on FamilyGuy. Although this set of entities obvi- ously contains the correct answer to the question (assuming the topic entity FamilyGuy is correct), it also includes incorrect entities that do not sat- isfy additional constraints implicitly or explicitly mentioned in the question.</p><p>To further restrict the set of answer entities, the graph with only the core inferential chain can be expanded by two types of actions: A c and A a . A c is the set of possible ways to attach an entity to a variable node, where the edge denotes one of the valid predicates that can link the variable to the entity. For instance, in <ref type="figure" target="#fig_7">Fig. 7</ref>, s 6 is created by attaching MegGriffin to y with the predicate character. This is equivalent to the last con- junctive term in the corresponding λ-expression: λx.∃y.cast(FamilyGuy, y) ∧ actor(y, x) ∧ character(y, MegGriffin). Sometimes, the constraints are described over the entire answer set through the aggregation function, such as the word "first" in our example question q ex . This is handled similarly by actions A a , which attach an aggregation node on a variable node. For exam- ple, the arg min node of s 7 in <ref type="figure" target="#fig_7">Fig. 7</ref> chooses the grounding with the smallest from attribute of y.</p><p>The full possible constraint set can be derived by first issuing the core inferential chain as a query to the knowledge base to find the bindings of vari- ables y's and x, and then enumerating all neigh- boring nodes of these entities. This, however, often results in an unnecessarily large constraint pool. In this work, we employ simple rules to re- tain only the nodes that have some possibility to be legitimate constraints. For instance, a constraint node can be an entity that also appears in the ques- tion (detected by our entity linking component), or an aggregation constraint can only be added if cer- tain keywords like "first" or "latest" occur in the question. The complete set of these rules can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning the reward function</head><p>Given a state s, the reward function γ(s) basically judges whether the query graph represented by s is the correct semantic parse of the input ques- tion q. We use a log-linear model to learn the re- ward function. Below we describe the features and the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Features</head><p>The features we designed essentially match spe- cific portions of the graph to the question, and gen- erally correspond to the staged actions described previously, including:</p><p>Topic Entity The score returned by the entity linking system is directly used as a feature.</p><p>Core Inferential Chain We use similarity scores of different CNN models described in Sec. 3.2.1 to measure the quality of the core infer- ential chain. PatChain compares the pattern (re- placing the topic entity with an entity symbol) and the predicate sequence. QuesEP concatenates the canonical name of the topic entity and the predi- cate sequence, and compares it with the question. This feature conceptually tries to verify the entity linking suggestion. These two CNN models are learned using pairs of the question and the infer- ential chain of the parse in the training data. In addition to the in-domain similarity features, we also train a ClueWeb model using the Freebase annotation of ClueWeb corpora ( <ref type="bibr" target="#b12">Gabrilovich et al., 2013</ref>). For two entities in a sentence that can be linked by one or two predicates, we pair the sen- tences and predicates to form a parallel corpus to train the CNN model.</p><p>Constraints &amp; Aggregations When a con- straint node is present in the graph, we use some simple features to check whether there are words in the question that can be associated with the con- straint entity or property. Examples of such fea- tures include whether a mention in the question can be linked to this entity, and the percentage of the words in the name of the constraint entity ap- pear in the question. Similarly, we check the ex- istence of some keywords in a pre-compiled list, such as "first", "current" or "latest" as features for aggregation nodes such as arg min. The complete list of these simple word matching features can also be found in Appendix B.</p><p>Overall The number of the answer entities re- trieved when issuing the query to the knowledge base and the number of nodes in the query graph are both included as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Family Guy cast actor</head><p>Meg Griffin argmin x y q = Who first voiced Meg on Family Guy? (5) indicates 50% of the words in "Meg Griffin" appear in the question q. (6) is 1 when the mention "Meg" in q is correctly linked to MegGriffin by the entity linking component. <ref type="formula">(8)</ref> is the number of nodes in s. The knowledge base returns only 1 entity when issuing this query, so (9) is 1.</p><p>To illustrate our feature design, <ref type="figure" target="#fig_8">Fig. 8</ref> presents the active features of an example query graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Learning</head><p>In principle, once the features are extracted, the model can be trained using any standard off-the- shelf learning algorithm. Instead of treating it as a binary classification problem, where only the cor- rect query graphs are labeled as positive, we view it as a ranking problem. Suppose we have several candidate query graphs for each question <ref type="bibr">4</ref> . Let g a and g b be the query graphs described in states s a and s b for the same question q, and the entity sets A a and A b be those retrieved by executing g a and g b , respectively. Suppose that A is the labeled an- swers to q. We first compute the precision, recall and F 1 score of A a and A b , compared with the gold answer set A. We then rank s a and s b by their F 1 scores 5 . The intuition behind is that even if a query is not completely correct, it is still preferred than some other totally incorrect queries. In this work, we use a one-layer neural network model based on lambda-rank (Burges, 2010) for training the ranker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first introduce the dataset and evaluation met- ric, followed by the main experimental results and some analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data &amp; evaluation metric</head><p>We use the WEBQUESTIONS dataset <ref type="bibr" target="#b3">(Berant et al., 2013)</ref>, which consists of 5,810 ques- tion/answer pairs. These questions were collected using Google Suggest API and the answers were obtained from Freebase with the help of Amazon MTurk. The questions are split into training and testing sets, which contain 3,778 questions (65%) and 2,032 questions (35%), respectively. This dataset has several unique properties that make it appealing and was used in several recent papers on semantic parsing and question answering. For instance, although the questions are not directly sampled from search query logs, the selection pro- cess was still biased to commonly asked questions on a search engine. The distribution of this ques- tion set is thus closer to the "real" information need of search users than that of a small number of human editors. The system performance is ba- sically measured by the ratio of questions that are answered correctly. Because there can be more than one answer to a question, precision, recall and F 1 are computed based on the system output for each individual question. The average F 1 score is reported as the main evaluation metric <ref type="bibr">6</ref> .</p><p>Because this dataset contains only question and answer pairs, we use essentially the same search procedure to simulate the semantic parses for training the CNN models and the overall reward function. Candidate topic entities are first gener- ated using the same entity linking system for each question in the training data. Paths on the Free- base knowledge graph that connect a candidate entity to at least one answer entity are identified as the core inferential chains <ref type="bibr">7</ref> . If an inferential- chain query returns more entities than the correct answers, we explore adding constraint and aggre- gation nodes, until the entities retrieved by the query graph are identical to the labeled answers, or the F 1 score cannot be increased further. Negative examples are sampled from of the incorrect can- didate graphs generated during the search process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Prec. <ref type="bibr">Rec. F1 (Berant et al., 2013)</ref> 48.0 41.3 35.7 ( <ref type="bibr" target="#b7">Bordes et al., 2014b)</ref> - - 29.7 ( <ref type="bibr" target="#b24">Yao and Van Durme, 2014</ref>) - - 33.0 ( <ref type="bibr" target="#b2">Berant and Liang, 2014)</ref> 40.5 46.6 39.9 ( <ref type="bibr" target="#b1">Bao et al., 2014</ref>) - - 37.5 ( <ref type="bibr" target="#b6">Bordes et al., 2014a</ref>) - - 39.2 ( <ref type="bibr" target="#b23">Yang et al., 2014</ref>) - - 41.3 ( <ref type="bibr" target="#b21">Wang et al., 2014</ref>) - - 45.3 Our approach -STAGG 52.8 60.7 52.5 <ref type="table">Table 1</ref>: The results of our approach compared to existing work. The numbers of other systems are either from the original papers or derived from the evaluation script, when the output is available.</p><p>In the end, we produce 17,277 query graphs with none-zero F 1 scores from the training set questions and about 1.7M completely incorrect ones. For training the CNN models to identify the core inferential chain (Sec. 3.2.1), we only use 4,058 chain-only query graphs that achieve F 1 = 0.5 to form the parallel question and pred- icate sequence pairs. The hyper-parameters in CNN, such as the learning rate and the numbers of hidden nodes at the convolutional and semantic layers were chosen via cross-validation. We re- served 684 pairs of patterns and inference-chains from the whole training examples as the held-out set, and the rest as the initial training set. The optimal hyper-parameters were determined by the performance of models trained on the initial train- ing set when applied to the held-out data. We then fixed the hyper-parameters and retrained the CNN models using the whole training set. The performance of CNN is insensitive to the hyper- parameters as long as they are in a reasonable range (e.g., 1000 ± 200 nodes in the convolutional layer, 300 ± 100 nodes in the semantic layer, and learning rate 0.05 ∼ 0.005) and the training pro- cess often converges after ∼ 800 epochs.</p><p>When training the reward function, we created up to 4,000 examples for each question that con- tain all the positive query graphs and randomly se- lected negative examples. The model is trained as a ranker, where example query graphs are ranked by their F 1 scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Tab. 1 shows the results of our system, STAGG (Staged query graph generation), compared to ex- isting work <ref type="bibr">8</ref> . As can be seen from the table, our <ref type="bibr">8</ref> We do not include results of ( <ref type="bibr" target="#b18">Reddy et al., 2014)</ref> because they used only a subset of 570 test questions, which are not Method #Entities # Covered Ques. # Labeled Ent. Freebase API 19,485 3,734 (98.8%) 3,069 (81.2%) Ours 9,147 3,770 (99.8%) 3,318 (87.8%) <ref type="table">Table 2</ref>: Statistics of entity linking results on train- ing set questions. Both methods cover roughly the same number of questions, but Freebase API sug- gests twice the number of entities output by our entity linking system and covers fewer topic enti- ties labeled in the original data.</p><p>system outperforms the previous state-of-the-art method by a large margin -7.2% absolute gain. Given the staged design of our approach, it is thus interesting to examine the contributions of each component. Because topic entity linking is the very first stage, the quality of the entities found in the questions, both in precision and recall, af- fects the final results significantly. To get some insight about how our topic entity linking com- ponent performs, we also experimented with ap- plying Freebase Search API to suggest entities for possible mentions in a question. As can be ob- served in Tab. 2, to cover most of the training questions, we only need half of the number of suggestions when using our entity linking compo- nent, compared to Freebase API. Moreover, they also cover more entities that were selected as the topic entities in the original dataset. Starting from those 9,147 entities output by our component, an- swers of 3,453 questions (91.4%) can be found in their neighboring nodes. When replacing our en- tity linking component with the results from Free- base API, we also observed a significant perfor- mance degradation. The overall system perfor- mance drops from 52.5% to 48.4% in F 1 (Prec = 49.8%, Rec = 55.7%), which is 4.1 points lower.</p><p>Next we test the system performance when the query graph has just the core inferential chain. Tab. 3 summarizes the results. When only the PatChain CNN model is used, the performance is already very strong, outperforming all existing work. Adding the other CNN models boosts the performance further, reaching 51.8% and is only slightly lower than the full system performance. This may be due to two reasons. First, the ques- tions from search engine users are often short and a large portion of them simply ask about properties of an entity. Examining the query graphs gener- ated for training set questions, we found that 1,888 directly comparable to results from other work. On these 570 questions, our system achieves 67.0% in F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Prec. Rec. F1 PatChain 48.8 59.3 49.6 +QuesEP 50.7 60.6 50.9 +ClueWeb 51.3 62.6 51.8 <ref type="table">Table 3</ref>: The system results when only the inferential-chain query graphs are generated. We started with the PatChain CNN model and then added QuesEP and ClueWeb sequentially. See Sec. 3.4 for the description of these models.</p><p>(50.0%) can be answered exactly (i.e., F 1 = 1) us- ing a chain-only query graph. Second, even if the correct parse requires more constraints, the less constrained graph still gets a partial score, as its results cover the correct answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Error Analysis</head><p>Although our approach substantially outperforms existing methods, the room for improvement seems big. After all, the accuracy for the intended application, question answering, is still low and only slightly above 50%. We randomly sampled 100 questions that our system did not generate the completely correct query graphs, and catego- rized the errors. About one third of errors are in fact due to label issues and are not real mistakes. This includes label error (2%), incomplete labels (17%, e.g., only one song is labeled as the an- swer to "What songs did Bob Dylan write?") and acceptable answers (15%, e.g., "Time in China" vs. "UTC+8"). 8% of the errors are due to incor- rect entity linking; however, sometimes the men- tion is inherently ambiguous (e.g., AFL in "Who founded the AFL?" could mean either "American Football League" or "American Federation of La- bor"). 35% of the errors are because of the incor- rect inferential chains; 23% are due to incorrect or missing constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work and Discussion</head><p>Several semantic parsing methods use a domain- independent meaning representation derived from the combinatory categorial grammar (CCG) parses (e.g., <ref type="bibr" target="#b10">(Cai and Yates, 2013;</ref><ref type="bibr" target="#b15">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b18">Reddy et al., 2014)</ref>). In contrast, our query graph design matches closely the graph knowl- edge base. Although not fully demonstrated in this paper, the query graph can in fact be fairly ex- pressive. For instance, negations can be handled by adding tags to the constraint nodes indicating that certain conditions cannot be satisfied. Our graph generation method is inspired by <ref type="bibr" target="#b24">(Yao and Van Durme, 2014;</ref><ref type="bibr" target="#b1">Bao et al., 2014</ref>). Unlike tra- ditional semantic parsing approaches, it uses the knowledge base to help prune the search space when forming the parse. Similar ideas have also been explored in <ref type="bibr" target="#b17">(Poon, 2013)</ref>. Empirically, our results suggest that it is cru- cial to identify the core inferential chain, which matches the relationship between the topic en- tity in the question and the answer. Our CNN models can be analogous to the embedding ap- proaches ( <ref type="bibr" target="#b6">Bordes et al., 2014a;</ref><ref type="bibr" target="#b23">Yang et al., 2014</ref>), but are more sophisticated. By allowing param- eter sharing among different question-pattern and KB predicate pairs, the matching score of a rare or even unseen pair in the training data can still be predicted precisely. This is due to the fact that the prediction is based on the shared model parame- ters (i.e., projection matrices) that are estimated using all training pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a semantic parsing frame- work for question answering using a knowledge base. We define a query graph as the meaning rep- resentation that can be directly mapped to a logical form. Semantic parsing is reduced to query graph generation, formulated as a staged search prob- lem. With the help of an advanced entity linking system and a deep convolutional neural network model that matches questions and predicate se- quences, our system outperforms previous meth- ods substantially on the WEBQUESTIONS dataset.</p><p>In the future, we would like to extend our query graph to represent more complicated questions, and explore more features and models for match- ing constraints and aggregation functions. Apply- ing other structured-output prediction methods to graph generation will also be investigated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Freebase subgraph of Family Guy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Query graph that represents the question "Who first voiced Meg on Family Guy?"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The legitimate actions to grow a query graph. See text for detail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Two possible topic entity linking actions applied to an empty graph, for question "Who first voiced [Meg] on [Family Guy]?"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Family</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>300 Figure 6 :</head><label>3006</label><figDesc>Figure 6: The architecture of the convolutional neural networks (CNN) used in this work. The CNN model maps a variable-length word sequence (e.g., a pattern or predicate sequence) to a low-dimensional vector in a latent semantic space. See text for the description of each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Extending an inferential chain with constraints and aggregation functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Active features of a query graph s. (1) is the entity linking score of the topic entity. (2)(4) are different model scores of the core chain. (5) indicates 50% of the words in "Meg Griffin" appear in the question q. (6) is 1 when the mention "Meg" in q is correctly linked to MegGriffin by the entity linking component. (8) is the number of nodes in s. The knowledge base returns only 1 entity when issuing this query, so (9) is 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Convolutional layer: ht Max pooling layer: v Semantic layer: y &lt;s&gt; w1 w2 wT &lt;/s&gt; Word sequence: xt Word hashing matrix: Wf Convolution matrix: Wc Max pooling operation</head><label></label><figDesc></figDesc><table>15K 

15K 
15K 
15K 
15K 

1000 
1000 
1000 

max 
max 

... 

... 

... 

max 

300 

... 

... 

Word hashing layer: ft 

Semantic projection matrix: Ws 

... 
... 

</table></figure>

			<note place="foot" n="1"> In the rest of the paper, we use the term entity for both real-world and CVT entities, as well as properties like date or height. The distinction is not essential to our approach.</note>

			<note place="foot" n="3"> Decomposing relations in the utterance can be done using decoding methods (e.g., (Bao et al., 2014)). However, similar to ontology mismatch, the relation in text may not have a corresponding single predicate, such as grandparent needs to be mapped to parent-parent in Freebase.</note>

			<note place="foot" n="4"> We will discuss how to create these candidate query graphs from question/answer pairs in Sec. 4.1. 5 We use F1 partially because it is the evaluation metric used in the experiments.</note>

			<note place="foot" n="6"> We used the official evaluation script from http:// www-nlp.stanford.edu/software/sempre/. 7 We restrict the path length to 2. In principle, parses of shorter chains can be used to train the initial reward function, for exploring longer paths using the same search procedure.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their thoughtful comments, Ming Zhou, Nan Duan and Xuchen Yao for sharing their experience on the question answering problem studied in this work, and Chris Meek for his valuable suggestions. We are also grateful to Siva Reddy and Jonathan Be-rant for providing us additional data.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>See supplementary notes.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge-based question answering as machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="967" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1415" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="615" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open question answering with weakly supervised embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;Siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roopak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From RankNet to LambdaRank to LambdaMart: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="23" to="581" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Largescale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bulgaria</forename><surname>Sofia</surname></persName>
		</author>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">FACC1: Freebase annotation of ClueWeb corpora, version 1. Technical report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling interestingness with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for Web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1545" to="1556" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lambda dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Grounded unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="933" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing without question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="377" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning semantic representations using convolutional neural networks for web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Companion Publication of the 23rd International Conference on World Wide Web Companion</title>
		<meeting>the Companion Publication of the 23rd International Conference on World Wide Web Companion</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="373" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An overview of Microsoft Deep QA system on Stanford WebQuestions benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengquan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<idno>MSR-TR-2014-121</idno>
		<imprint>
			<date type="published" when="2014-09" />
			<pubPlace>Microsoft</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">S-MART: Novel tree-based structured learning algorithms applied to tweet entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint relational embeddings for knowledge-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Chul</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haechang</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="645" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with Freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="643" to="648" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
