<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Novel Content Enriching Model for Microblog Using News Corpus</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Yu</surname></persName>
							<email>yuhongliang324@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Machine Perception (Ministry of Education)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Novel Content Enriching Model for Microblog Using News Corpus</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="218" to="223"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a novel model for enriching the content of microblogs by exploiting external knowledge, thus improving the data sparseness problem in short text classification. We assume that mi-croblogs share the same topics with external knowledge. We first build an optimization model to infer the topics of mi-croblogs by employing the topic-word distribution of the external knowledge. Then the content of microblogs is further enriched by relevant words from external knowledge. Experiments on microblog classification show that our approach is effective and outperforms traditional text classification methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During the past decade, the short text represen- tation has been intensively studied. Previous re- searches ( <ref type="bibr" target="#b9">Phan et al., 2008;</ref> show that while traditional methods are not so powerful due to the data sparseness problem, some semantic analysis based approaches are proposed and proved effective, and various topic models are among the most frequently used techniques in this area. Meanwhile, external knowledge has been found helpful ( <ref type="bibr" target="#b7">Hu et al., 2009</ref>) in tackling the da- ta scarcity problem by enriching short texts with informative context. Well-organized knowledge bases such as Wikipedia and WordNet are com- mon tools used in relevant methods.</p><p>Nowadays, most of the work on short text fo- cuses on microblog. As a new form of short tex- t, microblog has some unique features like infor- mal spelling and emerging words, and many mi- croblogs are strongly related to up-to-date topics as well. Every day, a great quantity of microblogs * Corresponding author more than we can read is pushed to us, and find- ing what we are interested in becomes rather dif- ficult, so the ability of choosing what kind of mi- croblogs to read is urgently demanded by common user. Such ability can be implemented by effective short text classification.</p><p>Treating microblogs as standard texts and di- rectly classifying them cannot achieve the goal of effective classification because of sparseness prob- lem. On the other hand, news on the Internet is of information abundance and many microblogs are news-related. They share up-to-date topics and sometimes quote each other. Thus, external knowledge, such as news, provides rich supple- mentary information for analysing and mining mi- croblogs.</p><p>Motivated by the idea of using topic model and external knowledge mentioned above, we present an LDA-based enriching method using the news corpus, and apply it to the task of microblog clas- sification. The basic assumption in our model is that news articles and microblogs tend to share the same topics. We first infer the topic distribution of each microblog based on the topic-word distri- bution of news corpus obtained by the LDA esti- mation. With the above two distributions, we then add a number of words from news as additional information to microblogs by evaluating the relat- edness of between each word and microblog, since words not appearing in the microblog may still be highly relevant.</p><p>To sum up, our contributions are:</p><p>(1) We formulate the topic inference problem for short texts as a convex optimization problem.</p><p>(2) We enrich the content of microblogs by infer- ring the association between microblogs and external words in a probabilistic perspective.</p><p>(3) We evaluate our method on the real dataset- s and experiment results outperform the base- line methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>218</head><p>Based on the idea of exploiting external knowl- edge, many methods are proposed to improve the representation of short texts for classification and clustering. Among them, some directly utilize the structure information of organized knowledge base or search engine. <ref type="bibr" target="#b0">Banerjee et al. (2007)</ref> use the title and the description of news article as two separate query strings to select related concept- s as additional feature. <ref type="bibr" target="#b7">Hu et al. (2009)</ref> present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet. However, to better leverage external resource, some other methods introduce topic models. <ref type="bibr" target="#b9">Phan et al. (2008)</ref> present a framework including an approach for short text topic inference and adds abstract words as extra feature.  modify classic topic models and propos- es a matrix-factorization based model for sentence similarity calculation tasks.</p><p>Those methods without topic model usually re- ly greatly on the performance of search system or the completeness of knowledge base, and lack in- depth analysis for external resources. Compared with our method, the topic model based method- s mentioned above remain in finding latent space representation of short text and ignore that rele- vant words from external knowledge are informa- tive as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Model</head><p>We formulate the problem as follows.</p><p>Let</p><formula xml:id="formula_0">EK = {d e 1 , . . . , d e M e } denote external knowl- edge consisting of M e documents. V e = {w e 1 , . . . , w e N e } represents its vocabulary. Let M B = {d m 1 , . . . , d m M m } denote microblog set and its vocabulary is V m = {w m 1 , . . . , w m N m }.</formula><p>Our task is to enrich each microblog with additional information so as to improve microblog's repre- sentation.</p><p>The model we proposed mainly consists of three steps:</p><p>(a) Topic inference for external knowledge by running LDA estimation.</p><p>(b) Topic inference for microblogs by employing the word distributions of topics obtained from step (a).</p><p>(c) Select relevant words from external knowl- edge to enrich the content of microblogs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Topic Inference for External Knowledge</head><p>We do topic analysis for EK using LDA esti- mation ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) in this section and we choose LDA as the topic analysis model because of its broadly proved effectivity and ease of under- standing.</p><p>In LDA, each document has a distribution over all topics P (z k |d j ), and each topic has a distri- bution over all words P (w i |z k ), where z k , d j and w i represent the topic, document and word respec- tively. The optimization problem is formulated as maximizing the log likelihood on the corpus:</p><formula xml:id="formula_1">max i j X ij log k P (z k |d j )P (w i |z k ) (1)</formula><p>In this formulation, X ij represents the term fre- quency of word w i in document d j . P (z k |d j ) and P (w i |z k ) are parameters to be inferred, cor- responding to the topic distribution of each doc- ument and the word distribution of each topic re- spectively. Estimating parameters for LDA by di- rectly and exactly maximizing the likelihood of the corpus in (1) is intractable, so we use Gibbs Sampling for estimation.</p><p>After performing LDA model (K topics) esti- mation on EK, we obtain the topic distribution- s of document d e j (j = 1, . . . , M e ), denoted as P (z e k |d e j ) (k = 1, . . . , K), and the word distri- bution of topic z e k (k = 1, . . . , K), denoted as</p><formula xml:id="formula_2">P (w e i |z e k ) (i = 1, . . . , N e ).</formula><p>Step (b) greatly re- lies on the word distributions of topics we have obtained here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Topic Inference for Microblog</head><p>In this section, we infer the topic distribution of each microblog. Because of the assumption that microblogs share the same topics with external corpus, the "topic distribution" here refers to a dis- tribution over all topics on EK.</p><p>Differing from step (a), the method used for topic inference for microblogs is not directly run- ning LDA estimation on microblog collection but following the topics from external knowledge to ensure topic consistence. We employ the word distributions of topics obtained from step (a), i.e. P (w e i |z e k ), and formulate the optimization prob- lem in a similar form to Formula (1) as follows:</p><formula xml:id="formula_3">max P (z e k |d m j ) i j X ij log k P (z e k |d m j )P (w e i |z e k ),<label>(2)</label></formula><p>where X ij represents the term frequency of word w e i in microblog d m j , and P (z e k |d m j ) denote the dis- tribution of microblog d m j over all topics on EK. Obviously most X ij are zero and we ignore those words that do not appear in V e .</p><p>Compared with the original LDA optimization problem (1), the topic inference problem for mi- croblog (2) follows the idea of document gener- ation process, but replaces topics to be estimated with known topics from other corpus. As a result, parameters to be inferred are only the topic distri- bution of every microblog.</p><p>It is noteworthy that since the word distribution of every topic P (w e i |z e k ) is known, Formula (2) can be further solved by separating it into M m sub- problems:</p><formula xml:id="formula_4">max P (z e k |d m j ) i X ij log k P (z e k |d m j )P (w e i |z e k ) f or j = 1, . . . , M m<label>(3)</label></formula><p>These M m subproblems correspond to the M m microblogs and can be easily proved convexity. After solving them, we obtain the topic distribu- tions of microblog d m j (j = 1, . . . , M m ), denoted as P (z e k |d m j ) (k = 1, . . . , K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Select Relevant Words for Microblog</head><p>To enrich the content of every microblog, we s- elect relevant words from external knowledge in this section. Based on the results of step (a)&amp;(b), we calcu- late the word distributions of microblogs as fol- lows:</p><formula xml:id="formula_5">P (w e i |d m j ) = k P (z e k |d m j )P (w e i |z e k ),<label>(4)</label></formula><p>where P (w e i |d m j ) represents the probability that word w e i will appear in microblog d m j . In other words, though some words may not actually ap- pears in a microblog, there is still a probability that it is highly relevant to the microblog. Intuitively, this probability indicates the strength of associa- tion between a word and a microblog. The word distribution of every microblog is based on topic analysis and its accuracy relies heavily on the ac- curacy of topic inference in step (b). In fact, the more words a microblog includes, the more accu- rate its topic inference will be, and this can be re- garded as an explanation of the low efficiency of data sparseness problem.</p><p>For microblog d m j , we sort all words by P (w e i |d m j ) in descending order. Having known the top L relevant words according to the result of sorting, we redefine the "term frequency" of every word after adding these L words to microblog d m j as additional content. Supposing these L words are w e j1 , w e j2 , . . . , w e jL , the revised term frequency of word w ∈ {w e j1 , . . . , w e jL } is defined as fol- lows:</p><formula xml:id="formula_6">RT F (w, d m j ) = P (w|d m j ) L p=1 P (w e jp |d m j ) * L,<label>(5)</label></formula><p>where RT F (·) is the revised term frequency.</p><p>As the Equation <ref type="formula" target="#formula_6">(5)</ref> shows, the revised term fre- quency of every word is proportional to probabili- ty P (w i |d m j ) rather than a constant. So far, we can add these L words and their re- vised term frequency as additional information to microblog d m j . The revised term frequency plays the same role as TF in common text representation vector, so we calculate the TFIDF of the added words as:</p><formula xml:id="formula_7">T F IDF (w, d m j ) = RT F (w, d m j )·IDF (w) (6)</formula><p>Note that IDF (w) is changed as arrival of new words for each microblog. The TFIDF vector of a microblog with additional words is called en- hanced vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>To evaluate our method, we build our own dataset- s. We crawl 95028 Chinese news reports from Sina News website, segment them, and remove stop words and rare words. After preprocessing, these news documents are used as external knowl- edge. As for microblog, we crawl a number of microblogs from Sina Weibo, and ask unbiased assessors to manually classify them into 9 cate- gories following the column setting of Sina News.</p><p>After the manual classification, we remove short microblogs (less than 10 words), usernames, links and some special characters, then we segmen- t them and remove rare words as well. Finally, we get 1671 classified microblogs as our microblog dataset. The size of each category is shown in Ta- ble 1. There are some important details of our imple- mentation. In step (a) of Section 3.1 we estimate LDA model using GibbsLDA++, a C/C++ imple- mentation of LDA using Gibbs Sampling. In step (b) of Section3.2, OPTI toolbox on Matlab is used to help solve the convex problems. In the clas- sification tasks shown below, we use LibSVM as classifier and perform ten-fold cross validation to evaluate the classification accuracy. In this section, we report the average preci- sion of each method as shown in <ref type="table" target="#tab_1">Table 2</ref>. The enhanced vector is the representation generated by our method. Two baselines are TFIDF vec- tor <ref type="bibr" target="#b8">(Jones, 1972)</ref> and boolean vector (word oc- currence) of the original microblog. In the table, our method increases the classification accuracy GibbsLDA++: http://gibbslda.sourceforge.net OPTI Toolbox: http://www.i2c2.aut.ac.nz/Wiki/OPTI/ SVM.NET: http://www.matthewajohnson.org/software /svm.html from 75.52% to 84.53% when considering addi- tional information, which means our method in- deed improves the representation of microblogs. The experiment corresponding to <ref type="figure" target="#fig_0">Figure 1</ref> is to discover how the classification accuracy changes when we fix the number of topics (K = 100) and change the number of added words (L) in our method. Result shows that more added words do not mean higher accuracy. By studying some cas- es, we find out that if we add too many words, the proportion of "noisy words" will increase. We reach the best result when number of added words is 300. The experiment corresponding to <ref type="figure" target="#fig_1">Figure 2</ref> is to discover how the classification accuracy changes when we fix the number of added words (L = Microblog (Translated) Top Relevant Words (Translated) Kim Jong Un held an emergency meeting this morn- ing, and commanded the missile units to prepare for attacking U.S. military bases at any time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category #Microblog</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation Average Accuracy</head><note type="other">TFIDF vector 0.7552 Boolean vector 0.7203 Enhanced vector 0.8453</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Effect of Added Words</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Effect of Topic Number</head><p>South Korea, America, North Korea, work, safety, claim, military, exercise, united, report Shenzhou Nine will carry three astronauts, including the first Chinese female astronaut, and launch in a proper time during the middle of June. day, satellite, launch, research, technology, system, mission, aerospace, success, Chang'e Two <ref type="table">Table 3</ref>: Case study (Translated from Chinese) 300) and change the number of topics (K) in our method. As we can see, the accuracy does not grow monotonously as the number of topic- s increases. Blindly enlarging the topic number will not improve the accuracy. The best result is reached when topic number is 100, and similar ex- periments adding different number of words show the same condition of reaching the best result. The experiment corrsponding to <ref type="figure" target="#fig_2">Figure 3</ref> is to discover whether our redefining "term frequency" as revised term frequency in step (c) of Section 3.3 will affect the classification accuracy and how. The results should be analysed in two aspects. On one hand, without redefinition, the accuracy re- mains in a stable high level and tends to decrease as we add more words. One reason for the de- creasing is that "noisy words" have a increasing negative impact on the accuracy as the propor- tion of "noisy words" grows with the number of added words. On the other hand, the best result is reached when we use the revise term frequen- cy. This suggests that our redefinition for term fre- quency shows better improvement for microblog representation under certain conditions, but is not optimal under all situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Effect of Revised Term Frequency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>In <ref type="table">Table 3</ref>, we select several cases consisting of microblogs and their top relevant words .</p><p>In the first case, we successfully find the country name according to its leader's name and limited information in the sentence. Other related coun- tries and events are also selected by our model as they often appear together in news. In the other case, relevant words are among the most frequent- ly used words in news and have close semantic re- lations with the microblogs in certain aspects.</p><p>As we can see, based on topic analysis, our model shows strong ability of mining relevan- t words. Other cases show that the model can be further improved by removing the noisy and mean- ingless ones among added words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We propose an effective content enriching method for microblog, to enhance classification accuracy. News corpus is exploited as external knowledge. As for techniques, our method uses LDA as its topic analysis model and formulates topic infer- ence for new data as convex optimization prob- lems. Compared with traditional representation, enriched microblog shows great improvement in classification tasks.</p><p>As we do not control the quality of added words, our future work starts from building a filter to se- lect better additional information. And to make the most of external knowledge, better ways to build topic space should be considered.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Classification accuracy changes according to topics and added words</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Classification accuracy changing according to the number of topics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Classification accuracy changing according to the redefinition of term frequency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Classification accuracy with different rep-
resentations 

</table></figure>

			<note place="foot">Sina News: http://news.sina.com.cn/ Sina Weibo: http://www.weibo.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by National Natural Sci-ence Foundation of China (Grant No. 61170091).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clustering short texts using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007-07" />
			<biblScope unit="page" from="787" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Measuring semantic similarity between words using web search engines. www</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ishizuka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="757" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computing Semantic Relatedness Using Wikipediabased Explicit Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1606" to="1611" />
			<date type="published" when="2007-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling sentences in the latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="864" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning the latent semantics of a concept from its definition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="140" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting internal and external semantics for the clustering of short texts using world knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
		<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009-11" />
			<biblScope unit="page" from="919" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of documentation</title>
		<imprint>
			<date type="published" when="1972" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Classify Short and Sparse Text &amp; Web with Hidden Topics from Large-scale Data Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Horiguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on World Wide Web</title>
		<meeting>the 17th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008-04" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A webbased kernel function for measuring the similarity of short text snippets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Heilman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on World Wide Web</title>
		<meeting>the 15th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006-05" />
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Harnessing web page directories for large-scale classification of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web companion</title>
		<meeting>the 22nd international conference on World Wide Web companion</meeting>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="225" to="226" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
