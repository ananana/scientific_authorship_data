<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Learning for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
							<email>chengyong3001@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
							<email>weixu@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Learning for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1965" to="1974"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (mono-lingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>End-to-end neural machine translation (NMT), which leverages a single, large neural network to directly transform a source-language sentence into a target-language sentence, has attracted increas- ing attention in recent several years <ref type="bibr" target="#b11">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>). Free of latent structure design and feature engineering that are critical in conventional statistical machine translation (SMT) ( <ref type="bibr" target="#b3">Brown et al., 1993;</ref><ref type="bibr" target="#b14">Koehn et al., 2003;</ref><ref type="bibr" target="#b4">Chiang, 2005</ref>), NMT has proven to excel in model- * Yang Liu is the corresponding author.</p><p>ing long-distance dependencies by enhancing re- current neural networks (RNNs) with the gating <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1993;</ref><ref type="bibr" target="#b5">Cho et al., 2014;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014</ref>) and attention mecha- nisms ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>.</p><p>However, most existing NMT approaches suf- fer from a major drawback: they heavily rely on parallel corpora for training translation mod- els. This is because NMT directly models the probability of a target-language sentence given a source-language sentence and does not have a sep- arate language model like SMT <ref type="bibr" target="#b11">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>. Unfortunately, parallel corpora are usually only available for a handful of research- rich languages and restricted to limited domains such as government documents and news reports. In contrast, SMT is capable of exploiting abundant target-side monolingual corpora to boost fluency of translations. Therefore, the unavailability of large-scale, high-quality, and wide-coverage par- allel corpora hinders the applicability of NMT.</p><p>As a result, several authors have tried to use abundant monolingual corpora to improve NMT. <ref type="bibr" target="#b8">Gulccehre et al. (2015)</ref> propose two methods, which are referred to as shallow fusion and deep fusion, to integrate a language model into NMT. The basic idea is to use the language model to score the candidate words proposed by the transla- tion model at each time step or concatenating the hidden states of the language model and the de- coder. Although their approach leads to signifi- cant improvements, one possible downside is that the network architecture has to be modified to in- tegrate the language model. Alternatively, <ref type="bibr" target="#b20">Sennrich et al. (2015)</ref> propose two approaches to exploiting monolingual corpora that is transparent to network architectures. The first approach pairs monolingual sentences with dummy input. Then, the parameters of encoder Figure 1: Examples of (a) source autoencoder and (b) target autoencoder on monolingual corpora. Our idea is to leverage autoencoders to exploit monolingual corpora for NMT. In a source autoencoder, the source-to-target model P (y|x; − → θ ) serves as an encoder to transform the observed source sentence x into a latent target sentence y (highlighted in grey), from which the target-to-source model P (x |y; ← − θ ) reconstructs a copy of the observed source sentence x from the latent target sentence. As a result, monolingual corpora can be combined with parallel corpora to train bidirectional NMT models in a semi-supervised setting. and attention model are fixed when training on these pseudo parallel sentence pairs. In the sec- ond approach, they first train a nerual translation model on the parallel corpus and then use the learned model to translate a monolingual corpus. The monolingual corpus and its translations con- stitute an additional pseudo parallel corpus. Simi- lar ideas have also been suggested in conventional SMT ( <ref type="bibr" target="#b24">Ueffing et al., 2007;</ref><ref type="bibr" target="#b2">Bertoldi and Federico, 2009)</ref>. <ref type="bibr" target="#b20">Sennrich et al. (2015)</ref> report that their ap- proach significantly improves translation quality across a variety of language pairs.</p><p>In this paper, we propose semi-supervised learning for neural machine translation. Given la- beled (i.e., parallel corpora) and unlabeled (i.e., monolingual corpora) data, our approach jointly trains source-to-target and target-to-source trans- lation models. The key idea is to append a re- construction term to the training objective, which aims to reconstruct the observed monolingual cor- pora using an autoencoder. In the autoencoder, the source-to-target and target-to-source models serve as the encoder and decoder, respectively. As the inference is intractable, we propose to sample the full search space to improve the efficiency. Specif- ically, our approach has the following advantages:</p><p>1. Transparent to network architectures: our ap- proach does not depend on specific architec- tures and can be easily applied to arbitrary end-to-end NMT systems.</p><p>2. Both the source and target monolingual cor- pora can be used: our approach can bene- fit NMT not only using target monolingual corpora in a conventional way, but also the monolingual corpora of the source language.</p><p>Experiments on Chinese-English NIST datasets show that our approach results in significant im- provements in both directions over state-of-the-art SMT and NMT systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Semi-Supervised Learning for Neural</head><p>Machine Translation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Supervised Learning</head><p>Given a parallel corpus D = {{x (n) , y (n) } N n=1 , the standard training objective in NMT is to max- imize the likelihood of the training data:</p><formula xml:id="formula_0">L(θ) = N n=1 log P (y (n) |x (n) ; θ),<label>(1)</label></formula><p>where P (y|x; θ) is a neural translation model and θ is a set of model parameters. D can be seen as labeled data for the task of predicting a target sentence y given a source sentence x. As P (y|x; θ) is modeled by a single, large neu- ral network, there does not exist a separate target language model P (y; θ) in NMT. Therefore, par- allel corpora have been the only resource for pa- rameter estimation in most existing NMT systems. Unfortunately, even for a handful of resource-rich languages, the available domains are unbalanced and restricted to government documents and news reports. Therefore, the availability of large-scale, high-quality, and wide-coverage parallel corpora becomes a major obstacle for NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Autoencoders on Monolingual Corpora</head><p>It is appealing to explore the more readily avail- able, abundant monolingual corpora to improve NMT. Let us first consider an unsupervised set- ting: how to train NMT models on a monolingual corpus T = {y (t) } T t=1 ? Our idea is to leverage autoencoders ( <ref type="bibr" target="#b25">Vincent et al., 2010;</ref><ref type="bibr" target="#b21">Socher et al., 2011</ref>): (1) encoding an ob- served target sentence into a latent source sentence using a target-to-source translation model and (2) decoding the source sentence to reconstruct the observed target sentence using a source-to-target model. For example, as shown in <ref type="figure">Figure 1</ref>(b), given an observed English sentence "Bush held a talk with Sharon", a target-to-source translation model (i.e., encoder) transforms it into a Chinese translation "bushi yu shalong juxing le huitan" that is unobserved on the training data (highlighted in grey). Then, a source-to-target translation model (i.e., decoder) reconstructs the observed English sentence from the Chinese translation.</p><p>More formally, let P (y|x; − → θ) and P (x|y; ← − θ ) be source-to-target and target-to-source transla- tion models respectively, where − → θ and ← − θ are cor- responding model parameters. An autoencoder aims to reconstruct the observed target sentence via a latent source sentence:</p><formula xml:id="formula_1">P (y |y; − → θ , ← − θ ) = x P (y , x|y; − → θ , ← − θ ) = x P (x|y; ← − θ ) encoder P (y |x; − → θ ) decoder ,<label>(2)</label></formula><p>where y is an observed target sentence, y is a copy of y to be reconstructed, and x is a latent source sentence. We refer to Eq. (2) as a target autoencoder. 1 Likewise, given a monolingual corpus of source language S = {x (s) } S s=1 , it is natural to introduce a source autoencoder that aims at reconstructing <ref type="bibr">1</ref> Our definition of auotoencoders is inspired by <ref type="bibr" target="#b0">Ammar et al. (2014)</ref>. Note that our autoencoders inherit the same spirit from conventional autoencoders <ref type="bibr" target="#b25">(Vincent et al., 2010;</ref><ref type="bibr" target="#b21">Socher et al., 2011</ref>) except that the hidden layer is denoted by a latent sentence instead of real-valued vectors. the observed source sentence via a latent target sentence:</p><formula xml:id="formula_2">P (x |x; − → θ , ← − θ ) = y P (x , y|x; ← − θ ) = y P (y|x; − → θ ) encoder P (x |y; ← − θ ) decoder .<label>(3)</label></formula><p>Please see <ref type="figure">Figure 1</ref>(a) for illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semi-Supervised Learning</head><p>As the autoencoders involve both source-to-target and target-to-source models, it is natural to com- bine parallel corpora and monolingual corpora to learn birectional NMT translation models in a semi-supervised setting. Formally, given a parallel corpus</p><formula xml:id="formula_3">D = {{x (n) , y (n) } N n=1 , a monolingual corpus of target language T = {y (t) } T t=1</formula><p>, and a monolingual cor- pus of source language S = {x (s) } S s=1 , we intro- duce our new semi-supervised training objective as follows:</p><formula xml:id="formula_4">J( − → θ , ← − θ ) = N n=1 log P (y (n) |x (n) ; − → θ ) source-to-target likelihood + N n=1 log P (x (n) |y (n) ; ← − θ ) target-to-source likelihood +λ 1 T t=1 log P (y |y (t) ; − → θ , ← − θ ) target autoencoder +λ 2 S s=1 log P (x |x (s) ; − → θ , ← − θ ) source autoencoder , (4)</formula><p>where λ 1 and λ 2 are hyper-parameters for balanc- ing the preference between likelihood and autoen- coders.</p><p>Note that the objective consists of four parts: source-to-target likelihood, target-to-source likeli- hood, target autoencoder, and source autoencoder. In this way, our approach is capable of exploiting abundant monolingual corpora of both source and target languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The optimal model parameters are given by</head><formula xml:id="formula_5">− → θ * = argmax N n=1 log P (y (n) |x (n) ; − → θ ) + λ 1 T t=1 log P (y |y (t) ; − → θ , ← − θ ) + λ 2 S s=1 log P (x |x (s) ; − → θ , ← − θ )<label>(5)</label></formula><formula xml:id="formula_6">← − θ * = argmax N n=1 log P (x (n) |y (n) ; ← − θ ) + λ 1 T t=1 log P (y |y (t) ; − → θ , ← − θ ) + λ 2 S s=1 log P (x |x (s) ; − → θ , ← − θ )<label>(6)</label></formula><p>It is clear that the source-to-target and target-to- source models are connected via the autoencoder and can hopefully benefit each other in joint train- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>We use mini-batch stochastic gradient descent to train our joint model. For each iteration, be- sides the mini-batch from the parallel corpus, we also construct two additional mini-batches by ran- domly selecting sentences from the source and tar- get monolingual corpora. Then, gradients are col- lected from these mini-batches to update model parameters.</p><p>The partial derivative of J( − → θ , ← − θ ) with respect to the source-to-target model − → θ is given by</p><formula xml:id="formula_7">∂J( − → θ , ← − θ ) ∂ − → θ = N n=1 ∂ log P (y (n) |x (n) ; − → θ ) ∂ − → θ +λ 1 T t=1 ∂ log P (y |y (t) ; − → θ , ← − θ ) ∂ − → θ +λ 2 S s=1 ∂ log P (x |x (s) ; − → θ , ← − θ ) ∂ − → θ .<label>(7)</label></formula><p>The partial derivative with respect to ← − θ can be cal- culated similarly.</p><p>Unfortunately, the second and third terms in Eq. (7) are intractable to calculate due to the exponen- tial search space. For example, the derivative in</p><formula xml:id="formula_8">Chinese English # Sent. 2.56M Parallel # Word 67.54M 74.82M Vocab. 0.21M 0.16M # Sent.</formula><p>18.75M 22.32M Monolingual # Word 451.94M 399.83M</p><p>Vocab. 0.97M 1.34M <ref type="table" target="#tab_0">Table 1</ref>: Characteristics of parallel and monolin- gual corpora.</p><p>the third term in Eq. <ref type="formula" target="#formula_7">(7)</ref> is given by x∈X (y) P (x|y;</p><formula xml:id="formula_9">← − θ )P (y |x; − → θ ) ∂ log P (y |x; − → θ ) ∂ − → θ x∈X (y) P (x|y; ← − θ )P (y |x; − → θ ) .<label>(8)</label></formula><p>It is prohibitively expensive to compute the sums due to the exponential search space of X (y).</p><p>Alternatively, we propose to use a subset of the full space˜Xspace˜ space˜X (y) ⊂ X (y) to approximate Eq. <ref type="formula" target="#formula_9">(8)</ref>:</p><formula xml:id="formula_10">x∈˜Xx∈˜ x∈˜X (y) P (x|y; ← − θ )P (y |x; − → θ ) ∂ log P (y |x; − → θ ) ∂ − → θ x∈˜Xx∈˜ x∈˜X (y) P (x|y; ← − θ )P (y |x; − → θ ) .<label>(9)</label></formula><p>In practice, we use the top-k list of candidate translations of y as˜Xas˜ as˜X (y). As | ˜ X (y)| X |(y)|, it is possible to calculate Eq. (9) efficiently by enumerating all candidates iñ X (y). In practice, we find this approximation results in significant improvements and k = 10 seems to suffice to keep the balance between efficiency and transla- tion quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We evaluated our approach on the Chinese- English dataset.</p><p>As shown in   For Chinese-to-English translation, we use the NIST 2006 Chinese-English dataset as the vali- dation set for hyper-parameter optimization and model selection. <ref type="bibr">The NIST 2002</ref><ref type="bibr" target="#b17">, 2003</ref><ref type="bibr">, 2004</ref>, and 2005 datasets serve as test sets. Each Chi- nese sentence has four reference translations. For English-to-Chinese translation, we use the NIST datasets in a reverse direction: treating the first English sentence in the four reference transla- tions as a source sentence and the original input Chinese sentence as the single reference trans- lation. The evaluation metric is case-insensitive BLEU ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>) as calculated by the multi-bleu.perl script.</p><p>We compared our approach with two state-of- the-art SMT and NMT systems: For MOSES, we use the default setting to train the phrase-based translation on the parallel corpus and optimize the parameters of log-linear models using the minimum error rate training algorithm <ref type="bibr" target="#b17">(Och, 2003)</ref>. We use the SRILM toolkit <ref type="bibr" target="#b22">(Stolcke, 2002</ref>) to train 4-gram language models.</p><p>For RNNSEARCH, we use the parallel corpus to train the attention-based neural translation models. We set the vocabulary size of word embeddings to 30K for both Chinese and English. We follow <ref type="bibr" target="#b16">Luong et al. (2015)</ref> to address rare words.</p><p>On top of RNNSEARCH, our approach is capa- ble of training bidirectional attention-based neural translation models on the concatenation of parallel and monolingual corpora. The sample size k is set to 10. We set the hyper-parameter λ 1 = 0.1 and λ 2 = 0 when we add the target monolingual cor- pus, and λ 1 = 0 and λ 2 = 0.1 for source monolin- gual corpus incorporation. The threshold of gra- dient clipping is set to 0.05. The parameters of our model are initialized by the model trained on parallel corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effect of Sample Size k</head><p>As the inference of our approach is intractable, we propose to approximate the full search space with the top-k list of candidate translations to improve efficiency (see Eq. <ref type="formula" target="#formula_10">(9)</ref>). <ref type="figure" target="#fig_1">Figure 2</ref> shows the BLEU scores of various set- tings of k over time. Only the English mono- lingual corpus is appended to the training data. We observe that increasing the size of the approx- imate search space generally leads to improved BLEU scores. There are significant gaps between k = 1 and k = 5. However, keeping increas- ing k does not result in significant improvements and decreases the training efficiency. We find that k = 10 achieves a balance between training effi- ciency and translation quality. As shown in <ref type="figure" target="#fig_2">Fig- ure 3</ref>, similar findings are also observed on the English-to-Chinese validation set. Therefore, we set k = 10 in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effect of OOV Ratio</head><p>Given a parallel corpus, what kind of monolingual corpus is most beneficial for improving transla- tion quality? To answer this question, we investi- gate the effect of OOV ratio on translation quality, which is defined as</p><formula xml:id="formula_11">ratio = y∈y y / ∈ V Dt |y| ,<label>(10)</label></formula><p>where y is a target-language sentence in the mono- lingual corpus T , y is a target-language word in y, V Dt is the vocabulary of the target side of the par- allel corpus D. Intuitively, the OOV ratio indicates how a sen- tence in the monolingual resembles the parallel corpus. If the ratio is 0, all words in the mono- lingual sentence also occur in the parallel corpus. <ref type="figure">Figure 4</ref> shows the effect of OOV ratio on the Chinese-to-English validation set. Only En- glish monolingual corpus is appended to the par- allel corpus during training. We constructed four monolingual corpora of the same size in terms of sentence pairs. "0% OOV" means the OOV ra- tio is 0% for all sentences in the monolingual cor- pus. "10% OOV" suggests that the OOV ratio is no greater 10% for each sentence in the mono- lingual corpus. We find that using a monolingual corpus with a lower OOV ratio generally leads to higher BLEU scores. One possible reason is that low-OOV monolingual corpus is relatively easier to reconstruct than its high-OOV counterpart and results in better estimation of model parameters. <ref type="figure">Figure 5</ref> shows the effect of OOV ratio on the English-to-Chinese validation set. Only English monolingual corpus is appended to the parallel corpus during training. We find that "0% OOV" still achieves the highest BLEU scores. <ref type="table">Table 2</ref> shows the comparison between MOSES and our work. MOSES used the monolingual corpora as shown in <ref type="table" target="#tab_0">Table 1</ref>: 18.75M Chinese sentences and 22.32M English sentences. We find that exploiting monolingual corpora dramat- ically improves translation performance in both Chinese-to-English and English-to-Chinese direc- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with SMT</head><p>Relying only on parallel corpus, RNNSEARCH outperforms MOSES trained also only on par- allel corpus.</p><p>But the capability of making use of abundant monolingual corpora enables MOSES to achieve much higher BLEU scores than RNNSEARCH only using parallel corpus.</p><p>Instead of using all sentences in the monolin- gual corpora, we constructed smaller monolingual corpora with zero OOV ratio: 2.56M Chinese sen- tences with 47.51M words and 2.56M English English sentences with 37.47M words. In other words, the monolingual corpora we used in the experiments are much smaller than those used by MOSES.</p><p>By adding English monolingual corpus, our approach achieves substantial improvements over RNNSEARCH using only parallel corpus (up to +4.7 BLEU points). In addition, significant im- provements are also obtained over MOSES using both parallel and monolingual corpora (up to +3.5 BLEU points).</p><p>An interesting finding is that adding English monolingual corpora helps to improve English-to- Chinese translation over RNNSEARCH using only parallel corpus (up to +3.2 BLEU points), sug- gesting that our approach is capable of improving NMT using source-side monolingual corpora.</p><p>In the English-to-Chinese direction, we ob- tain similar findings. In particular, adding Chi-  <ref type="table">Table 2</ref>: Comparison with MOSES and RNNSEARCH. MOSES is a phrase-based statistical machine translation system ( <ref type="bibr" target="#b15">Koehn et al., 2007)</ref>. RNNSEARCH is an attention-based neural machine translation system ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>). "CE" donates Chinese-English parallel corpus, "C" donates Chinese monolingual corpus, and "E" donates English monolingual corpus. " √ " means the corpus is included in the training data and × means not included. "NIST06" is the validation set and "NIST02-05" are test sets. The BLEU scores are case-insensitive. "*": significantly better than MOSES (p &lt; 0.05); "**": significantly better than MOSES (p &lt; 0.01);"+": significantly better than RNNSEARCH (p &lt; 0.05); "++": significantly better than RNNSEARCH (p &lt; 0.01).   nese monolingual corpus leads to more benefits to English-to-Chinese translation than adding En- glish monolingual corpus. We also tried to use both Chinese and English monolingual corpora through simply setting all the λ to 0.1 but failed to obtain further significant improvements. Therefore, our findings can be summarized as follows:</p><formula xml:id="formula_12">System Training Data Direction NIST06 NIST02 NIST03 NIST04 NIST05 CE C E MOSES √ × × C → E 32</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>1. Adding target monolingual corpus improves over using only parallel corpus for source-to- target translation;</p><p>2. Adding source monolingual corpus also im- proves over using only parallel corpus for source-to-target translation, but the improve- ments are smaller than adding target mono- lingual corpus;</p><p>3. Adding both source and target monolingual corpora does not lead to further significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Comparison with Previous Work</head><p>We re-implemented <ref type="bibr" target="#b20">Sennrich et al. (2015)</ref>'s method on top of RNNSEARCH as follows:</p><p>1. Train the target-to-source neural translation model P (x|y;</p><formula xml:id="formula_13">← − θ ) on the parallel corpus D = {{x (n) , y (n) } N n=1 .</formula><p>2. The trained target-to-source model</p><formula xml:id="formula_14">← − θ * is</formula><p>used to translate a target monolingual corpus</p><formula xml:id="formula_15">T = {y (t) } T t=1 into a source monolingual corpus˜Scorpus˜ corpus˜S = {˜x{˜x (t) } T t=1 .</formula><p>3. The target monolingual corpus is paired with its translations to form a pseudo parallel cor- pus, which is then appended to the original parallel corpus to obtain a larger parallel cor- pus:</p><formula xml:id="formula_16">˜ D = D ∪ ˜ S, T .</formula><p>4. Re-train the the source-to-target neural trans- lation model oñ D to obtain the final model parameters</p><formula xml:id="formula_17">− → θ * .</formula><p>Monolingual hongsen shuo , ruguo you na jia famu gongsi dangan yishenshifa , name tamen jiang zihui qiancheng . Reference hongsen said, if any logging companies dare to defy the law, then they will destroy their own future . Translation hun sen said , if any of those companies dare defy the law , then they will have their own fate . [iteration 0] hun sen said if any tree felling company dared to break the law , then they would kill themselves . [iteration 40K] hun sen said if any logging companies dare to defy the law , they would destroy the future themselves .</p><p>[  <ref type="table">Table 4</ref>: Example translations of sentences in the monolingual corpus during semi-supervised learning. We find our approach is capable of generating better translations of the monolingual corpus over time. <ref type="table">Table 3</ref> shows the comparison results. Both the two approaches use the same parallel and mono- lingual corpora. Our approach achieves signifi- cant improvements over <ref type="bibr" target="#b20">Sennrich et al. (2015)</ref> in both Chinese-to-English and English-to-Chinese directions (up to +1.8 and +1.0 BLEU points). One possible reason is that <ref type="bibr" target="#b20">Sennrich et al. (2015)</ref> only use the pesudo parallel corpus for parame- ter estimation for once (see Step 4 above) while our approach enables source-to-target and target- to-source models to interact with each other itera- tively on both parallel and monolingual corpora.</p><p>To some extent, our approach can be seen as an iterative extension of <ref type="bibr" target="#b20">Sennrich et al. (2015)</ref>'s ap- proach: after estimating model parameters on the pseudo parallel corpus, the learned model param- eters are used to produce a better pseudo parallel corpus. <ref type="table">Table 4</ref> shows example Viterbi transla- tions on the Chinese monolingual corpus over it- erations:</p><formula xml:id="formula_18">x * = argmax x P (y |x; − → θ )P (x|y; ← − θ ) .<label>(11)</label></formula><p>We observe that the quality of Viterbi transla- tions generally improves over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our work is inspired by two lines of research: (1) exploiting monolingual corpora for machine trans- lation and (2) autoencoders in unsupervised and semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Exploiting Monolingual Corpora for Machine Translation</head><p>Exploiting monolingual corpora for conventional SMT has attracted intensive attention in recent years. Several authors have introduced transduc- tive learning to make full use of monolingual corpora ( <ref type="bibr" target="#b24">Ueffing et al., 2007;</ref><ref type="bibr" target="#b2">Bertoldi and Federico, 2009</ref>). They use an existing translation model to translate unseen source text, which can be paired with its translations to form a pseudo parallel corpus. This process iterates until con- vergence. While <ref type="bibr" target="#b13">Klementiev et al. (2012)</ref> pro- pose an approach to estimating phrase translation probabilities from monolingual corpora, Zhang and Zong (2013) directly extract parallel phrases from monolingual corpora using retrieval tech- niques. Another important line of research is to treat translation on monolingual corpora as a de- cipherment problem ( <ref type="bibr" target="#b19">Ravi and Knight, 2011;</ref><ref type="bibr" target="#b7">Dou et al., 2014</ref>).</p><p>Closely related to <ref type="bibr" target="#b8">Gulccehre et al. (2015)</ref> and <ref type="bibr" target="#b20">Sennrich et al. (2015)</ref>, our approach focuses on learning birectional NMT models via autoen- coders on monolingual corpora. The major ad- vantages of our approach are the transparency to network architectures and the capability to exploit both source and target monolingual corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Autoencoders in Unsupervised and Semi-Supervised Learning</head><p>Autoencoders and their variants have been widely used in unsupervised deep learning <ref type="bibr" target="#b25">((Vincent et al., 2010;</ref><ref type="bibr" target="#b21">Socher et al., 2011;</ref><ref type="bibr" target="#b0">Ammar et al., 2014</ref>), just to name a few). Among them, Socher et al. (2011)'s approach bears close resemblance to our approach as they introduce semi-supervised recur- sive autoencoders for sentiment analysis. The dif- ference is that we are interested in making a bet- ter use of parallel and monolingual corpora while they concentrate on injecting partial supervision to conventional unsupervised autoencoders. <ref type="bibr" target="#b6">Dai and Le (2015)</ref> introduce a sequence autoencoder to reconstruct an observed sequence via RNNs. Our approach differs from sequence autoencoders in that we use bidirectional translation models as encoders and decoders to enable them to interact within the autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a semi-supervised approach to training bidirectional neural machine translation models. The central idea is to introduce autoen- coders on the monolingual corpora with source-to- target and target-to-source translation models as encoders and decoders. Experiments on Chinese- English NIST datasets show that our approach leads to significant improvements.</p><p>As our method is sensitive to the OOVs present in monolingual corpora, we plan to integrate <ref type="bibr" target="#b10">Jean et al. (2015)</ref>'s technique on using very large vo- cabulary into our approach. It is also necessary to further validate the effectiveness of our approach on more language pairs and NMT architectures. Another interesting direction is to enhance the connection between source-to-target and target-to- source models (e.g., letting the two models share the same word embeddings) to help them benefit more from interacting with each other.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Bush held a talk with Sharon Bush held a talk with Sharon Bush held a talk with Sharon bushi yu shalong juxing le huitan encoder decoder encoder decoder (a) (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effect of sample size k on the Chineseto-English validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effect of sample size k on the Englishto-Chinese validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 .Figure 4 :Figure 5 :</head><label>145</label><figDesc>Figure 4: Effect of OOV ratio on the Chinese-toEnglish validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 :</head><label>3</label><figDesc>Comparison with Sennrich et al. (2015). Both Sennrich et al. (2015) and our approach build on top of RNNSEARCH to exploit monolingual corpora. The BLEU scores are case-insensitive. "*": significantly better than Sennrich et al. (2015) (p &lt; 0.05); "**": significantly better than Sennrich et al. (2015) (p &lt; 0.01).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 ,</head><label>1</label><figDesc></figDesc><table>we use both a parallel 
corpus and two monolingual corpora as the train-
ing set. The parallel corpus from LDC consists of 
2.56M sentence pairs with 67.53M Chinese words 
and 74.81M English words. The vocabulary sizes 
of Chinese and English are 0.21M and 0.16M, re-
spectively. We use the Chinese and English parts 
of the Xinhua portion of the GIGAWORD cor-
pus as the monolingual corpora. The Chinese 
monolingual corpus contains 18.75M sentences 
with 451.94M words. The English corpus contains 
22.32M sentences with 399.83M words. The vo-
cabulary sizes of Chinese and English are 0.97M 
and 1.34M, respectively. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was done while Yong Cheng was vis-iting Baidu. This research is supported by the 973 <ref type="bibr">Program (2014CB340501, 2014CB340505)</ref>, the National Natural Science Foundation of China (No. 61522204, 61331013, 61361136003), 1000 Talent Plan grant, Tsinghua Initiative Research Program grants 20151080475 and a Google Fac-ulty Research Award. We sincerely thank the viewers for their valuable suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Conditional random field autoencoders for unsupervised structred prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WMT</title>
		<meeting>WMT</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguisitics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunhyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8</title>
		<meeting>SSST-8</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond parallel data: Joint word alignment and decipherment improves machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulccehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">On using monolingual corpora in neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguisitics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Toward statistical machine translation without paralel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>demo session</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a methof for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deciphering foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improving nerual machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semisupervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Srilm-am extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Trasductive learning for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Autoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a phrase-based translation model from monolingual data with application to domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
