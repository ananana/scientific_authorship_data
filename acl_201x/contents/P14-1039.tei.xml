<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">That&apos;s Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 23-25</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjuan</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">The Ohio State University Columbus</orgName>
								<address>
									<postCode>43210</postCode>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">The Ohio State University Columbus</orgName>
								<address>
									<postCode>43210</postCode>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">That&apos;s Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="413" to="423"/>
							<date type="published">June 23-25</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate whether parsers can be used for self-monitoring in surface realization in order to avoid egregious errors involving &quot;vicious&quot; ambiguities, namely those where the intended interpretation fails to be considerably more likely than alternative ones. Using parse accuracy in a simple reranking strategy for self-monitoring, we find that with a state-of-the-art averaged perceptron realization ranking model, BLEU scores cannot be improved with any of the well-known Treebank parsers we tested, since these parsers too often make errors that human readers would be unlikely to make. However , by using an SVM ranker to combine the realizer&apos;s model score together with features from multiple parsers, including ones designed to make the ranker more robust to parsing mistakes, we show that significant increases in BLEU scores can be achieved. Moreover, via a targeted manual analysis, we demonstrate that the SVM reranker frequently manages to avoid vicious ambiguities, while its ranking errors tend to affect fluency much more often than adequacy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Rajkumar &amp; <ref type="bibr" target="#b35">2012)</ref> have recently shown that some rather egregious surface realiza- tion errors-in the sense that the reader would likely end up with the wrong interpretation-can be avoided by making use of features inspired by psycholinguistics research together with an other- wise state-of-the-art averaged perceptron realiza- tion ranking model , as reviewed in the next section. However, one is apt to wonder: could one use a parser to check whether the intended interpretation is easy to re- cover, either as an alternative or to catch additional mistakes? Doing so would be tantamount to self- monitoring in <ref type="bibr" target="#b20">Levelt's (1989)</ref> model of language production. <ref type="bibr" target="#b21">Neumann &amp; van Noord (1992)</ref> pursued the idea of self-monitoring for generation in early work with reversible grammars. As <ref type="bibr">Neumann &amp; van</ref> Noord observed, a simple, brute-force way to gen- erate unambiguous sentences is to enumerate pos- sible realizations of an input logical form, then to parse each realization to see how many inter- pretations it has, keeping only those that have a single reading; they then went on to devise a more efficient method of using self-monitoring to avoid generating ambiguous sentences, targeted to the ambiguous portion of the output. We might question, however, whether it is really possible to avoid ambiguity entirely in the general case, since <ref type="bibr" target="#b0">Abney (1996)</ref> and others have argued that nearly every sentence is potentially ambiguous, though we (as human comprehenders) may not notice the ambiguities if they are unlikely. Tak- ing up this issue, <ref type="bibr" target="#b17">Khan et al. (2008)</ref>-building on <ref type="bibr">Chantree et al.'s (2006)</ref> approach to identifying "innocuous" ambiguities-conducted several ex- periments to test whether ambiguity could be bal- anced against length or fluency in the context of generating referring expressions involving coordi- nate structures. Though Khan et al.'s study was limited to this one kind of structural ambiguity, they do observe that generating the brief variants when the intended interpretation is clear instanti- ates Van Deemter's (2004) general strategy of only avoiding vicious ambiguities-that is, ambigui- ties where the intended interpretation fails to be considerably more likely than any other distractor interpretations-rather than trying to avoid all am- biguities.</p><p>In this paper, we investigate whether Neumann &amp; van Noord's brute-force strategy for avoid-ing ambiguities in surface realization can be up- dated to only avoid vicious ambiguities, extend- ing (and revising) Van Deemter's general strategy to all kinds of structural ambiguity, not just the one investigated by Khan et al. To do so-in a nutshell-we enumerate an n-best list of realiza- tions and rerank them if necessary to avoid vicious ambiguities, as determined by one or more auto- matic parsers. A potential obstacle, of course, is that automatic parsers may not be sufficiently rep- resentative of human readers, insofar as errors that a parser makes may not be problematic for human comprehension; moreover, parsers are rarely suc- cessful in fully recovering the intended interpreta- tion for sentences of moderate length, even with carefully edited news text. Consequently, we ex- amine two reranking strategies, one a simple base- line approach and the other using an SVM reranker <ref type="bibr" target="#b16">(Joachims, 2002</ref>).</p><p>Our simple reranking strategy for self- monitoring is to rerank the realizer's n-best list by parse accuracy, preserving the original order in case of ties. In this way, if there is a realization in the n-best list that can be parsed more accurately than the top-ranked realization-even if the intended interpretation cannot be recovered with 100% accuracy-it will become the preferred output of the combined realization-with-self- monitoring system. With this simple reranking strategy and each of three different Treebank parsers, we find that it is possible to improve BLEU scores on Penn Treebank development data with <ref type="bibr" target="#b26">White &amp; Rajkumar's (2011;</ref><ref type="bibr" target="#b35">2012)</ref> baseline generative model, but not with their averaged perceptron model. In inspecting the results of reranking with this strategy, we observe that while it does sometimes succeed in avoiding egregious errors involving vicious ambiguities, common parsing mistakes such as PP-attachment errors lead to unnecessarily sacrificing conciseness or fluency in order to avoid ambiguities that would be easily tolerated by human readers. Therefore, to develop a more nuanced self-monitoring reranker that is more robust to such parsing mistakes, we trained an SVM using dependency precision and recall features for all three parses, their n-best parsing results, and per-label precision and recall for each type of dependency, together with the realizer's normalized perceptron model score as a feature. With the SVM reranker, we obtain a significant improvement in BLEU scores over White &amp; Rajkumar's averaged perceptron model on both development and test data. Additionally, in a targeted manual analysis, we find that in cases where the SVM reranker improves the BLEU score, improvements to fluency and adequacy are roughly balanced, while in cases where the BLEU score goes down, it is mostly fluency that is made worse (with reranking yielding an acceptable paraphrase roughly one third of the time in both cases).</p><p>The paper is structured as follows. In Sec- tion 2, we review the realization ranking mod- els that serve as a starting point for the paper. In Section 3, we report on our experiments with the simple reranking strategy, including a discus- sion of the ways in which this method typically fails. In Section 4, we describe how we trained an SVM reranker and report our results using BLEU scores ( <ref type="bibr" target="#b23">Papineni et al., 2002</ref>). In Section 5, we present a targeted manual analysis of the devel- opment set sentences with the greatest change in BLEU scores, discussing both successes and er- rors. In Section 6, we briefly review related work on broad coverage surface realization. Finally, in Section 7, we sum up and discuss opportunities for future work in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We use the OpenCCG 1 surface realizer for the ex- periments reported in this paper. The OpenCCG realizer generates surface strings for input seman- tic dependency graphs (or logical forms) using a chart-based algorithm <ref type="bibr" target="#b36">(White, 2006</ref>) for Combi- natory Categorial Grammar <ref type="bibr" target="#b30">(Steedman, 2000</ref>) to- gether with a "hypertagger" for probabilistically assigning lexical categories to lexical predicates in the input ( <ref type="bibr" target="#b9">Espinosa et al., 2008</ref>). An exam- ple input appears in <ref type="figure" target="#fig_0">Figure 1</ref>. In the figure, nodes correspond to discourse referents labeled with lexical predicates, and dependency relations between nodes encode argument structure (gold standard CCG lexical categories are also shown); note that semantically empty function words such as infinitival-to are missing. The grammar is ex- tracted from a version of the CCGbank <ref type="bibr" target="#b14">(Hockenmaier and Steedman, 2007)</ref>   <ref type="figure" target="#fig_0">Figure 1</ref>: Example OpenCCG semantic depen- dency input for he has a point he wants to make, with gold standard lexical categories for each node and assignment of consistent semantic roles across diathesis alternations <ref type="bibr" target="#b3">(Boxwell and White, 2008)</ref>, using PropBank ( <ref type="bibr" target="#b22">Palmer et al., 2005</ref>).</p><p>To select preferred outputs from the chart, we use <ref type="bibr" target="#b35">2012)</ref> realization ranking model, recently augmented with a large- scale 5-gram model based on the Gigaword cor- pus. The ranking model makes choices addressing all three interrelated sub-tasks traditionally con- sidered part of the surface realization task in natu- ral language generation research <ref type="bibr" target="#b28">(Reiter and Dale, 2000;</ref><ref type="bibr" target="#b29">Reiter, 2010)</ref>: inflecting lemmas with gram- matical word forms, inserting function words and linearizing the words in a grammatical and natu- ral order. The model takes as its starting point two probabilistic models of syntax that have been de- veloped for CCG parsing, <ref type="bibr" target="#b13">Hockenmaier &amp; Steedman's (2002)</ref> generative <ref type="bibr">model and Clark &amp; Curran's (2007)</ref> normal-form model. Using the aver- aged perceptron algorithm <ref type="bibr" target="#b7">(Collins, 2002</ref>), White &amp; Rajkumar (2009) trained a structured predic- tion ranking model to combine these existing syn- tactic models with several n-gram language mod- els. This model improved upon the state-of-the-art in terms of automatic evaluation scores on held- out test data, but nevertheless an error analysis re- vealed a surprising number of word order, func- tion word and inflection errors. For each kind of error, subsequent work investigated the utility of employing more linguistically motivated features to improve the ranking model. To improve word ordering decisions, <ref type="bibr" target="#b35">White &amp; Rajkumar (2012)</ref> demonstrated that incorporat- ing a feature into the ranker inspired by Gib- son's (2000) dependency locality theory can de- liver statistically significant improvements in au- tomatic evaluation scores, better match the distri- butional characteristics of sentence orderings, and significantly reduce the number of serious order- ing errors (some involving vicious ambiguities) as confirmed by a targeted human evaluation. Sup- porting Gibson's theory, comprehension and cor- pus studies have found that the tendency to min- imize dependency length has a strong influence on constituent ordering choices; see <ref type="bibr" target="#b31">Temperley (2007)</ref> and <ref type="bibr" target="#b12">Gildea and Temperley (2010)</ref> for an overview. <ref type="table">Table 1</ref> shows examples from White and Rajku- mar (2012) of how the dependency length feature (DEPLEN) affects the OpenCCG realizer's output even in comparison to a model (DEPORD) with a rich set of discriminative syntactic and depen- dency ordering features, but no features directly targeting relative weight. In wsj 0015.7, the de- pendency length model produces an exact match, while the DEPORD model fails to shift the short temporal adverbial next year next to the verb, leav- ing a confusingly repetitive this year next year at the end of the sentence. Note how shifting next year from its canonical VP-final position to appear next to the verb shortens its dependency length considerably, while barely lengthening the depen- dency to based on; at the same time, it avoids ambiguity in what next year is modifying. In wsj 0020.1 we see the reverse case: the depen- dency length model produces a nearly exact match with just an equally acceptable inversion of closely watching, keeping the direct object in its canoni- cal position. By contrast, the DEPORD model mis- takenly shifts the direct object South Korea, Tai- wan and Saudia Arabia to the end of the sentence where it is difficult to understand following two very long intervening phrases.</p><p>With function words, Rajkumar and White (2011) showed that they could improve upon the earlier model's predictions for when to employ that-complementizers using features inspired by Jaeger's (2010) work on using the principle of uniform information density, which holds that human language use tends to keep information density relatively constant in order to optimize communicative efficiency. In news text, com-wsj 0015.7 the exact amount of the refund will be determined next year based on actual collections made until Dec. 31 of this year .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DEPLEN</head><p>[same]</p><p>DEPORD the exact amount of the refund will be determined based on actual collections made until Dec. 31 of this year next year . wsj 0020.1 the U.S. , claiming some success in its trade diplomacy , removed South Korea , Taiwan and Saudi Arabia from a list of countries it is closely watching for allegedly failing to honor U.S. patents , copyrights and other intellectual-property rights .</p><p>DEPLEN the U.S. claiming some success in its trade diplomacy , removed South Korea , Taiwan and Saudi Arabia from a list of countries it is watching closely for allegedly failing to honor U.S. patents , copyrights and other intellectual-property rights .</p><p>DEPORD the U.S. removed from a list of countries it is watching closely for allegedly failing to honor U.S. patents , copyrights and other intellectual-property rights , claiming some success in its trade diplomacy , South Korea , Taiwan and Saudi Arabia . <ref type="table">Table 1</ref>: Examples of realized output for full models with and without the dependency length feature <ref type="bibr" target="#b35">(White and Rajkumar, 2012)</ref> plementizers are left out two times out of three, but in some cases the presence of that is cru- cial to the interpretation. Generally, inserting a complementizer makes the onset of a complement clause more predictable, and thus less informa- tion dense, thereby avoiding a potential spike in information density that is associated with com- prehension difficulty. Rajkumar &amp; White's exper- iments confirmed the efficacy of the features based on Jaeger's work, including information density- based features, in a local classification model. <ref type="bibr">2</ref> Their experiments also showed that the improve- ments in prediction accuracy apply to cases in which the presence of a that-complementizer ar- guably makes a substantial difference to fluency or intelligiblity. For example, in (1), the pres- ence of that avoids a local ambiguity, helping the reader to understand that for the second month in a row modifies the reporting of the shortage; with- out that, it is very easy to mis-parse the sentence as having for the second month in a row modifying the saying event.</p><p>(1) He said that/∅? for the second month in a row, food processors reported a shortage of nonfat dry milk. (PTB WSJ0036.61)</p><p>Finally, to reduce the number of subject-verb agreement errors, Rajkumar and White (2010) ex- tended the earlier model with features enabling it to make correct verb form choices in sentences involving complex coordinate constructions and 2 Note that the features from the local classification model for that-complementizer choice have not yet been incorpo- rated into OpenCCG's global realization ranking model, and thus do not inform the baseline realization choices in this work.</p><p>with expressions such as a lot of where the correct choice is not determined solely by the head noun. They also improved animacy agreement with rela- tivizers, reducing the number of errors where that or which was chosen to modify an animate noun rather than who or whom (and vice-versa), while also allowing both choices where corpus evidence was mixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simple Reranking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Methods</head><p>We ran two OpenCCG surface realization models on the CCGbank dev set (derived from Section 00 of the Penn Treebank) and obtained n-best (n = 10) realizations. The first one is the baseline gen- erative model (hereafter, generative model) used in training the averaged perceptron model. This model ranks realizations using the product of the Hockenmaier syntax model, n-gram models over words, POS tags and supertags in the training sec- tions of the CCGbank, and the large-scale 5-gram model from Gigaword. The second one is the averaged perceptron model (hereafter, perceptron model), which uses all the features reviewed in Section 2. In order to experiment with multiple parsers, we used the Stanford dependencies (de <ref type="bibr" target="#b8">Marneffe et al., 2006</ref>), obtaining gold dependen- cies from the gold-standard PTB parses and auto- matic dependencies from the automatic parses of each realization. Using dependencies allowed us to measure parse accuracy independently of word order. We chose the Berkeley parser ( <ref type="bibr" target="#b24">Petrov et al., 2006</ref>), Brown parser <ref type="bibr" target="#b5">(Charniak and Johnson, 2005</ref>) and Stanford parser ( <ref type="bibr" target="#b18">Klein and Manning, 2003)</ref>     </p><note type="other">That's Not What I Meant! Using Parsers to Avoid Structural Ambiguities in Generated Text Manjuan Duan and Michael White Department of</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We investigate . . . two realization models and calculated precision, recall and F 1 of the dependencies for each realiza- tion by comparing them with the gold dependen- cies. We then ranked the realizations by their F 1 score of parse accuracy, keeping the original rank- ing in case of ties. We also tried using unlabeled (and unordered) dependencies, in order to possi- bly make better use of parses that were close to being correct. In this setting, as long as the right pair of tokens occur in a dependency relation, it was counted as a correctly recovered dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Simple ranking with the Berkeley parser of the generative model's n-best realizations raised the BLEU score from 85.55 to 86.07, well below the averaged perceptron model's BLEU score of 87.93. However, as shown in <ref type="table" target="#tab_2">Table 2</ref>, none of the parsers yielded significant improvements on the top of the perceptron model. Inspecting the results of simple ranking re- vealed that while simple ranking did success- fully avoid vicious ambiguities in some cases, parser mistakes with PP-attachments, noun-noun compounds and coordinate structures too often blocked the gold realization from emerging on top. To illustrate, <ref type="figure" target="#fig_1">Figure 2</ref> shows an example with a PP-attachment mistake. In the figure, the key gold dependencies of the reference sentence are shown in (a), the dependencies of the realization selected by the simple ranker are shown in (b), and the de- pendencies of the realization selected by the per- ceptron ranker (same as gold) appear in (c), with the parsing mistake indicated by the dashed line. The simple ranker ends up choosing (b) as the best realization because it has the most accurate parse compared to the reference sentence, given the mis- take with (c).</p><p>Other common parse errors are illustrated in <ref type="figure">Figure 3</ref>. Here, (b) ends up getting chosen by the simple ranker as the realization with the most ac- curate parse given the failures in (c), where the ad- ditional technology, personnel training is mistak- enly analyzed as one noun phrase, a reading un- likely to be considered by human readers.</p><p>In sum, although simple ranking helps to avoid vicious ambiguity in some cases, the overall re- sults of simple ranking are no better than the per- ceptron model (according to BLEU, at least), as parse failures that are not reflective of human in- tepretive tendencies too often lead the ranker to choose dispreferred realizations. As such, we turn now to a more nuanced model for combining the results of multiple parsers in a way that is less sen- sitive to such parsing mistakes, while also letting the perceptron model have a say in the final rank- ing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Reranking with SVMs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Methods</head><p>Since different parsers make different errors, we conjectured that dependencies in the intersection of the output of multiple parsers may be more re- liable and thus may more reliably reflect human comprehension preferences. Similarly, we conjec- tured that large differences in the realizer's percep- tron model score may more reliably reflect human fluency preferences than small ones, and thus we combined this score with features for parser accu- racy in an SVM ranker. Additionally, given that parsers may more reliably recover some kinds of dependencies than others, we included features for each dependency type, so that the SVM ranker might learn how to weight them appropriately. Finally, since the differences among the n-best parses reflect the least certain parsing decisions, the additional technology, personnel training and promotional efforts and thus ones that may require more common sense inference that is easy for humans but not machines, we conjectured that including features from the n-best parses may help to better match human performance. In more detail, we made use of the following feature classes for each candidate realization:</p><p>perceptron model score the score from the real- izer's model, normalized to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> for the real- izations in the n-best list precision and recall labeled and unlabeled preci- sion and recall for each parser's best parse per-label precision and recall (dep) precision and recall for each type of dependency obtained from each parser's best parse (using zero if not defined for lack of predicted or gold dependencies with a given label) n-best precision and recall (nbest) labeled and unlabeled precision and recall for each parser's top five parses, along with the same features for the most accurate of these parses</p><p>In training, we used the BLEU scores of each realization compared with its reference sentence to establish a preference order over pairs of candi- date realizations, assuming that the original corpus sentences are generally better than related alterna- tives, and that BLEU can somewhat reliably pre- dict human preference judgments.</p><p>We trained the SVM ranker <ref type="bibr" target="#b16">(Joachims, 2002</ref>) with a linear kernel and chose the hyper-parameter c, which tunes the trade-off between training error and margin, with 6-fold cross-validation on the de- vset. We trained different models to investigate the contribution made by different parsers and differ- ent types of features, with the perceptron model score included as a feature in all models. For each parser, we trained a model with its overall preci- sion and recall features, as shown at the top of Ta- ble 3. Then we combined these three models to get a new model <ref type="table">(Bkl+Brw+St in the table)</ref> . Next, to this combined model we separately added (i) the per-label precision and recall features from all the parsers (BBS+dep), and (ii) the n-best features from the parsers (BBS+nbest). The full model (BBS+dep+nbest) includes all the features listed above. Finally, since the Berkeley parser yielded the best results on its own, we also tested mod- els using all the feature classes but only using this parser by itself. <ref type="table" target="#tab_5">Table 3</ref> shows the results of different SVM rank- ing models on the devset. We calculated signifi- cance using paired bootstrap resampling <ref type="bibr" target="#b19">(Koehn, 2004</ref>   tures and the n-best parse features contributed to achieving a significant improvement compared to the perceptron model. Somewhat surprisingly, the Berkeley parser did as well as all three parsers us- ing just the overall precision and recall features, but not quite as well using all features. The com- plete model, BBS+dep+nbest, achieved a BLEU score of 88.73, significantly improving upon the perceptron model (p &lt; 0.02). We then confirmed this result on the final test set, Section 23 of the CCGbank, as shown in <ref type="table" target="#tab_6">Table 4</ref> (p &lt; 0.02 as well).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Targeted Manual Analysis</head><p>In order to gain a better understanding of the suc- cesses and failures of our SVM ranker, we present here a targeted manual analysis of the develop- ment set sentences with the greatest change in BLEU scores, carried out by the second author (a native speaker). In this analysis, we consider whether the reranked realization improves upon or detracts from realization quality-in terms of adequacy, fluency, both or neither-along with a linguistic categorization of the differences be- tween the reranked realization and the original top-ranked realization according to the averaged perceptron model. Unlike the broad-based and ob- jective evaluation in terms of BLEU scores pre- sented above, this analysis is narrowly targeted and subjective, though the interested reader is in- vited to review the complete set of analyzed ex- amples that accompany the paper as a supplement.</p><p>We leave a more broad-based human evaluation by naive subjects for future work. <ref type="table">Table 5</ref> shows the results of the analysis, both overall and for the most frequent categories of changes. Of the 50 sentences where the BLEU score went up the most, 15 showed an improve- ment in adequacy (i.e., in conveying the intended meaning), 22 showed an improvement in fluency (with 3 cases also improving adequacy), and 16 yielded no discernible change in fluency or ade- quacy. By contrast, with the 50 sentences where the BLEU score went down the most, adequacy was only affected 4 times, though fluency was af- fected 32 times, and 15 remained essentially un- changed. <ref type="bibr">4</ref> The table also shows that differences in the order of VP constituents usually led to a change in adequacy or fluency, as did ordering changes within NPs, with noun-noun compounds and named entities as the most frequent subcate- gories of NP-ordering changes. Of the cases where adequacy and fluency were not affected, contrac- tions and subject-verb inversions were the most frequent differences.</p><p>Examples of the changes yielded by the SVM ranker appear in <ref type="table">Table 6</ref>. With wsj 0036.54, the averaged perceptron model selects a realiza- tion that regrettably (though amusingly) swaps purchasing and more than 250-yielding a sen- tence that suggests that the executives have been purchased!-while the SVM ranker succeeds in ranking the original sentence above all competing realizations. With wsj 0088.25, self-monitoring with the SVM ranker yields a realization nearly identical to the original except for an extra comma, where it is clear that in public modifies do this; by contrast, in the perceptron-best realization, in public mistakenly appears to modify be disclosed. With wsj 0041.18, the SVM ranker unfortunately prefers a realization where presumably seems to modify shows rather than of two politicians as ±adq ±flu =eq ±vpord ±npord ±nn ±ne =vpord =sbjinv =cntrc <ref type="table" target="#tab_2">BLEU wins  15  22  16  10  9  7  3  4  - 11  BLEU losses  4  32  15  8  13  5  5  4</ref> 7 - <ref type="table">Table 5</ref>: Manual analysis of devset sentences where the SVM ranker achieved the greatest in- crease/decrease in BLEU scores (50 each of wins/losses) compared to the averaged perceptron baseline model in terms of positive or negative changes in adequacy (±adq), fluency (±flu) or neither (=eq); changes in VP ordering (±vpord), NP ordering (±npord), noun-noun compound ordering (±nn) and named entities (±ne); and neither positive nor negative changes in VP ordering (=vpord), subject- inversion (=sbjinv) and contractions (=cntrc). In all but one case (counted as =eq here), the BLEU wins saw positive changes and the BLEU losses saw negative changes.</p><p>wsj 0036.54 the purchasing managers ' report is based on data provided by more than 250 purchasing executives . SVM RANKER <ref type="bibr">[same]</ref> PERCEP BEST the purchasing managers ' report is based on data provided by purchasing more than 250 executives . wsj 0088.25</p><p>Markey said we could have done this in public because so little sensitive information was disclosed , the aide said . SVM RANKER Markey said , we could have done this in public because so little sensitive information was disclosed , the aide said .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PERCEP BEST</head><p>Markey said , we could have done this because so little sensitive information was disclosed in public , the aide said .</p><p>wsj 0041.18 the screen shows two distorted , unrecognizable photos , presumably of two politicians . SVM RANKER the screen shows two distorted , unrecognizable photos presumably , of two politicians .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PERCEP BEST</head><p>[same as original] wsj 0044.111 " I was dumbfounded " , Mrs. Ward recalls . SVM RANKER " I was dumbfounded " , recalls Mrs. Ward .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PERCEP BEST</head><p>[same as original] <ref type="table">Table 6</ref>: Examples of devset sentences where the SVM ranker improved adequacy (top), made it worse (middle) or left it the same (bottom) in the original, which the averaged perceptron model prefers. Finally, wsj 0044.111 is an exam- ple where a subject-inversion makes no difference to adequacy or fluency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discussion</head><p>The BLEU evaluation and targeted manual analy- sis together show that the SVM ranker increases the similarity to the original corpus of realizations produced with self-monitoring, often in ways that are crucial for the intended meaning to be apparent to human readers.</p><p>A limitation of the experiments reported in this paper is that OpenCCG's input semantic depen- dency graphs are not the same as the Stanford de- pendencies used with the Treebank parsers, and thus we have had to rely on the gold parses in the PTB to derive gold dependencies for measur- ing accuracy of parser dependency recovery. In a realistic application scenario, however, we would need to measure parser accuracy relative to the re- alizer's input. We initially tried using OpenCCG's parser in a simple ranking approach, but found that it did not improve upon the averaged perceptron model, like the three parsers used subsequently. Given that with the more refined SVM ranker, the Berkeley parser worked nearly as well as all three parsers together using the complete feature set, the prospects for future work on a more realistic scenario using the OpenCCG parser in an SVM ranker for self-monitoring now appear much more promising, either using OpenCCG's reimplemen- tation of Hockenmaier &amp; Steedman's generative CCG model, or using the Berkeley parser trained on OpenCCG's enhanced version of the CCG- bank, along the lines of <ref type="bibr" target="#b10">Fowler and Penn (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Approaches to surface realization have been de- veloped for LFG, HPSG, and TAG, in addition to CCG, and recently statistical dependency-based approaches have been developed as well; see the report from the first surface realization shared task ( <ref type="bibr" target="#b1">Belz et al., 2010;</ref><ref type="bibr" target="#b2">Belz et al., 2011</ref>) for an overview. To our knowledge, however, a com- prehensive investigation of avoiding vicious struc- tural ambiguities with broad coverage statistical parsers has not been previously explored. As our SVM ranking model does not make use of CCG-specific features, we would expect our self- monitoring method to be equally applicable to re- alizers using other frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have shown that while using parse accuracy in a simple reranking strategy for self-monitoring fails to improve BLEU scores over a state-of-the-art averaged perceptron realiza- tion ranking model, it is possible to significantly increase BLEU scores using an SVM ranker that combines the realizer's model score together with features from multiple parsers, including ones de- signed to make the ranker more robust to parsing mistakes that human readers would be unlikely to make. Additionally, via a targeted manual analy- sis, we showed that the SVM reranker frequently manages to avoid egregious errors involving "vi- cious" ambiguities, of the kind that would mislead human readers as to the intended meaning.</p><p>As noted in Reiter's (2010) survey, many NLG systems use surface realizers as off-the-shelf com- ponents. In this paper, we have focused on broad coverage surface realization using widely- available PTB data-where there are many sen- tences of varying complexity with gold-standard annotations-following the common assumption that experiments with broad coverage realization are (or eventually will be) relevant for NLG ap- plications. Of course, the kinds of ambiguity that can be problematic in news text may or may not be the same as the ones encountered in particular ap- plications. Moreover, for certain applications (e.g. ones with medical or legal implications), it may be better to err on the side of ambiguity avoidance, even at some expense to fluency, thereby requir- ing training data reflecting the desired trade-off to adapt the methods described here. We leave these application-centered issues for investigation in fu- ture work.</p><p>The current approach is primarily suitable for offline use, for example in report generation where there are no real-time interaction demands. In fu- ture work, we also plan to investigate ways that self-monitoring might be implemented more effi- ciently as a combined process, rather than running independent parsers as a post-process following realization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example parsing mistake in PPattachment (wsj 0043.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example parsing mistake in PPattachment (wsj 0043.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example parsing mistakes in a noun-noun compound and a coordinate structure (wsj 0085.45) Figure 3: Example parsing mistakes in a noun-noun compound and a coordinate structure (wsj 0085.45)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>he h3 he h2 &lt;Det&gt; &lt;Arg0&gt; &lt;Arg1&gt; &lt;TENSE&gt;pres &lt;NUM&gt;sg &lt;Arg0&gt; w1 want.01 m1 &lt;Arg1&gt; &lt;GenRel&gt; &lt;Arg1&gt; &lt;TENSE&gt;pres p1 point h1 have.03</head><label></label><figDesc></figDesc><table>enhanced for realiza-
tion; the enhancements include: better analyses of 
punctuation (White and Rajkumar, 2008); less er-
ror prone handling of named entities (Rajkumar et 
al., 2009); re-inserting quotes into the CCGbank; a 
a1 

make.03 

&lt;Arg0&gt; 

s[b]\np/np 

np/n 

np 

n 

s[dcl]\np/np 

s[dcl]\np/(s[to]\np) 

np 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>to parse the realizations generated by the</figDesc><table>Berkeley Brown Stanford 
No reranking 
87.93 
87.93 
87.93 
Labeled 
87.77 
87.87 
87.12 
Unlabeled 
87.90 
87.97 
86.97 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Devset BLEU scores for simple ranking 
on top of n-best perceptron model realizations 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Devset results of SVM ranking on top 
of perceptron model. Significance codes:  *  *  for 
p &lt; 0.05,  *  for p &lt; 0.1. 

BLEU sig. 
perceptron baseline 86.94 
-
BBS+dep+nbest 
87.64 ** 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Final test results of SVM ranking on top 
of perceptron model. Significance codes:  *  *  for 
p &lt; 0.05,  *  for p &lt; 0.1. 

</table></figure>

			<note place="foot" n="1"> http://openccg.sf.net</note>

			<note place="foot" n="3"> Kudos to Kevin Gimpel for making his implementation available: http://www.ark.cs.cmu.edu/MT/ paired_bootstrap_v13a.tar.gz</note>

			<note place="foot" n="4"> The difference in the distribution of adequacy change, fluency change and no change counts between the two conditions is highly significant statistically (χ 2 = 9.3, df = 2, p &lt; 0.01). In this comparison, items where both fluency and adequacy were affected were counted as adequacy cases.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Mark Johnson, Micha Elsner, the OSU Clippers Group and the anonymous reviewers for helpful comments and discussion. This work was supported in part by NSF grants IIS-1143635 and IIS-1319318.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical methods and linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The balancing act: Combining symbolic and statistical approaches to language</title>
		<editor>Judith Klavans and Philip Resnik</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding common ground: Towards a surface realisation shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INLG-10</title>
		<meeting>INLG-10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="267" to="272" />
		</imprint>
	</monogr>
	<note>Generation Challenges</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The first surface realisation shared task: Overview and evaluation results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deirdre</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation</title>
		<meeting>the Generation Challenges Session at the 13th European Workshop on Natural Language Generation<address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-09" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Projecting Propbank roles onto the CCGbank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LREC08</title>
		<meeting>LREC08</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying nocuous ambiguities in natural language requirements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chantree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nuseibeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Roeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Willis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
	<note>Requirements Engineering</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Proceedings of ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">WideCoverage Efficient Statistical Parsing with CCG and Log-Linear Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="552" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP-02</title>
		<meeting>EMNLP-02</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypertagging: Supertagging for surface realization with CCG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Espinosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Mehay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="183" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate context-free parsing with Combinatory Categorial Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Fowler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dependency locality theory: A distance-based theory of linguistic complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image, Language, brain: Papers from the First Mind Articulation Project Symposium</title>
		<editor>Alec Marantz, Yasushi Miyashita, and Wayne O&apos;Neil</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Do grammars minimize dependency length?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Temperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="310" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative models for statistical parsing with Combinatory Categorial Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-02</title>
		<meeting>ACL-02</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="396" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Redundancy and reduction: Speakers manage information density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Florian</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="62" />
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generation of referring expressions: Managing structural ambiguities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ritchie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<editor>Dekang Lin and Dekai Wu</editor>
		<meeting>EMNLP 2004<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Speaking: From Intention to Articulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Willem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levelt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Selfmonitoring with reversible grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th conference on Computational linguistics</title>
		<meeting>the 14th conference on Computational linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="700" to="706" />
		</imprint>
	</monogr>
	<note>COLING &apos;92. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The proposition bank: A corpus annotated with semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-02</title>
		<meeting>ACL-02</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Designing agreement features for realization ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Coling 2010: Posters</title>
		<meeting>Coling 2010: Posters<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="1032" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Linguistically motivated complementizer choice in surface realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop</title>
		<meeting>the UCNLG+Eval: Language Generation and Evaluation Workshop<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="39" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting named entity classes in CCG surface realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Espinosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL HLT 2009 Short Papers</title>
		<meeting>NAACL HLT 2009 Short Papers</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building natural generation systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Natural Language Processing</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Natural language generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Computational Linguistics and Natural Language Processing</title>
		<editor>Alexander Clark, Chris Fox, and Shalom Lappin</editor>
		<imprint>
			<publisher>Wiley-Blackwell</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
	<note>Blackwell Handbooks in Linguistics. 1 edition</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The syntactic process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Minimization of dependency length in written English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Temperley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="333" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards a probabilistic version of bidirectional OT syntax and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Deemter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Semantics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="280" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A more precise analysis of punctuation for broadcoverage surface realization with CCG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perceptron reranking for CCG realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="410" to="419" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Minimal dependency length in realization ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajakrishnan</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="244" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Efficient Realization of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Coordinate Structures in Combinatory Categorial Grammar. Research on Language &amp; Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="75" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
