<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-based Dependency Parsing with Bidirectional LSTM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<addrLine>No.5 Yiheyuan Road</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Collaborative Innovation Center for Language Ability</orgName>
								<orgName type="institution">Haidian District</orgName>
								<address>
									<postCode>100871, 221009</postCode>
									<settlement>Beijing, Xuzhou</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-based Dependency Parsing with Bidirectional LSTM</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2306" to="2315"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a neural network model for graph-based dependency parsing which utilizes Bidirectional LSTM (BLSTM) to capture richer contextual information instead of using high-order fac-torization, and enable our model to use much fewer features than previous work. In addition, we propose an effective way to learn sentence segment embedding on sentence-level based on an extra forward LSTM network. Although our model uses only first-order factorization, experiments on English Peen Treebank and Chinese Penn Treebank show that our model could be competitive with previous higher-order graph-based dependency parsing models and state-of-the-art models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsing is a fundamental task for lan- guage processing which has been investigated for decades. It has been applied in a wide range of ap- plications such as information extraction and ma- chine translation. Among a variety of dependency parsing models, graph-based models are attractive for their ability of scoring the parsing decisions on a whole-tree basis. Typical graph-based mod- els factor the dependency tree into subgraphs, in- cluding single arcs ( <ref type="bibr" target="#b14">McDonald et al., 2005</ref>), sib- ling or grandparent arcs <ref type="bibr" target="#b13">(McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b2">Carreras, 2007)</ref> or higher-order substruc- tures ( <ref type="bibr" target="#b9">Koo and Collins, 2010;</ref><ref type="bibr" target="#b11">Ma and Zhao, 2012)</ref> and then score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as high-dimensional feature vectors which are then fed into a linear model to learn the feature weights.</p><p>However, conventional graph-based models heavily rely on feature engineering and their per- formance is restricted by the design of features. In addition, standard decoding algorithm <ref type="bibr" target="#b6">(Eisner, 2000</ref>) only works for the first-order model which limits the scope of feature selection. To incor- porate high-order features, Eisner algorithm must be somehow extended or modified, which is usu- ally done at high cost in terms of efficiency. The fourth-order graph-based model <ref type="bibr" target="#b11">(Ma and Zhao, 2012)</ref>, which seems the highest-order model so far to our knowledge, requires O(n 5 ) time and O(n 4 ) space. Due to the high computational cost, high- order models are normally restricted to produc- ing only unlabeled parses to avoid extra cost in- troduced by inclusion of arc-labels into the parse trees.</p><p>To alleviate the burden of feature engineering, <ref type="bibr" target="#b16">Pei et al. (2015)</ref> presented an effective neural net- work model for graph-based dependency parsing. They only use atomic features such as word uni- grams and POS tag unigrams and leave the model to automatically learn the feature combinations. However, their model requires many atomic fea- tures and still relies on high-order factorization strategy to further improve the accuracy.</p><p>Different from previous work, we propose an LSTM-based dependency parsing model in this paper and aim to use LSTM network to capture richer contextual information to support parsing decisions, instead of adopting a high-order factor- ization. The main advantages of our model are as follows:</p><p>• By introducing Bidirectional LSTM, our model shows strong ability to capture poten- tial long range contextual information and ex- hibits improved accuracy in recovering long distance dependencies. It is different to pre- vious work in which a similar effect is usually achieved by high-order factorization. More-over, our model also eliminates the need for setting feature selection windows and re- duces the number of features to a minimum level.</p><p>• We propose an LSTM-based sentence seg- ment embedding method named LSTM- Minus, in which distributed representation of sentence segment is learned by using subtrac- tion between LSTM hidden vectors. Experi- ment shows this further enhances our model's ability to access to sentence-level informa- tion.</p><p>• Last but important, our model is a first-order model using standard Eisner algorithm for decoding, the computational cost remains at the lowest level among graph-based models.</p><p>Our model does not trade-off efficiency for accuracy.</p><p>We evaluate our model on the English Penn Treebank and Chinese Penn Treebank, experi- ments show that our model achieves competi- tive parsing accuracy compared with conventional high-order models, however, with a much lower computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Graph-based dependency parsing</head><p>In dependency parsing, syntactic relationships are represented as directed arcs between head words and their modifier words. Each word in a sen- tence modifies exactly one head, but can have any number of modifiers itself. The whole sentence is rooted at a designated special symbol ROOT, thus the dependency graph for a sentence is constrained to be a rooted, directed tree.</p><p>For a sentence x, graph-based dependency pars- ing model searches for the highest-scoring tree of x:</p><formula xml:id="formula_0">y * (x) = arg maxˆy∈Y maxˆ maxˆy∈Y (x) Score(x, ˆ y; θ)<label>(1)</label></formula><p>Here y * (x) is the tree with the highest score, Y (x) is the set of all valid dependency trees for x and Score(x, ˆ y; θ) measures how likely the treê y is the correct analysis of the sentence x, θ are the model parameters. However, the size of Y (x) grows exponentially with respect to the length of the sentence, directly solving equation <ref type="formula" target="#formula_0">(1)</ref> is im- practical.</p><p>The common strategy adopted in the graph- based model is to factor the dependency treê y into <ref type="figure">Figure 1</ref>: First-order, Second-order and Third- order factorization strategy. Here h stands for head word, m stands for modifier word, s and t stand for the sibling of m. g stands for the grandparent of m.</p><p>a set of subgraph c which can be scored in isola- tion, and score the whole treê y by summing score of each subgraph:</p><formula xml:id="formula_1">Score(x, ˆ y; θ) = c∈ˆyc∈ˆy</formula><p>ScoreC(x, c; θ) (2) <ref type="figure">Figure 1</ref> shows several factorization strategies. The order of the factorization is defined accord- ing to the number of dependencies that subgraph contains. The simplest first-order factorization <ref type="bibr" target="#b14">(McDonald et al., 2005</ref>) decomposes a depen- dency tree into single dependency arcs. Based on the first-order factorization, second-order fac- torization ( <ref type="bibr" target="#b13">McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b2">Carreras, 2007</ref>) brings sibling and grandparent information into their model. Third-order factorization <ref type="bibr" target="#b9">(Koo and Collins, 2010)</ref> further incorporates richer con- textual information by utilizing grand-sibling and tri-sibling parts.</p><p>Conventional graph-based models <ref type="bibr" target="#b14">(McDonald et al., 2005;</ref><ref type="bibr" target="#b13">McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b2">Carreras, 2007;</ref><ref type="bibr" target="#b9">Koo and Collins, 2010;</ref><ref type="bibr" target="#b11">Ma and Zhao, 2012</ref>) score subgraph by a linear model, which heavily depends on feature engineering. The neu- ral network model proposed by <ref type="bibr" target="#b16">Pei et al. (2015)</ref> alleviates the dependence on feature engineering to a large extent, but not completely. We follow <ref type="bibr" target="#b16">Pei et al. (2015)</ref> to score dependency arcs using neural network model. However, different from their work, we introduce a Bidirectional LSTM to capture long range contextual information and an extra forward LSTM to better represent segments of the sentence separated by the head and modi- fier. These make our model more accurate in re- covering long-distance dependencies and further decrease the number of atomic features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Network Model</head><p>In this section, we describe the architecture of our neural network model in detail, which is summa- rized in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Input layer</head><p>In our neural network model, the words, POS tags are mapped into distributed embeddings. We represent each input token x i which is the in- put of Bidirectional LSTM by concatenating POS tag embedding e p i ∈ R de and word embedding e w i ∈ R de , d e is the the dimensionality of em- bedding, then a linear transformation w e is per- formed and passed though an element-wise acti- vation function g:</p><formula xml:id="formula_2">x i = g(w e [e w i ; e p i ] + b e )<label>(3)</label></formula><p>where x i ∈ R de , w e ∈ R de×2de is weight matrix, b e ∈ R de is bias term. the dimensionality of input token x i is equal to the dimensionality of word and POS tag embeddings in our experiment, ReLU is used as our activation function g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bidirectional LSTM</head><p>Given an input sequence x = (x 1 , . . . , x n ), where n stands for the number of words in a sentence, a standard LSTM recurrent network computes the hidden vector sequence h = (h 1 , . . . , h n ) in one direction.</p><p>Bidirectional LSTM processes the data in both directions with two separate hidden layers, which are then fed to the same output layer. It com- putes the forward hidden sequence − → h , the back- ward hidden sequence ← − h and the output sequence v by iterating the forward layer from t = 1 to n, the backward layer from t = n to 1 and then up- dating the output layer:</p><formula xml:id="formula_3">v t = − → h t + ← − h t (4)</formula><p>where</p><formula xml:id="formula_4">v t ∈ R d l is the output vector of Bidirec- tional LSTM for input x t , − → h t ∈ R d l , ← − h t ∈ R d l , d l is the dimensionality of LSTM hidden vector.</formula><p>We simply add the forward hidden vector − → h t and the backward hidden vector ← − h t together, which gets similar experiment result as concatenating them together with a faster speed.</p><p>The output vectors of Bidirectional LSTM are used as word feature embeddings. In addition, they are also fed into a forward LSTM network to learn segment embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Segment Embedding</head><p>Contextual information of word pairs 1 has been widely utilized in previous work <ref type="bibr" target="#b14">(McDonald et al., 2005;</ref><ref type="bibr" target="#b13">McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b16">Pei et al., 2015</ref>). For a dependency pair (h, m), previ- ous work divides a sentence into three parts (pre- fix, infix and suffix) by head word h and modifier word m. These parts which we call segments in our work make up the context of the dependency pair (h, m).</p><p>Due to the problem of data sparseness, conven- tional graph-based models can only capture con- textual information of word pairs by using bigrams or tri-grams features. Unlike conventional mod- els, <ref type="bibr" target="#b16">Pei et al. (2015)</ref> use distributed representa- tions obtained by averaging word embeddings in segments to represent contextual information of the word pair, which could capture richer syn- tactic and semantic information. However, their method is restricted to segment-level since their segment embedding only consider the word infor- mation within the segment. Besides, averaging operation simply treats all the words in segment equally. However, some words might carry more salient syntactic or semantic information and they are expected to be given more attention.</p><p>A useful property of forward LSTM is that it could keep previous useful information in their memory cell by exploiting input, output and for- get gates to decide how to utilize and update the memory of previous information. Given an in- put sequence v = (v 1 , . . . , v n ), previous work ( ) of- ten uses the last hidden vector h n of the forward LSTM to represent the whole sequence. Each hid- den vector h t (1 ≤ t ≤ n) can capture useful in- formation before and including v t .</p><p>Inspired by this, we propose a method named LSTM-Minus to learn segment embedding. We utilize subtraction between LSTM hidden vectors to represent segment's information. As illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>, the segment infix can be described as h m − h 2 , h m and h 2 are hidden vector of the for- ward LSTM network. The segment embedding of suffix can also be obtained by subtraction between the last LSTM hidden vector of the sequence (h 7 ) and the last LSTM hidden vector in infix (h m ). For prefix, we directly use the last LSTM hidden vec- tor in prefix to represent it, which equals to sub- tract a zero embedding. When no prefix or suffix exists, the corresponding embedding is set to zero.</p><p>Specifically, we place an extra forward LSTM layer on top of the Bidirectional LSTM layer and learn segment embeddings using LSTM-Minus based on this forward LSTM. LSTM-minus en- ables our model to learn segment embeddings from information both outside and inside the seg- ments and thus enhances our model's ability to ac- cess to sentence-level information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hidden layer and output layer</head><p>As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, we map all the feature embeddings to a hidden layer. Following <ref type="bibr" target="#b16">Pei et al. (2015)</ref>, we use direction-specific transformation to model edge direction and tanh-cube as our activa- tion function:</p><formula xml:id="formula_5">h = g i W d h i a i + b d h (5)</formula><p>where a i ∈ R da i is the feature embedding, d a i indicates the dimensionality of feature embedding</p><formula xml:id="formula_6">a i , W d h i ∈ R d h ×da i is weight matrices which cor- responding to a i , d h indicates the dimensionality of hidden layer vector, b d h ∈ R d h is bias term. W d h i</formula><p>and b d h are bound with index d ∈ {0, 1} which in- dicates the direction between head and modifier.</p><p>A output layer is finally added on the top of the hidden layer for scoring dependency arcs:</p><formula xml:id="formula_7">ScoreC(x, c) = W d o h + b d o<label>(6)</label></formula><p>Where</p><formula xml:id="formula_8">W d o ∈ R L×d h is weight matrices, b d o ∈ R L is bias term, ScoreC(x, c) ∈ R L is the output vec- tor,</formula><p>L is the number of dependency types. Each di- mension of the output vector is the score for each kind of dependency type of head-modifier pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Features in our model</head><p>Previous neural network models ( <ref type="bibr" target="#b16">Pei et al., 2015;</ref><ref type="bibr" target="#b15">Pei et al., 2014;</ref><ref type="bibr" target="#b27">Zheng et al., 2013</ref>) normally set context window around a word and extract atomic features within the window to represent the con- textual information. However, context window limits their ability in detecting long-distance in- formation. Simply increasing the context window size to get more contextual information puts their model in the risk of overfitting and heavily slows down the speed.</p><p>Unlike previous work, we apply Bidirectional LSTM to capture long range contextual informa- tion and eliminate the need for context windows, avoiding the limit of the window-based feature selection approach. Compared with <ref type="bibr" target="#b16">Pei et al. (2015)</ref>, the cancellation of the context window al- lows our model to use much fewer features. More- over, by combining a word's atomic features (word form and POS tag) together, our model further de- creases the number of features. <ref type="bibr" target="#b16">Pei et al. (2015)</ref> h−2.w, h−1.w, h.w, h1.w, h2.w h−2.p, h−1.p, h.p, h1.p, h2.p m−2.w, m−1.w, m.w, m1.w, m2.w m−2.p, m−1.p, m.p, m1.p, m2.p dis(h, m)</p><p>Our basic model v h , vm dis(h, m)  <ref type="table" target="#tab_0">Table 1</ref> lists the atomic features used in 1st- order atomic model of <ref type="bibr" target="#b16">Pei et al. (2015)</ref> and atomic features used in our basic model. Our basic model only uses the outputs of Bidirectional LSTM for head word and modifier word, and the distance be- tween them as features. Distance features are en- coded as randomly initialized embeddings. As we can see, our basic model reduces the number of atomic features to a minimum level, making our model run with a faster speed. Based on our ba- sic model, we incorporate additional segment in- formation <ref type="bibr">(prefix, infix and suffix)</ref>, which further improves the effect of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Training</head><p>In this section, we provide details about training the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Max-Margin Training</head><p>We use the Max-Margin criterion to train our model. Given a training instance (x (i) , y (i) ), we use Y (x (i) ) to denote the set of all possible depen- dency trees and y (i) is the correct dependency tree for sentence x (i) . The goal of Max Margin train- ing is to find parameters θ such that the difference in score of the correct tree y (i) from an incorrect treê</p><formula xml:id="formula_9">y ∈ Y (x (i) ) is at least (y (i) , ˆ y).</formula><p>Score(x (i) ,y (i) ; θ) ≥ Score(x (i) ,ˆ y; θ)+(y (i) ,ˆ y)</p><p>The structured margin loss (y (i) , ˆ y) is defined as:</p><formula xml:id="formula_10">(y (i) , ˆ y) = n j κ1{h(y (i) , x (i) j ) = h(ˆ y, x (i) j )}</formula><p>where n is the length of sentence x, h(y (i) , x (i) j ) is the head (with type) for the j-th word of x (i) in tree y (i) and κ is a discount parameter. The loss is proportional to the number of word with an incor- rect head and edge type in the proposed tree.</p><p>Given a training set with size m, The regular- ized objective function is the loss function J(θ) including a l 2 -norm term:</p><formula xml:id="formula_11">J(θ) = 1 m m i=1 l i (θ) + λ 2 ||θ|| 2 l i (θ) = maxˆy∈Y maxˆ maxˆy∈Y (x (i) ) (Score(x (i) ,ˆ y; θ)+(y (i) ,ˆ y)) −Score(x (i) ,y (i) ; θ)<label>(7)</label></formula><p>By minimizing this objective, the score of the correct tree is increased and score of the highest scoring incorrect tree is decreased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimization Algorithm</head><p>Parameter optimization is performed with the di- agonal variant of AdaGrad ( <ref type="bibr" target="#b4">Duchi et al., 2011</ref>) with minibatchs (batch size = 20) . The param- eter update for the i-th parameter θ t,i at time step t is as follows:</p><formula xml:id="formula_12">θ t,i = θ t−1,i − α t τ =1 g 2 τ,i g t,i<label>(8)</label></formula><p>where α is the initial learning rate (α = 0.2 in our experiment) and g τ ∈ R |θ i | is the subgradient at time step τ for parameter θ i . To mitigate overfitting, dropout <ref type="bibr" target="#b8">(Hinton et al., 2012</ref>) is used to regularize our model. we apply dropout on the hidden layer with 0.2 rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Initialization&amp;Hyperparameters</head><p>The following hyper-parameters are used in all experiments: word embedding size = 100, POS tag embedding size = 100, hidden layer size = 200, LSTM hidden vector size = 100, Bidirec- tional LSTM layers = 2, regularization parameter λ = 10 −4 .</p><p>We initialized the parameters using pretrained word embeddings. Following , we use a variant of the skip n-gram model in- troduced by <ref type="bibr">Ling</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present our experimental setup and the main result of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiments Setup</head><p>We conduct our experiments on the English Penn Treebank (PTB) and the Chinese Penn Treebank (CTB) datasets. For English, we follow the standard splits of PTB3. Using section 2-21 for training, section 22 as development set and 23 as test set. We con- duct experiments on two different constituency-to- dependency-converted Penn Treebank data sets. The first one, Penn-YM, was created by the Penn2Malt tool 2 based on <ref type="bibr" target="#b21">Yamada and Matsumoto (2003)</ref> head rules. The second one, Penn-SD, use Stanford Basic Dependencies ( <ref type="bibr" target="#b12">Marneffe et al., 2006)</ref> and was converted by version 3.3.0 3 of the Stanford parser. The Stanford POS Tagger ( <ref type="bibr" target="#b18">Toutanova et al., 2003</ref>) with ten-way jackknifing of the training data is used for assigning POS tags (accuracy ≈ 97.2%).</p><p>For Chinese, we adopt the same split of CTB5 as described in <ref type="bibr" target="#b22">(Zhang and Clark, 2008)</ref>. Follow- ing ( <ref type="bibr" target="#b22">Zhang and Clark, 2008;</ref><ref type="bibr" target="#b3">Chen and Manning, 2014</ref>), we use gold segmen- tation and POS tags for the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments Results</head><p>We first make comparisons with previous graph- based models of different orders as shown in Ta-ble 2. We use MSTParser 4 for conventional first- order model <ref type="bibr" target="#b14">(McDonald et al., 2005)</ref> and second- order model <ref type="bibr" target="#b13">(McDonald and Pereira, 2006</ref>). We also include the results of conventional high-order models ( <ref type="bibr" target="#b9">Koo and Collins, 2010;</ref><ref type="bibr" target="#b11">Ma and Zhao, 2012;</ref><ref type="bibr" target="#b23">Zhang and McDonald, 2012;</ref><ref type="bibr" target="#b26">Zhang et al., 2013;</ref><ref type="bibr" target="#b24">Zhang and McDonald, 2014</ref>) and the neu- ral network model of <ref type="bibr" target="#b16">Pei et al. (2015)</ref>. Different from typical high-order models ( <ref type="bibr" target="#b9">Koo and Collins, 2010;</ref><ref type="bibr" target="#b11">Ma and Zhao, 2012)</ref>, which need to extend their decoding algorithm to score new types of higher-order dependencies. <ref type="bibr" target="#b23">Zhang and McDonald (2012)</ref> generalized the Eisner algorithm to handle arbitrary features over higher-order dependencies and controlled complexity via approximate decod- ing with cube pruning. They further improve their work by using perceptron update strategies for in- exact hypergraph search ( <ref type="bibr" target="#b26">Zhang et al., 2013)</ref> and forcing inference to maintain both label and struc- tural ambiguity through a secondary beam <ref type="bibr" target="#b24">(Zhang and McDonald, 2014)</ref>.</p><p>Following previous work, UAS (unlabeled at- tachment scores) and LAS (labeled attachment scores) are calculated by excluding punctuation <ref type="bibr">5</ref> . The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM which is same to <ref type="bibr" target="#b16">Pei et al. (2015)</ref>. We measure the parsing speeds of <ref type="bibr" target="#b16">Pei et al. (2015)</ref> according to their codes 6 and parameters.</p><p>On accuracy, as shown in table 2, our</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Penn-YM Penn-SD CTB5 UAS LAS UAS LAS UAS LAS ( <ref type="bibr" target="#b25">Zhang and Nivre, 2011)</ref> 92.9 91.8 - - 86.0 84.4 <ref type="bibr" target="#b1">(Bernd Bohnet, 2012)</ref> 93.39 92.38 - - 87.5 85.9 ( <ref type="bibr" target="#b24">Zhang and McDonald, 2014</ref>) 93.57 92.48 93.01 <ref type="bibr">90.64 87.96 86.34 (Dyer et al., 2015)</ref> - - 93.1 90.9 87.2 85.7 ( <ref type="bibr" target="#b20">Weiss et al., 2015)</ref> - - 93.99 92.05 - - Our basic model + segment 93.51 92.45 94.08 91.82 87.55 86.23 <ref type="table">Table 3</ref>: Comparison with previous state-of-the-art models on Penn-YM, Penn-SD and CTB5.</p><p>basic model outperforms previous first-order graph-based models by a substantial margin, even outperforms Zhang and McDonald (2012)'s unlimited-order model. Moreover, incorporating segment information further improves our model's accuracy, which shows that segment embeddings do capture richer contextual information. By using segment embeddings, our improved model could be comparable to high-order graph-based models <ref type="bibr">7</ref> .</p><p>With regard to parsing speed, our model also shows advantage of efficiency. Our model uses only first-order factorization and requires O(n 3 ) time to decode. Third-order model requires O(n 4 ) time and fourth-order model requires O(n 5 ) time. By using approximate decoding, the unlimited- order model of <ref type="bibr" target="#b23">Zhang and McDonald (2012)</ref> re- quires O(k ·log(k)·n 3 ) time, where k is the beam size. The computational cost of our model is the lowest among graph-based models. Moreover, al- though using LSTM requires much computational cost. However, compared with Pei's 1st-order model, our model decreases the number of atomic features from 21 to 3, this allows our model to re- quire a much smaller matrix computation in the scoring model, which cancels out the extra compu- tation cost introduced by the LSTM computation. Our basic model is the fastest among first-order and second-order models. Incorporating segment information slows down the parsing speed while it is still slightly faster than conventional first-order model. To compare with conventional high-order models on practical parsing speed, we can make an indirect comparison according to <ref type="bibr" target="#b23">Zhang and McDonald (2012)</ref>. Conventional first-order model is about 10 times faster than <ref type="bibr">Zhang and McDon7</ref> Note that our model can't be strictly comparable with third-order model ( <ref type="bibr" target="#b9">Koo and Collins, 2010)</ref> and fourth- order model <ref type="bibr" target="#b11">(Ma and Zhao, 2012)</ref> since they are unlabeled model. However, our model is comparable with all the three unlimited-order models presented in <ref type="bibr" target="#b23">(Zhang and McDonald, 2012)</ref>, <ref type="bibr" target="#b26">(Zhang et al., 2013)</ref> and <ref type="bibr" target="#b24">(Zhang and McDonald, 2014</ref>), since they all are labeled models as ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Peen  ald <ref type="formula" target="#formula_0">(2012)</ref>'s unlimited-order model and about 40 times faster than conventional third-order model, while our model is faster than conventional first- order model. Our model should be much faster than conventional high-order models.</p><p>We further compare our model with previous state-of-the-art systems for English and Chinese. <ref type="table">Table 3</ref> lists the performances of our model as well as previous state-of-the-art systems on on Penn- YM, Penn-SD and CTB5. We compare to conven- tional state-of-the-art graph-based model <ref type="bibr" target="#b24">(Zhang and McDonald, 2014)</ref>, conventional state-of-the- art transition-based model using beam search ( <ref type="bibr" target="#b25">Zhang and Nivre, 2011)</ref>, transition-based model combining graph-based approach <ref type="bibr" target="#b1">(Bernd Bohnet, 2012)</ref> , transition-based neural network model us- ing stack LSTM (  and transition- based neural network model using beam search ( <ref type="bibr" target="#b20">Weiss et al., 2015)</ref>. Overall, our model achieves competitive accuracy on all three datasets. Al- though our model is slightly lower in accuarcy than unlimited-order double beam model <ref type="bibr" target="#b24">(Zhang and McDonald, 2014</ref>) on Penn-YM and CTB5, our model outperforms their model on Penn-SD. It seems that our model performs better on data sets with larger label sets, given the number of la- bels used in Penn-SD data set is almost four times more than Penn-YM and CTB5 data sets.</p><p>To show the effectiveness of our segment em- bedding method LSTM-Minus, we compare with averaging method proposed by <ref type="bibr" target="#b16">Pei et al. (2015)</ref>. We get segment embeddings by averaging the out- put vectors of Bidirectional LSTM in segments. To make comparison as fair as possible, we let two models have almost the same number parameters. <ref type="table" target="#tab_4">Table 4</ref> lists the UAS of two methods on test set. As we can see, LSTM-Minus shows better per- formance because our method further incorporates more sentence-level information into our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Impact of Network Structure</head><p>In this part, we investigate the impact of the com- ponents of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSTM Recurrent Network</head><p>To evaluate the impact of LSTM, we make er- ror analysis on Penn-YM. We compare our model with <ref type="bibr" target="#b16">Pei et al. (2015)</ref> on error rates of different distance between head and modifier.</p><p>As we can see, the five models do not show much difference for short dependencies whose dis- tance less than three. For long dependencies, both our two models show better performance com- pared with the 1st-order model of <ref type="bibr" target="#b16">Pei et al. (2015)</ref>, which proves that LSTM can effectively capture long-distance dependencies. Moreover, our mod- els and Pei's 2nd-order phrase model both im- prove accuracy on long dependencies compared with Pei's 1st-order model, which is in line with our expectations. Using LSTM shows the same effect as high-order factorization strategy. Com- pared with 2nd-order phrase model of <ref type="bibr" target="#b16">Pei et al. (2015)</ref>, our basic model occasionally performs worse in recovering long distant dependencies. However, this should not be a surprise since higher order models are also motivated to recover long- distance dependencies. Nevertheless, with the in- troduction of LSTM-minus segment embeddings, our model consistently outperforms the 2nd-order phrase model of <ref type="bibr" target="#b16">Pei et al. (2015)</ref> in accuracies of all long dependencies. We carried out significance test on the difference between our and Pei's mod- els. Our basic model performs significantly better than all 1st-order models of <ref type="bibr" target="#b16">Pei et al. (2015)</ref> (t- test with p&lt;0.001) and our basic+segment model (still a 1st-order model) performs significantly bet- ter than their 2nd-order phrase model (t-test with p&lt;0.001) in recovering long-distance dependen- cies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization of pre-trained word embeddings</head><p>We further analyze the influence of using pre- trained word embeddings for initialization. with- out using pretrained word embeddings, our im- proved model achieves 92.94% UAS / 91.83% LAS on Penn-YM, 93.46% UAS / 91.19% LAS on Penn-SD and 86.5% UAS / 85.0% LAS on CTB5. Using pre-trained word embeddings can obtain around 0.5%∼1.0% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Dependency parsing has gained widespread inter- est in the computational linguistics community. There are a lot of approaches to solve it. Among them, we will mainly focus on graph-based de- pendency parsing model here. Dependency tree factorization and decoding algorithm are neces- sary for graph-based models. <ref type="bibr" target="#b14">McDonald et al. (2005)</ref> proposed the first-order model which de- composes a dependency tree into its individual edges and use a effective dynamic programming algorithm <ref type="bibr" target="#b6">(Eisner, 2000</ref>) to decode. Based on first- order model, higher-order models <ref type="bibr" target="#b13">(McDonald and Pereira, 2006;</ref><ref type="bibr" target="#b2">Carreras, 2007;</ref><ref type="bibr" target="#b9">Koo and Collins, 2010;</ref><ref type="bibr" target="#b11">Ma and Zhao, 2012</ref>) factor a dependency tree into a set of high-order dependencies which bring interactions between head, modifier, siblings and (or) grandparent into their model. However, for above models, scoring new types of higher- order dependencies requires extensions of the un- derlying decoding algorithm, which also requires higher computational cost. Unlike above models, unlimited-order models <ref type="bibr" target="#b23">(Zhang and McDonald, 2012;</ref><ref type="bibr" target="#b26">Zhang et al., 2013;</ref><ref type="bibr" target="#b24">Zhang and McDonald, 2014</ref>) could handle arbitrary features over higher- order dependencies by generalizing the Eisner al- gorithm.</p><p>In contrast to conventional methods, neural net- work model shows their ability to reduce the effort in feature engineering. <ref type="bibr" target="#b16">Pei et al. (2015)</ref> proposed a model to automatically learn high-order feature combinations via a novel activation function, al- lowing their model to use a set of atomic features instead of millions of hand-crafted features.</p><p>Different from previous work, which is sensi- tive to local state and accesses to larger context by higher-order factorization. Our model makes pars- ing decisions on a global perspective with first- order factorization, avoiding the expensive com- putational cost introduced by high-order factoriza- tion.</p><p>LSTM network is heavily utilized in our model. LSTM network has already been explored in transition-based dependency parsing.  presented stack LSTMs with push and pop operations and used them to imple- ment a state-of-the-art transition-based depen- dency parser.  replaced lookup-based word representations with character- based representations obtained by Bidirectional LSTM in the continuous-state parser of , which was proved experimentally to be useful for morphologically rich languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose an LSTM-based neural network model for graph-based dependency pars- ing. Utilizing Bidirectional LSTM and segment embeddings learned by LSTM-Minus allows our model access to sentence-level information, mak- ing our model more accurate in recovering long- distance dependencies with only first-order factor- ization. Experiments on PTB and CTB show that our model could be competitive with conventional high-order models with a faster speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of the Neural Network. x 1 to x 5 stand for the input token of Bidirectional LSTM. a 1 to a 5 stand for the feature embeddings used in our model.</figDesc><graphic url="image-2.png" coords="3,83.95,62.81,194.38,219.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration for learning segment embeddings based on an extra forward LSTM network, v h , v m and v 1 to v 7 indicate the output vectors of Bidirectional LSTM for head word h, modifier word m and other words in sentence, h h , h m and h 1 to h 7 indicate the hidden vectors of the forward LSTM corresponding to v h , v m and v 1 to v 7 .</figDesc><graphic url="image-3.png" coords="4,76.34,62.81,209.59,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Error rates of different distance between head and modifier on Peen-YM.</figDesc><graphic url="image-4.png" coords="8,72.00,62.81,226.78,158.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Atomic features in our basic model and 
Pei's 1st-order atomic model. w is short for word 
and p for POS tag. h indicates head and m indi-
cates modifier. The subscript represents the rela-
tive position to the center word. dis(h, m) is the 
distance between head and modifier. v h and v m in-
dicate the outputs of Bidirectional LSTM for head 
word and modifier word. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Comparison with previous graph-based models on Penn-YM.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Model performance of different way to 
learn segment embeddings. 

</table></figure>

			<note place="foot" n="1"> A word pair is limited to the dependency pair (h, m) in our work since we use only first-order factorization. In previous work, word pair could be any pair with particular relation (e.g., sibling pair (s, m) in Figure 1).</note>

			<note place="foot" n="2"> http://stp.lingfil.uu.se/nivre/ research/Penn2Malt.html 3 http://nlp.stanford.edu/software/ lex-parser.shtml</note>

			<note place="foot" n="4"> http://sourceforge.net/projects/ mstparser 5 Following previous work, a token is a punctuation if its POS tag is {&quot; &quot; : , .} 6 https://github.com/Williammed/ DeepParser</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The best of both worlds: a graph-based completion model for transition-based parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the European Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Experiments with a higherorder projective dependency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="957" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bilexical Grammars and their Cubic-Time Parsing Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer Netherlands</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>English gigaword. Linguistic Data Consortium</publisher>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fourth-order dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2012, 24th International Conference on Computational Linguistics, Proceedings of the Conference: Posters, 8-15 December 2012</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="785" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="454" />
			<pubPlace>Lrec</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
		<editor>EACL. Citeseer</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maxmargin tensor neural network for chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="293" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An effective neural network model for graph-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing System</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1412.7449</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Statistical dependency analysis with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized higher-order dependency parsing with cube pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="320" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enforcing structural diversity in cube-pruned dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="656" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online learning for inexact hypergraph search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Emnlp</title>
		<meeting>Emnlp</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning for Chinese word segmentation and POS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
