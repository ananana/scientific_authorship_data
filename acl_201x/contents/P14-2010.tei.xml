<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sprinkling Topics for Weakly Supervised Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Hingmire</surname></persName>
							<email>swapnil.hingmire@tcs.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tata Research Development and Design Center</orgName>
								<orgName type="laboratory">Systems Research Lab</orgName>
								<address>
									<settlement>Pune</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<region>Chennai</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutanu</forename><surname>Chakraborti</surname></persName>
							<email>sutanuc@cse.iitm.ac.in</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Madras</orgName>
								<address>
									<region>Chennai</region>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sprinkling Topics for Weakly Supervised Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="55" to="60"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Supervised text classification algorithms require a large number of documents labeled by humans, that involve a labor-intensive and time consuming process. In this paper, we propose a weakly supervised algorithm in which supervision comes in the form of labeling of Latent Dirichlet Allocation (LDA) topics. We then use this weak supervision to &quot;sprin-kle&quot; artificial words to the training documents to identify topics in accordance with the underlying class structure of the corpus based on the higher order word associations. We evaluate this approach to improve performance of text classification on three real world datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In supervised text classification learning algo- rithms, the learner (a program) takes human la- beled documents as input and learns a decision function that can classify a previously unseen doc- ument to one of the predefined classes. Usually a large number of documents labeled by humans are used by the learner to classify unseen documents with adequate accuracy. Unfortunately, labeling a large number of documents is a labor-intensive and time consuming process.</p><p>In this paper, we propose a text classification algorithm based on Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) which does not need la- beled documents. LDA is an unsupervised prob- abilistic topic model and it is widely used to dis- cover latent semantic structure of a document col- lection by modeling words in the documents. <ref type="bibr" target="#b1">Blei et al. (Blei et al., 2003)</ref> used LDA topics as fea- tures in text classification, but they use labeled documents while learning a classifier. sLDA <ref type="bibr" target="#b0">(Blei and McAuliffe, 2007)</ref>, DiscLDA ( <ref type="bibr" target="#b13">Lacoste-Julien et al., 2008</ref>) and MedLDA ( <ref type="bibr" target="#b19">Zhu et al., 2009</ref>) are few extensions of LDA which model both class labels and words in the documents. These models can be used for text classification, but they need expensive labeled documents.</p><p>An approach that is less demanding in terms of knowledge engineering is ClassifyLDA <ref type="bibr" target="#b10">(Hingmire et al., 2013)</ref>. In this approach, a topic model on a given set of unlabeled training documents is constructed using LDA, then an annotator assigns a class label to some topics based on their most probable words. These labeled topics are used to create a new topic model such that in the new model topics are better aligned to class labels. A class label is assigned to a test document on the ba- sis of its most prominent topics. We extend Clas- sifyLDA algorithm by "sprinkling" topics to unla- beled documents.</p><p>Sprinkling ( <ref type="bibr">Chakraborti et al., 2007</ref>) integrates class labels of documents into Latent Semantic In- dexing (LSI) <ref type="bibr" target="#b5">(Deerwester et al., 1990</ref>). The ba- sic idea involves encoding of class labels as ar- tificial words which are "sprinkled" (appended) to training documents. As LSI uses higher or- der word associations ( <ref type="bibr" target="#b12">Kontostathis and Pottenger, 2006</ref>), sprinkling of artificial words gives better and class-enriched latent semantic structure. How- ever, Sprinkled LSI is a supervised technique and hence it requires expensive labeled documents. The paper revolves around the idea of labeling top- ics (which are far fewer in number compared to documents) as in ClassifyLDA, and using these la- beled topic for sprinkling.</p><p>As in ClassifyLDA, we ask an annotator to as- sign class labels to a set of topics inferred on the unlabeled training documents. We use the labeled topics to find probability distribution of each train- ing document over the class labels. We create a set of artificial words corresponding to a class la- bel and add (or sprinkle) them to the document. The number of such artificial terms is propor-tional to the probability of generating the docu- ment by the class label. We then infer a set of topics on the sprinkled training documents. As LDA uses higher order word associations ( <ref type="bibr" target="#b14">Lee et al., 2010</ref>) while discovering topics, we hypothe- size that sprinkling will improve text classification performance of ClassifyLDA. We experimentally verify this hypothesis on three real world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several researchers have proposed semi- supervised text classification algorithms with the aim of reducing the time, effort and cost involved in labeling documents. These algorithms can be broadly categorized into three categories depending on how supervision is provided. In the first category, a small set of labeled documents and a large set of unlabeled documents is used while learning a classifier. Semi-supervised text classification algorithms proposed in ( <ref type="bibr" target="#b16">Nigam et al., 2000</ref>), (Joachims, 1999), ( <ref type="bibr" target="#b18">Zhu and Ghahramani, 2002</ref>) and <ref type="bibr" target="#b2">(Blum and Mitchell, 1998</ref>) are a few examples of this type. However, these algo- rithms are sensitive to initial labeled documents and hyper-parameters of the algorithm.</p><p>In the second category, supervision comes in the form of labeled words (features). ( <ref type="bibr" target="#b15">Liu et al., 2004)</ref> and <ref type="bibr" target="#b6">(Druck et al., 2008</ref>) are a few examples of this type. An important limitation of these algorithms is coming up with a small set of words that should be presented to the annotators for labeling. Also a human annotator may discard or mislabel a pol- ysemous word, which may affect the performance of a text classifier.</p><p>The third type of semi-supervised text classifi- cation algorithms is based on active learning. In active learning, particular unlabeled documents or features are selected and queried to an oracle (e.g. human annotator).( <ref type="bibr" target="#b8">Godbole et al., 2004</ref>), <ref type="bibr" target="#b17">(Raghavan et al., 2006</ref>), <ref type="bibr" target="#b7">(Druck et al., 2009</ref>) are a few ex- amples of active learning based text classification algorithms. However, these algorithms are sensi- tive to the sampling strategy used to query docu- ments or features.</p><p>In our approach, an annotator does not label documents or words, rather she labels a small set of interpretable topics which are inferred in an un- supervised manner. These topics are very few, when compared to the number of documents. As the most probable words of topics are representa- tive of the dataset, there is no need for the annota- tor to search for the right set of features for each class. As LDA topics are semantically more mean- ingful than individual words and can be acquired easily, our approach overcomes limitations of the semi-supervised methods discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LDA</head><p>LDA is an unsupervised probabilistic generative model for collections of discrete data such as text documents. The generative process of LDA can be described as follows:</p><p>1. for each topic t, draw a distribution over words:</p><formula xml:id="formula_0">φt ∼ Dirichlet(βw) 2. for each document d ∈ D a.</formula><p>Draw a vector of topic proportions:</p><formula xml:id="formula_1">θ d ∼ Dirichlet(αt)</formula><p>b. for each word w at position n in d i. Draw a topic assignment:</p><formula xml:id="formula_2">z d,n ∼ Multinomial(θ d )</formula><p>ii. Draw a word:</p><formula xml:id="formula_3">w d,n ∼ Multinomial(z d,n )</formula><p>Where, T is the number of topics, φ t is the word probabilities for topic t, θ d is the topic probabil- ity distribution, z d,n is topic assignment and w d,n is word assignment for nth word position in docu- ment d respectively. α t and β w are topic and word Dirichlet priors.</p><p>The key problem in LDA is posterior inference. The posterior inference involves the inference of the hidden topic structure given the observed doc- uments. However, computing the exact posterior inference is intractable. In this paper we estimate approximate posterior inference using collapsed Gibbs sampling ( <ref type="bibr" target="#b9">Griffiths and Steyvers, 2004</ref>).</p><p>The Gibbs sampling equation used to update the assignment of a topic t to the word w ∈ W at the position n in document d, conditioned on α t , β w is:</p><formula xml:id="formula_4">P (z d,n = t|z d,¬n , w d,n = w, αt, βw) ∝ ψ w,t + β w − 1 v∈W ψ v,t + β v − 1 × (Ω t,d + α t − 1) (1)</formula><p>where ψ w,c is the count of the word w assigned to the topic c, Ω c,d is the count of the topic c assigned to words in the document d and W is the vocabulary of the corpus. We use a subscript d, ¬n to denote the current token, z d,n is ignored in the Gibbs sampling update. After performing collapsed Gibbs sampling using equation 1, we use word topic assignments to compute a point estimate of the distribution over words φ w,c and a point estimate of the posterior distribution over topics for each document d (θ d ) is:</p><formula xml:id="formula_5">φw,t = ψw,t + βw v∈W ψv,t + βv (2) θ t,d = Ω t,d + αt T i=1 Ω i,d + αi<label>(3)</label></formula><p>Let M D =&lt; Z, Φ, Θ &gt; be the hidden topic structure, where Z is per word per document topic assignment, Φ = {φ t } and Θ = {θ d }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sprinkling</head><p>( <ref type="bibr">Chakraborti et al., 2007)</ref> propose a simple ap- proach called "sprinkling" to incorporate class la- bels of documents into LSI. In sprinkling, a set of artificial words are appended to a training docu- ment which are specific to the class label of the document. Consider a case of binary classification with classes c 1 and c 2 . If a document d belongs to the class c 1 then a set of artificial words which represent the class c 1 are appended into the doc- ument d, otherwise a set of artificial words which represent the class c 2 are appended.</p><p>Singular Value Decomposition (SVD) is then performed on the sprinkled training documents and a lower rank approximation is constructed by ignoring dimensions corresponding to lower singular values. Then, the sprinkled terms are removed from the lower rank approximation. ( <ref type="bibr">Chakraborti et al., 2007)</ref> empirically show that sprinkled words boost higher order word associ- ations and projects documents with same class la- bels close to each other in latent semantic space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Topic Sprinkling in LDA</head><p>In our text classification algorithm, we first infer a set of topics on the given unlabeled document cor- pus. We then ask a human annotator to assign one or more class labels to the topics based on their most probable words. We use these labeled topics to create a new LDA model as follows. If the topic assigned to the word w at the position n in docu- ment d is t, then we replace it by the class label assigned to the topic t. If more than one class la- bels are assigned to the topic t, then we randomly select one of the class labels assigned to the topic t. If the annotator is unable to label a topic then we randomly select a class label from the set of all class labels. We then update the new LDA model using collapsed Gibbs sampling.</p><p>We use this new model to infer the probability distribution of each unlabeled training document over the class labels. Let, θ c,d be the probability of generating document d by class c. We then sprin- kle s artificial words of class label c to document d, such that s = K * θ c,d for some constant K.</p><p>We then infer a set of |C| number of topics on the sprinkled dataset using collapsed Gibbs sam- pling, where C is the set of class labels of the training documents. We modify collapsed Gibbs sampling update in Equation 1 to carry class label information while inferring topics. If a word in a document is a sprinkled word then while sampling a class label for it, we sample the class label asso- ciated with the sprinkled word, otherwise we sam- ple a class label for the word using Gibbs update in Equation 1.</p><p>We name this model as Topic Sprinkled LDA (TS-LDA). While classifying a test document, its probability distribution over class labels is inferred using TS-LDA model and it is classified to its most probable class label. Algorithm for TS-LDA is summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>We determine the effectiveness of our algorithm in relation to ClassifyLDA algorithm proposed in ( <ref type="bibr" target="#b10">Hingmire et al., 2013)</ref>. We evaluate and com- pare our text classification algorithm by comput- ing Macro averaged F1. As the inference of LDA is approximate, we repeat all the experiments for each dataset ten times and report average Macro- F1. Similar to ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) we also learn supervised SVM classifier (LDA-SVM) for each dataset using topics as features and report average Macro-F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We use the following datasets in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>20 Newsgroups: This dataset contains messages across twenty newsgroups. In our experiments, we use bydate version of the 20Newsgroup dataset <ref type="bibr">1</ref> . This version of the dataset is divided into training (60%) and test (40%) datasets. We construct classifiers on training datasets and evaluate them on test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>SRAA: Simulated/Real/Aviation/Auto UseNet data 2 : This dataset contains 73,218</p><p>• Input: unlabeled document corpus-D, number of topics-T and number of sprinkled terms-K</p><p>1. Infer T number of topics on D for LDA using col- lapsed Gibbs sampling. Let MD be the hidden topic structure of this model.</p><p>2. Ask an annotator to assign one or more class labels ci ∈ C to a topic based on its 30 most probable words.</p><p>3. Initialization: For nth word in document d ∈ D if z d,n = t and the annotator has labeled topic t with ci then, z d,n = ci 4. Update MD using collapsed Gibbs sampling up- date in Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Sprinkling: For each document d ∈ D:</head><p>(a) Infer a probability distribution θ d over class labels using MD using Equation 3. 6. Infer |C| number of topics on the sprinkled docu- ment corpus D using collapsed Gibbs sampling up- date.</p><p>7. Let M D be the new hidden topic structure. Let us call this hidden structure as TS-LDA. <ref type="table">Table 1</ref>: Algorithm for sprinkling LDA topics for text classification UseNet articles from four discussion groups, for simulated auto racing (sim auto), simulated aviation (sim aviation), real autos (real auto), real aviation (real aviation). Following are the three classification tasks associated with this dataset. 1. sim auto vs sim aviation vs real auto vs real aviation 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Classification of an unlabled document</head><formula xml:id="formula_6">d (a) Infer θ d for document d using M D . (b) k = argmax i θ i,d (c) y d = c k</formula><p>auto (sim auto + real auto) vs aviation (sim aviation + real aviation) 3. simulated (sim auto + sim aviation) vs real (real auto + real aviation) We randomly split SRAA dataset such that 80% is used as training data and remaining is used as test data. 3. WebKB: The WebKB dataset 3 contains 8145 web pages gathered from university computer science departments. The task is to classify the webpages as student, course, faculty or project. We randomly split this dataset such that 80% is used as training and 20% is used as test data.</p><p>We preprocess these datasets by removing HTML tags and stop-words.</p><p>For various subsets of the 20Newsgroups and WebKB datasets discussed above, we choose number of topics as twice the number of classes. For SRAA dataset we infer 8 topics on the train- ing dataset and label these 8 topics for all the three classification tasks. While labeling a topic, we show its 30 most probable words to the human an- notator.</p><p>Similar to ( <ref type="bibr" target="#b9">Griffiths and Steyvers, 2004</ref>), we set symmetric Dirichlet word prior (β w ) for each topic to 0.01 and symmetric Dirichlet topic prior (α t ) for each document to 50/T, where T is number of topics. We set K i.e. maximum number of words sprinkled per class to 10. <ref type="table" target="#tab_1">Table 2</ref> shows experimental results. We can ob- serve that, TS-LDA performs better than Classi- fyLDA in 5 of the total 9 subsets. For the comp- religion-sci dataset TS-LDA and ClassifyLDA have the same performance. However, Classi- fyLDA performs better than TS-LDA for the three classification tasks of SRAA dataset. We can also observe that, performance of TS-LDA is close to supervised LDA-SVM. We should note here that in TS-LDA, the annotator only labels a few topics and not a single document. Hence, our approach exerts a low cognitive load on the annotator, at the same time achieves text classification perfor- mance close to LDA-SVM which needs labeled documents. <ref type="table" target="#tab_2">Table 3</ref> shows most prominent words of four topics inferred on the med-space subset of the 20Newsgroup dataset. We can observe here that most prominent words of the first topic do not rep- resent a single class, while other topics represent either med (medical) or space class. We can say here that, these topics are not "coherent".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Example</head><p>We use these labeled topics and create a TS- LDA model using the algorithm described in <ref type="table">Table  1</ref>. <ref type="table" target="#tab_3">Table 4</ref> shows words corresponding to the top two topics of the TS-LDA model. We can observe here that these two topics are more coherent than the topics in <ref type="table" target="#tab_2">Table 3</ref>.    Hence, we can say here that, in addition to text classification, sprinkling improves coherence of topics.</p><p>We should note here that, in ClassifyLDA, the annotator is able to assign a single class label to a topic. If the annotator assigns a wrong class la- bel to a topic representing multiple classes (e.g. first topic in <ref type="table" target="#tab_2">Table 3</ref>), then it may affect the perfor- mance of the resulting classifier. However, in our approach the annotator can assign multiple class labels to a topic, hence our approach is more flexi- ble for the annotator to encode her domain knowl- edge efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper we propose a novel algorithm that classifies documents based on class labels over few topics. This reduces the need to label a large collection of documents. We have used the idea of sprinkling originally proposed in the context of supervised Latent Semantic Analysis, but the setting here is quite different. Unlike the work in ( <ref type="bibr">Chakraborti et al., 2007</ref>), we do not assume that we have class labels over the set of training documents. Instead, to realize our goal of reduc- ing knowledge acquisition overhead, we propose a way of propagating knowledge of few topic labels to the words and inducing a new topic distribu- tion that has its topics more closely aligned to the class labels. The results show that the approach can yield performance comparable to entirely su- pervised settings. In future work, we also envi- sion the possibility of sprinkling knowledge from background knowledge sources like Wikipedia ( <ref type="bibr" target="#b20">Gabrilovich and Markovitch, 2007)</ref> to realize an alignment of topics to Wikipedia concepts. We would like to study effect of change in number of topics on the text classification performance. We will also explore techniques which will help an- notators to encode their domain knowledge effi- ciently when the topics are not well aligned to the class labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>b) Let, θ c,d be probability of generating docu- ment d by class c. (c) Insert K * θ c,d distinct words associated with the class c to the document d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Experimental results of text classification on various datasets.</head><label>2</label><figDesc></figDesc><table>ID Most prominent words in the 
topic 
Class (med 
/ space) 
0 
science scientific idea large theory 
bit pat thought problem isn 
med 
+ 
space 
1 
information health research medi-
cal water cancer hiv aids children 
institute newsletter 

med 

2 
msg food doctor disease pain 
day treatment blood steve dyer 
medicine symptoms 

med 

3 
space nasa launch earth orbit 
moon shuttle data lunar satellite 
space 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Topic labeling on the med-space subset of the 

20Newsgroup dataset 

ID Most prominent words in the 
topic 
Class (med 
/ space) 
0 
msg medical health food disease 
years problem information doctor 
pain cancer 

med 

1 
space launch earth data orbit 
moon program shuttle lunar satel-
lite 

space 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Topics inferred on the med-space subset of the 

20Newsgroup dataset after sprinkling labeled topics from Ta-
ble 3. 

</table></figure>

			<note place="foot" n="1"> http://qwone.com/ ˜ jason/20Newsgroups/ 2 http://people.cs.umass.edu/ ˜ mccallum/ data.html</note>

			<note place="foot" n="3"> http://www.cs.cmu.edu/ ˜ webkb/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Supervised Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining Labeled and Unlabeled Data with Co-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh annual conference on Computational learning theory</title>
		<meeting>the eleventh annual conference on Computational learning theory</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutanu</forename><surname>Chakraborti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahman</forename><surname>Mukras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Lothian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirmalie</forename><surname>Wiratunga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised Latent Semantic Indexing Using Adaptive Sprinkling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1582" to="1587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Indexing by Latent Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIS</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning from Labeled Features using Generalized Expectation criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Druck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active Learning by Labeling Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Druck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Document Classification through Interactive Supervision of Document and Term Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Shantanu Godbole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Harpale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PKDD</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="185" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Finding Scientific Topics. PNAS</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
	<note>suppl. 1</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document Classification by Topic Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Hingmire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Chougule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><forename type="middle">K</forename><surname>Palshikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutanu</forename><surname>Chakraborti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="877" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Transductive Inference for Text Classification using Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Framework for Understanding Latent Semantic Indexing (LSI) Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">April</forename><surname>Kontostathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">M</forename><surname>Pottenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="73" />
			<date type="published" when="2006-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An Empirical Comparison of Four Text Mining Methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangno</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeki</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">C</forename><surname>Wetherbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 43rd Hawaii International Conference on System Sciences</title>
		<meeting>the 2010 43rd Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Text Classification by Labeling Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th national conference on Artifical intelligence</title>
		<meeting>the 19th national conference on Artifical intelligence</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text Classification from Labeled and Unlabeled Documents using EM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning-Special issue on information retrieval</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<date type="published" when="2000-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Hema Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosie</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jones</surname></persName>
		</author>
		<title level="m">Active Learning with Feedback on Features and Instances. JMLR</title>
		<imprint>
			<date type="published" when="2006-12" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1655" to="1686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning from Labeled and Unlabeled Data with Label Propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MedLDA: Maximum Margin Supervised Topic Models for Regression and Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Computing Semantic Relatedness Using Wikipediabased Explicit Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
