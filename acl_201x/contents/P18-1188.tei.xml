<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document Modeling with External Attention for Sentence Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
							<email>shashi.narayan@ed.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Cardenas</surname></persName>
							<email>ronald.cardenas@matfyz.cz</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Papasarantopoulos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangsheng</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Edinburgh</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh</orgName>
								<orgName type="institution" key="instit3">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Document Modeling with External Attention for Sentence Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">July 15 -20, 2018. c 2018 Association for Computational Linguistics 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Document modeling is essential to a variety of natural language understanding tasks. We propose to use external information to improve document modeling for problems that can be framed as sentence extraction. We develop a framework composed of a hierarchical document encoder and an attention-based ex-tractor with attention over external information. We evaluate our model on extrac-tive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question). We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks have become one of the most widely used models in natural lan- guage processing (NLP). A number of variants of RNNs such as Long Short-Term Memory networks (LSTM; <ref type="bibr" target="#b9">Hochreiter and Schmidhuber, 1997)</ref> and Gated Recurrent Unit networks (GRU; <ref type="bibr" target="#b3">Cho et al., 2014)</ref> have been designed to model text capturing long-term dependencies in prob- lems such as language modeling. However, doc- ument modeling, a key to many natural language * The first three authors made equal contributions to this paper. The work was done when the second author was visit- ing Edinburgh. <ref type="bibr">1</ref> Our TensorFlow code and datasets are publicly avail- able at https://github.com/shashiongithub/ Document-Models-with-Ext-Information. understanding tasks, is still an open challenge. Re- cently, some neural network architectures were proposed to capture large context for modeling text <ref type="bibr" target="#b20">(Mikolov and Zweig, 2012;</ref><ref type="bibr" target="#b7">Ghosh et al., 2016;</ref><ref type="bibr" target="#b11">Ji et al., 2015;</ref><ref type="bibr" target="#b35">Wang and Cho, 2016)</ref>. <ref type="bibr" target="#b18">Lin et al. (2015)</ref> and <ref type="bibr" target="#b40">Yang et al. (2016)</ref> proposed a hi- erarchical RNN network for document-level mod- eling as well as sentence-level modeling, at the cost of increased computational complexity. <ref type="bibr" target="#b31">Tran et al. (2016)</ref> further proposed a contextual lan- guage model that considers information at inter- document level.</p><p>It is challenging to rely only on the document for its understanding, and as such it is not sur- prising that these models struggle on problems such as document summarization <ref type="bibr" target="#b2">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b1">Chen et al., 2016;</ref><ref type="bibr" target="#b22">Nallapati et al., 2017;</ref><ref type="bibr" target="#b28">See et al., 2017;</ref><ref type="bibr" target="#b30">Tan and Wan, 2017)</ref> and machine reading comprehension ( <ref type="bibr" target="#b32">Trischler et al., 2016;</ref><ref type="bibr" target="#b21">Miller et al., 2016;</ref><ref type="bibr" target="#b38">Weissenborn et al., 2017;</ref><ref type="bibr" target="#b10">Hu et al., 2017;</ref>. In this pa- per, we formalize the use of external information to further guide document modeling for end goals.</p><p>We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with "external attention." Our model includes a neural hierarchical docu- ment encoder (or a machine reader) and a hier- archical attention-based sentence extractor. Our hierarchical document encoder resembles the ar- chitectures proposed by <ref type="bibr" target="#b2">Cheng and Lapata (2016)</ref> and <ref type="bibr" target="#b24">Narayan et al. (2018)</ref> in that it derives the doc- ument meaning representation from its sentences and their constituent words. Our novel sentence extractor combines this document meaning repre- sentation with an attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) over the external information to label sentences from the input document. Our model ex- plicitly biases the extractor with external cues and implicitly biases the encoder through training.</p><p>We demonstrate the effectiveness of our model on two problems that can be naturally framed as sentence extraction with external information. These two problems, extractive document summa- rization and answer selection for machine reading comprehension, both require local and global con- textual reasoning about a given document. Extrac- tive document summarization systems aim at cre- ating a summary by identifying (and subsequently concatenating) the most important sentences in a document, whereas answer selection systems se- lect the candidate sentence in a document most likely to contain the answer to a query. For docu- ment summarization, we exploit the title and im- age captions which often appear with documents (specifically newswire articles) as external infor- mation. For answer selection, we use word overlap features, such as the inverse sentence frequency <ref type="bibr">(ISF, Trischler et al., 2016</ref>) and the inverse doc- ument frequency (IDF) together with the query, all formulated as external cues.</p><p>Our main contributions are three-fold: First, our model ensures that sentence extraction is done in a larger (rich) context, i.e., the full document is read first before we start labeling its sentences for extraction, and each sentence labeling is done by implicitly estimating its local and global relevance to the document and by directly attending to some external information for importance cues.</p><p>Second, while external information has been shown to be useful for summarization systems using traditional hand-crafted features <ref type="bibr" target="#b5">(Edmundson, 1969;</ref><ref type="bibr" target="#b14">Kupiec et al., 1995;</ref><ref type="bibr" target="#b19">Mani, 2001</ref>), our model is the first to exploit such information in deep learning-based summarization. We evalu- ate our models automatically (in terms of ROUGE scores) on the CNN news highlights dataset <ref type="bibr" target="#b8">(Hermann et al., 2015)</ref>. Experimental results show that our summarizer, informed with title and im- age captions, consistently outperforms summariz- ers that do not use this information. We also con- duct a human evaluation to judge which type of summary participants prefer. Our results over- whelmingly show that human subjects find our summaries more informative and complete.</p><p>Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be "read" to produce high quality extracts al- lowing a rich contextual reasoning, in contrast to previous answer selection approaches that often measure a score between each sentence in the doc- ument and the question and return the sentence with highest score in an isolated manner <ref type="bibr" target="#b41">(Yin et al., 2016;</ref>. Our model with ISF and IDF scores as ex- ternal features achieves competitive results for an- swer selection. Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-of- the-art results on the popular question answering datasets WikiQA ( ) and NewsQA ( <ref type="bibr" target="#b32">Trischler et al., 2016)</ref>, and it obtains comparable results to the state of the art for SQuAD ( <ref type="bibr" target="#b26">Rajpurkar et al., 2016</ref>). We also evaluate our approach on the MSMarco dataset <ref type="bibr" target="#b25">(Nguyen et al., 2016)</ref> and elab- orate on the behavior of our machine reader in a scenario where each candidate answer sentence is contextually independent of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Document Modeling For Sentence Extraction</head><p>Given a document D consisting of a sequence of n sentences (s 1 , s 2 , ..., s n ) , we aim at labeling each sentence s i in D with a label y i ∈ {0, 1} where y i = 1 indicates that s i is extraction-worthy and 0 otherwise. Our architecture resembles those pre- viously proposed in the literature <ref type="bibr" target="#b2">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b22">Nallapati et al., 2017</ref>). The main com- ponents include a sentence encoder, a document encoder, and a novel sentence extractor (see <ref type="figure" target="#fig_1">Fig- ure 1</ref>) that we describe in more detail below. The novel characteristics of our model are that each sentence is labeled by implicitly estimating its (lo- cal and global) relevance to the document and by directly attending to some external information for importance cues.</p><p>Sentence Encoder A core component of our model is a convolutional sentence encoder <ref type="bibr" target="#b12">(Kim, 2014;</ref><ref type="bibr" target="#b13">Kim et al., 2016</ref>) which encodes sentences into continuous representations. We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature. This filter is applied to each possible window of words in s to pro- duce a feature map f ∈ R k−h+1 where k is the sentence length. We then apply max-pooling over time over the feature map f and take the maximum value as the feature corresponding to this particu- lar filter K. We use multiple kernels of various sizes and each kernel multiple times to construct the representation of a sentence. In <ref type="figure" target="#fig_1">Figure 1</ref>, ker-nels of size 2 (red) and 4 (blue) are applied three times each. The max-pooling over time operation yields two feature lists f K 2 and f K 4 ∈ R 3 . The final sentence embeddings have six dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document Encoder</head><p>The document encoder composes a sequence of sentences to obtain a doc- ument representation. We use a recurrent neural network with LSTM cells to avoid the vanishing gradient problem when training long sequences <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref>. Given a document D consisting of a sequence of sentences (s 1 , s 2 , . . . , s n ), we follow common practice and feed the sentences in reverse order <ref type="bibr" target="#b29">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b6">Filippova et al., 2015</ref>).</p><p>Sentence Extractor Our sentence extractor se- quentially labels each sentence in a document with 1 or 0 by implicitly estimating its relevance in the document and by directly attending to the external information for importance cues. It is im- plemented with another RNN with LSTM cells with an attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) and a softmax layer. Our attention mech- anism differs from the standard practice of attend- ing intermediate states of the input (encoder). In- stead, our extractor attends to a sequence of p pieces of external information E : (e 1 , e 2 , ..., e p ) relevant for the task (e.g., e i is a title or an im- age caption for summarization) for cues. At time t i , it reads sentence s i and makes a binary predic- tion, conditioned on the document representation (obtained from the document encoder), the previ- ously labeled sentences and the external informa- tion. This way, our labeler is able to identify lo- cally and globally important sentences within the document which correlate well with the external information. Given sentence s t at time step t, it returns a probability distribution over labels as:</p><formula xml:id="formula_0">p(y t |s t , D, E) = softmax(g(h t , h t )) (1) g(h t , h t ) = U o (V h h t + W h h t ) (2) h t = LSTM(s t , h t−1 ) h t = p i=1 α (t,i) e i , where α (t,i) = exp(h t e i ) j exp(h t e j )</formula><p>where g(·) is a single-layer neural network with parameters</p><formula xml:id="formula_1">U o , V h and W h . h t is an intermedi- Document encoder s5 s4 s3 s2 s1</formula><p>Sentence Extractor</p><formula xml:id="formula_2">s1 s2 s3 s4 s5 y1 y2 y3 y4 y5</formula><p>Convolutional Sentence encoder Document External s5 s4 s3 s2 s1 e1 e2 e3</p><p>North Korea fired a missile over Japan  ate RNN state at time step t. The dynamic con- text vector h t is essentially the weighted sum of the external information (e 1 , e 2 , . . . , e p ). <ref type="figure" target="#fig_1">Figure 1</ref> summarizes our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentence Extraction Applications</head><p>We validate our model on two sentence extrac- tion problems: extractive document summariza- tion and answer selection for machine reading comprehension. Both these tasks require local and global contextual reasoning about a given docu- ment. As such, they test the ability of our model to facilitate document modeling using external in- formation.</p><p>Extractive Summarization An extractive sum- marizer aims to produce a summary S by select- ing m sentences from D (where m &lt; n). In this setting, our sentence extractor sequentially predicts label y i ∈ {0, 1} (where 1 means that s i should be included in the summary) by as- signing score p(y i |s i , D, E , θ) quantifying the rel- evance of s i to the summary. We assemble a summary S by selecting m sentences with top p(y i = 1|s i , D, E , θ) scores.</p><p>We formulate external information E as the se- quence of the title and the image captions associ- ated with the document. We use the convolutional sentence encoder to get their sentence-level repre- sentations.</p><p>Answer Selection Given a question q and a doc- ument D , the goal of the task is to select one candidate sentence s i ∈ D in which the answer exists. In this setting, our sentence extractor se- quentially predicts label y i ∈ {0, 1} (where 1 means that s i contains the answer) and assign score p(y i |s i , D, E , θ) quantifying s i 's relevance to the query. We return as answer the sentence s i with the highest p(y i = 1|s i , D, E , θ) score.</p><p>We treat the question q as external information and use the convolutional sentence encoder to get its sentence-level representation. This simplifies Eq. (1) and (2) as follow:</p><formula xml:id="formula_3">p(y t |s t , D, q) = softmax(g(h t , q)) (3) g(h t , q) = U o (V h h t + W q q),</formula><p>where V h and W q are network parameters. We ex- ploit the simplicity of our model to further assimi- late external features relevant for answer selection: the inverse sentence frequency <ref type="bibr">(ISF, (Trischler et al., 2016)</ref>), the inverse document frequency (IDF) and a modified version of the ISF score which we call local ISF. <ref type="bibr" target="#b32">Trischler et al. (2016)</ref> have shown that a simple ISF baseline (i.e., a sen- tence with the highest ISF score as an answer) correlates well with the answers. The ISF score α s i for the sentence s i is computed as α s i = w∈s i ∩q IDF(w), where IDF is the inverse doc- ument frequency score of word w, defined as: IDF(w) = log N Nw , where N is the total number of sentences in the training set and N w is the number of sentences in which w appears. Note that, s i ∩ q refers to the set of words that appear both in s i and in q. Local ISF is calculated in the same manner as the ISF score, only with setting the total num- ber of sentences (N ) to the number of sentences in the article that is being analyzed.</p><p>More formally, this modifies Eq. (3) as follows:</p><formula xml:id="formula_4">p(y t |s t , D, q) = softmax(g(h t , q, α t , β t , γ t )),<label>(4)</label></formula><p>where α t , β t and γ t are the ISF, IDF and local ISF scores (real values) of sentence s t respectively .</p><p>The function g is calculated as follows:</p><formula xml:id="formula_5">g(h t , q, α t , β t , γ t ) =U o (V h h t + W q q + W isf (α t · 1)+ W idf (β t · 1) + W lisf (γ t · 1) ,</formula><p>where W isf , W idf and W lisf are new parameters added to the network and 1 is a vector of 1s of size equal to the sentence embedding size. In <ref type="figure" target="#fig_1">Figure  1</ref>, these external feature vectors are represented as 6-dimensional gray vectors accompanied with dashed arrows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>This section presents our experimental setup and results assessing our model in both the extractive summarization and answer selection setups. In the rest of the paper, we refer to our model as XNET for its ability to exploit eXternal information to improve document representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Extractive Document Summarization</head><p>Summarization Dataset We evaluated our models on the CNN news highlights dataset (Her- mann et al., 2015). <ref type="bibr">2</ref> We used the standard splits of <ref type="bibr" target="#b8">Hermann et al. (2015)</ref> for training, validation, and testing (90,266/1,220/1,093 documents). We followed previous studies <ref type="bibr" target="#b2">(Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b23">Nallapati et al., 2016</ref><ref type="bibr" target="#b22">Nallapati et al., , 2017</ref><ref type="bibr" target="#b28">See et al., 2017;</ref><ref type="bibr" target="#b30">Tan and Wan, 2017</ref>) in assuming that the "story highlights" associated with each article are gold-standard abstractive summaries. We trained our network on a named-entity-anonymized version of news articles. However, we generated deanonymized summaries and evaluated them against gold summaries to facilitate human evalu- ation and to make human evaluation comparable to automatic evaluation. To train our model, we need documents anno- tated with sentence extraction information, i.e., each sentence in a document is labeled with 1 (summary-worthy) or 0 (not summary-worthy). We followed <ref type="bibr" target="#b22">Nallapati et al. (2017)</ref> and automat- ically extracted ground truth labels such that all positively labeled sentences from an article col- lectively give the highest ROUGE ( <ref type="bibr" target="#b16">Lin and Hovy, 2003</ref>) score with respect to the gold summary.</p><p>We used a modified script of <ref type="bibr" target="#b8">Hermann et al. (2015)</ref> to extract titles and image captions, and we associated them with the corresponding arti- cles. All articles get associated with their titles. The availability of image captions varies from 0 to 414 per article, with an average of 3 image cap- tions. There are 40% CNN articles with at least one image caption.</p><p>All sentences, including titles and image cap- tions, were padded with zeros to a sentence length of 100. All input documents were padded with zeros to a maximum document length of 126. For each document, we consider a maximum of 10 im- age captions. We experimented with various num- bers <ref type="bibr">(1, 3, 5, 10 and 20)</ref> of image captions on the validation set and found that our model performed best with 10 image captions. We refer the reader to the supplementary material for more implemen- tation details to replicate our results.</p><p>Comparison Systems We compared the output of our model against the standard baseline of sim- ply selecting the first three sentences from each document as the summary. We refer to this base- line as LEAD in the rest of the paper.</p><p>We also compared our system against the sen- tence extraction system of <ref type="bibr" target="#b2">Cheng and Lapata (2016)</ref>. We refer to this system as POINTERNET as the neural attention architecture in <ref type="bibr" target="#b2">Cheng and Lapata (2016)</ref>  Automatic Evaluation To automatically assess the quality of our summaries, we used ROUGE ( <ref type="bibr" target="#b16">Lin and Hovy, 2003)</ref>, a recall-oriented met- ric, to compare our model-generated summaries to manually-written highlights. <ref type="bibr">6</ref> Previous work has reported ROUGE-1 (R1) and ROUGE-2 (R2) scores to access informativeness, and ROUGE-L (RL) to access fluency. In addition to R1, R2 and RL, we also report ROUGE-3 (R3) and ROUGE-4 (R4) capturing higher order n-grams overlap to as- sess informativeness and fluency simultaneously.</p><p>teresting direction of research but we do not pursue it here. It requires decoding with multiple types of attentions and this is not the focus of this paper. <ref type="bibr">5</ref> We are unable to compare our results to the extractive system of Nallapati et al. (2017) because they report their re- sults on the DailyMail dataset and their code is not available. The abstractive systems of <ref type="bibr" target="#b1">Chen et al. (2016)</ref> and <ref type="bibr" target="#b30">Tan and Wan (2017)</ref> report their results on the CNN dataset, however, their results are not comparable to ours as they report on the full-length F1 variants of ROUGE to evaluate their abstrac- tive summaries. We report ROUGE recall scores which is more appropriate to evaluate our extractive summaries. <ref type="bibr">6</ref> We used pyrouge, a Python package, to compute all our ROUGE scores with parameters "-a -c 95 -m -n 4 -w 1.2."</p><p>We report our results on both full length (three sentences with the top scores as the summary) and fixed length (first 75 bytes and 275 bytes as the summary) summaries. For full length summaries, our decision of selecting three sentences is guided by the fact that there are 3.11 sentences on aver- age in the gold highlights of the training set. We conduct our ablation study on the validation set with full length ROUGE scores, but we report both fixed and full length ROUGE scores for the test set.</p><p>We experimented with two types of external information: title (TITLE) and image captions (CAPTION). In addition, we experimented with the first sentence (FS) of the document as external in- formation. Note that the latter is not external in- formation, it is a sentence in the document. How- ever, we wanted to explore the idea that the first sentence of the document plays a crucial part in generating summaries ( <ref type="bibr" target="#b27">Rush et al., 2015;</ref><ref type="bibr" target="#b23">Nallapati et al., 2016)</ref>. XNET with FS acts as a baseline for XNET with title and image captions.</p><p>We report the performance of several variants of XNET on the validation set in <ref type="table">Table 1</ref>. We also compare them against the LEAD baseline and POINTERNET. These two systems do not use any additional information. Interestingly, all the vari- ants of XNET significantly outperform LEAD and POINTERNET. When the title (TITLE), image cap- tions (CAPTION) and the first sentence (FS) are used separately as additional information, XNET performs best with TITLE as its external informa- tion. Our result demonstrates the importance of the title of the document in extractive summariza- tion <ref type="bibr" target="#b5">(Edmundson, 1969;</ref><ref type="bibr" target="#b14">Kupiec et al., 1995;</ref><ref type="bibr" target="#b19">Mani, 2001</ref>). The performance with TITLE and CAP- TION is better than that with FS. We also tried possible combinations of TITLE, CAPTION and FS. All XNET models are superior to the ones with- out any external information. XNET performs best when TITLE and CAPTION are jointly used as ex- ternal information (55.4%, 21.8%, 11.8%, 7.5%, and 49.2% for R1, R2, R3, R4, and RL respec- tively). It is better than the the LEAD baseline by 3.7 points on average and than POINTERNET by 1.8 points on average, indicating that external in- formation is useful to identify the gist of the doc- ument. We use this model for testing purposes.</p><p>Our final results on the test set are shown in   to XNET. This result could be because LEAD (al- ways) and POINTERNET (often) include the first sentence in their summaries, whereas, XNET is better capable at selecting sentences from vari- ous document positions. This is not captured by smaller summaries of 75 bytes, but it becomes more evident with longer summaries (275 bytes and full length) where XNET performs best across all ROUGE scores. We note that POINTERNET outperforms LEAD for 75-byte summaries, then its performance drops behind LEAD for 275-byte summaries, but then it outperforms LEAD for full length summaries on the metrics R1, R2 and RL. It shows that POINTERNET with its attention over sentences in the document is capable of exploring more than first few sentences in the document, but it is still behind XNET which is better at identi- fying salient sentences in the document. XNET performs significantly better than POINTERNET by 0.8 points for 275-byte summaries and by 1.9 points for full length summaries, on average for all ROUGE scores.</p><p>Human Evaluation We complement our auto- matic evaluation results with human evaluation. We randomly selected 20 articles from the test set.</p><p>Annotators were presented with a news article and summaries from four different systems. These in- clude the LEAD baseline, POINTERNET, XNET and the human authored highlights. We followed the guidelines in <ref type="bibr" target="#b2">Cheng and Lapata (2016)</ref>, and asked our participants to rank the summaries from best (1st) to worst (4th) in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary writ- ten in well-formed English?). We did not allow any ties and we only sampled articles with non- identical summaries. We assigned this task to five annotators who were proficient English speakers. Each annotator was presented with all 20 articles. The order of summaries to rank was randomized per article. An example of summaries our subjects ranked is provided in the supplementary material.</p><p>The results of our human evaluation study are shown in <ref type="table" target="#tab_3">Table 3</ref>. As one might imagine, HUMAN gets ranked 1st most of the time (41%). How- ever, it is closely followed by XNET which ranked 1st 28% of the time. In comparison, POINTER- NET and LEAD were mostly ranked at 3rd and 4th places. We also carried out pairwise com- parisons between all models in <ref type="table" target="#tab_3">Table 3</ref> for their statistical significance using a one-way ANOVA with post-hoc Tukey HSD tests with (p &lt; 0.01). It showed that XNET is significantly better than LEAD and POINTERNET, and it does not differ significantly from HUMAN. On the other hand, POINTERNET does not differ significantly from LEAD and it differs significantly from both XNET and HUMAN. The human evaluation results cor- roborates our empirical results in <ref type="table" target="#tab_1">Table 1 and Ta- ble 2:</ref> XNET is better than LEAD and POINT- ERNET in producing informative and fluent sum- maries. NewsQA was especially designed to present lexical and syntactic divergence between ques- tions and answers. It contains 119,633 questions posed by crowdworkers on 12,744 CNN articles previously collected by <ref type="bibr" target="#b8">Hermann et al. (2015)</ref>. In a similar manner, SQuAD associates 100,000+ question with a Wikipedia article's first paragraph, for 500+ previously chosen articles. WikiQA was collected by mining web-searching query logs and then associating them with the summary section of the Wikipedia article presumed to be related to the topic of the query. A similar collection procedure was followed to create MSMarco with the differ- ence that each candidate answer is a whole para- graph from a different browsed website associated with the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Answer Selection</head><p>We follow the widely used setup of leaving out unanswered questions <ref type="bibr" target="#b32">(Trischler et al., 2016;</ref> and adapt the format of each dataset to our task of answer sentence selection by label- ing a candidate sentence with 1 if any answer span is contained in that sentence. In the case of MS- Marco, each candidate paragraph comes associ- ated with a label, hence we treat each one as a sin- gle long sentence. Since SQuAD keeps the official test dataset hidden and MSMarco does not provide labels for its released test set, we report results on their official validation sets. For validation, we set apart 10% of each official training set.</p><p>Our dataset splits consist of 92,525, 5,165 and 5,124 samples for NewsQA; <ref type="bibr">79,032, 8,567, and 10,570 for SQuAD; 873, 122, and 237 for WikiQA; and 79,704, 9,706, and 9,650</ref> for MSMarco, for training, validation, and testing respectively.</p><p>Comparison Systems We compared the output of our model against the ISF ( <ref type="bibr" target="#b32">Trischler et al., 2016)</ref> and LOCALISF baselines. Given an ar- ticle, the sentence with the highest ISF score is selected as an answer for the ISF baseline and the sentence with the highest local ISF score for the LOCALISF baseline. We also compare our model against a neural network (PAIRCNN) that encodes (question, candidate) in an isolated man- ner as in previous work <ref type="bibr" target="#b41">(Yin et al., 2016;</ref>). The architec- ture uses the sentence encoder explained in earlier sections to learn the question and candidate repre- sentations. The distribution over labels is given by p(y t |q) = p(y t |s t , q) = softmax(g(s t , q)) where g(s t , q) = ReLU(W sq · [s t ; q] + b sq ). In ad- dition, we also compare our model against AP- CNN (dos , <ref type="bibr">ABCNN (Yin et al., 2016</ref>), L.D.C ( <ref type="bibr" target="#b34">Wang and Jiang, 2017)</ref>, KV- MemNN ( <ref type="bibr" target="#b21">Miller et al., 2016)</ref>, and COMPAGGR, a state-of-the-art system by .</p><p>We experiment with several variants of our model. XNET is the vanilla version of our sen-  <ref type="table" target="#tab_1">AP-CNN  - - - - 68.86 69.57  - - - - - - ABCNN  - - - - 69.21 71.08  - - - - - - L.D.C  - - - - 70.58 72.26  - - - - - - KV-MemNN  - - - - 70.69 72.65  - - - - - - LOCALISF</ref>    <ref type="bibr" target="#b34">Wang and Jiang, 2017)</ref>, KV-MemNN ( <ref type="bibr" target="#b21">Miller et al., 2016)</ref>, and COMPAGGR, a state-of-the-art system by . (WGT) WRD CNT stands for the (weighted) word count baseline. See text for more details.</p><p>tence extractor conditioned only on the query q as external information (Eq. <ref type="formula">(3)</ref>). XNET+ is an extension of XNET which uses ISF, IDF and lo- cal ISF scores in addition to the query q as exter- nal information (Eqn. <ref type="formula" target="#formula_4">(4)</ref>). We also experimented with a baseline XNETTOPK where we choose the top k sentences with highest ISF score, and then among them choose the one with the highest prob- ability according to XNET. In our experiments, we set k = 5. In the end, we experimented with an ensemble network LRXNET which com- bines the XNET score, the COMPAGGR score and other word-overlap-based scores (tweaked and op- timized for each dataset separately) for each sen- tence using a logistic regression classifier. It uses ISF and LocalISF scores for NewsQA, IDF and ISF scores for SQuAD, sentence length, IDF and ISF scores for WikiQA, and word overlap and ISF score for MSMarco. We refer the reader to the supplementary material for more implementation and optimization details to replicate our results.</p><p>Evaluation Metrics We consider metrics that evaluate systems that return a ranked list of can- didate answers: mean average precision (MAP), mean reciprocal rank (MRR), and accuracy (ACC).</p><p>Results <ref type="table" target="#tab_6">Table 4</ref> gives the results for the test sets of NewsQA and WikiQA, and the original vali- dation sets of SQuAD and MSMarco. Our first observation is that XNET outperforms PAIRCNN, supporting our claim that it is beneficial to read the whole document in order to make decisions, instead of only observing each candidate in isola- tion. Secondly, we can observe that ISF is indeed a strong baseline that outperforms XNET. This means that just "reading" the document using a vanilla version of XNET is not sufficient, and help is required through a coarse filtering. Indeed, we observe that XNET+ outperforms all baselines except for COMPAGGR. Our ensemble model LRXNET can ultimately surpass COMPAGGR on majority of the datasets.</p><p>This consistent behavior validates the machine reading capabilities and the improved document representation with external features of our model for answer selection. Specifically, the combination of document reading and word overlap features is required to be done in a soft manner, using a clas- sification technique. Using it as a hard constraint, with XNETTOPK, does not achieve the best re- sult. We believe that often the ISF score is a bet- ter indicator of answer presence in the vicinity of certain candidate instead of in the candidate itself. As such, XNET+ is capable of using this feature in datasets with richer context.</p><p>It is worth noting that the improvement gained by LRXNET over the state-of-the-art follows a pattern. For the SQuAD dataset, the results are comparable (less than 1%). However, the im- provement for WikiQA reaches ∼3% and then the gap shrinks again for NewsQA, with an improve- ment of ∼1%. This could be explained by the fact that each sample of the SQuAD is a paragraph, compared to an article summary for WikiQA, and to an entire article for NewsQA. Hence, we further strengthen our hypothesis that a richer context is needed to achieve better results, in this case ex- pressed as document length, but as the length of the context increases the limitation of sequential models to learn from long rich sequences arises. <ref type="bibr">7</ref> Interestingly, our model lags behind COM- PAGGR on the MSMarco dataset. It turns out this is due to contextual independence between can- didates in the MSMarco dataset, i.e., each candi- date is a stand-alone paragraph in this dataset, in contrast to contextually dependent candidate sen- tences from a document in the NewsQA, SQuAD and WikiQA datasets. As a result, our models (XNET+ and LRXNET) with document reading abilities perform poorly. This can be observed by the fact that XNET and PAIRCNN obtain com- parable results. COMPAGGR performs better be- cause comparing each candidate independently is a better strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We describe an approach to model documents while incorporating external information that in- forms the representations learned for the sentences in the document. We implement our approach through an attention mechanism of a neural net- work architecture for modeling documents.</p><p>Our experiments with extractive document sum- marization and answer selection tasks validates our model in two ways: first, we demonstrate that external information is important to guide docu- ment modeling for natural language understanding tasks. Our model uses image captions and the title of the document for document summarization, and the query with word overlap features for answer selection and outperforms its counterparts that do not use this information. Second, our external at- tention mechanism successfully guides the learn- ing of the document representation for the relevant end goal. For answer selection, we show that in- serting the query with word overlap features us- ing our external attention mechanism outperforms state-of-the-art systems that naturally also have ac- cess to this information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hierarchical encoder-decoder model for sentence extraction with external attention. s 1 ,. .. , s 5 are sentences in the document and, e 1 , e 2 and e 3 represent external information. For the extractive summarization task, e i s are external information such as title and image captions. For the answers selection task, e i s are the query and word overlap features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Question</head><label></label><figDesc>Answering Datasets We run experi- ments on four datasets collected for open domain question-answering tasks: WikiQA (Yang et al., 2015), SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MSMarco (Nguyen et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 . It turns out that for smaller summaries (75 bytes) LEAD and POINTERNET are superior</head><label>2</label><figDesc></figDesc><table>MODELS 
R1 
R2 
R3 
R4 
RL 
Fixed length: 75b 

LEAD 

20.1 
7.1 
3.5 
2.1 14.6 
POINTERNET 20.3 
7.2 
3.5 
2.2 14.8 
XNET 
20.2 
7.1 
3.4 
2.0 14.6 
Fixed length: 275b 

LEAD 

39.1 14.5 
7.6 
4.7 34.6 
POINTERNET 38.6 13.9 
7.3 
4.4 34.3 
XNET 
39.7 14.7 
7.9 
5.0 35.2 
Full length summaries 

LEAD 

49.3 19.5 10.7 6.9 43.8 
POINTERNET 51.7 19.7 10.6 6.6 45.7 
XNET 
54.2 21.6 12.0 7.9 48.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Final results on the test set. POINTER-
NET is the sentence extraction system of Cheng 
and Lapata. XNET is our best model from Table 
1. Best ROUGE score in each block and each col-
umn is highlighted in boldface. 

Models 
1st 
2nd 
3rd 
4th 

LEAD 

0.15 0.17 0.47 0.21 
POINTERNET 0.16 0.05 0.31 0.48 
XNET 
0.28 0.53 0.15 0.04 

HUMAN 

0.41 0.25 0.07 0.27 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Human evaluations: Ranking of various 
systems. Rank 1st is best and rank 4th, worst. 
Numbers show the percentage of times a system 
gets ranked at a certain position. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>SQuAD WikiQA NewsQA MSMarco ACC MAP MRR ACC MAP MRR ACC MAP MRR ACC MAP MRR WRD CNT 77.84 27.50 27.77 51.05 48.91 49.24 44.67 46.48 46.91 20.16 19.37 19.51 WGT WRD CNT 78.43 28.10 28.38 49.79 50.99 51.32 45.24 48.20 48.64 20.50 20.06 20.23</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results (in percentage) for answer selection comparing our approaches (bottom part) to base-
lines (top): AP-CNN (dos Santos et al., 2016), ABCNN (Yin et al., 2016), L.D.C (</table></figure>

			<note place="foot" n="2"> Hermann et al. (2015) have also released the DailyMail dataset, but we do not report our results on this dataset. We found that the script written by Hermann et al. to crawl DailyMail articles mistakenly extracts image captions as part of the main body of the document. As image captions often do not have sentence boundaries, they blend with the sentences of the document unnoticeably. This leads to the production of erroneous summaries.</note>

			<note place="foot" n="7"> See the supplementary material for an example supporting our hypothesis.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Jianpeng Cheng for providing us with the CNN dataset and the implementation of Point-erNet. We also thank the members of the Edin-burgh NLP group for participating in our human evaluation experiments. This work greatly benefit-ted from discussions with Jianpeng Cheng, Annie Louis, Pedro Balage, Alfonso Mendes, Sebastião Miranda, and members of the Edinburgh NLP group. We gratefully acknowledge the support of the European Research Council (Lapata; award number 681760), the European Union under the Horizon 2020 SUMMA project (Narayan, Cohen; grant agreement 688139), and Huawei Technolo-gies (Cohen).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distraction-based neural networks for modeling documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2754" to="2760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods on Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attentive pooling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR abs/1602.03609</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">New methods in automatic extracting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">P</forename><surname>Edmundson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Computing Machinery</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="285" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentence compression by deletion with LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Contextual LSTM (CLSTM) models for large scale NLP tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<idno>CoRR abs/1602.06291</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno>CoRR abs/1705.02798</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Document context language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<idno>CoRR abs/1511.03962</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A trainable document summarizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Kupiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francine</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="406" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1106" to="1115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using N-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="71" to="78" />
			<pubPlace>Edmonton, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for document modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods on Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic Summarization. Natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>John Benjamins Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Spoken Language Technology Workshop. IEEE</title>
		<meeting>the Spoken Language Technology Workshop. IEEE</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods on Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
	<note>AmirHossein Karimi, Antoine Bordes, and Jason Weston</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SummaRuNNer: A recurrent neural network based sequence model for extractive summarization of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31st AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3075" to="3081" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-tosequence RNNs and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ranking sentences for extractive summarization with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. New Orleans</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. New Orleans<address><addrLine>US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MS Marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches, co-located with the 30th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Workshop on Cognitive Computation: Integrating neural and symbolic approaches, co-located with the 30th Annual Conference on Neural Information Processing Systems<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graph-based attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1171" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inter-document contextual language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Quan Hung Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Zukerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="762" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Newsqa: A machine comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno>CoRR abs/1611.09830</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A compareaggregate model for matching text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Larger-context language modelling with recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1319" to="1329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sentence similarity learning by lexical decomposition and composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1340" to="1349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Making neural QA as simple as possible but not simpler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">WikiQA: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ABCNN: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schtze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
