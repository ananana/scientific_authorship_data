<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning How to Actively Learn: A Deep Imitation Learning Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wray</forename><surname>Buntine</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Information Technology</orgName>
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning How to Actively Learn: A Deep Imitation Learning Approach</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1874" to="1883"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary. We introduce a method that learns an AL policy using imitation learning (IL). Our IL-based approach makes use of an efficient and effective algorithmic expert, which provides the policy learner with good actions in the encountered AL situations. The AL strategy is then learned with a feedforward network, mapping situations to most informative query datapoints. We evaluate our method on two different tasks: text classification and named entity recognition. Experimental results show that our IL-based AL strategy is more effective than strong previous methods using heuristics and reinforcement learning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For many real-world NLP tasks, labeled data is rare while unlabelled data is abundant. Active learning (AL) seeks to learn an accurate model with minimum amount of annotation cost. It is inspired by the observation that a model can get better performance if it is allowed to choose the data points on which it is trained. For example, the learner can identify the areas of the space where it does not have enough knowledge, and query those data points which bridge its knowledge gap.</p><p>Traditionally, AL is performed using engi- neered heuristics in order to estimate the useful- ness of unlabeled data points as queries to an an- notator. Recent work <ref type="bibr" target="#b7">(Fang et al., 2017;</ref><ref type="bibr" target="#b1">Bachman et al., 2017;</ref><ref type="bibr" target="#b26">Woodward and Finn, 2017)</ref> have fo- cused on learning the AL querying strategy, as en- gineered heuristics are not flexible to exploit char- acteristics inherent to a given problem. The basic idea is to cast AL as a decision process, where the most informative unlabeled data point needs to be selected based on the history of previous queries. However, previous works train for the AL pol- icy by a reinforcement learning (RL) formulation, where the rewards are provided at the end of se- quences of queries. This makes learning the AL policy difficult, as the policy learner needs to deal with the credit assignment problem. Intuitively, the learner needs to observe many pairs of query sequences and the resulting end-rewards to be able to associate single queries with their utility scores.</p><p>In this work, we formulate learning AL strate- gies as an imitation learning problem. In par- ticular, we consider the popular pool-based AL scenario, where an AL agent is presented with a pool of unlabelled data. Inspired by the Dataset Aggregation (DAGGER) algorithm <ref type="bibr" target="#b19">(Ross et al., 2011</ref>), we develop an effective AL policy learn- ing method by designing an efficient and effective algorithmic expert, which provides the AL agent with good decisions in the encountered states. We then use a deep feedforward network to learn the AL policy to associate states to actions. Unlike the RL approach, our method can get observa- tions and actions directly from the expert's trajec- tory. Therefore, our trained policy can make better rankings of unlabelled datapoints in the pool, lead- ing to more effective AL strategies.</p><p>We evaluate our method on text classification and named entity recognition. The results show our method performs better than strong AL meth- ods using heuristics and reinforcement learning, in that it boosts the performance of the under- lying model with fewer labelling queries. An open source implementation of our model is avail- able at: https://github.com/Grayming/ ALIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pool-based AL as a Decision Process</head><p>We consider the popular pool-based AL setting where we are given a small set of initial labeled data and a large pool of unlabelled data, and a bud- get for getting the annotation of some unlabelled data by querying an oracle, e.g. a human anno- tator. The goal is to intelligently pick those unla- belled data for which if the annotations were avail- able, the performance of the underlying re-trained model would be improved the most.</p><p>The main challenge in AL is how to identify and select the most beneficial unlabelled data points. Various heuristics have been proposed to guide the unlabelled data selection <ref type="bibr" target="#b20">(Settles, 2010)</ref>. How- ever, there is no one AL heuristic which performs best for all problems. The goal of this paper is to provide an approach to learn an AL strategy which is best suited for the problem at hand, instead of resorting to ad-hoc heuristics.</p><p>The AL strategy can be learned by attempting to actively learn on tasks sampled from a distribu- tion over the tasks ( <ref type="bibr" target="#b1">Bachman et al., 2017</ref>). The idea is to simulate the AL scenario on instances of the problem created using available labeled data, where the label of some part of the data is kept hidden. This allows to have an automatic oracle to reveal the labels of the queried data, resulting in an efficient way to quickly evaluate a hypothe- sised AL strategy. Once the AL strategy is learned on simulations, it is then applied to real AL sce- narios. The more related are the tasks in the real scenario to those used to train the AL strategy, the more effective the AL strategy would be.</p><p>We are interested to train a model m φ φ φ which maps an input x x x ∈ X to its label y y y ∈ Y x x x , where Y x x x is the set of labels for the input x x x and φ φ φ is the pa- rameter vector of the underling model. For exam- ple, in the named entity recognition (NER) task, the input is a sentence and the output is its label se- quence, e.g. in the IBO format. Let D = {(x x x, y y y)} be a support set of labeled data, which is randomly partitioned into labeled D lab , unlabelled D unl , and evaluation D evl datasets. Repeated random parti- tioning creates multiple instances of the AL prob- lem. At each time step t of an AL problem, the algorithm interacts with the oracle and queries the label of a datapoint x x x t ∈ D unl t . As the result of this action, the followings happen:</p><p>• The automatic oracle reveals the label y y y t ;</p><p>• The labeled and unlabelled datasets are up- dated to include and exclude the recently queried data point, respectively;</p><p>• The underlying model is re-trained based on the enlarged labeled data to update φ φ φ; and where loss(y y y , y y y) is the loss incurred due to predicting y y y instead of the ground truth y y y.</p><p>More formally, a pool-based AL problem is a Markov decision process (MDP), denoted by (S, A, P r(s s s t+1 |s s s t , a t ), R) where S is the state space, A is the set of actions, P r(s s s t+1 |s s s t , a t ) is the transition function, and R is the reward func- tion. The state s s s t ∈ S at time t consists of the labeled D lab t and unlabelled D unl t datasets paired with the parameters of the currently trained model φ t . An action a t ∈ A corresponds to the selec- tion of a query datapoint, and the reward function R(s s s t , a t , s s s t+1 ) := −loss(m φ φ φt , D evl ).</p><p>We aim to find the optimal AL policy prescrib- ing which datapoint needs to be queried in a given state to get the most benefit. The optimal policy is found by maximising the following objective over the parameterised policies:</p><formula xml:id="formula_0">E (D lab ,D unl ,D evl )∼D Eπ θ θ θ B t=1 R(s s st, at, s s st+1)<label>(1)</label></formula><p>where π θ θ θ is the policy network parameterised by θ θ θ, D is a distribution over possible AL problem in- stances, and B is the maximum number of queries made in an AL run, a.k.a. an episode. Following ( <ref type="bibr" target="#b1">Bachman et al., 2017)</ref>, we maximise the sum of the rewards after each time step to encourage the anytime behaviour, i.e. the model should perform well after each label query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Imitation Learning to Train the AL Policy</head><p>The question remains as how can we train the policy network to maximise the training objec- tive in eqn 1. Typical learning approaches resort to deep reinforcement learning (RL) and provide training signal at the end of each episode to learn the optimal policy ( <ref type="bibr" target="#b7">Fang et al., 2017;</ref><ref type="bibr" target="#b1">Bachman et al., 2017)</ref> e.g., using policy gradient methods. These approaches, however, need a large number of training episodes to learn a reasonable policy as they need to deal with the credit assignment prob- lem, i.e. discovery of the utility of individual ac- tions in the sequence based on the achieved reward at the end of the episode. This exacerbates the dif- ficulty of finding a good AL policy. We formulate learning for the AL policy as an imitation learning problem. At each state, we pro- vide the AL agent with a correct action which is computed by an algorithmic expert. The AL agent uses the sequence of states observed in an episode paired with the expert's sequence of actions to up- date its policy. This directly addresses the credit assignment problem, and reduces the complexity of the problem compared to the RL approaches. In what follows, we describe the ingredients of our deep imitation learning (IL) approach, which is summarised in Algorithm 1.</p><p>Algorithmic Expert. At a given AL state s s s t , our algorithmic expert computes an action by evaluat- ing the current pool of unlabeled data. More con- cretely, for each x x x ∈ D pool rnd and its correct label y y y , the underlying model m φ φ φt is re-trained to get</p><formula xml:id="formula_1">m x x x φ φ φt , where D pool rnd ⊂ D unl t</formula><p>is a small subset of the current large pool of unlabeled data. The expert action is then computed as:</p><formula xml:id="formula_2">arg min x x x ∈D pool rnd loss(m x x x φ φ φt (x x x), D evl ).<label>(2)</label></formula><p>In other words, our algorithmic expert tries a sub- set of actions to roll-out one step from the current state, in order to efficiently compute a reasonable action. Searching for the optimal action would be O(|D unl | B ), which is computationally challeng- ing due to (i) the large action set, and (ii) the ex- ponential dependence on the length of the roll out.</p><p>We will see in the experiments that our method ef- ficiently learns effective AL policies.</p><p>Policy Network. Our policy network is a feed- forward network with two fully-connected hidden layers. It receives the current AL state, and pro- vides a preference score for a given unlabeled data point, allowing to select the most beneficial one corresponding to the highest score. The input to our policy network consists of three parts: (i) a fixed dimensional representation of the content and the predicted label of the unlabeled data point under consideration, (ii) a fixed-dimensional rep- resentation of the content and the labels of the la- beled dataset, and (iii) a fixed-dimensional repre- sentation of the content of the unlabeled dataset. Imitation Learning Algorithm. A typical ap- proach to imitation learning (IL) is to train the policy network so that it mimics the expert's be- haviour given training data of the encountered states (input) and actions (output) performed by the expert. The policy network's prediction af- fects future inputs during the execution of the pol- icy. This violates the crucial independent and identically distributed (iid) assumption, inherent to most statistical supervised learning approaches for learning a mapping from states to actions. We make use of Dataset Aggregation (DAGGER) <ref type="bibr" target="#b19">(Ross et al., 2011</ref>), an iterative algorithm for IL which addresses the non-iid nature of the encountered states during the AL process (see Algorithm 1). In round τ of DAG- GER, the learned policy networkˆπnetworkˆ networkˆπ τ is applied to the AL problem to collect a sequence of states which are paired with the expert actions. The collected pair of states and actions are aggregated to the dataset of such pairs M , collected from the previous iterations of the algorithm. The policy network is then re-trained on the aggregated set, resulting inˆπinˆ inˆπ τ +1 for the next iteration of the algo- rithm. The intuition is to build up the set of states that the algorithm is likely to encounter during its execution, in order to increase the generalization of the policy network. To better leverage the training signal from the algorithmic expert, we allow the algorithm to collect state-action pairs according to a modified policy which is a mixture ofˆπofˆ ofˆπ τ and the expert policy˜πpolicy˜ policy˜π * τ , i.e.</p><formula xml:id="formula_3">π τ = β τ ˜ π * + (1 − β τ )ˆ π τ</formula><p>where β τ ∈ [0, 1] is a mixing coefficient. This amounts to tossing a coin with parameter β τ in each iteration of the algorithm to decide one of these two policies for data collection.</p><p>Re-training the Policy Network. To train our policy network, we turn the preference scores to probabilities, and optimise the parameters such that the probability of the action prescribed by the expert is maximized. More specifically, let M := {(s s s i , a a a i )} I i=1 be the collected states paired with their expert's prescribed actions. Let D pool i be the set of unlabelled datapoints in the pool within the state, and a a a i denote the datapoint selected by the expert in the set. Our training objective is I i=1 log P r(a a a i |D pool i ) where</p><formula xml:id="formula_4">P r(a a a i |D pool i ) := expˆπexpˆ expˆπ(a a a i ; s s s i ) x x x∈D pool i expˆπexpˆ expˆπ(x x x; s s s i ) .</formula><p>The above can be interpreted as the probability of a a a i being the best action among all possible actions in the state. Following <ref type="bibr" target="#b15">(Mnih et al., 2015)</ref>, we ran- domly sample multiple 1 mini-batches from the re- play memory M, in addition to the current round's stat-action pair, in order to retrain the policy net- work. For each mini-batch, we make one SGD step to update the policy, where the gradients of the network parameters are calculated using the backpropagation algorithm.</p><p>Transferring the Policy. We now apply the pol- icy learned on the source task to AL in the tar- get task. We expect the learned policy to be effec- tive for target tasks which are related to the source task in terms of the data distribution and charac- teristics. Algorithm 2 illustrates the policy trans- fer. The pool-based AL scenario in Algorithm 2 is cold-start; however, extending to incorporate ini- tially available labeled data is straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments on text classification and named entity recognition (NER). The AL sce- narios include cross-domain sentiment classifica- tion, cross-lingual authorship profiling, and cross- lingual named entity recognition (NER), whereby an AL policy trained on a source domain/language is transferred to the target domain/language. We compare our proposed AL method using im- itation learning (ALIL) with the followings:</p><p>• Random sampling: The query datapoint is cho- sen randomly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learn active learning policy via imi- tation learning</head><p>Input: large labeled data D, max episodes T , budget B, sample size K, the coin parameter β Output: The learned policy 1: M ← ∅ the aggregated dataset 2: initialisê π1 with a random policy x x x ∈D lab Jaccard(x x x, x x x ), where the Jac- card coefficient between the unigram features of the two given texts is used as the similarity measure.</p><note type="other">3: for τ =1, . . . , T do 4: D lab , D unl , D evl ← dataPartition(D) 5: φ φ φ1 ← trainModel(D lab ) 6: c ← coinToss(β) 7: for t ∈ 1, . . . , B do 8: D pool rnd ← sampleUniform(D unl , K) 9: s s st ← (D lab , D pool rnd , φ φ φt) 10: a a at ← arg min x x x ∈D pool rnd loss(m x x x φ φ φ t , D evl ) 11: if c is head then the expert 12: x x xt ← a a at 13: else the policy 14: x x xt ← arg max x x x ∈D pool rndˆπτ rndˆ rndˆπτ (x x x ; s s st) 15: end if 16:</note><formula xml:id="formula_5">D lab ← D lab + {(x x xt, y y yt)} 17: D unl ← D unl − {x x xt} 18: M ← M + {(s s st, a</formula><note type="other">a at)} 19: φ φ φt+1 ← retrainModel(φ φ φt, D lab ) 20: end for 21: ˆ πτ+1 ← retrainPolicy(ˆ πτ , M ) 22: end for 23: returnˆπTreturnˆ returnˆπT +1 Algorithm 2 Active learning by policy transfer</note><p>• Uncertainty-based sampling:</p><p>For text classification, we use the datapoint with the highest predictive entropy,   which has been shown to be the best heuristic for this task among 17 different heuristics <ref type="bibr" target="#b21">(Settles and Craven, 2008</ref>).</p><p>• PAL: A reinforcement learning based approach <ref type="bibr" target="#b7">(Fang et al., 2017)</ref>, which makes use a deep Q-network to make the selection decision for stream-based active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Classification</head><p>Datasets and Setup. The first task is senti- ment classification, in which product reviews ex- press either positive or negative sentiment. The data comes from the Amazon product reviews <ref type="bibr" target="#b14">(McAuley and Yang, 2016)</ref>; see <ref type="table">Table 1</ref> for data statistics.</p><p>The second task is Authorship Profiling, in which we aim to predict the gender of the text author. The data comes from the gender profil- ing task in PAN 2017 ( <ref type="bibr" target="#b17">Rangel et al., 2017)</ref>, which consists of a large Twitter corpus in multiple lan- guages: English (en), Spanish (es) and Portuguese (pt). For each language, all tweets collected from a user constitute one document; <ref type="table">Table 1</ref> shows data statistics. The multilingual embeddings for this task come from off-the-shelf CCA-trained embed- dings (Ammar et al., 2016) for twelve languages, including English, Spanish and Portuguese. We fix these word embeddings during training of both the policy and the underlying classification model. For training, 10% of the source data is used as the evaluation set for computing the best action in imitation learning. We run T = 100 episodes with the budget B = 100 documents in each episode, set the sample size K = 5, and fix the mixing coefficient β τ = 0.5. For testing, we take 90% of the target data as the unlabeled pool, and the remaining 10% as the test set. We show the test accuracy w.r.t. the number of labelled documents selected in the AL process.</p><p>As the underlying model m φ φ φ , we use a fast and efficient text classifier based on convolutional neu- ral networks. More specifically, we apply 50 con- volutional filters with ReLU activation on the em- bedding of all words in a document x x x, where the width of the filters is 3. The filter outputs are aver- aged to produce a 50-dimensional document rep- resentation h h h(x x x), which is then fed into a softmax to predict the class.</p><p>Representing state-action. The input to the policy network, i.e. the feature vector represent- ing a state-action pair, includes: the candidate doc- ument represented by the convolutional net h h h(x x x), the distribution over the document's class labels m φ φ φ (x x x), the sum of all document vector represen- tations in the labeled set x x x ∈D lab h h h(x x x ), the sum of all document vectors in the random pool of un- labelled data x x x ∈D pool rnd h h h(x x x ), and the empirical distribution of class labels in the labeled dataset.</p><p>Results. <ref type="figure" target="#fig_5">Fig 2 shows</ref> the results on product sentiment prediction and authorship profiling, in cross-domain and cross-lingual AL scenarios 2 . Our ALIL method consistently outperforms both heuristic-based and RL-based (PAL) <ref type="bibr" target="#b7">(Fang et al., 2017</ref>) approaches across all tasks. ALIL tends to convergence faster than other methods, which in- dicates its policy can quickly select the most infor- mative datapoints. Interestingly, the uncertainty and diversity sampling heuristics perform worse than random sampling on sentiment classification. We speculate this may be due to these two heuris- tics not being able to capture the polarity informa- tion during the data selection process. PAL per- forms on-par with uncertainty with rationals on musical device, both of which outperform the tra- ditional diversity and uncertainty sampling heuris- tics. Interestingly, PAL is outperformed by ran- dom sampling on movie reviews, and by the tra- ditional uncertainty sampling heuristic on author- ship profiling tasks. We attribute this to ineffec- tiveness of the RL-based approach for learning a reasonable AL query strategy.</p><p>We further investigate combining the transfer of the policy network with the transfer of the under- lying classifier. That is, we first train a classi- fier on all of the annotated data from the source domain/language. Then, this classifier is ported to the target domain/language; for cross-language transfer, we make use of multilingual word em- beddings. We start the AL process starting from the transferred classifier, referred to as the warm- start AL. We compare the performance of the di- rectly transferred classifier with those obtained af- ter the AL process in the warm-start and cold-start scenarios. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. We have run the cold-start and warm-start AL for 25 times, and reported the average accuracy in Ta- ble 2. As seen from the results, both the cold and warm start AL settings outperform the direct trans- fer significantly, and the warm start consistently gets higher accuracy than the cold start. The dif- ference between the results are statistically signif- icant, with a p-value of .001, according to McNe- mar test <ref type="bibr">3 (Dietterich, 1998)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Named Entity Recognition</head><p>Data and setup We use NER corpora from the CONLL2002/2003 shared tasks, which include annotated text in English (en), German (de), Span- ish (es), and Dutch (nl). The original annotation is based on IOB1, which we convert to the IO labelling scheme. Following <ref type="bibr" target="#b7">Fang et al. (2017)</ref>, we consider two experimental conditions: (i) the bilingual scenario where English is the source (used for policy training) and other languages are the target, and (ii) the multilingual scenario where one of the languages (except English) is the target and the remaining ones are the source used in joint training of the AL policy. The underlying model m φ φ φ is a conditional random field (CRF) treating NER as a sequence labelling task. The prediction is made using the Viterbi algorithm.</p><p>In the existing corpus partitions from CoNLL, each language has three subsets: train, testa and testb. During policy training with the source lan- guage(s), we combine these three subsets, shuf- fle, and re-split them into simulated training, unla- belled pool, and evaluation sets in every episode. We run N = 100 episodes with the budget B = 200, and set the sample size k = 5. When we transfer the policy to the target language, we do one episode and select B datapoints from train (treated as the pool of unlabeled data) and report F1 scores on testa.</p><p>Representing state-action. The input to the policy network includes the representation of the candidate sentence using the sum of its words' embeddings h h h(x x x), the representation of the la- belling marginals using the label-level convolu- tional network cnn lab (E m φ φ φ (y y y|x x x) [y y y]) <ref type="bibr" target="#b7">(Fang et al., 2017)</ref>, the representation of sentences in the la- beled data (x x x ,y y y )∈D lab h h h(x x x ), the representa- tion of sentences in the random pool of un- labelled data x x x ∈D pool rnd h h h(x x x ), the representa- tion of ground-truth labels in the labeled data (x x x ,y y y )∈D lab cnn lab (y y y ) using the empirical distri- butions, and the confidence of the sequential pre-  Results. <ref type="figure" target="#fig_7">Fig. 3</ref> shows the results for three tar- get languages. In addition to the strong heuristic- based methods, we compare our imitation learn- ing approach (ALIL) with the reinforcement learn- ing approach (PAL) <ref type="bibr" target="#b7">(Fang et al., 2017)</ref>, in both bilingual (bi) and multilingual (mul) transfer set- tings. Across all three languages, ALIL.bi and ALIL.mul outperform the heuristic methods, in- cluding Uncertainty Sampling based on TTE. This is expected as the uncertainty sampling largely re- lies on a high quality underlying model, and di- versity sampling ignores the labelling information. In the bilingual case, ALIL.bi outperforms PAL.bi on Spanish (es) and Dutch (nl), and performs sim- ilarly on German (de). In the multilingual case, ALIL.mul achieves the best performance on Span- ish, and performs competitively with PAL.mul on German and Dutch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>Insight on the selected data. We compare the data selected by ALIL to other methods. This will confirm that ALIL learns policies which are suit- able for the problem at hand, without resorting to a fixed engineered heuristics. For this analysis, we report the mean reciprocal rank (MRR) of the data points selected by the ALIL policy under rank- ings of the unlabelled pool generated by the un- certainty and diversity sampling. Furthermore, we measure the fraction of times the decisions made by the ALIL policy agrees with those which would movie sentiment gender pt NER es acc Unc.</p><p>0  have been made by the heuristic methods, which is measured by the accuracy (acc). <ref type="table" target="#tab_3">Table 3</ref> re- port these measures. As we can see, for sentiment classification since uncertainty and diversity sam- pling perform badly, ALIL has a big disagreement with them on the selected data points. While for gender classification on Portuguese and NER on Spanish, ALIL shows much more agreement with other three heuristics. Lastly, we compare chosen queries by ALIL to those by PAL, to investigate the extent of the agreement between these two methods. This is simply measure by the fraction of identical query data points among the total number of queries (i.e. accuracy). Since PAL is stream-based and sen- sitive to the order in which it receives the data points, we report the average accuracy taken over multiple runs with random input streams. The ex- pected accuracy numbers are reported in <ref type="table" target="#tab_3">Table 3</ref>. As seen, ALIL has higher overlap with PAL than the heuristic-based methods, in terms of the se- lected queries.</p><p>Sensitivity to K. As seen in Algorithm 1, we re- sort to an approximate algorithmic expert, which selects the best action in a random subset of the pool of unlabelled data with size K, in order to make the policy training efficient. Note that, in policy training, setting K to one and the size of the unlabelled data pool correspond to stream-based and pool-based AL scenarios, respectively. By changing K to values between these two extremes, we can analyse the effect of the quality of the al- gorithmic expert on the trained policy; <ref type="figure" target="#fig_8">Figure 4</ref> shows the results. A larger candidate set may cor- respond to a better learned policy, needed to be traded off with the training time growing linearly with K. Interestingly, even small candidate sets lead to strong AL policies as increasing K beyond 10 does not change the performance significantly.</p><p>Dynamically changing β. In our algorithm, β plays an important role as it trades off exploration versus exploitation. In the above experiments, we fix it to 0.5; however, we can change its value throughout trajectory collection as a function of τ (see Algorithm 1). We investigate schedules which tend to put more emphasis on exploration and exploitation towards the beginning and end of data collection, respectively. We investigate the following schedules: (i) linear β τ = max(0.5, 1 − 0.01τ ), (ii) exponential β τ = 0.9 τ , and (iii) and inverse sigmoid β τ = 5 5+exp(τ /5) , as a function of iterations. <ref type="figure" target="#fig_9">Fig. 5</ref> shows the comparisons of these schedules. The learned policy seems to perform competitively with either a fixed or an exponential schedule. We have also investigated tossing the coin in each step within the trajectory roll out, but found that it is more effective to have it before the full trajectory roll out (as currently done in Algo- rithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Traditional active learning algorithms rely on various heuristics <ref type="bibr" target="#b20">(Settles, 2010)</ref>, such as un- certainty sampling <ref type="bibr" target="#b21">(Settles and Craven, 2008;</ref><ref type="bibr" target="#b9">Houlsby et al., 2011)</ref>, query-by-committee <ref type="bibr">(GiladBachrach et al., 2006</ref>), and diversity sampling <ref type="bibr" target="#b2">(Brinker, 2003;</ref><ref type="bibr" target="#b11">Joshi et al., 2009;</ref><ref type="bibr" target="#b28">Yang et al., 2015</ref>). Apart from these, different heuristics can be combined, thus creating integrated strat- egy which consider one or more heuristics at the same time. Combined with transfer learning, pre-existing labeled data from related tasks can help improve the performance of an active learner ( <ref type="bibr" target="#b27">Xiao and Guo, 2013;</ref><ref type="bibr" target="#b12">Kale and Liu, 2013;</ref><ref type="bibr" target="#b10">Huang and Chen, 2016;</ref><ref type="bibr" target="#b13">Konyushkova et al., 2017</ref>). More recently, deep reinforcement learning is used as the framework for learning active learning algo- rithms, where the active learning cycle is consid- ered as a decision process. <ref type="bibr" target="#b26">(Woodward and Finn, 2017)</ref> extended one shot learning to active learn- ing and combined reinforcement learning with a deep recurrent model to make labeling decisions. ( <ref type="bibr" target="#b1">Bachman et al., 2017</ref>) introduced a policy gradi- ent based method which jointly learns data repre- sentation, selection heuristic as well as the model prediction function. <ref type="bibr" target="#b7">(Fang et al., 2017</ref>) designed an active learning algorithm based on a deep Q- network, in which the action corresponds to bi- nary annotation decisions applied to a stream of data. The learned policy can then be transferred between languages or domains.</p><p>Imitation learning (IL) refers to an agent's ac- quisition of skills or behaviours by observing an expert's trajectory in a given task. It helps re- duce sequential prediction tasks into supervised learning by employing a (near) optimal oracle at training time. Several IL algorithms has been proposed in sequential prediction tasks, including SEARA ( <ref type="bibr" target="#b5">Daumé et al., 2009</ref>), AggreVaTe (Ross and Bagnell, 2014), <ref type="bibr">DaD (Venkatraman et al., 2015</ref>), LOLS( , DeeplyAggre- VaTe ( <ref type="bibr" target="#b24">Sun et al., 2017)</ref>. Our work is closely re- lated to <ref type="bibr">Dagger (Ross et al., 2011)</ref>, which can guarantee to find a good policy by addressing the dependency nature of encountered states in a tra- jectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have proposed a new method for learning active learning algorithms using deep im- itation learning. We formalize pool-based active learning as a Markov decision process, in which active learning corresponds to the selection de- cision of the most informative data points from the pool. Our efficient algorithmic expert pro- vides state-action pairs from which effective active learning policies can be learned. We show that the algorithmic expert allows direct policy learning, while at the same time, the learned policies trans- fer successfully between domains and languages, demonstrating improvement over previous heuris- tic and reinforcement learning approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>The AL algorithm receives a reward −loss(m φ φ φ , D evl ), which is the negative loss of the current trained model on the evaluation set, defined as loss(m φ φ φ , D evl ) := (x x x,y y y)∈D evl loss(m φ φ φ (x x x), y y y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The policy network and its inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Input: unlabeled pool D unl , budget B, policyˆπpolicyˆ policyˆπ Output: labeled dataset and trained model 1: D lab ← ∅ 2: initialise φ φ φ randomly 3: for t ∈ 1, . . . , B do 4: s s st ← (D lab , D unl , φ φ φ) 5: x x xt ← arg max x x x ∈D unî π(x x x ; s s st) 6: y y yt ← askAnnotation(x x xt) 7: D lab ← D lab + {(x x xt, y y yt)} 8: D unl ← D unl − {x x xt} 9: φ ← retrainModel(φ φ φ, D lab ) 10: end for 11: return D lab and φ φ φ • Diversity sampling: The query datapoint is arg minx x x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>arg maxx x x − y p(y|x x x, D lab ) log p(y|x x x, D lab ) where p(y y y|x x x, D lab ) comes from the underlying model. We further use a state-of-the-art exten- sion of this method, called uncertainty with rationals (Sharma et al., 2015), which not only considers uncertainty but also looks whether</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The performance of different active learning methods for cross domain sentiment classification (left two plots) and cross lingual authorship profiling (right two plots).</figDesc><graphic url="image-1.png" coords="6,72.00,62.81,453.55,149.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The performance of active learning methods on the bilingual and multilingual settings for three target languages: German (de), Spanish (es) and Dutch (nl).</figDesc><graphic url="image-2.png" coords="7,73.53,62.81,312.95,132.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The learning curves of agents with different K on Spanish (es) NER.</figDesc><graphic url="image-3.png" coords="7,392.49,69.24,131.53,119.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The learning curves of agents with different schedules for β before the trajectory on Spanish (es) NER.</figDesc><graphic url="image-4.png" coords="8,113.10,62.81,136.07,123.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Classifiers performance under three dif-
ferent transfer settings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The first four rows show MRR and accu-
racy of instances returned by ALIL under the rank-
ings of uncertainty and diversity sampling, the last 
row give average accuracy of instances under PAL. 

</table></figure>

			<note place="foot" n="1"> In our experiments, we use 10 mini-bathes, each of which of size 100.</note>

			<note place="foot" n="2"> Uncertainty with rationale cannot be done for authorship profiling as the rationales come from a sentiment dictionary.</note>

			<note place="foot" n="3"> As the contingency table needed for the McNemar test, we have used the average counts across the 25 runs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the feedback from anony-mous reviewers. G. H. is grateful to Trevor Cohn for interesting discussions. This work was sup-ported by computational resources from the Multi-modal Australian ScienceS Imaging and Visuali-sation Environment (MASSIVE) at Monash Uni-versity.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively multilingual word embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning algorithms for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia. PMLR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incorporating diversity in active learning with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Brinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03)</title>
		<meeting>the 20th International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to search better than your teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2058" to="2066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selective sampling in natural language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sean P Engelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-95 Workshop on New Approaches to Learning for Natural Language Processing</title>
		<meeting>IJCAI-95 Workshop on New Approaches to Learning for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Search-based structured prediction. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="297" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximate statistical tests for comparing supervised classification learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning how to active learn: A deep reinforcement learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02383</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Query by committee made real</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Navot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Máté</forename><surname>Lengyel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1112.5745</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transfer learning with active queries from source domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1592" to="1598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-class active learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Papanikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2372" to="2379" />
		</imprint>
	</monogr>
	<note>IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accelerating active learning with transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 13th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1085" to="1090" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning active learning from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ksenia</forename><surname>Konyushkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Sznitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4225" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Addressing complex and subjective product-related queries with customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
		<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="625" to="635" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Charles Beattie</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
			<publisher>Shane Legg, and Demis Hassabis</publisher>
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Poolbased active learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Automated Learning and Discovery (CONALD)</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Overview of the 5th author profiling task at PAN 2017: Gender and language variety identification in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Rangel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Working Notes Papers of the CLEF</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Reinforcement and imitation learning via interactive noregret learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Ross</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5979</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">11</biblScope>
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An analysis of active learning strategies for sequence labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A note on the concept of entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Tech. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Active learning with rationales for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manali</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="441" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deeply aggrevated: Differentiable imitation learning for sequential prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01030</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving multi-step prediction of learned time series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Andrew</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3024" to="3030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Woodward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06559</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Active oneshot learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online active learning for cost sensitive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-class active learning by uncertainty sampling with diversity maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
