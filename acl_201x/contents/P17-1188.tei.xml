<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Character-level Compositionality with Visual Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Lo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
						</author>
						<title level="a" type="main">Learning Character-level Compositionality with Visual Features</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2059" to="2068"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1188</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Previous work has modeled the composi-tionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics , creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry semantic content, resulting in embeddings that are coherent in visual space.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Compositionality-the fact that the meaning of a complex expression is determined by its structure and the meanings of its constituents-is a hall- mark of every natural language ( <ref type="bibr" target="#b8">Frege and Austin, 1980;</ref><ref type="bibr">Szabó, 2010)</ref>. Recently, neural models have provided a powerful tool for learning how to com- pose words together into a meaning representation of whole sentences for many downstream tasks. This is done using models of various levels of sophistication, from simpler bag-of-words <ref type="bibr" target="#b11">(Iyyer et al., 2015</ref>) and linear recurrent neural network (RNN) models <ref type="bibr">(Sutskever et al., 2014;</ref><ref type="bibr">Kiros et al., 2015)</ref>, to more sophisticated models using tree- structured <ref type="bibr">(Socher et al., 2013</ref>) or convolutional networks ( <ref type="bibr" target="#b12">Kalchbrenner et al., 2014</ref>). In fact, a growing body of evidence shows that it is essential to look below the word-level and con- sider compositionality within words themselves. For example, several works have proposed mod- els that represent words by composing together the characters into a representation of the word it- self ( <ref type="bibr">Ling et al., 2015;</ref><ref type="bibr">Zhang et al., 2015;</ref><ref type="bibr" target="#b7">Dhingra et al., 2016)</ref>. Additionally, for languages with pro- ductive word formation (such as agglutination and compounding), models calculating morphology- sensitive word representations have been found ef- fective ( <ref type="bibr">Luong et al., 2013;</ref><ref type="bibr" target="#b0">Botha and Blunsom, 2014</ref>). These models help to learn more robust representations for rare words by exploiting mor- phological patterns, as opposed to models that op- erate purely on the lexical level as the atomic units.</p><p>For many languages, compositionality stops at the character-level: characters are atomic units of meaning or pronunciation in the language, and no further decomposition can be done. <ref type="bibr">1</ref>   <ref type="table">Table 1</ref>: By-category statistics for the Wikipedia dataset. Note that Food is the abbreviation for "Food and Culture" and Religion is the abbreviation for "Religion and Belief".</p><p>be derived from the sum of its parts, is very much a reality. Perhaps the most compelling example of compositionality of sub-character units can be found in logographic writing systems such as the Han and Kanji characters used in Chinese and Japanese, respectively. 2 As shown on the left side of <ref type="figure" target="#fig_0">Fig. 1</ref>, each part of a Chinese character (called a "radical") potentially contributes to the meaning (i.e., <ref type="figure" target="#fig_0">Fig. 1(a)</ref>) or pronunciation (i.e., <ref type="figure" target="#fig_0">Fig. 1(b)</ref>) of the overall character. This is similar to how English characters combine into the meaning or pronunciation of an English word. Even in lan- guages with phonemic orthographies, where each character corresponds to a pronunciation instead of a meaning, there are cases where composition occurs. <ref type="figure" target="#fig_0">Fig. 1</ref>(c) and (d) show the examples of Ko- rean and German, respectively, where morpholog- ical inflection can cause single characters to make changes where some but not all of the component parts are shared. In this paper, we investigate the feasibility of modeling the compositionality of characters in a way similar to how humans do: by visually ob- serving the character and using the features of its shape to learn a representation encoding its mean- ing. Our method is relatively simple, and gener- alizable to a wide variety of languages: we first transform each character from its Unicode repre- sentation to a rendering of its shape as an image, then calculate a representation of the image us- ing Convolutional Neural Networks (CNNs) ( <ref type="bibr" target="#b3">Cun et al., 1990</ref>). These features then serve as inputs to a down-stream processing task and trained in an end-to-end manner, which first calculates a loss function, then back-propagates the loss back to the CNN.</p><p>As demonstrated by our motivating examples in <ref type="figure" target="#fig_0">Fig. 1</ref>, in logographic languages character-level semantic or phonetic similarity is often indicated by visual cues; we conjecture that CNNs can appropriately model these visual patterns. Con- sequently, characters with similar visual appear- ances will be biased to have similar embeddings, allowing our model to handle rare characters ef- fectively, just as character-level models have been effective for rare words.</p><p>To evaluate our model's ability to learn repre- sentations, particularly for rare characters, we per- form experiments on a downstream task of classi- fying Wikipedia titles for three Asian languages: Chinese, Japanese, and Korean. We show that our proposed framework outperforms a baseline model that uses standard character embeddings for instances containing rare characters. A qualita- tive analysis of the characteristics of the learned embeddings of our model demonstrates that visu- ally similar characters share similar embeddings. We also show that the learned representations are particularly effective under low-resource scenar- ios and complementary with standard character embeddings; combining the two representations through three different fusion methods <ref type="bibr">(Snoek et al., 2005;</ref><ref type="bibr" target="#b13">Karpathy et al., 2014</ref>) leads to con- sistent improvements over the strongest baseline without visual features. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank</head><p>Frequency ⎯⎯ Chinese ⎯⎯"Japanese ⎯⎯ Korean</p><p>Rank &lt; 20% Freq. &gt; 80% <ref type="figure">Figure 2</ref>: The character rank-frequency distribu- tion of the corpora we considered in this paper. All three languages have a long-tail distribution.</p><p>positionality in the characters of the language. To satisfy these desiderata, we create a text classifi- cation dataset where the input is a Wikipedia ar- ticle title in Chinese, Japanese, or Korean, and the output is the category to which the article be- longs. <ref type="bibr">3</ref> This satisfies (1), because Wikipedia titles are short and thus each character in the title will be important to our decision about its category. It also satisfies (2), because Chinese, Japanese, and Korean have writing systems with large numbers of characters that decompose regularly as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. While this task in itself is novel, it is similar to previous work in named entity type in- ference using Wikipedia ( <ref type="bibr">Toral and Munoz, 2006;</ref><ref type="bibr" target="#b14">Kazama and Torisawa, 2007;</ref><ref type="bibr">Ratinov and Roth, 2009)</ref>, which has proven useful for downstream named entity recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Collection</head><p>As the labels we would like to predict, we use 12 different main categories from the Wikipedia web page: Geography, Sports, Arts, Military, Eco- nomics, Transportation, Health Science, Educa- tion, Food Culture, Religion and Belief, Agricul- ture and Electronics. Wikipedia has a hierarchical structure, where each of these main categories has a number of subcategories, and each subcategory has its own subcategories, etc. We traverse this hierarchical structure, adding each main category tag to all of its descendants in this subcategory tree structure. In the case that a particular article is the descendant of multiple main categories, we favor the main category that minimizes the depth of the <ref type="bibr">3</ref> The link to the dataset and the crawling scripts - https://github.com/frederick0329/ Wikipedia_title_dataset article in the tree (e.g., if an article is two steps away from Sports and three steps away from Arts, it will receive the "Sports" label). We also per- form some rudimentary filtering, removing pages that match the regular expression ".*:.*", which catches special pages such as "title:agriculture".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Statistics</head><p>For Chinese, Japanese, and Korean, respectively, the number of articles is 593k/810k/46.6k, and the average length and standard deviation of the ti- tle is 6.25±3.96/8.60±5.58/6.10±3.71. As shown in <ref type="figure">Fig. 2</ref>, the character rank-frequency distribu- tions of all three languages follows the 80/20 rule (Newman, 2005) (i.e., top 20% ranked char- acters that appear more than 80% of total frequen- cies), demonstrating that the characters in these languages belong to a long tail distribution.</p><p>We further split the dataset into training, valida- tion, and testing sets with a 6:2:2 ratio. The cat- egory distribution for each language can be seen in Tab. 1. Chinese has two varieties of characters, traditional and simplified, and the dataset is a mix of the two. Hence, we transform this dataset into two separate sets, one completely simplified and the other completely traditional using the Chinese text converter provided with Mac OS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer#</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3-layer CNN Configuration 1 Spatial Convolution (3, 3) → 32 2 ReLu 3</head><p>MaxPool (2, 2) 4</p><p>Spatial Convolution (3, 3) → 32 5 ReLu 6</p><p>MaxPool (2, 2) 7</p><p>Spatial Convolution (3, 3) → 32 8 ReLu 9</p><p>Linear (800, 128) 10 ReLu 11</p><p>Linear (128, 128) 12 ReLu We calculate character representations, use a RNN to combine the character representations into a sentence representation, and then add a softmax layer after that to predict the probability for each class. As shown in <ref type="figure">Fig. 2</ref>.1, the baseline model, which we call it the LOOKUP model, calculates the representation for each character by looking it up in a character embedding matrix. Our proposed model, the VISUAL model instead learns the rep- resentation of each character from its visual ap- pearance via CNN.</p><p>LOOKUP model Given a character vocabulary C, for the LOOKUP model as in the bottom part of <ref type="figure">Fig. 2</ref>.1, the input to the network is a stream of characters c 1 , c 2 , ...c N , where c n ∈ C. Each char- acter is represented by a 1-of-|C| (one-hot) en- coding. This one-hot vector is then multiplied by the lookup matrix T C ∈ R |C|×dc , where d c is the dimension of the character embedding. The ran- domly initialized character embeddings were opti- mized with classification loss. loss. Because we are training embeddings based on this classification loss, we expect that the CNN will focus on parts of the image that contain se- mantic information useful for category classifica- tion, a hypothesis that we examine in the experi- ments (see Section 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VISUAL model</head><p>In more detail, the specific structure of the CNN that we utilize consists of three convolution layers where each convolution layer is followed by the max pooling and ReLU nonlinear activation lay- ers. The configurations of each layer are listed in Tab. 2. The output vector for the image embed- dings also has size d c which is the same as the LOOKUP model.</p><p>Encoder and Classifier For both the LOOKUP and the VISUAL models, we adopt an RNN encoder using Gated Recurrent Units (GRUs) ( <ref type="bibr" target="#b1">Chung et al., 2014</ref>). Each of the GRU units processes the character embeddings sequen- tially. At the end of the sequence, the incremental GRU computation results in a hidden state e embedding the sentence. The encoded sentence embedding is passed through a linear layer whose output is the same size as the number of classes. We use a softmax layer to compute the posterior class probabilities:</p><formula xml:id="formula_0">P (y = j|e) = exp(w T j e + b j ) L i=1 exp(w T i e + b i )<label>(1)</label></formula><p>To train the model, we use cross-entropy loss between predicted and true targets:</p><formula xml:id="formula_1">J = 1 B B i=1 L j=1 −t i,j log(p i,j )<label>(2)</label></formula><p>where t i,j ∈ {0, 1} represents the ground truth la- bel of the j-th class in the i-th Wikipedia page ti- tle. B is the batch size and L is the number of categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fusion-based Models</head><p>One thing to note is that the LOOKUP and the VISUAL models have their own advantages. The LOOKUP model learns embedding that captures the semantics of each character symbol without sharing information with each other. In con- trast, the proposed VISUAL model directly learns embedding from visual information, which natu- rally shares information between visually similar characters. This characteristic gives the VISUAL  model the ability to generalize better to rare char- acters, but also has the potential disadvantage of introducing noise for characters with similar ap- pearances but different meanings. With the complementary nature of these two models in mind, we further combine the two em- beddings to achieve better performances. We adopt three fusion schemes, early fusion, late fu- sion (described by <ref type="bibr">Snoek et al. (2005)</ref> and Karpa- thy et al. <ref type="formula" target="#formula_0">(2014)</ref>), and fallback fusion, a method specific to this paper.</p><p>Early Fusion Early fusion works by concatenat- ing the two varieties of embeddings before feeding them into the RNN. In order to ensure that the di- mensions of the RNN are the same after concate- nation, the concatenated vector is fed through a hidden layer to reduce the size from 2 × d c to d c . The whole model is then fine-tuned with training data.</p><p>Late Fusion Instead of learning a joint represen- tation like early fusion, late fusion averages the model predictions. Specifically, it takes the output of the softmax layers from both models and aver- ages the probabilities to create a final distribution used to make the prediction.</p><p>Fallback Fusion Our final fallback fusion method hypothesizes that our VISUAL model does better with instances which contain more rare characters. First, in order to quantify the over- all rareness of an instance consisting of multiple characters, we calculate the average training set frequency of the characters therein. The fallback fusion method uses the VISUAL model to predict testing instances with average character frequency below or equal to a threshold (here we use 0.0 fre- quency as cutoff, which means all characters in the instance do not appear in the training set), and uses the LOOKUP model to predict the rest of the in- stances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>In this section, we compare our proposed VISUAL model with the baseline LOOKUP model through three different sets of experiments. First, we ex- amine whether our model is capable of classify- ing text and achieving similar performance as the baseline model. Next, we examine the hypothesis that our model will outperform the baseline model when dealing with low frequency characters. Fi- nally, we examine the fusion methods described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Configurations</head><p>The dimension of the embeddings and batch size for both models are set to d c = 128 and B = 400, respectively. We build our proposed model using Torch <ref type="bibr" target="#b2">(Collobert et al., 2002</ref>), and use Adam ( <ref type="bibr">Kingma and Ba, 2014</ref>) with a learning rate η = 0.001 for stochastic optimization. The length of each instance is cut off or padded to 10 characters for batch training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with the Baseline Model</head><p>In this experiment, we examine whether our VI- SUAL model achieves similar performance with the baseline LOOKUP model in classification ac- curacy.</p><p>The results in <ref type="bibr">Tab. 3</ref> show that the baseline model performs 1-2% better across four datasets; this is due to the fact that the LOOKUP model can directly learn character embeddings that capture the semantics of each character symbol for fre- quent characters. In contrast, the VISUAL model learns embeddings from visual information, which constraints characters that has similar appearance to have similar embeddings. This is an advantage for rare characters, but a disadvantage for high fre- quency characters because being similar in appear- ance does not always lead to similar semantics.</p><p>To demonstrate that this is in fact the case, be- sides looking at the overall classification accuracy, we also examine the performance on classifying low frequency instances which are sorted accord- ing to the average training set frequency of the characters therein. Tab. 4 and <ref type="figure" target="#fig_3">Fig. 4</ref> both show that our model performs better in the 100 lowest fre- quency instances (the intersection point of the two models). More specifically, take <ref type="figure" target="#fig_3">Fig. 4(a)</ref>' as ex- ample, the solid (proposed) line is higher than the dashed (baseline) line up to 10 2 , indicating that the proposed model outperforms the baseline for the . This figure is a log-log plot, where x-axis shows rarity (rarest to the left), y-axis shows cumulative correctly classified instances up to this rank; a perfect classifier will result in a diagonal line.</p><p>first 100 instances. Lines depart the x-axis when the model classifies its first instance correctly, and the LOOKUP model did not correctly classify any of the first 80 rarest instances, resulting in it cross- ing later than the proposed model. This confirms that the VISUAL model can share visual informa- tion among characters and help to classify low fre- quency instances. For training time, visual features take signifi- cantly more time, as expected. VISUAL is 30x slower than LOOKUP, although they are equiv- alent at test time. For space, images of Chinese characters took 36MB to store for 8985 characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiments on Different Training Sizes</head><p>In our second experiment, we consider two smaller training sizes (i.e., 50% and 12.5% of the full training size) indicated by green and red lines in <ref type="figure" target="#fig_3">Fig. 4</ref>. We performed this experiment under the hypothesis that because the proposed method was more robust to infrequent characters, the proposed model may perform better in low-resourced sce- narios. If this is the case, the intersection point of the two models will shift right because of the in- crease of the number of instances with low average character frequency.  <ref type="table">Table 4</ref>: Classification results for the LOOKUP / VISUAL of the k lowest frequency instances across four datasets. The 100 lowest frequency in- stances for traditional and simplified Chinese and Korean were both significant (p-value &lt; 0.05).</p><p>Those for Japanese were not (p-value = 0.13); likely because there was less variety than Chinese and more data than Korean.</p><p>As we can see in <ref type="figure" target="#fig_3">Fig. 4</ref>, the intersection point for 100% training data lies between the intersec- tion point for 50% training data and 12.5%. This disagrees with our hypothesis; this is likely be- cause while the number of low-frequency charac- ters increases, smaller amounts of data also ad- versely impact the ability of CNN to learn useful visual features, and thus there is not a clear gain nor loss when using the proposed method.</p><p>As a more extreme test of the ability of our pro- posed framework to deal with the unseen char-zh trad zh simp ja ko Lookup 0.5503 0.5543 0.4914 0.4765 Visual 0.5434 0.5403 0.4775 0.4207 early 0.5520 0.5546 0.4896 0.4796 late 0.5658 0.5685 0.5029 0.4869 fall 0.5507 0.5547 0.4914 0.4766 acters in the test set, we use traditional Chinese as our training data and simplified Chinese as our testing data. The model was able to achieve around 40% classification accuracy when we use the full training set, compared to 55%, which is achieved by the model trained on simplified Chi- nese. This result demonstrates that the model is able to transfer between similar scripts, similarly to how most Chinese speakers can guess the mean- ing of the text, even if it is written in the other script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Experiment on Different Fusion Methods</head><p>Results of different fusion methods can be found in Tab. 5. The results show that late fusion gives the best performance among all the fu- sion schemes combining the LOOKUP model and the proposed VISUAL model. Early fusion achieves small improvements for all languages ex- cept Japanese, where it displays a slight drop. Unsurprisingly, fallback fusion performs better than the LOOKUP model and the VISUAL model alone, since it directly targets the weakness of the LOOKUP model (e.g., rare characters) and replaces the results with the VISUAL model. These re- sults show that simple integration, no matter which schemes we use, is beneficial, demonstrating that both methods are capturing complementary infor- mation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Visualization of Character Embeddings</head><p>Finally, we qualitatively examine what is learned by our proposed model in two ways. First, we visualize which parts of the image are most im- portant to the VISUAL model's embedding calcu- lation. Second, we show the 6-nearest neighbor results for characters using both the LOOKUP and the VISUAL embeddings. Emphasis of the VISUAL Model In order to delve deeper into what the VISUAL model has learned, we measure a modified version of the oc- clusion sensitivity proposed by <ref type="bibr">Zeiler and Fergus (2014)</ref> by masking the original character image in four ways, and examine the importance of each part of the character to the model's calculated rep- resentations. Specifically, we leave only the up- per half, bottom half, left half, or right half of the image, and mask the remainder with white pix- els since Chinese characters are usually formed by combining two radicals vertically or horizon- tally. We run these four images forward through the CNN part of the model and calculate the L 2 distance between the masked image embeddings with the full image embedding. The larger the dis- tance, the more the masked part of the character contributes to the original embedding. The contri- bution of each part (e.g. the L 2 distance) is repre- sented as a heat map, and then it is normalized to adjust the opacity of the character strokes for bet- ter visualization. The value of each corner of the heatmap is calculated by adding the two L 2 dis- tances that contribute to this corner. The visualization is shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. The mean- ing of each Chinese character in English is shown below the Chinese character. The opacity of the character strokes represent how much the corre- sponding parts contribute to the original embed- ding (the darker the more). In general, the darker part of the character is related to its semantics. For example, "金" means gold in Chinese, which is highlighted in both "鐵" (Iron) and "銅" (Bronze). We can also find similar results for other exam- ples shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. <ref type="figure" target="#fig_5">Fig. 5</ref> also demonstrated that our model captures the compositionality of Chinese characters, both meaning of sub-character units and their structure (e.g. the semantic content tends to be structurally localized on one side of a Chinese character).</p><formula xml:id="formula_2">! ! ! ! ! ! Visual'model Lookup'model Visual'model Lookup'model</formula><p>K-nearest neighbors Finally, to illustrate the difference of the learned embeddings between the two models, we display 6-nearest neighbors (L 2 distance) for selected characters in <ref type="figure" target="#fig_6">Fig. 6</ref>. As can be seen, the VISUAL embedding for characters with similar appearances are close to each other. In addition, similarity in the radical part indicates semantic similarity between the characters. For example, the characters with radical "鳥" all refer to different type of birds.</p><p>The LOOKUP embedding do not show such fea- ture, as it learns the embedding individually for each symbol and relies heavily on the training set and the task. In fact, the characters shown in <ref type="figure" target="#fig_6">Fig. 6</ref> for the LOOKUP model do not exhibit semantic similarity either. There are two potential expla- nations for this: First, the category classification task that we utilized do not rely heavily on the fine- grained semantics of each character, and thus the LOOKUP model was able to perform well without exactly capturing the semantics of each character precisely. Second, the Wikipedia dataset contains a large number of names and location and the char- acters therein might not have the same semantic meaning used in daily vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Methods that utilize neural networks to learn distributed representations of words or charac- ters have been widely developed. However, word2vec <ref type="bibr">(Mikolov et al., 2013)</ref>, for example, re- quires storing an extremely large table of vectors for all word types. For example, due to the size of word types in twitter tweets, work has been done to generate vector representations of tweets at character-level ( <ref type="bibr" target="#b7">Dhingra et al., 2016)</ref>.</p><p>There is also work done in understanding math- ematical expressions with a convolutional net- work for text and layout recognition by using an attention-based neural machine translation sys- tem ( <ref type="bibr" target="#b6">Deng et al., 2016)</ref>. They tested on real- world rendered mathematical expressions paired with LaTeX markup and show the system is ef- fective at generating accurate markup. Other than that, there are several works that combine visual information with text in improving machine trans- lation ( <ref type="bibr">Sutskever et al., 2014</ref>), visual question an- swering, caption generation ( <ref type="bibr">Xu et al., 2015)</ref>, etc. These works extract image representations from a pre-trained CNN ( <ref type="bibr">Zhu et al., 2016;</ref><ref type="bibr">Wang et al., 2016)</ref>.</p><p>Unrelated to images, CNNs have also been used for text classification <ref type="bibr" target="#b15">(Kim, 2014;</ref><ref type="bibr">Zhang et al., 2015</ref>). These models look at the sequential depen- dencies at the word or character-level and achieve the state-of-the-art results. These works inspire us to use CNN to extract features from image and serve as the input to the RNN. Our model is able to directly back-propagate the gradient all the way through the CNN, which generates visual embed- dings, in a way such that the embedding can con- tain both semantic and visual information.</p><p>Several techniques for reducing the rare words effects have been introduced in the literature, in- cluding spelling expansion <ref type="bibr" target="#b10">(Habash, 2008)</ref>, dictio- nary term expansion <ref type="bibr" target="#b10">(Habash, 2008)</ref>, proper name transliteration ( <ref type="bibr" target="#b5">Daumé and Jagarlamudi, 2011)</ref>, treating words as a sequence of characters <ref type="bibr">(Luong and Manning, 2016)</ref>, subword units ( <ref type="bibr">Sennrich et al., 2015)</ref>, and reading text as bytes ( <ref type="bibr">Gillick et al., 2015)</ref>. However, most of these techniques still have no mechanism for handling low fre- quency characters, which are the target of this work.</p><p>Finally, there are works on improving embed- dings with radicals, which explicitly splits Chi- nese characters into radicals based on a dictionary of what radicals are included in which characters ( <ref type="bibr">Li et al., 2015;</ref><ref type="bibr">Shi et al., 2015;</ref><ref type="bibr">Yin et al., 2016)</ref>. The motivation of this method is similar to ours, but is only applicable to Chinese, in contrast to the method in this paper, which works on any lan- guage for which we can render text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we proposed a new framework that utilizes appearance of characters, convolu- tional neural networks, recurrent neural networks to learn embeddings that are compositional in the component parts of the characters. More specif- ically, we collected a Wikipedia dataset, which consists of short titles of three different languages and satisfies the compositionality in the characters of the language. Next, we proposed an end-to-end model that learns visual embeddings for characters using CNN and showed that the features extracted from the CNN include both visual and semantic information. Furthermore, we showed that our VISUAL model outperforms the LOOKUP baseline model in low frequency instances. Additionally, by examining the character embeddings visually, we found that our VISUAL model is able to learn visually related embeddings.</p><p>In summary, we tackled the problem of rare characters by using embeddings learned from im- ages. In the future, we hope to further general- ize this method to other tasks such as pronuncia- tion estimation, which can take advantage of the fact that pronunciation information is encoded in parts of the characters as demonstrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, or machine translation, which could benefit from a wholistic view that considers both semantics and pronunciation. We also hope to apply the model to other languages with complicated compositional writing systems, potentially including historical texts such as hieroglyphics or cuneiform.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of character-level compositionality in (a, b) Chinese, (c) Korean, and (d) German. The red part of the characters are shared, and affects the pronunciation (top) or meaning (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of two models, our proposed VISUAL model at the top and the baseline LOOKUP model at the bottom using the same RNN architecture. A string of characters (e.g. "温 病学"), each converted into a 36x36 image, serves as input of our VISUAL model. d c is the dimension of the character embedding for the LOOKUP model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Experiments on different training sizes for four different datasets. More specifically, we consider three different training data size percentages (TPs) (100%, 50%, and 12.5%) and four datasets: (a) traditional Chinese, (b) simplified Chinese, (c) Japanese, and (d) Korean. We calculate the accumulated number of correctly predicted instances for the VISUAL model (solid lines) and the LOOKUP model (dashed lines). This figure is a log-log plot, where x-axis shows rarity (rarest to the left), y-axis shows cumulative correctly classified instances up to this rank; a perfect classifier will result in a diagonal line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of how much each part of the character contributes to its embedding (the darker the more). Two characters are shown per radical to emphasize that characters with same radical have similar patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of the Chinese traditional characters by finding the 6-nearest neighbors of the query (i.e., center) characters. The highlighted red indicates the radical along with the meaning of the characters.</figDesc><graphic url="image-72.png" coords="8,73.32,125.74,107.82,53.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Architecture of the CNN used in the ex-
periments. All the convolutional layers have 32 
3×3 filters. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The classification results of the LOOKUP 
/ VISUAL models for different percentages of full 
training size. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Experiment results for three different fu-
sion methods across 4 datasets. The late fusion 
model was better (p-value &lt; 0.001) across four 
datasets. 

</table></figure>

			<note place="foot" n="1"> In English, for example, this is largely the case.</note>

			<note place="foot" n="2"> Other prominent examples are largely for extinct languages: Egyptian hieroglyphics, Mayan glyphs, and Sumerian cuneiform scripts (Daniels and Bright, 1996).</note>

			<note place="foot" n="2"> Dataset Before delving into the details of our model, we first describe a dataset we constructed to examine the ability of our model to capture the compositional characteristics of characters. Specifically, the dataset must satisfy two desiderata: (1) it must be necessary to fully utilize each character in the input in order to achieve high accuracy, and (2) there must be enough regularity and com</note>

			<note place="foot" n="3"> Model Our overall model for the classification task follows the encoder model by Sutskever et al. (2014).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Taylor Berg-Kirkpatrick, Adhiguna Kuncoro, Chen-Hsuan Lin, Wei-Cheng Chang, Wei-Ning Hsu and the anonymous reviewers for their enlightening comments and feedbacks.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1899" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Torch: A modular machine learning software library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Marithoz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Habbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The world&apos;s writing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain adaptation for machine translation by mining unseen words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">What you get is what you see: A visual markup decompiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anssi</forename><surname>Kanervisto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04938</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Tweet2vec: Character-based distributed representations for social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Muehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The foundations of arithmetic: A logico-mathematical enquiry into the concept of number</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gottlob</forename><surname>Frege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Langshaw</forename><surname>Austin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Northwestern University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Brunk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00103</idno>
		<title level="m">Oriol Vinyals, and Amarnag Subramanya. 2015. Multilingual language processing from bytes</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-Short</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan L</forename><surname>Boydgraber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting Wikipedia as external knowledge for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torisawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
