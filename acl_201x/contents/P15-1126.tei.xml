<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Orthogonality of Syntax and Semantics within Distributional Spaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
							<email>jeff.mitchell@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Informatics</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
							<email>steedman@inf.ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Orthogonality of Syntax and Semantics within Distributional Spaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1301" to="1310"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A recent distributional approach to word-analogy problems (Mikolov et al., 2013b) exploits interesting regularities in the structure of the space of representations. Investigating further, we find that performance on this task can be related to orthogonality within the space. Explicitly designing such structure into a neu-ral network model results in representations that decompose into orthogonal semantic and syntactic subspaces. We demonstrate that learning from word-order and morphological structure within En-glish Wikipedia text to enable this decomposition can produce substantial improvements on semantic-similarity, pos-induction and word-analogy tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributional methods have become widely used across computational linguistics. Recent applica- tions include predicate clustering for question an- swering ( <ref type="bibr" target="#b1">Lewis and Steedman, 2013)</ref>, bilingual embeddings for machine translation ( <ref type="bibr" target="#b12">Zou et al., 2013)</ref> and enhancing the coverage of POS tag- ging ( <ref type="bibr" target="#b0">Huang et al., 2013)</ref>. The popularity of these methods, stemming from their conceptual simplic- ity and wide applicability, motivates a deeper anal- ysis of the structure of the representations they produce.</p><p>Commonly, these representations are made in a single vector space with similarity being the main structure of interest. However, recent work by <ref type="bibr" target="#b6">Mikolov et al. (2013b)</ref> on a word-analogy task suggests that such spaces may have further use- ful internal regularities. They found that seman- tic differences, such as between big and small, and also syntactic differences, as between big and bigger, were encoded consistently across their space. In particular, they solved the word-analogy problems by exploiting the fact that equivalent re- lations tended to correspond to parallel vector- differences.</p><p>In this paper, we investigate orthogonality be- tween relations rather than parallelism. While par- allelism serves to ensure that the same relation is encoded consistently, our hypothesis is that or- thogonality serves to ensure that distinct relations are clearly differentiable. We focus specifically on semantic and syntactic relations as these are probably the most distinct classes of properties en- coded in distributional spaces.</p><p>Empirically, we demonstrate that orthogonal- ity predicts performance on the word-analogy task for three existing approaches to constructing word vectors. We also attempt to enhance the weak- est of these three models by imposing an orthog- onal structure in its construction. In these exten- sions, word representations decompose into or- thogonal semantic and syntactic spaces, and we use word-order and morphology to drive this sep- aration. This decomposition also allows us to de- fine a novel approach to solving the word-analogy problems and our extended models become com- petitive with the other two original models. In addition, we show that the separate semantic and syntactic sub-spaces gain improved performance on semantic-similarity and POS-induction tasks respectively.</p><p>Our experiments here are based on models that construct vector-representations within a model that predicts the occurence of words in context. In particular we focus on the CBOW and Skip-gram models of Mikolov etal. (2013b) and <ref type="bibr">Pennington et al.'s (2014)</ref> GloVe model. These models share the property of producing a single general repre- sentation for each word, which can be utilized in a variety of tasks, from POS tagging to semantic role labelling. In contrast, here we attempt to de- compose the representations into separate seman- To motivate this decomposition, consider the analogical reasoning task that <ref type="bibr" target="#b6">Mikolov et al. (2013b)</ref> apply neural embeddings to. In this task, given vectors for the words big, bigger and small, we try to predict the vector for smaller. They find that in practice smaller ≈ small + bigger − big produces an estimate that is frequently closer to the actual representation of smaller than any other word vector. We can think of the vector bigger − big as representing the syntactic rela- tion that holds between an adjective and its com- parative. Adding this syntactic structure to small thus ends up at, or near, the relevant comparative, smaller. Alternatively, we could think of the vec- tor small−big as representing the semantic differ- ence between small and big, and adding this rela- tion to bigger produces a semantic transformation to smaller.  question of what happens at the corners. In other words, what is the relationship between the semantic differences, e.g. smaller − bigger, and the syntactic differences, e.g. smaller − small?</p><p>In this paper we explore the idea that such se- mantic and syntactic relations ought to be orthogo- nal to each other. This hypothesis arises both from the intuition that such distinct types of informa- tion ought to be represented distinctly within our space and also from the observation that solving the word-analogy task requires that words can be uniquely identified by combining these vector dif- ferences and so small − big ought to be easily differentiable from bigger − big as these relations point to different end results starting from big. Es- sentially, orthogonality will make better use of the volume within the space, spreading words with different semantic or syntactic characteristics fur- ther from each other.</p><p>In terms of predicting smaller from big, bigger and small, orthogonality of the relationship be- tween smaller − bigger and smaller − small can be expressed in terms of their dot product:</p><formula xml:id="formula_0">(smaller −bigger)·(smaller −small) = 0 (1)</formula><p>If all semantic relations were genuinely orthog- onal to all syntactic relations, then their space would be decomposable into two orthogonal sub- spaces: one semantic, the other syntactic. Any word representation, v, would then be the combi- nation of a unique semantic vector, b, within the semantic subspace and a unique syntactic vector, s, within the syntactic subspace. If b were given a representation in terms of e components, and s in terms of f components, then v would have a repre- sentation in terms of d = e + f components which would just be the concatenation of the two sets of components, which we will represent in terms of the operator ⊕.</p><formula xml:id="formula_1">v = b ⊕ s<label>(2)</label></formula><p>Achieving this differentiation within the repre- sentations requires that the model have a means of differentiating semantic and syntactic informa- tion in the raw text. We consider two very simple approaches for this purpose, based on morpholog- ical and word order features. Both these types of features have been previously employed in simple word co-occurrence models (e.g., <ref type="bibr" target="#b4">McDonald and Lowe, 1998;</ref><ref type="bibr">Clark, 2003)</ref>, with bag-of-words and</p><formula xml:id="formula_2">b −2 w t−2 b −1 w t−1 b 1 w t+1 b 2 w t+2 ¡ ¡ ¡ ¡ ! e e e e u Q       k b context T w t</formula><p>Figure 2: CBOW model predicting w t from of a bag-of-words representation, b context , of a 4-word window around it. lemmatization being good for semantic applica- tions, while sequential order and suffixes is more useful for syntax. More recently, <ref type="bibr" target="#b7">Mitchell (2013)</ref> demonstrated that word order could be used to sep- arate syntactic from semantic structure, but only within a simple bigram language model, rather than a neural network model, and without exploit- ing morphology.</p><p>Our enhanced models are based on Mikolov et al.'s (2013a) CBOW architecture, which is de- scribed in Section 2. The novel extensions to it, employing a semantic-syntactic decomposition, are proposed in Section 3. We then describe our evaluation tasks and provide their results in Sec- tions 5 and 6 respectively. These evaluations are based on the word-analogy dataset of <ref type="bibr" target="#b6">Mikolov et al. (2013b)</ref>, a noun-verb similarity task <ref type="bibr" target="#b7">(Mitchell, 2013</ref>) and a POS clustering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Continuous Bag-of-Words Model (CBOW)</head><p>In the original CBOW model, the probability of a central target word, w t , is predicted from a bag- of-words representation of the context it occurs in, as illustrated in <ref type="figure">Figure 2</ref>. This context representa- tion, b context , is a simple sum of the CBOW vec- tors, b i , that represent each item, w t+i , in a k-word window either side of the target.</p><formula xml:id="formula_3">b context = k i=−k,i =0 b i (3)</formula><p>For speed, the output layer uses a hierarchi- cal softmax function <ref type="bibr" target="#b8">(Morin and Bengio, 2005</ref>).</p><p>Each word is given a Huffman code correspond- ing to a path through a binary tree, and the output predicts the binary choices on nodes of the tree as independent variables. In comparison to the computational cost of doing the full softmax over the whole vocabulary, this hierarchical approach is much more efficient.</p><p>Each node is associated with a vector, n, and the output at that node, given a context vector, b context , is:</p><formula xml:id="formula_4">p = logistic(n · b context ) (4)</formula><p>Here, p is the probability of choosing 1 over 0 at this node of the tree, or equivalently finding a 1 in the Huffman code of w t at the relevant position.</p><p>The objective function is the negative log- likelihood of the data given the model.</p><formula xml:id="formula_5">O = − log(p)<label>(5)</label></formula><p>Where the sum is over tokens in the training cor- pus and the relevant nodes in the tree. Training is then based on stochastic gradient descent, with a decreasing learning rate.</p><formula xml:id="formula_6">p = logistic( k i=−k,i =0 n i · s i )<label>(6)</label></formula><p>Here each node of the tree is associated with one vector, n i , for each position, i, in the input context, giving 2k vectors in total at each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Continuous Bag and Sequence of Words (CBSOW)</head><p>Having introduced a sequential version of the CBOW model, what is really desired is a model that combines both bag and sequence components. Each word will have both an e-dimensional bag- vector b and an f -dimensional sequence-vector s.</p><p>The full representation of a word, v, is then the concatenation of the components of b and s. Given this structure, the representation of a con- text of 2k words will be made up of the sum, b context , of their bag vectors, b i , as in the CBOW model given by Equation 3, along with the ordered sequence vectors, s i , as in the CSOW model. Each node in the tree then requires both a bag vector, n b , to handle the bag context, and 2k sequence vec- tors, n s i , to handle the sequence context vectors, with probabilities given by:</p><formula xml:id="formula_7">p = logistic(n b · b context + k i=−k,i =0 n s i · s i ) (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Continuous Bag of Morphemes (CBOM)</head><p>A second source of information which might be used to differentiate semantic from syntactic rep- resentations is morphology. Specifically, English has the useful characteristic that the written words themselves can often be broken into a semantic stem on the left and a syntactic ending on the right. For example, dancing = dance + ing and swim- mer = swim + er. In fact, stemming or lemma- tization is commonly used in constructing distri- butional vectors precisely because throwing away the syntactic information helps to enhance their se- mantic content. Here, we want to use both the left and right halves separately to enhance both the se- mantic and syntactic components of the represen- tations.</p><p>Our starting point is to break each word into a left-hand stem and a right-hand ending using CELEX ( <ref type="bibr">Baayen et al., 1995)</ref>, as explained in more detail in Section 4.1.</p><p>The simplest model is then to represent each of these with its own vector, l i and r i respectively, and sum these vectors to form context representa- tions of words in the input.</p><formula xml:id="formula_8">l context = k i=−k,i =0 l i (8) r context = k i=−k,i =0 r i<label>(9)</label></formula><p>The output function takes much the same form as the original model but now each node needs both a left and a right vector, corresponding to the two context representations.</p><formula xml:id="formula_9">p = logistic(n l · l context + n r · r context ) (10)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Continuous Bag and Sequence of Words and Morphemes (CBSOWM)</head><p>Finally, we want to incorporate all these elements in a single model, with the morphological and word order elements of the model working in har- mony. In particular, we want the sequential part of the model to be guided by morphological infor- mation without being constrained to give all words with same ending the same representation. Our solution is to add a constraint term to the objec- tive function, which penalizes sequence vectors that stray far from the relevant morphological rep- resentation. The bag vectors, in contrast, are de- termined directly by the left hand stems, with all words having the same stem then sharing the same bag vector, b = l. The main structure of the model remains as in the CBSOW model, with the context being rep- resented by the sum of bag vectors alongside the ordered sequence vectors. Output probabilities are as given by Equation 7, and we add a morpholog- ical penalty, m, to the objective function.</p><formula xml:id="formula_10">m = k i=−k,i =0 1 2 λ|s i − r i | 2<label>(11)</label></formula><p>The morphological representations r enter into the model only through the penalty term, and they adapt during training solely in terms of this in- teraction with the sequence vectors. Gradient de- scent results in the r vectors moving towards the centre of the corresponding s vectors, and the s vectors in turn being drawn towards that centre.</p><p>The result is to elastically connect all the s vec- tors corresponding to a single morphological ele- ment through their r vectors, so that they are drawn together, but can still develop idiosyncratically if there is sufficient evidence in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Application to the Word-Analogy Task</head><note type="other">Decomposition of representations into separate se- mantic and syntactic spaces enables us to utilise a new approach to solving the word-analogy prob- lems. Rather than using vector differences to predict a vector, we can instead construct it by copying the relevant bag and sequence vectors. So, since small and smaller share very similar semantic content, we can use the bag vector of small as the bag vector of smaller, since that is where the semantic content is mainly represented: b smaller ≈ b small . Similarly, we can use the se- quence vector of bigger as the sequence vector for smaller, since these words share common syntac- tic behaviour: s smaller ≈ s bigger .</note><p>The predicted representation of smaller is then given by the concatenation of the components.</p><formula xml:id="formula_11">v smaller ≈ b small ⊕ s bigger<label>(12)</label></formula><p>We find that this gives the best performance on the models that use word-order features (CBSOW and CBSOWM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head><p>Our experiments are based on the publicly avail- able word2vec 1 and GloVe 2 packages. We mod- ified the original CBOW code to incorporate the CBSOW, CBOM and CBSOWM extensions de- scribed above, and trained models on three En- glish Wikipedia corpora of varying sizes, includ- ing the enwik8 and enwik9 files 3 suggested in the word2vec documentation, containing the first 10 8 and 10 9 characters of a 2006 download, and also a full download from 2009. On the smallest 17M word corpus we explored a range of vector dimen- sionalities from 10 to 1000. On the larger 120M and 1.6B word corpus, we trained extended mod- els with a 200-dimensional semantic component and a 100-dimensional syntactic component com- paring to 300-dimensional CBOW, Skip-gram and GloVe models. The parameter, λ, in Equation 11 was set to 0.1 and the recommended window sizes of 5, 10 and 15 words either side of the central word were used as context for the CBOW, Skip- gram and GloVe models respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CELEX</head><p>We attempted to split all the words in the training data into a left hand and a right hand using CELEX ( <ref type="bibr">Baayen et al., 1995)</ref>, an electronic dictionary con- taining morphological structure. In the cases of words that were not found in the dictionary and also those that were found but had no morpholog- ical substructure, the left hand was just the whole word and the right hand was a −N U LL− token. For the remaining words, we treated short suf- fixes as being syntactic inflections and stripped all these off to leave a left hand 'semantic' compo- nent. The 'syntactic' component was then right- most of these suffixes, with any additional suffixes being ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The hypothesis that orthogonality is useful to word vector representations is investigated empirically in two ways. Firstly, we attempt to quantify the orthogonality that is already implicitly present in the original CBOW, Skip-gram and GloVe repre- sentations and relate that to their success in the word-analogy task. Secondly, the extensions de- scribed above are evaluated on a number of tasks in order to evaluate the benefits of their explicit orthogonality between components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Orthogonality within the Original Models</head><p>Equation 1 relates orthogonality of vector differ- ences to their dot product being zero, which cor- responds to the fact the cosine of 90 • is zero. Thus, we can use the cosine as a quantifica- tion of how close to orthogonal the vector dif- ferences are and then relate that to performance on the word-analogy dataset distributed with the word2vec toolkit.</p><p>That task involves predicting a word vector given vectors for other related words. So, for ex- ample, given vectors for big, bigger and small, we would try to predict a vector for smaller. We then judge the success of this prediction in terms of whether the predicted vector is in fact closer to smaller's actual word vector than any other word vector. The dataset contains 19,544 items, bro- ken down into 14 subtasks (e.g. capitals of com- mon countries or adjective to adverb conversion).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">CBOW Extensions</head><p>We evaluate the extensions on three tasks. Along- side the word-analogy problems, we also evalu- ate the separate semantic and syntactic sub-spaces on their own individual tasks. The semantic task correlates predicted semantic similarities with the noun-verb similarity ratings gathered by <ref type="bibr" target="#b7">Mitchell (2013)</ref>, and the remaining task clusters the syntac- tic representations and evaluates these clusters in relation to the POS classes found in the Penn Tree- bank.</p><p>On the word-analogy problem we compare to the original CBOW, Skip-gram and GloVe mod- els. In the case of these original models and also the CBOM model, we follow Mikolov et al.'s (2013b) method for making the word-analogy predictions in terms of addition and subtraction: smaller ≈ bigger − big + small. However, in the case of the CBSOW and CBSOWM models, we use the novel approach described in Section 3.5: v smaller ≈ b small ⊕ s bigger . Similarity is then based on the cosine measure for all types of repre- sentation.</p><p>The noun-verb similarity task is based on cor- relating the model's predicted semantic similarity for words with human ratings gathered in an on- line experiment. Such evaluations have been com- monly used to evaluate distributional representa- tions, with higher correlations indicating a model which is more effective at forming vectors whose relations to each other mirror human notions of se- mantic similarity. <ref type="bibr" target="#b7">Mitchell (2013)</ref> argued that pre- dicting semantic similarity relations across syntac- tic categories provided a measure of the extent to which word representations succeed in separating semantic from syntactic content, and gathered a dataset of similarities for noun-verb pairs. Each rated item consists of a noun paired with a verb, and the pairs are constucted to range from high se- mantic similarity, e.g. disappearance -vanish, to low, e.g. transmitter -grieve. The dataset contains ratings for 108 different pairs, each of which was rated by 20 participants. For the CBOW model, we predict similarities in terms of the cosine mea- sure for the two word vectors. For the other mod- els, we predict similarities from cosine applied to just the bag or left-hand vectors.</p><p>The syntactic component of the representations is evaluated by clustering the vectors and then comparing the induced classes to the POS classes found in the Penn Treebank. We use the many- to-one measure ( <ref type="bibr">Christodoulopoulos et al., 2010;</ref><ref type="bibr" target="#b11">Yatbaz et al., 2012</ref>) to determine the extent to which the clusters agree with the POS classes. Each cluster is mapped to its most frequent gold tag and the reported score is the proportion of word tokens correctly tagged using this mapping. The clustering itself is a form of k-means cluster- ing, where similarity is measured in terms of the cosine measure. Each vector is assigned to a clus- ter based on which cluster centroid it is most sim- ilar to and then the cluster centroids are updated given the new cluster assignments and the process repeats. This clustering was applied to either the sequence or right-hand vectors in the case of the CBSOW, CBOM and CBSOWM models, and to the whole vectors in the case of CBOW. We ran- domly initialized 45 clusters and then evaluated af- ter 100 iterations of the k-means algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Original Models</head><p>Figure 3 is a plot of the proportion of correct pre- dictions made by 100-dimensional CBOW, Skip- Gram and GloVe models on the word-analogy task against cosine of the angle between the vector dif- ferences. The range of the cosine distribution was broken into twenty intervals and the plotted values were derived by calculating the proportion correct and average cosine value within each interval. It is clear from the resulting curves that cosine is a fairly strong predictor for all models of whether the model gets a word-analogy item correct, with higher rates of success for smaller cosine values -i.e. angles closer to orthogonality. This is con- firmed by a significant (p &lt; 0.001) result from a logistic regression of correctness against cosine value. Similar results are found for both the se- mantic subtasks (e.g. capitals of common coun- tries) and syntactic subtasks (e.g. adjective to ad- verb conversion) considered separately. The actual distribution of cosine values for each type of model is given in <ref type="figure" target="#fig_5">Figure 4</ref>. This analy-  sis reveals that while the Skip-Gram and GloVe models have fairly similar cosine distributions, the CBOW model's distribution is shifted to the right, with more angles further from othogonality. This begs the question of what the effect on perfor- mance would be if we managed to push more of the CBOW distribution towards zero, and in the next section we examine the extensions that im- plement this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">CBOW Extensions</head><p>We first consider the models trained on the smaller 17M word corpus, and the evaluations of these models on the noun-verb similarity and POS clus- tering tasks are presented in <ref type="figure" target="#fig_6">Figures 5 and 6</ref> re- spectively. These graphs depict the performance as the representations grow in size. For the CBOW model, this is just the dimension of the induced vectors. For the other models, we consider mod- els with equal sizes of semantic and syntactic sub- spaces and report performance against the total di- mensionality of the combined representation. For both these tasks, the results were averaged over ten repetitions of training with random initializations. On the noun-verb similarity task, morphol- ogy produces the largest performance gains, with the CBOM model substantially outperforming the CBOW model. Word order structure has no clear impact.</p><p>On the syntactic task, in contrast, it is word or- der that produces reliable gains, with the CBSOW model clearly improving on the CBOW model. The simplistic use of morphology in the CBOM model results in a degradation of performance in comparison to the CBOW model, but the CB- SOWM model's performance is comparable to that of the CBSOW for larger representations.</p><p>Thus for these two tasks, the CBSOWM re- sults appear to show a reasonable integration of morphology and word order information giving good performance on both semantic and syntac- tic tasks. This conclusion is borne out the results of the word-analogy tasks in <ref type="figure" target="#fig_8">Figure 7</ref>, where the CBSOWM model outperforms all the other mod- els. Here, morphology gives the greatest benefit on its own, as evidenced in the differences be- tween the CBOW and CBOM models. Nonethe- less, word order still produces noticeable improve- ments, with the CBSOW result beating the CBOW results, and the CBSOWM beating the CBOM at larger dimensions. There is considerable variation in the effects on performance among the various analogy subtasks, but even a task such as capi- tals of common countries (e.g. predicting Iraq as having Baghdad as its capital, given that Greece has Athens) appears to benefit from decomposi- tion of representations, despite not obviously in- volving syntactic structure.  <ref type="table" target="#tab_2">Table 1</ref>: Performance of 300-Dimensional Models on the Word-Analogy Task gains than word order, and the combined CB- SOWM model outperforming both. This perfor- mance advantage of the CBOM over CBSOW ap- pears to weaken as the training data grows, which is probably the effect of both the lack of morpho- logical information for rare words encountered in the larger datasets and also the diminishing returns on that information as more data provides better supervision of the training process. The sequen- tial information, in contrast, is internal to the train- ing data and seems to provide the same, or greater, performance boost as the training set grows.</p><p>Comparing the results of our extended models to the Skip-gram and GloVe models, we can see that on the two smaller corpora CBSOWM outper- forms both these models, while on the largest cor- pus, it only beats the Skip-gram results and GloVe achieves the best performance. Of course, nei- ther the Skip-gram nor GloVe models has access to the morphological information that the CBSOWM model uses, but the results demonstrate that the performance of the CBOW model can be sub- stantially boosted by exploiting a representational structure that decomposes into semantic and syn- tactic sub-spaces. Similar methods could in prin- ciple be applied to most word embedding models, including Skip-gram and GloVe.</p><p>We can also examine the distribution of cosine values for the new models. <ref type="figure" target="#fig_9">Figure 8</ref> compares the distribution of cosine values for CBOW, CB- SOW, CBOM and CBSOWM models. Although, in comparison to the original CBOW model, each of the extended models shifts the distribution to- wards zero, i.e. towards orthogonality, this shift for the CBSOW model is marginal. In contrast, the CBOM model has a large number of instances where the cosine is exactly zero, corresponding to cases where all of the relevant morphological information is found in CELEX. The remainder of the data, however, seems to be less orthogo- nal than the original CBOW distribution, suggest- ing that words without a morphological analysis need a more sophisticated treatment. The shift in the CBSOWM distribution, in comparison, is less radically bimodal, with more continuity between those words with and without morphology. This reflects the difference in these models handling of suffixes, with the CBSOWM model's greater flex- ibility resulting in gains over the CBOM model on the POS induction and word analogy tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Our experiments demonstrate the utility of orthog- onality within vector-space representations in a number of ways. In terms of existing models, we find that the cosines of vector-differences is a strong predictor of the performance of CBOW, Skip-gram and GloVe representations on the word analogy task, with smaller cosine values -corre- sponding to angles closer to orthogonality -being associated with a greater proportion of correct pre- dictions. With regard to developing new models, this orthogonality of relationships inspired three models which used word-order and morphology to separate semantic and syntactic representations. These separate sub-spaces were shown to have enhanced performance in semantic similarity and POS-induction tasks and the combined representa- tions showed enhanced performance on the word- analogy task, using a novel approach to solving this problem that exploits the decomposable struc- ture of the representations. Both <ref type="bibr">Botha and Blunsom (2014)</ref> and <ref type="bibr" target="#b3">Luong et al. (2013)</ref> take a more sophisticated approach to morphology <ref type="bibr">4</ref> , constructing a word's embedding by recursively combining representations of all its morphemes, though only within a single non- decomposed space. Future work ought to pursue models in which all morphemes contribute both semantic and syntactic content to the word repre- sentations. It would also be desirable to explore more prac- tical applications of these representations than the limited evaluations presented here. It seems fea- sible that our decomposition of representations could benefit tasks that need to differentiate their treatment of semantic and syntactic content. In particular, applications of word embeddings that mainly involve syntax, such as POS tagging (e.g., <ref type="bibr" target="#b10">Tsuboi, 2014</ref>) or supertagging for parsing (e.g., <ref type="bibr" target="#b2">Lewis and Steedman, 2014</ref>), may be a reasonable starting point.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Geometric relationships between small, smaller, big and bigger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Mikolov et al. (2013b) represent these sort of relations in terms of a diagram similar to Figure 1. The image places the four words in a 2D space and represents the relations between them in terms of arrows. The solid black arrows represent the syn- tactic relations smaller −small and bigger −big, while the gray dashed arrows represent the seman- tic differences smaller − bigger and small − big. Their solution to the analogy problem exploits the fact that these pairs of relations are approximately parallel to each other, i.e. that we can approx- imate smaller − small with bigger − big, or smaller − bigger with small − big. However, knowing that opposite sides of the square in Fig- ure 1 are parallel to each other still leaves open</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>the</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Proportion Correct against Average Cosine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Frequency against Cosine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average Correlation on Noun-Verb Evaluation Task against Size of Representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average Many-To-One Evaluation against Size of Representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Proportion Correct on the Analogy Task against Size of Representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Frequency against Cosine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 compares</head><label>1</label><figDesc>300-dimensional models across different sizes of training data. In the case of the CBSOW, CBOM and CBSOWM mod- els we use representations with 200 semantic and 100 syntactic dimensions and compare these to CBOW, Skip-gram and GloVe models of the same total size. It is clear for all quantities of train- ing data that all the extensions outperform the ba- sic CBOW model, with morphology giving greater</figDesc><table>Training Words 
Model 
17M 
120M 
1.6B 
GloVe 
29.53% 58.18% 72.54% 
Skip-Gram 30.03% 52.67% 62.34% 
CBOW 
18.47% 38.48% 54.17% 
CBSOW 
20.83% 42.00% 59.41% 
CBOM 
44.29% 53.60% 61.87% 
CBSOWM 48.92% 63.19% 68.32% 

</table></figure>

			<note place="foot" n="3"> Extensions 3.1 Continuous Sequence of Words (CSOW) A major feature of the CBOW model is its use of a bag-of-words representation of the context and this is achieved by summing over the vectors representing words in the input. Although the model does seem to produce representations that are effective on both semantic and syntactic tasks, we want to be able to exploit word order information to separate these two characteristics. We therefore need to consider models which do not reduce the context to a structureless bag-of-words. Modifying the original model to retain the sequential information in the input is relatively straightforward. Instead of summing the input representations, we simply leave them as an ordered sequence of vectors, s i. Then in the output layer, we require a vector for every input position, i, on every node. In this way, the output of the network depends on which context word is in which position, rather than just the set of words, irrespective of position in the input. The network still learns a single representation for each word independently of position, but the output function has more parameters.</note>

			<note place="foot" n="1"> https://code.google.com/p/word2vec/ 2 http://nlp.stanford.edu/projects/glove/ 3 http://mattmahoney.net/dc/text.html</note>

			<note place="foot" n="4"> Though not neccessarily better performing. Luong et al.&apos;s published 50-dimensional embeddings trained on 986M words scored only 13.57% on the word-analogy task, well behind 40-dimensional CBOM (34.68%) and CBSOWM (36.71%) models trained on 17M words.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Stella Frank, Sharon Gold-water and other colleagues along with our review-ers for criticism, advice and discussion. This work was supported by ERC Advanced Fellow-ship 249520 GRAMPLUS and EU Cognitive Sys-tems project FP7-ICT-270273 Xperience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head><p>Harald Baayen, <ref type="bibr">Richard Piepenbrock, and Hedderik van Rijn. 1995</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Representations for Weakly Supervised Natural Language Processing Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="85" to="120" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combined distributional and logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A* CCG parsing with a supertag-factored model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="990" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modelling functional priming and the associative boost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Meeting of the Cognitive Science Society</title>
		<meeting>the 20th Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<publisher>Erlbaum</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="675" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning semantic representations in a bigram language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013)-Short Papers</title>
		<meeting>the 10th International Conference on Computational Semantics (IWCS 2013)-Short Papers<address><addrLine>Potsdam, Germany, March</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="362" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar, October</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural networks leverage corpuswide information for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Tsuboi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="938" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning syntactic categories using paradigmatic representations of word context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enis</forename><surname>Mehmet Ali Yatbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="940" to="951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
