<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yow-Ting</forename><surname>Shiue</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hen-Hsen</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Detection of Chinese Word Usage Errors for Non-Native Chinese Learners with Bidirectional LSTM</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="404" to="410"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2064</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Selecting appropriate words to compose a sentence is one common problem faced by non-native Chinese learners. In this paper , we propose (bidirectional) LSTM sequence labeling models and explore various features to detect word usage errors in Chinese sentences. By combining CWIN-DOW word embedding features and POS information, the best bidirectional LSTM model achieves accuracy 0.5138 and MRR 0.6789 on the HSK dataset. For 80.79% of the test data, the model ranks the ground-truth within the top two at position level.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, more and more people around the world choose Chinese as their second language. That re- sults in an increasing need for automatic grammat- ical error detection and correction (GEC) tools. To measure the performance of GEC systems in a standardized manner, several shared tasks have been conducted for English ( <ref type="bibr" target="#b3">Dale and Kilgarriff, 2011;</ref><ref type="bibr" target="#b2">Dale et al., 2012;</ref><ref type="bibr" target="#b16">Ng et al., 2013</ref><ref type="bibr" target="#b15">Ng et al., , 2014</ref>) and Chinese ( <ref type="bibr" target="#b22">Yu et al., 2014;</ref><ref type="bibr" target="#b9">Lee et al., 2015</ref><ref type="bibr" target="#b8">Lee et al., , 2016</ref>.</p><p>In Chinese sentences, a word usage error (WUE) is a grammatically or semantically incor- rect token which is written in a wrong form itself, or is an existent word but is improper for its con- text (refer to example (E1)). In fact, many Chi- nese WUEs result from subtle semantic unsuitabil- ity instead of violation of syntactic constraints. In example (E1), both 權力 (power) and 權利 (right) are nouns in Chinese, and both versions are gram- matically correct. It is difficult to formulate an ex- plicit rule for recognizing this kind of errors.</p><p>(E1) 人們 有 (*權力,權利) 吃 安全 的 食品 。 ( People have the (*power, right) to enjoy safe food. )</p><p>Shiue and Chen (2016) adopted the HSK cor- pus, a dynamic composition corpus built by Bei- jing Language and Culture University, to study the detection of WUEs. Instead of specific position in- formation, their model only determines whether a sentence segment contains WUEs.  used the HSK corpus to study the prepo- sition selection problem. They proposed gated recurrent unit (GRU)-based models to select the most suitable one from a closed set of Chinese prepositions given the sentential context. Al- though their approach can be utilized to detect and correct preposition errors, it is still worth inves- tigating how to recognize WUEs involving other types of words such as verbs and nouns.</p><p>In the past few years, distributed word rep- resentations derived from neural network mod- els ( <ref type="bibr" target="#b13">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b17">Pennington et al., 2014</ref>) have become popular among various stud- ies in natural language processing. Beyond sur- face forms, these low-dimensional vector repre- sentations can encode syntactic and semantic in- formation implicitly <ref type="bibr" target="#b14">(Mikolov et al., 2013b</ref>). Be- cause WUEs involve syntactic or semantic prob- lems, vector representations could be promising for finding the erroneous tokens.</p><p>One challenging aspect of dealing with gram- matical errors is that the errors usually do not stand on their own, but are dependent on the context ( <ref type="bibr" target="#b0">Chollampatt et al., 2016</ref>). Therefore, we need a model that considers the sequence of words in a sentence as a whole to determine which position needs correction. One possible model for this task is the Long Short-Term Memory (LSTM) model <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber, 1997)</ref>, which pro- cesses sequential data and generates the output based not only on the information of the current time step, but also on the past information stored in the memory layer. <ref type="bibr" target="#b18">Rei and Yannakoudakis (2016)</ref> adopted neural network models, including LSTM, to detect errors in English learner writ- ing. However, they mainly focused on compar- ing different composition architectures under the same word representation, so it remained unclear to what extent pre-trained word embeddings can help. <ref type="bibr" target="#b6">Huang and Wang (2016)</ref> used LSTM for Chinese grammatical error diagnosis, but their models are trained only on learner data, without external well-formed text. That means the per- formance might be limited by the relatively small amount of annotated sentences written by foreign learners.</p><p>This paper utilizes LSTM and its extension (Bidirectional LSTM) along with the information derived from external resources to deal with Chi- nese WUE detection. Several types of pre-trained word embeddings and additional token-level fea- tures are considered. Each token in a sentence will be labeled correct or incorrect. Experimental re- sults show that our models can rank the ground- truth error position toward the top of the candidate list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">WUE Detection Based on Bidirectional LSTM</head><p>We formulate the Chinese WUE detection task as a sequence labeling problem. Each token, the fun- damental unit after word segmentation, is labeled either correct (0) or incorrect (1).</p><p>We utilize the LSTM model for labeling. LSTM models long sequences better than simple re- current neural network (RNN) does, since it is equipped with input, output and forget gates to control how much information is used. The ability of LSTM to capture longer dependencies among time steps makes it suitable for modeling the com- plex dependencies of the erroneous token on the other parts of the sentence.</p><p>We train the LSTM model with the Adam op- timizer ( <ref type="bibr" target="#b7">Kingma and Ba, 2014</ref>) implemented in Keras <ref type="bibr" target="#b1">(Chollet, 2015)</ref>. The loss function is bi- nary cross entropy. The batch size and the initial learning rate is set to 32 and 0.001 respectively. The training process is stopped when the valida- tion accuracy does not increase for two consecu- tive epochs. The model with the highest validation accuracy is selected as the final model.</p><p>We apply a sigmoid activation function before the output layer, so the output score of each token, which is between 0 and 1, can be interpreted as the predicted level of incorrectness. With these scores, our system can output a ranked list of candidate error positions. The positions with the highest in- correctness scores will be marked as incorrect. In (E2) we show an example labeling result of our system. The tokens 差 (bad) and 知識 (knowl- edge), with the highest scores, are most likely to be incorrect.</p><formula xml:id="formula_0">(E2) 學習 的 知 知 知識 識 識 也 很 差 差 差 0.056 0.035 0.153 0.039 0.030 0.429</formula><p>( The knowledge learned is also very bad. ) Bidirectional LSTM ( <ref type="bibr" target="#b19">Schuster and Paliwal, 1997</ref>) is an extension of LSTM which includes a backward LSTM layer. Both information before and after the current time step are taken into con- sideration. We need the "future" information to detect the error in example (E3). The incorrect- ness of the token 留在 (left at) cannot be deter- mined without considering its object 我們 (us).</p><formula xml:id="formula_1">(E3) 店 是 爸爸 (*留在,留給) 我們 的 。 (</formula><p>The store is our father left (*at,to) us. )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequence Embedding Features</head><p>We consider the word sequence in a sentence and the corresponding POS tag sequence. They are mapped to sequences of real-valued vectors through an embedding layer. These vectors are also updated during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Embeddings</head><p>We set the word embedding size to 400. Besides randomly initialized embedding, we also tried sev- eral types of pre-trained word vectors. To train the word embeddings, we utilize the Chinese part of the ClueWeb09 dataset 1 . The Chinese part was extracted and segmented by <ref type="bibr" target="#b21">Yu et al. (2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CBOW/Skip-gram Word Embeddings</head><p>We trained word vectors with the two architec- tures included in the word2vec software ( <ref type="bibr" target="#b13">Mikolov et al., 2013a</ref>). The continuous bag-of-words model (CBOW) uses the words in a context win- dow to predict the target word, while the skip- gram model (SG) uses the target word to predict every word in the context window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CWINDOW/Structured Skip-gram Word Embeddings</head><p>Taking the order of the context words into con- sideration, we also employ the continuous win- dow model (CWIN) and the structured skip-gram model (Struct-SG) ( <ref type="bibr" target="#b10">Ling et al., 2015)</ref>. The former replaces the summation of context word vectors in CBOW with a concatenation operation, and the latter applies different projection matrices for pre- dicting context words in different relative position with the target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">POS Embeddings</head><p>The POS embeddings are randomly initialized. We set the embedding size to 20, which is slightly smaller than the number of different POS tags <ref type="formula">(30)</ref> in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Token Features</head><p>In addition to representing each token as a real- valued vector, we also incorporate some abstract features. These features are derived from the Google Chinese Web 5-gram corpus ( <ref type="bibr" target="#b11">Liu et al., 2010)</ref> and will be referred to as "n-gram features".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Out-of-Vocabulary Indicator</head><p>This feature is simply a bit indicating whether a word is an out-of-vocabulary word or not. If a to- ken never appears in the Web 5-gram corpus, the bit is set to 1; otherwise it is set to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">N-gram Probability Features</head><p>We compute the n-gram probability of each token using the occurrence count in the Web 5-gram cor- pus. We consider only up to trigrams since the probabilities are mostly zero when n &gt; 3. Given the limited amount of available learner data, these probabilities may serve as useful features indicat- ing how likely an expression is valid in Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>We obtain the "wrong" part of the HSK dataset used in <ref type="bibr" target="#b20">(Shiue and Chen, 2016)</ref>. Each sentence segment has exactly one token-level position that is erroneous. Word segmentation and POS tagging are performed with the Stanford CoreNLP toolkit ( ). We filter out any sentence segment whose corrected version differs from it by more than one token due to segmentation is- sue. That is, we only focus on the cases in which the error can be corrected by replacing one single token. After filtering, we end up with 10,510 sen- tence segments. We use 10% data for validation and testing respectively, and the remaining 80% data as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head><p>We use the detection accuracy as our main eval- uation metric. A test instance is regarded as cor- rect only if our system gives the highest score of incorrectness for the ground-truth position. This metric is relatively strict as the average length of the sentence segments in our dataset is 9.24. The McNemar's test is adopted to perform statistical significance test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean Reciprocal Rank (MRR)</head><p>The mean reciprocal rank rewards the test in- stances for which the model ranks the ground-truth near the top of the candidate list. MRR is defined as 1</p><formula xml:id="formula_2">N N i=1 1 rank(i) ,</formula><p>where N is the total number of test instances and rank(i) is the rank of the ground-truth position of test instance i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hit@k Rate</head><p>The Hit@k rate regards a test instance as correct if the answer is ranked within the top k places. In the experiments, k is set to 2. We report this met- ric since one of the most common types of WUEs is collocation error. In example (E2), the prob- lem involves a pair of words, i.e., the adjective 差 (bad) is not a suitable modifier of the noun 知識 (knowledge). (E4) and (E5) are both acceptable.</p><p>(E4) 學習 的 知識 也 很 不 不 不足 足 足 ( The knowledge learned is also insufficient. ) (E5) 學習 的 態 態 態度 度 度 也 很 差 ( The attitude of learning is also very bad. ) Which correction is better highly depends on the context or even the intended meaning in the writer's mind. If the model proposes two poten- tially erroneous tokens which are closely related to each other, it can be useful for Chinese learners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hit@r% Rate</head><p>Finding the exact position of the error could be more challenging in a longer sentence segment. We propose another hit rate measure which takes the segment length (len) into account. Specif- ically, we regard one test instance as correct if the answer is ranked within the top max(1, len * r%) candidates. We report hit@20%. That is, for segments shorter than 10 tokens, the system is al- lowed to propose one candidate; for those whose length is between 10 and 14, the system is al- lowed to propose two, and so on. Equivalently, this measure judges whether our system can rank the ground-truth error position within the top 20%  <ref type="table">Table 1</ref> shows the performance of our WUE de- tection models with different input features. The random baseline is a system randomly choosing one token as the incorrect position. The LSTM model using only randomly initialized word em- beddings largely outperforms the random base- line. The pre-trained CBOW/SG word embed- dings seem not very useful, leading to detection performance slightly lower than the model with random initial word embeddings. For both CBOW and SG, introducing the POS sequence improves the detection accuracy by about 2% and also im- proves all other measurements. The n-gram fea- tures further increase the accuracy by about 1%. On the other hand, the CWIN and Struct-SG embeddings themselves are very powerful. In- corporating the POS and n-gram features leads to only slight improvements in terms of accu- racy. Despite the small impact on accuracy, the n-gram features bring obvious improvements on hit@2 and hit@20% rates, indicating that they do facilitate the model in promoting the rank of the ground-truth position. Under the same set of features, all models with CWIN/Struct-SG signif- icantly outperform their CBOW/SG counterparts (p &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>Bidirectional LSTM (Bi-LSTM) further en- hance the performance of LSTM. The Bi-LSTM with CWIN+POS features achieves the best accu- racy and MRR, and significantly outperforms its LSTM counterpart (p &lt; 0.005). Bi-LSTM with CWIN+POS+n-gram features achieves the best Hit@2 and Hit@20%. To take a closer look, we analyze the performance of the two types of mod- els on different length of segments in <ref type="table" target="#tab_1">Table 2</ref>. We use the versions with all set of features and report hit@20% rates. Using Bi-LSTM leads to some improvement on short (≤ 9 tokens) segments, and larger improvement on mid-length (10~14 tokens) ones. Even longer (≥ 15 tokens) segments are relatively rare since foreign learners seldom con- struct complex sentences.</p><p>In Section 5.2 we justify the use of the hit@2 metric by pointing out that a WUE usually in- volves a pair of words dependent on each other. We can verify whether the top two candidates pro- posed by our model are closely related by exam- ining the dependency distance. We take the out-  # correct (c1 = a) 520 (49.48%) # tests where c2 = a 339 (32.25%) Average dis(c1, c2) when c2 = a 2.07 # tests where c2 = a and dis(c1, c2) = 1 129 (12.27%) <ref type="table">Table 3</ref>: Summary of the analysis of the depen- dency between the top two candidates proposed by the CWIN+POS+n-gram Bi-LSTM model. a de- notes the ground-truth error position. c 1 and c 2 denote the first and the second candidate positions proposed by the model. dis(c 1 , c 2 ) is the distance between c 1 and c 2 on the dependency graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS (# tests) CWIN CWIN+POS</head><p>VV <ref type="formula">(325)</ref>   each dependency corresponds to an edge, and cal- culate the shortest distance between the top two candidates in these cases. The results are summa- rized in <ref type="table">Table 3</ref>. The average distance (2.07) is small compared to the average length of the seg- ments <ref type="bibr">(9.24)</ref>, indicating that our model can con- sider the dependencies among words when rank- ing the candidate positions. A factor that might limit the effectiveness of POS features is that the POS tagger trained on well-formed text may not perform well on noisy learner data. In fact, for 26.7% of the test data, the POS tag of the original erroneous token dif- fers from that of its corrected version. We com- pare the performance of the model with or without POS features on three most frequent POS tags in <ref type="table" target="#tab_3">Table 4</ref>. As can be seen, the POS information of the erroneous segment, which potentially contains errors, can still be helpful for detecting anomaly of the segment. In example (E6) we show the scores of incorrectness predicted by models with or without POS features. The "DEC + AD" con- struction is invalid in Chinese, so in this case the error can be detected more easily if POS informa- tion is available. 0.571 <ref type="bibr">(8/14)</ref> 0.700 <ref type="bibr">(7/10)</ref> 0.875 (7/8) 經驗 (experience) 0.500 <ref type="bibr">(5/10)</ref> 0.667 (4/6) 0.800 (4/5) 發生 (happen) 0.455 <ref type="bibr">(5/11)</ref> 0.571 (4/7) 0.800 (4/5) 而 (so) 0.417 (20/48) 0.550 <ref type="bibr">(11/20)</ref> 0.550 (11/20) <ref type="table">Table 5</ref>: Precision/recall of Bi-LSTM mod- els with CWIN+POS features on four most com- monly misused (err rate(w) &gt; 0.4) words.</p><formula xml:id="formula_3">(E6) 應該 有 別人 的 *盡 盡 盡力 力 力</formula><p>In <ref type="table">Table 5</ref> we show the precision/recall of the Bi-LSTM model with CWIN+POS features on four most commonly misused words. The error rate of a word w is calculated on the test set by err rate(w) = # segments in which w is misused # segments containing w . We exclude words that occur in less than 10 segments regardless of their error rates. In general, our model achieves high recall and fair precision. Dis- criminating correct and wrong usage of the con- junction 而 (so), which often connects more than one segment, seems to be the most difficult. For example, in (E7) the inappropriateness of 而 can- not be recognized unless we consider the wider context of this segment.</p><p>(E7) (*而,並) 當成 此生 做人 的 道理 ( ..., (*so,and) take it as a lifelong way to behave around others. )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we propose an LSTM-based se- quence labeling model for detecting WUEs in sentences written by non-native Chinese learn- ers. The experimental results suggest that the CWIN/Struct-SG embeddings, which consider word orders, are better word features for Chinese WUE detection. Moreover, Bi-LSTM is more pre- ferred than LSTM. While a wrong usage often in- volves more than one token, making it difficult to determine which one should be corrected, the best model can rank the ground-truth error posi- tion within the top two in 80.97% of the cases. One possible future direction is to exploit more sophisticated structural information such as de- pendency paths. Moreover, it is also worth study- ing how to extend our system to cope with the cor- rection task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>put of the Bi-LSTM model with CWIN+POS+n- gram features and analyze the error cases where the model ranks the ground-truth error position second. We use the dependency parsing output of CoreNLP to construct an undirected graph, where Length (# tests) # proposed LSTM Bi-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Hit@20% rates of LSTM and Bi-LSTM 
on segments with different lengths. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Hit@20% rates of Bi-LSTM models 
with or without POS features on three most fre-
quent POS tags of the erroneous token. 

</table></figure>

			<note place="foot" n="1"> http://lemurproject.org/clueweb09.php</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially supported by Min-istry of Science and Technology, Taiwan, un-der grants MOST-104-2221-E-002-061-MY3 and MOST-105-2221-E-002-154-MY3.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural network translation models for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shamil</forename><surname>Chollampatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2768" to="2774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hoo 2012: A report on the preposition and determiner error correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Anisimoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Narroway</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W12-2006" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP. Association for Computational Linguistics</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Helping our own: The hoo 2011 pilot shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W11-2838" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation. Association for Computational Linguistics</title>
		<meeting>the 13th European Workshop on Natural Language Generation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chinese preposition selection for grammatical error diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hen-Hsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1085" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="888" to="899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bi-lstm neural networks for chinese grammatical error diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W16-4919" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2016). The COLING 2016 Organizing Committee</title>
		<meeting>the 3rd Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2016). The COLING 2016 Organizing Committee</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="148" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of nlp-tea 2016 shared task for chinese grammatical error diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lung-Hao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaoqi</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chih</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Endong</forename><surname>Xun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Ping</forename><surname>Chang</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W16-4906" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2016</title>
		<meeting>the 3rd Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overview of the nlp-tea 2015 shared task for chinese grammatical error diagnosis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lung-Hao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chih</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Ping</forename><surname>Chang</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/W15-4401</idno>
		<ptr target="https://doi.org/10.18653/v1/W15-4401" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2015). Association for Computational Linguistics</title>
		<meeting>the 2nd Workshop on Natural Language Processing Techniques for Educational Applications (NLPTEA 2015). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two/too simple adaptations of word2vec for syntax problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/N15-1142</idno>
		<ptr target="https://doi.org/10.3115/v1/N15-1142" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1299" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Chinese web 5-gram version 1. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P14-5010</idno>
		<ptr target="https://doi.org/10.3115/v1/P14-5010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Association for Computational Linguistics</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/N13-1090" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL). Association for Computational Linguistics</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The conll-2014 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tou Hwee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><forename type="middle">Siew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendy</forename><forename type="middle">Raymond</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bryant</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/W14-1701</idno>
		<ptr target="https://doi.org/10.3115/v1/W14-1701" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning: Shared Task. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The conll2013 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tou Hwee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><forename type="middle">Siew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/D14-1162</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association 409 for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association 409 for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Compositional sequence labeling models for error detection in learner writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1112</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1112" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1181" to="1191" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detecting word usage errors in chinese sentences for learning chinese as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yow-Ting</forename><surname>Shiue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="220" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Development of a web-scale chinese word n-gram corpus with parts of speech information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Hsin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="320" to="324" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overview of grammatical error diagnosis for learning chinese as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chih</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lung-Hao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Ping</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Natural Language Processing Techniques for Educational Applications</title>
		<meeting>the 1st Workshop on Natural Language Processing Techniques for Educational Applications<address><addrLine>NLPTEA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
