<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Grained Entity Typing with High-Multiplicity Assignments</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
						</author>
						<title level="a" type="main">Fine-Grained Entity Typing with High-Multiplicity Assignments</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="330" to="334"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2052</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as Wikipedia that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Motivated by potential applications to information retrieval, coreference resolution, question answer- ing, and other downstream tasks, recent work on entity typing has moved beyond coarse-grained systems towards richer ontologies with much more detailed information, and therefore correspond- ingly more specific types ( <ref type="bibr" target="#b4">Ling and Weld, 2012;</ref><ref type="bibr" target="#b2">Gillick et al., 2014;</ref><ref type="bibr" target="#b9">Yogatama et al., 2015)</ref>.</p><p>As types become more specific, entities will tend to belong to more types (i.e. there will tend to be higher type multiplicity). However, most data used in previous work exhibits an extremely low degree of multiplicity.</p><p>In this paper, we focus on the high multiplic- ity case, which we argue naturally arises in large- scale knowledge resources. To illustrate this point, we construct a corpus of entity mentions paired with higher-multiplicity type assignments. Our corpus is based on mentions and categories drawn from Wikipedia, but we generalize and denoise the raw Wikipedia categories to provide more coher- ent supervision. <ref type="table" target="#tab_0">Table 1</ref> gives examples of type Figure 1: Comparison of type set size CDFs for the our Wikipedia corpus and the prior FIGER cor- pus ( <ref type="bibr" target="#b4">Ling and Weld, 2012</ref>). The figure illustrates that our corpus exhibits much greater type assign- ment multiplicity. assignments from our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0">1 2 3 4 5 6 7 8 9 111 12 13 14 15</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Types</head><p>As type multiplicity grows, it is natural to con- sider type prediction as an inherently set-valued problem and ask questions about how such sets might be modeled. To this end, we develop a struc- tured prediction approach in which the sets of as- signed types are predicted as first-class objects, in- cluding a preliminary consideration of how to ef- ficiently search over them. The resulting model captures type correlations and ultimately outper- forms a strong unstructured baseline.</p><p>Related work The fine-grained entity typing problem was first investigated in detail by <ref type="bibr" target="#b4">Ling and Weld (2012)</ref>. Subsequently, <ref type="bibr" target="#b2">Gillick et al. (2014)</ref> introduced a larger evaluation corpus for this task and introduced methods for training pre- dictors based on multiclass classification. Both used the Freebase typing system, coarsened to approximately 100 types, and subsequent work David Foster Wallace novelist suicide sportswriter writer alumnus ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Albert Einstein</head><p>physicist agnostic emigrant people pacifist ... NATO organization treaty document organisation alliance ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Federal Reserve</head><p>agency authorities banks institution organization ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Industrial Revolution</head><p>concept history evolution revolution past ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Black Death</head><p>concept epidemic pandemic disaster ...  <ref type="table">Table 2</ref>: Example of an entity and its types, before and after projection. The projection operation col- lapses related types that would be very difficult to learn in their original, highly specific forms.</p><p>has mostly followed this lead <ref type="bibr" target="#b8">(Yaghoobzadeh and Schütze, 2016;</ref><ref type="bibr" target="#b9">Yogatama et al., 2015</ref>), although types based on WordNet have recently also been investigated ( <ref type="bibr" target="#b0">Corro et al., 2015</ref>). Most prior work has focused on unstructured predictors using some form of multiclass logistic regression ( <ref type="bibr" target="#b4">Ling and Weld, 2012;</ref><ref type="bibr" target="#b2">Gillick et al., 2014;</ref><ref type="bibr" target="#b6">Shimaoka et al., 2016;</ref><ref type="bibr" target="#b8">Yaghoobzadeh and Schütze, 2016;</ref><ref type="bibr" target="#b9">Yogatama et al., 2015</ref>). Some of these approaches implicitly incorporate struc- ture during decoding by enforcing hierarchy con- straints ( <ref type="bibr" target="#b2">Gillick et al., 2014</ref>), while neural ap- proaches can encode correlations in a soft manner via shared hidden layers ( <ref type="bibr" target="#b6">Shimaoka et al., 2016;</ref><ref type="bibr" target="#b8">Yaghoobzadeh and Schütze, 2016)</ref>.</p><p>Our work differs from these lines of work in two respects: its use of a corpus exhibiting high type multiplicity with types derived from a semi-open inventory and its use of a fully structured model and decoding procedure, one that can in principle be integrated with neural models if desired. Previ- ously, most results focused on the low-multiplicity Freebase-based FIGER corpus. The only work we are aware of that uses a type system similar to ours used a rule-based system and evaluated on their own newswire-and Twitter-based evaluation cor- pora ( <ref type="bibr" target="#b0">Corro et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our structured prediction framework is based on modeling type assignments as sets. Each entity e is assigned a set of types T * drawn from the over- all set of types T . Our goal is thus to predict, given an input sentence-entity pair, the set of types asso- ciated with that entity.</p><p>We take the commonly-used linear model ap- proach to this structured prediction problem. Given a featurizer ϕ that takes an input sentence x and entity e, we seek to learn a weight vector w such that</p><formula xml:id="formula_0">f (x, e) = argmax T w ϕ (x, e, T ) (1)</formula><p>predicts T correctly with high accuracy.</p><p>Our approach stands in contrast to prior work, which deployed several techniques, of similar ef- ficacy, to port single-type learning and inference strategies to the multi-type setting ( <ref type="bibr" target="#b2">Gillick et al., 2014</ref>). Provided type interactions can be ne- glected, equation <ref type="formula">(1)</ref> can be simplified to</p><formula xml:id="formula_1">f single (x, e) = t ∈ T : w ϕ (x, e, t) ≥ r .</formula><p>This simplification corresponds to expanding each multi-type example triple (x, e, T * ) into a set of single-type example triples</p><formula xml:id="formula_2">(x, e, t * ) t * ∈T * .</formula><p>Learning can then be done using any technique for multiclass logistic regression, and inference can be carried out by specifying a threshold r and predicting all types that score above that thresh- old: In prior work, a simple r = 0 threshold was used ( <ref type="bibr" target="#b4">Ling and Weld, 2012)</ref>. In this paper, we focus on the more general specification (1), though in Section 2.2, we explain a simplification that can be used to speed up infer- ence if desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Features</head><p>Modeling type assignments as sets in principle opens the door to non-decomposable set features (a simple instance of which would be set size). For reasons of tractability, we assume our features fac- tor along type pairs:</p><formula xml:id="formula_3">ϕ (x, e, T ) = t∈T ϕ (x, e, t) + t, t ∈T ϕ t, t<label>(2)</label></formula><p>Note that in addition to enforcing factorization over type pairs, the specification (2) requires that any features linking the type assignment to the ob- served entity mention depend only on a single type at a time. We investigated non-decomposable fea- tures, but found they did not lead to improved per- formance.</p><p>We use entity mention features very similar to those in previous work:</p><p>1. Context unigrams and bigrams. Indicators on all uni-and bigrams within a certain win- dow of the entity mention.</p><p>2. Dependency parse features. Indicators on the lexical parent of the entity mention head, as well as the corresponding dependency type. Separately, indicators on the lexical children of the entity mention head and their dependency types.</p><p>3. Entity head and non-head tokens. Indica- tors on the syntactic head of the entity men- tion and on its non-head tokens.</p><p>4. Word shape features. Indicators on the shape of each token in the entity mention.</p><p>We combine these features with type-based fea- tures to obtain the features our model actually uses:</p><p>1. Conjunction features. These are simple conjunctions of mention features with indi- cators on type membership in the predicted set. Using only these features results in an unstructured model.</p><p>2. Type pair features. These are indicators on pairs of types appearing in the predicted set.</p><p>3. Graph-based features. As we discuss in Section 3, the type system in our corpus comes with a graph structure. We add indica- tors on certain patterns occurring within the set-e.g. a parent-child type pair, sibling type pairs, and so on, abstracting away the specific types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning and Inference</head><p>We train our system using structured max- margin ( <ref type="bibr" target="#b7">Tsochantaridis et al., 2005</ref>). Optimization is performed via AdaGrad on the primal <ref type="bibr" target="#b3">(Kummerfeld et al., 2015)</ref>. We use set-F1 as our loss function. Inference, for both prediction and loss- augmented decoding, poses a greater challenge, as solving the maximization problem (1) exactly re- quires iterating over all subsets of the type system.</p><p>Fortunately, we find a simple greedy algorithm is effective. Our decoder begins by choosing the type that scores highest individually, taking only single-type features into account. It then proceeds by iteratively adding new types into the set until doing so would decrease the score.</p><p>At the cost of restricting the permissible type sets slightly, we can speed up the greedy procedure further. Specifically, we can require that the pre- dicted type set T be connected in some constraint graph over the types-either the co-occurrence graph, the complete graph, or the graph underly- ing the type system. If we denote by C the set of all such connected sets, the corresponding predic- tor would be</p><formula xml:id="formula_4">f conn (x, e) = argmax T ∈C w ϕ (x, e, T ) 332 Level Features P R F1 Entity</formula><p>Unstructured 50.0 67.2 52.9 + Pairs 53.3 64.1 54.3 + Graph 53.9 63.9 54.5 Sentence Unstructured 42.6 58.9 44.4 + Pairs 46.5 54.1 45.6 + Graph 47.0 53.6 45.6 <ref type="table">Table 3</ref>: Results on our corpus. All quantities are macro-averaged.</p><p>The greedy decoding procedure for this predictor is faster because at each step, it need only consider adding types that are adjacent to some type that has already been included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpus</head><p>Our corpus construction methodology involves three key stages: mention identification, type sys- tem construction, and type assignment. <ref type="bibr">1</ref> We ex- plain each of these in turn.</p><p>Mention identification. We follow prior work on entity linking <ref type="bibr" target="#b1">(Durrett and Klein, 2014</ref>) and take all mentions that occur as anchor text. We filter the resulting collection of mentions down to those that pass a heuristic filter that removes men- tions of common nouns, as well as spurious sen- tences representing Wikipedia formatting.</p><p>Type system construction. Prior work on fine- grained entity typing has derived its type sys- tem from Freebase ( <ref type="bibr" target="#b4">Ling and Weld, 2012;</ref><ref type="bibr" target="#b2">Gillick et al., 2014</ref>). The resulting ontologies thus inherit the coverage and specificity limitations of Free- base, somewhat exacerbated by manual coarsen- ing. Motivated by efforts to inject broader cover- age, more complex knowledge resources into NLP systems, we instead derive our types from the Wikipedia category and WordNet graphs, in a manner similar to that of <ref type="bibr" target="#b5">Ponzetto and Strube (2007)</ref>.</p><p>Our base type set consists of all Wikipedia cat- egories. By following back-pointers in articles for categories, we derive a base underlying directed graph. To eliminate noise, we filter down to all categories whose syntactic heads can be found in WordNet and keep directed edges only when the head of the parent is a WordNet ancestor of the 1 Our corpus will be released at http://people. eecs.berkeley.edu/ ˜ rabinovich/. head of the child. We conclude by projecting each type down to its syntactic head.</p><p>Type assignment. The type set for an entity is obtained by taking its Wikipedia category as- signments, augmenting these with their ancestors in the category graph above, and then projecting these down to their syntactic heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our method on the dataset described in Section 3. For these experiments, we restrict to the 100 most frequent types and downsam- ple to 750K mentions. We use a baseline that closely replicates the FIGER system ( <ref type="bibr" target="#b4">Ling and Weld, 2012)</ref>. Within our framework, this can be thought of as a model that sets all type pair fea- tures in (2) to zero. <ref type="table">Table 3</ref> summarizes our results. Starting with the baseline, we incrementally add the type pair, graph-based, and set size features discussed in 2.1. Adding type pair features results in an appreciable performance gain, while the graph features bring little benefit-potentially because pairwise corre- lations suffice to summarize the set structure when the number of types is moderately low.</p><p>A concern when studying multiclass problems with large numbers of classes, whether predict- ing sets or individual labels, is that performance on instances associated with common classes will dominate the performance metric. <ref type="figure" target="#fig_2">Figure 3</ref> shows micro-averaged F1 for the binary prediction task associated with predicting the presence or absence of each type, demonstrating that our performance is strong even for many rare types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>333</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have highlighted the issue of multiplicity in fine-grained entity typing. Whereas most prior work has focused on corpora with low multiplicity assignments, we denoised the Wikipedia type sys- tem to construct a realistic corpus with high mul- tiplicity type assignments. Using this corpus as a testbed, we showed that an approach based on structured prediction of sets can outperform un- structured baselines when type assignments have high multiplicity. Our approach may therefore be preferable in such contexts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figer Wikipedia</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Fragment of the graph underlying our type system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Per-type F1 scores plotted by type frequency in the training corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>With types from a large corpus like Wikipedia, large type set assignments become common. 

Entity 
Raw Type 
Projected Type 

David Foster Wallace 

Short story writers 
writer 
Amherst alumni 
alumnus 
Illinois State faculty 
faculty 
People from New York 
people 
Essayists 
essayist 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Finet: Context-aware fine-grained named entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdalghani</forename><surname>Luciano Del Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Abujabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assoc. for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Contextdependent fine-grained entity type tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1820</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An empirical analysis of optimization for max-margin NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Jonathan K Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="page">279</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deriving a large scale taxonomy from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An attentive neural architecture for fine-grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.05525</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Corpus-level fine-grained entity typing using contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07901</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Embedding methods for fine grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
