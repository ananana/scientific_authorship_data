<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Sense-Based Translation Model for Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Provincial Key Laboratory for Computer Information Processing Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Provincial Key Laboratory for Computer Information Processing Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Sense-Based Translation Model for Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1459" to="1469"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The sense in which a word is used determines the translation of the word. In this paper, we propose a sense-based translation model to integrate word senses into statistical machine translation. We build a broad-coverage sense tagger based on a nonparametric Bayesian topic model that automatically learns sense clusters for words in the source language. The proposed sense-based translation model enables the decoder to select appropriate translations for source words according to the inferred senses for these words using maximum entropy classifiers. Our method is significantly different from previous word sense disambiguation reformu-lated for machine translation in that the latter neglects word senses in nature. We test the effectiveness of the proposed sense-based translation model on a large-scale Chinese-to-English translation task. Results show that the proposed model substantially outperforms not only the base-line but also the previous reformulated word sense disambiguation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of very common phenomena in language is that a plenty of words have multiple meanings. In the context of machine translation, such dif- ferent meanings normally produce different target translations. Therefore a natural assumption is that word sense disambiguation (WSD) may contribute to statistical machine translation (SMT) by provid- ing appropriate word senses for target translation selection with context features <ref type="bibr" target="#b4">(Carpuat and Wu, 2005</ref>). * Corresponding author</p><p>This assumption, however, has not been em- pirically verified in the early days. Carpuat and <ref type="bibr" target="#b4">Wu (2005)</ref> adopt a standard formulation of WSD: predicting word senses that are defined on an ontology for ambiguous words. As they apply WSD to Chinese-to-English translation, they pre- dict word senses from a Chinese ontology HowNet and project the predicted senses to English glosses provided by HowNet. These glosses, used as the sense predictions of their WSD system, are inte- grated into a word-based SMT system either to substitute for translation candidates of their trans- lation model or to postedit the output of their SMT system. They report that WSD degenerates the translation quality of SMT.</p><p>In contrast to the standard WSD formulation, <ref type="bibr" target="#b19">Vickrey et al. (2005)</ref> reformulate the task of WSD for SMT as predicting possible target translations rather than senses for ambiguous source words. They show that such a reformulated WSD can im- prove the accuracy of a simplified word translation task.</p><p>Following this WSD reformulation for SMT, <ref type="bibr" target="#b6">Chan et al. (2007)</ref> integrate a state-of-the-art WSD system into a hierarchical phrase-based sys- tem <ref type="bibr" target="#b8">(Chiang, 2005)</ref>. <ref type="bibr" target="#b5">Carpuat and Wu (2007)</ref> also use this reformulated WSD and further adapt it to multi-word phrasal disambiguation. They both re- port that the redefined WSD can significantly im- prove SMT.</p><p>Although this reformulated WSD has proved helpful for SMT, one question is not answered yet: are pure word senses useful for SMT? The early WSD for SMT <ref type="bibr" target="#b4">(Carpuat and Wu, 2005</ref>) uses projected word senses while the reformu- lated WSD sidesteps word senses. In this pa- per we would like to re-investigate this question by resorting to word sense induction (WSI) that is related to but different from WSD. <ref type="bibr">1</ref> We use WSI to obtain word senses for large-scale data. With these word senses, we study in particular: 1) whether word senses can be directly integrated to SMT to improve translation quality and 2) whether WSI-based model can outperform the reformu- lated WSD in the context of SMT.</p><p>In order to incorporate word senses into SMT, we propose a sense-based translation model that is built on maximum entropy classifiers. We use a nonparametric Bayesian topic model based WSI to infer word senses for source words in our training, development and test set. We collect training in- stances from the sense-tagged training data to train the proposed sense-based translation model. Spe- cially,</p><p>• Instead of predicting target translations for ambiguous source words as the previous re- formulated WSD does, we first predict word senses for ambiguous source words. The pre- dicted word senses together with other con- text features are then used to predict possible target translations for these words.</p><p>• Instead of using word senses defined by a prespecified sense inventory as the standard WSD does, we incorporate word senses that are automatically learned from data into our sense-based translation model.</p><p>We integrate the proposed sense-based transla- tion model into a state-of-the-art SMT system and conduct experiments on Chines-to-English trans- lation using large-scale training data. Results show that automatically learned word senses are able to improve translation quality and the sense- based translation model is better than the previous reformulated WSD.</p><p>The remainder of this paper proceeds as fol- lows. Section 2 introduces how we obtain word senses for our large-scale training data via a WSI- based broad-coverage sense tagger. Section 3 presents our sense-based translation model. Sec- tion 4 describes how we integrate the sense-based translation model into SMT. Section 5 elaborates our experiments on the large-scale Chinese-to- English translation task. Section 6 introduces re- lated studies and highlights significant differences from them. Finally, we conclude in Section 7 with future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">WSI-Based Broad-Coverage Sense Tagger</head><p>In order to obtain word senses for any source words, we build a broad-coverage sense tagger that relies on the nonparametric Bayesian model based word sense induction. We first describe WSI, especially WSI based on the Hierarchical Dirichlet Process (HDP) ( <ref type="bibr" target="#b18">Teh et al., 2004</ref>), a non- parametric version of Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>. We then elaborate how we use the HDP-based WSI to predict sense clus- ters and to annotate source words in our train- ing/development/test sets with these sense clus- ters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Sense Induction</head><p>Before we introduce WSI, we differentiate word type from word token. A word type refers to a unique word as a vocabulary entry while a word token is an occurrence of a word type. Take the first sentence of this paragraph as an example, it has 11 word tokens but 9 word types as there are two word tokens of the word type "we" and two tokens of the word type "word". Word sense induction is a task of automatically inducing the underlying senses of word tokens given the surrounding contexts where the word tokens occur. The biggest difference from word sense disambiguation lies in that WSI does not rely on a predefined sense inventory. Such a pre- specified list of senses is normally assumed by WSD which predicts senses of word tokens using this given inventory. From this perspective, WSI can be treated as a clustering problem while WSD a classification one.</p><p>Various clustering algorithms, such as k-means, have been previously used for WSI. Recently, we have also witnessed that WSI is cast as a topic modeling problem where the sense clusters of a word type are considered as underlying topics <ref type="bibr" target="#b3">(Brody and Lapata, 2009;</ref><ref type="bibr" target="#b25">Yao and Durme, 2011;</ref><ref type="bibr" target="#b13">Lau et al., 2012</ref>). We follow this line to tailor a topic modeling framework to induce word senses for our large-scale training data.</p><p>In the topic-based WSI, surrounding context of a word token is considered as a pseudo document of the corresponding word type. A pseudo docu- ment is composed of either a bag of neighboring words of a word token, or the Part-to-Speech tags of neighboring words, or other contextual infor- mation elements. In this paper, we define a pseudo document as ±N neighboring words centered on a given word token. <ref type="table">Table 1</ref> shows examples of pseudo documents for a Chinese word "wǎngluò" (network). These two pseudo documents are ex- tracted from a sentence listed in the first row of Ta- ble 1. Here we set N = 5. We can extract as many pseudo documents as the number of word tokens of a given word type that occur in training data. The collection of all these extracted pseudo docu- ments of the given word type forms a corpus. We can induce topics on this corpus for each pseudo document via topic modeling approaches. <ref type="figure" target="#fig_0">Figure 1(a)</ref> shows the LDA-based WSI for a given word type W . The outer plate represents replicates of pseudo documents which consist of N neighboring words centered on the tokens of the given word type W . w j,i is the i-th word of the j-th pseudo document of the given word type W . s j,i is the sense assigned to the word w j,i . The conventional topic distribution θ j for the j- th pseudo document is taken as the the distribu- tion over senses for the given word type W . The LDA generative process for sense induction is as follows: 1) for each pseudo document D j , draw a per-document sense distribution θ j from a Dirich- let distribution Dir(α); 2) for each item w j,i in the pseudo document D j , 2.1) draw a sense cluster s j,i ∼ Multinomial(θ j ); and 2.2) draw a word w j,i ∼ φ s j,i where φ s j,i is the distribution of sense s j,i over words drawn from a Dirichlet dis- tribution Dir(β).</p><p>As LDA needs to manually specify the num- ber of senses (topics), a better idea is to let the training data automatically determine the number of senses for each word type. Therefore we re- sort to the HDP, a natural nonparametric gener- alization of LDA, for the inference of both sense clusters and the number of sense clusters follow- ing <ref type="bibr" target="#b13">Lau et al. (2012)</ref> and <ref type="bibr" target="#b25">Yao and Durme (2011)</ref>. The HDP for WSI is shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). The HDP generative process for word sense induction is as follows: 1) sample a base distribution G 0 from a Dirichlet process DP(γ, H) with a con- centration parameter γ and a base distribution H; 2) for each pseudo document D j , sample a dis- tribution G j ∼ DP(α 0 , G 0 ); 3) for each item w j,i in the pseudo document D j , 3.1) sample a sense cluster s j,i ∼ G j ; and 3.2) sample a word w j,i ∼ φ s j,i . Here G 0 is a global distribution over sense clusters that are shared by all G j . G j is a per-document sense distribution over these sense clusters, which has its own document-specific pro- portions of these sense clusters. The hyperparam- eter γ, α 0 in the HDP are both concentration pa- rameters which control the variability of senses in the global distribution G 0 and document-specific distribution G j . The HDP/LDA-based WSI complies with the distributional hypothesis that states that words oc- curring in the same contexts tend to have similar meanings. We want to extend this hypothesis to machine translation by building sense-based trans- lation model upon the HDP-based word sense in- duction: words with the same meanings tend to be translated in the same way.</p><formula xml:id="formula_0">w j,i α θ j s j,i j ∈ [1, J ] ϕ k k ∈ [1, K] β G 0 G j s j,i j ∈ [1, J ] w j,i H γ α 0 (a) (b) i ∈ [1, Nj ] i ∈ [1, Nj ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Sense Tagging</head><p>We adopt the HDP-based WSI to automatically predict word senses and use these predicted senses to annotate source words. We individually build a HDP-based WSI model per word type and train these models on the training data. The sense for a word token is defined as the most probable sense according to the per-document sense distribution G j estimated for the corresponding pseudo doc- ument that represents the surrounding context of the word token. In particular, we take the follow- ing steps. tā tíxǐng wǒguó wǎngluò yùnyíng zhě zhùyì fángfàn hēikè gōngjī ， quèbǎo wǎngluò ānquán 。 Pseudo Documents for word "wǎngluò" tā tíxǐng wǒguó wǎngluò yùnyíng zhě zhùyì fángfàn hēikè fángfàn hēikè gōngjī ， quèbǎo wǎngluò ānquán 。 <ref type="table">Table 1</ref>: Examples of pseudo documents extracted from a Chinese sentence (written in Chinese Pinyin).</p><p>• Data preprocessing We preprocess the source side of our bilingual training data as well as development and test set by removing stop words and rare words.</p><p>• Training Data Sense Annotation From the preprocessed training data, we extract all possible pseudo documents for each source word type. The collection of these extracted pseudo documents is used as a corpus to train a HDP-based WSI model for the source word type. In this way, we can train as many HDP- based WSI models as the number of word types kept after preprocessing. The sense with the highest probability output by the HDP-based WSI model for each pseudo doc- ument is used as the sense cluster to label the corresponding word token.</p><p>• Test/Dev Data Sense Annotation From the preprocessed test data, we can also extract pseudo documents for each source word type that occur in the test/dev set. Using the trained HDP-based WSI model that corre- spond to the source word type in question, we can obtain the best sense assignment for each pseudo document of the word type, which in turn is used to annotate the corresponding word token in the test/dev data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sense-Based Translation Model</head><p>In this section we present our sense-based transla- tion model and describe the features that we use as well as the training process of this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>The sense-based translation model estimates the probability that a source word c is translated into a target phrase˜ephrase˜ phrase˜e given contextual information, in- cluding word senses that are obtained using the HDP-based WSI as described in the last section.</p><p>We allow the target phrase˜ephrase˜ phrase˜e to be either a phrase of length up to 3 words or NULL so that we can capture both multi-word and null translations. The essential component of the model is a maximum entropy (MaxEnt) based classifier that is used to predict the translation probability p(˜ e|C(c)). The MaxEnt classifier can be formulated as follows.</p><formula xml:id="formula_1">p(˜ e|C(c)) = exp( ∑ i θ i h i (˜ e, C(c))) ∑ ˜ e ′ exp( ∑ i θ i h i (˜ e ′ , C(c)))<label>(1)</label></formula><p>where h i s are binary features, θ i s are weights of these features, C(c) is the surrounding context of c.</p><p>We define two groups of binary features: 1) lex- icon features and 2) sense features. All these fea- tures take the following form.</p><formula xml:id="formula_2">h(˜ e, C(c)) = { 1, if˜eif˜ if˜e = 2 and C(c).µ = ν 0, else<label>(2)</label></formula><p>where 2 is a placeholder for a possible target translation (up to 3 words or NULL), µ is the name of a contextual (lexicon or sense) feature for the source word c, and the symbol ν represents the value of the feature µ.</p><p>We extract both the lexicon and sense features from a ±k-word window centered on the word c. The lexicon features are defined as the preceding k words, the succeeding k words and the word c itself: {c −k , ..., c −1 , c, c 1 , ..., c k }. The sense fea- tures are defined as the predicted senses for these words:</p><formula xml:id="formula_3">{s c −k , ..., s c −1 , s c , s c 1 , ..., s c k }.</formula><p>As we also use these neighboring words to pre- dict word senses in the HDP-based WSI, the infor- mation provided by the lexicon and sense features may overlap. This is not a issue for the MaxEnt classifier as it can deal with arbitrary overlapping features ( <ref type="bibr" target="#b1">Berger et al., 1996)</ref>. One may also won- der whether the sense features can contribute to SMT new information that can NOT be obtained from the lexicon features. First, we believe that the senses induced by the HDP-based WSI pro- vide a different view of data than that of the lex- icon features. Second, the sense features contain semantic distributional information learned by the HDP across contexts where lexical words occur. Third, we empirically investigate this doubt by comparing two MaxEnt-based translation models in Section 5. One model only uses the lexicon fea- tures while the other integrates both the lexicon and sense features. The former model can be con- sidered as a reformulated WSD for SMT as we de- scribed in Section 1.</p><p>Given a source sentence {c i } I 1 , the proposed sense-based translation model M s can be denoted as</p><formula xml:id="formula_4">M s = ∏ c i ∈W ( ˜ e i |C(c i ))<label>(3)</label></formula><p>where W is a set of words for which we build MaxEnt classifiers (see the next subsection for the discussion on how we build MaxEnt classifiers for our sense-based translation model).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>The training of the proposed sense-based transla- tion model is a process of estimating the feature weights θs in the equation <ref type="formula" target="#formula_1">(1)</ref>. There are two strategies that we can use to obtain these weights. We can either build an all-in-one MaxEnt clas- sifier that integrates all source word types c and their possible target translations˜etranslations˜ translations˜e or build multi- ple MaxEnt classifiers. If we train the all-in-one classifier, we have to predict millions of classes (target translations of length up to 3 words). This is normally intractable in practice. Therefore we take the second strategy: building multiple Max- Ent classifiers with one classifier per source word type.</p><p>In order to train these classifiers, we have to col- lect training events from our word-aligned bilin- gual training data where source words are anno- tated with their corresponding sense clusters pre- dicted by the HDP-based WSI as described in Section 2. A training event for a source word c consists of all contextual elements in the form of C(c).µ = ν defined in the last subsection and the target translatioñ e. Using these collected events, we can train our multiple classifiers. In prac- tice, we do not build MaxEnt classifiers for source words that occur less than 10 times in the train- ing data and run the MaxEnt toolkit in a parallel manner in order to expedite the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decoding with Sense-Based Translation Model</head><p>The sense-based translation model described above is integrated into the log-linear translation model of SMT as a sense-based knowledge source.</p><p>The weight of this model is tuned by the minimum error rate training (MERT) <ref type="bibr" target="#b15">(Och, 2003)</ref> together with other models such as the language model. <ref type="figure" target="#fig_1">Figure 2</ref> shows the architecture of the SMT system enhanced with the sense-based translation model. Before we translate a source sentence, we use the HDP-based WSI models trained on the training data to predict senses for word tokens oc- curring in the source sentence as discussed in Sec- tion 2.2. Note that the HDP-based WSI does not predict senses for all words due to the following two reasons.</p><p>• We do not train HDP-based WSI models for word types for which we extract more than T pseudo documents. <ref type="bibr">2</ref> • In the test/dev set, there are some words that are unseen in the training data. These un- seen words, of course, do not have their HDP- based WSI models.</p><p>For these words, we set a default sense (i.e. s c = s 1 ). Sense tagging on test sentences can be done in a preprocessing step. Once we get sense clus- ters for word tokens in test sentences, we load pre-trained MaxEnt classifiers of the correspond- ing word types. During decoding, we keep word alignments for each translation rule. Whenever a new source word c is translated, we find its trans- latioñ e via the kept word alignments. We then calculate the translation probability p(˜ e|C(c)) ac- cording to the equation (1) using the correspond- ing loaded classifier. In this way, we can easily calculate the sense-based translation model score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we carried out a series of ex- periments on Chinese-to-English translation us- ing large-scale bilingual training data. In order to build the proposed sense-based translation model, we annotated the source part of the bilingual train- ing data with word senses induced by the HDP- based WSI. With the trained sense-based transla- tion model, we would like to investigate the fol- lowing two questions:</p><p>• Do word senses automatically induced by the HDP-based WSI improve translation quality?</p><p>• Does the sense-based translation model out- perform the reformulated WSD for SMT?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Our baseline system is a state-of-the-art SMT system which adapts Bracketing Transduction Grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model ( <ref type="bibr" target="#b24">Xiong et al., 2006</ref>). We used LDC corpora LDC2004E12, LDC2004T08, LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07 as our bilingual training data which consists of 3.84M bilingual sentences, 109.5M English word tokens and 96.9M Chinese word tokens. We ran Giza++ on the training data in two directions and applied the "grow-diag-final" refinement rule ( <ref type="bibr" target="#b12">Koehn et al., 2003</ref>) to obtain word align- ments. From the word-aligned data, we extracted weighted phrase pairs to generate our phrase table. We trained a 5-gram language model on the Xinhua section of the English Gigaword corpus (306 million words) using the SRILM toolkit <ref type="bibr">(Stolcke, 2002</ref>) with the modified Kneser-Ney smoothing <ref type="bibr" target="#b7">(Chen and Goodman, 1996)</ref>. We trained our HDP-based WSI models via the C++ HDP toolkit 3 ( <ref type="bibr" target="#b20">Wang and Blei, 2012</ref>). We set the hyperparameters γ = 0.1 and α 0 = 1.0 following <ref type="bibr" target="#b13">Lau et al. (2012)</ref>.We extracted pseudo documents from a ±10-word window centered on the corresponding word token for each word type following <ref type="bibr" target="#b3">Brody and Lapata (2009)</ref>. As described in Section 2.2, we preprocessed the source part of our bilingual training data by removing stop words and infrequent words that occurs less than  10 times in the training data. From the prepro- cessed data, we extracted pseudo documents for each word type to train a HDP-based WSI model per word type. Note that we do not build WSI models for highly frequent words that occur more than 20,000 times in order to expedite the HDP training process. We trained our MaxEnt classifiers with the off- the-shelf MaxEnt tool. <ref type="bibr">4</ref> We performed 100 iter- ations of the L-BFGS algorithm implemented in the training toolkit on the collected training events from the sense-annotated data as described in Sec- tion 3.2. We set the Gaussian prior to 1 to avoid overfitting. On average, we obtained 346 classes (target translations) per source word type with the maximum number of classes being 256,243. It took an average of 57.5 seconds for training a Maxent classifier.</p><p>We used the NIST MT03 evaluation test data as our development set, and the NIST MT05 as the test set. We evaluated translation quality with the case-insensitive BLEU-4 ( <ref type="bibr" target="#b16">Papineni et al., 2002</ref>) and NIST <ref type="bibr" target="#b10">(Doddington, 2002)</ref>. In order to al- leviate the impact of MERT <ref type="bibr" target="#b15">(Och, 2003</ref>) insta- bility, we followed the suggestion of <ref type="bibr" target="#b9">Clark et al. (2011)</ref> to run MERT three times and report aver- age BLEU/NIST scores over the three runs for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Statistics and Examples of Word Senses</head><p>Before we present our experiment results of the sense-based translation model, we study some statistics of the HDP-based WSI on the training and test data. We show these statistics in <ref type="table" target="#tab_1">Table 2</ref>. There are 67,723 and 4,348 unique word types in the training and test data after the preprocessing step. For these word types, we extract 27.73M and 11,777 pseudo documents from the training and test set respectively. On average, there are 427.79 System BLEU(%) NIST STM (±5w) 34.64 9.4346 STM (±10w) 34.76 9.5114 STM (±15w) - - <ref type="table">Table 4</ref>: Experiment results of the sense-based translation model (STM) with lexicon and sense features extracted from a window of size varying from ±5 to ±15 words on the development set.</p><p>pseudo documents per word type in the training data and 2.71 in the test set. The HDP-based WSI learns 271,770 word senses in total using the pseudo documents collected from the training data and infers 24,162 word senses using the pseudo documents extracted from the test set. There are 4.01 different senses per word type in the training data and 5.56 in the test set on average. <ref type="table">Table 3</ref> illustrates six different senses of the word "运营 (operate)" learned by the HDP-based WSI in the training data. We also show the most probable 10 words for each sense cluster. Sense s 1 represents the operations of company or organi- zation, sense s 2 denotes country/institution/inter- nation operations, sense s 3 refers to market opera- tions, sense s 4 corresponds to business operations, sense s 5 to public facility operations, and finally s 6 to economy operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Impact of Window Size k used in MaxEnt Classifiers</head><p>Our first group of experiments were conducted to investigate the impact of the window size k on translation performance in terms of BLEU/NIST on the development set. We extracted both the lex- icon and sense features from a ±k-word window for our MaxEnt classifiers. We varied k from 5 to 15. Experiment results are shown in <ref type="table">Table 4</ref>. We achieve the best performance when k = 10. This suggests that a ±10-word window context is sufficient for predicting target translations for am- biguous source words. We therefore set k = 10 for all experiments thereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of the Sense-Based Translation Model</head><p>Our second group of experiments were carried out to investigate whether the sense-base translation model is able to improve translation quality by comparing the system enhanced with our sense- based translation model against the baseline. We also studied the impact of word senses induced by   the HDP-based WSI on translation performance by enforcing the sense-based translation model to use only sense features. <ref type="table" target="#tab_3">Table 5</ref> shows the experi- ment results. From the table, we can observe that</p><p>• Our sense-based translation model achieves a substantial improvement of 1.2 BLEU points over the baseline. This indicates that the sense-based translation model is able to help select correct translations for ambiguous source words.</p><p>• If we only integrate sense features into the sense-based translation model, we can still outperform the baseline by 0.62 BLEU points. This suggests that automatically in- duced word senses alone are indeed useful for machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison to Word Sense Disambiguation</head><p>As we mentioned in Section 3.1, our sense-based translation model can be degenerated to a reformu- lated WSD model for SMT if we only use lexicon features in MaxEnt classifiers. This allows us to directly compare our method against the reformu- lated WSD for SMT. <ref type="table" target="#tab_4">Table 6</ref> shows the compari- son result.</p><p>From the table, we can find that the sense- based translation model outperforms the reformu- lated WSD by 0.57 BLEU points. This suggests that the HDP-based word sense induction is bet- ter than the reformulated WSD in the context of SMT. Furthermore, as the reformulated WSD is a degenerated version of our sense-based transla- tion model which only uses the lexicon features, <ref type="table">Table 3</ref>: Six different senses learned for the word "运营" from the training data.</p><formula xml:id="formula_5">s 1 s 2 s 3 运营 (operate) 运营 (operate) 运营 (operate) 设施 (facility) 卫星 (satellite) 市场 (market) 计划 (plan) 系统 (system) 企业 (enterprise) 基础 (foundation) 国家 (country) 竞争 (competition) 项目 (project) 提供 (supply) 资产 (assets) 公司 (company) 国际 (inter-nation) 利润 (profit) 结构 (structure) 机构 (institution) 造成 (cause) 服务 (service) 进行 (proceed) 费用 (cost) 组织 (organization) 中心 (center) 资金 (capital) 提供 (supply) 合作 (cooperate) 业务 (business) s 4 s 5 s 6 费用 (cost) 城市 (city) 处于 (lie) 股价 (share price) 处理 (process) 拍照 (photograph) 27000 自来水 (tap-water) 119 科索沃 (Kosovo) 工厂 (factory) DPRK 额外 (extra) 汽车 (car) 保险 (insurance) 工资 (wage) 铁路 (railway) 超支 (overspend) 美元 (dollar) 污水 (sewage) 地位 (position) 商业 (commerce) 办事处 (office) 经济 (economy) 收入 (income) 保本 (break-even) 竞争者 (competitor) 铁路局 (railway administration) 部件 (component) 平衡 (balance)</formula><p>the sense features used in our model do provide new information that can not be obtained by the lexicon features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>In this section we introduce previous studies that are related to our work. For ease of comparison, we roughly divide them into 4 categories: 1) WSD for SMT, 2) topic-based WSI, 3) topic model for SMT and 4) lexical selection.</p><p>WSD for SMT As we mentioned in Section 1, WSD has been successfully reformulated and adapted to SMT <ref type="bibr" target="#b19">(Vickrey et al., 2005;</ref><ref type="bibr" target="#b5">Carpuat and Wu, 2007;</ref><ref type="bibr" target="#b6">Chan et al., 2007)</ref>. Rather than predict- ing word senses for ambiguous words, the refor- mulated WSD directly predicts target translations for source words with context information. Our sense-based translation model also predicts target translations for SMT. The significant difference is that we predict word senses automatically learned from data and incorporate these predicted senses into SMT. Our experiments show that such word senses are able to improve translation quality.</p><p>Topic-based WSI Topic-based WSI can be considered as the foundation of our work as we use it to obtain broad-coverage word senses to an- notate our large-scale training data. <ref type="bibr" target="#b3">Brody and Lapata (2009)</ref>'s work is the first attempt to approach WSI via topic modeling. They adapt LDA to word sense induction by building one topic model per word type. According to them, there are 3 sig- nificant differences between topic-based WSI and generic topic modeling.</p><p>• First, the goal of topic-based WSI is to di- vide contexts of a word type into different categories, each representing a sense cluster. However generic topic models aim at topic distributions of documents.</p><p>• Second, generic topic modeling explores whole documents for topic inference while topic-based WSI uses much smaller units in a document (e.g., surrounding words of a tar- get word) for word sense induction.</p><p>• Finally, the number of induced word senses in WSI is usually less than 10 while the num- ber of inferred topics in generic topic model- ing is tens or hundreds.</p><p>As LDA-based WSI needs to manually spec- ify the number of word senses, <ref type="bibr" target="#b25">Yao and Durme (2011)</ref> propose HDP-based WSI that is capable of determining the number of senses for each word type according to training data. <ref type="bibr" target="#b13">Lau et al. (2012)</ref> adopt the HDP-based WSI for novel sense de- tection and empirically show that the HDP-based WSI is better than the LDA-based WSI. We follow them to set the hyperparameters of HDP for train- ing and incorporate automatically induced word senses into SMT in our work.</p><p>Topic model for SMT Generic topic models are also explored for SMT. <ref type="bibr" target="#b26">Zhao and Xing (2007)</ref> propose a bilingual topic model and integrate a topic-specific lexicon translation model into SMT. <ref type="bibr" target="#b17">Tam et al. (2007)</ref> also explore a bilingual topic model for translation and language model adapta- tion. <ref type="bibr">Foster and Kunh (2007)</ref> introduce a mixture model approach for translation model adaptation. <ref type="bibr" target="#b22">Xiao et al. (2012)</ref> propose a topic-based similar- ity model for rule selection in hierarchical phrase- based translation. <ref type="bibr" target="#b23">Xiong and Zhang (2013)</ref> em- ploy a sentence-level topic model to capture co- herence for document-level machine translation. The difference between our work and these pre- vious studies on topic model for SMT lies in that we adopt topic-based WSI to obtain word senses rather than generic topics and integrate induced word senses into machine translation.</p><p>Lexical selection Our work is also related to lexical selection in SMT where appropriate target lexical items for source words are selected by a statistical model with context information <ref type="bibr" target="#b0">(Bangalore et al., 2007;</ref><ref type="bibr" target="#b14">Mauser et al., 2009</ref>). The refor- mulated WSD discussed before can also be con- sidered as a lexical selection model. The signif- icant difference from these studies is that we per- form lexical selection using automatically induced word senses by the HDP on the source side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a sense-based translation model that integrates word senses into machine translation. We capitalize on the broad-coverage word sense induction system that is built on the nonparametric Bayesian HDP to learn sense clus- ters for words in the source language. We gen- erate pseudo documents for word tokens in the training/test data for the HDP-based WSI system to infer topics. The most probable topic inferred for a pseudo document is taken as the sense of the corresponding word token. We incorporate these learned word senses as translation evidences into maximum entropy classifiers which form the foundation of the proposed sense-based translation model.</p><p>We carried out a series of experiments to vali- date the effectiveness of the sense-based transla- tion by comparing the model against the baseline and the previous reformulated WSD. Our experi- ment results show that</p><p>• The sense-based translation model is able to substantially improve translation quality in terms of both BLEU and NIST.</p><p>• The sense-based translation model is also better than the previous reformulated WSD for SMT.</p><p>• Word senses automatically induced by the HDP-based WSI on large-scale training data are very useful for machine translation. To the best of our knowledge, this is the first at- tempt to empirically verify the positive im- pact of word senses on translation quality.</p><p>Comparing with macro topics of documents in- ferred by LDA with bag of words from the whole documents, word senses inferred by the HDP- based WSI can be considered as micro topics. In the future, we would like to explore both the micro and macro topics for machine translation. Addi- tionally, we also want to induce sense clusters for words in the target language so that we can build sense-based language model and integrate it into SMT. We would like to investigate whether auto- matically learned senses of proceeding words are helpful for predicting succeeding words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical model representations of (a) Latent Dirichlet Allocation for WSI, (b) Hierarchical Dirichlet Process for WSI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of SMT system with the sense-based translation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the HDP-based word sense 
induction on the training and test data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 5 : Experiment results of the sense-based translation model (STM) against the baseline.</head><label>5</label><figDesc></figDesc><table>System 
BLEU(%) NIST 
Base 
33.53 
9.0561 
Reformulated WSD 
34.16 
9.3820 
STM 
34.73 
9.4184 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison results of the sense-based 
translation model vs. the reformulated WSD for 
SMT. 

</table></figure>

			<note place="foot" n="1"> We will discuss the relation and difference between WSI and WSD in Section 2.</note>

			<note place="foot" n="2"> we set T = 20, 000.</note>

			<note place="foot" n="3"> http://www.cs.cmu.edu/ ˜ chongw/ resource.html</note>

			<note place="foot" n="4"> http://homepages.inf.ed.ac.uk/ lzhang10/maxenttoolkit.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The work was sponsored by the National Natu-ral Science Foundation of China under projects 61373095 and 61333018. We would like to thank three anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical Machine Translation through Global Lexical Selection and Sentence Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Kanthak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<title level="m">A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="39" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian Word Sense Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009)</title>
		<meeting>the 12th Conference of the European Chapter of the ACL (EACL 2009)<address><addrLine>Athens, Greece, March</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation vs. Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="387" to="394" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving Statistical Machine Translation Using Word Sense Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Word Sense Disambiguation Improves Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An Empirical Study of Smoothing Techniques for Language Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual Meeting on Association for Computational Linguistics, ACL &apos;96</title>
		<meeting>the 34th Annual Meeting on Association for Computational Linguistics, ACL &apos;96<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Hierarchical Phrase-Based Model for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic Evaluation of Machine Translation Quality Using N-gram Cooccurrence Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Human Language Technology Research, HLT &apos;02</title>
		<meeting>the Second International Conference on Human Language Technology Research, HLT &apos;02<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MixtureModel Adaptation for SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Second Workshop on Statistical Machine Translation</title>
		<meeting>of the Second Workshop on Statistical Machine Translation<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical Phrase-Based Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Joseph</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05" />
			<biblScope unit="page" from="58" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Word Sense Induction for Novel Sense Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-04" />
			<biblScope unit="page" from="591" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saša</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003-07" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Sapporo</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bilingual LSA-based adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yik-Cheung</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">R</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="187" to="207" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hierarchical Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word-Sense Disambiguation for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vickrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Biewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Teyssier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT/EMNLP. The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A Split-Merge MCMC Algorithm for the Hierarchical Dirichlet Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Topic Similarity Model for Hierarchical Phrase-based Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="750" to="758" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Topic-Based Coherence Model for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence (AAAI-13)</title>
		<meeting>the Twenty-Seventh AAAI Conference on Artificial Intelligence (AAAI-13)<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonparametric Bayesian Word Sense Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing</title>
		<meeting>TextGraphs-6: Graph-based Methods for Natural Language Processing<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">HM-BiTAM: Bilingual Topic Exploration, Word Alignment, and Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
