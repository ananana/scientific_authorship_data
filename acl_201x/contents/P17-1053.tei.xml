<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Neural Relation Detection for Knowledge Base Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><forename type="middle">Saidul</forename><surname>Hasan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Dos Santos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Information and Language Processing</orgName>
								<orgName type="institution" key="instit1">AI Foundations</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<address>
									<country>USA ?</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">LMU Munich ‡ IBM Watson</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Neural Relation Detection for Knowledge Base Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="571" to="581"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1053</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Relation detection is a core component of many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning which detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Base Question Answering (KBQA) systems answer questions by obtaining informa- tion from KB tuples <ref type="bibr" target="#b2">(Berant et al., 2013;</ref><ref type="bibr" target="#b3">Bordes et al., 2015;</ref><ref type="bibr" target="#b1">Bast and Haussmann, 2015;</ref><ref type="bibr" target="#b26">Yih et al., 2015;</ref>. For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple <ref type="bibr" target="#b7">(Fader et al., 2013;</ref><ref type="bibr" target="#b27">Yih et al., 2014;</ref><ref type="bibr" target="#b3">Bordes et al., 2015)</ref>; and (b) a more complex case, where some constraints need to be handled for multiple entities in the question. The KBQA system in the figure performs two key tasks: (1) entity linking, which links n-grams in questions to KB entities, and (2) relation detection, which identifies the KB relation(s) a question refers to.</p><p>The main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system. Although general relation detection 1 methods are well stud- ied in the NLP community, such studies usually do not take the end task of KBQA into considera- tion. As a result, there is a significant gap between general relation detection studies and KB-specific relation detection. First, in most general relation detection tasks, the number of target relations is limited, normally smaller than 100. In contrast, in KBQA even a small KB, like Freebase2M ( <ref type="bibr" target="#b3">Bordes et al., 2015)</ref>, contains more than 6,000 relation types. Second, relation detection for KBQA often becomes a zero-shot learning task, since some test instances may have unseen relations in the training data. For example, the SimpleQuestions ( <ref type="bibr" target="#b3">Bordes et al., 2015</ref>) data set has 14% of the golden test relations not observed in golden training tuples. Third, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), for some KBQA tasks like WebQuestions ( <ref type="bibr" target="#b2">Berant et al., 2013)</ref>, we need to predict a chain of relations instead of a single relation. This increases the number of tar- get relation types and the sizes of candidate rela- tion pools, further increasing the difficulty of KB relation detection. Owing to these reasons, KB re- lation detection is significantly more challenging compared to general relation detection tasks.</p><p>This paper improves KB relation detection to cope with the problems mentioned above. First, in order to deal with the unseen relations, we propose to break the relation names into word sequences for question-relation matching. Second, noticing  entity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic entity). Based on the detected entity and relation, we form a query to search the KB for the correct answer "Love Will Find a Way". (b) A more complex question containing two entities. By using "Grant Show" as the topic entity, we could detect a chain of relations "starring roles-series" pointing to the answer. An additional constraint detection takes the other entity "2008" as a constraint, to filter the correct answer "SwingTown" from all candidates found by the topic entity and relation.</p><p>that original relation names can sometimes help to match longer question contexts, we propose to build both relation-level and word-level rela- tion representations. Third, we use deep bidirec- tional LSTMs (BiLSTMs) to learn different levels of question representations in order to match the different levels of relation information. Finally, we propose a residual learning method for sequence matching, which makes the model training easier and results in more abstract (deeper) question rep- resentations, thus improves hierarchical matching. In order to assess how the proposed improved relation detection could benefit the KBQA end task, we also propose a simple KBQA implemen- tation composed of two-step relation detection. Given an input question and a set of candidate enti- ties retrieved by an entity linker based on the ques- tion, our proposed relation detection model plays a key role in the KBQA process: (1) Re-ranking the entity candidates according to whether they con- nect to high confident relations detected from the raw question text by the relation detection model. This step is important to deal with the ambigui- ties normally present in entity linking results. (2) Finding the core relation (chains) for each topic entity 2 selection from a much smaller candidate entity set after re-ranking. The above steps are followed by an optional constraint detection step, when the question cannot be answered by single relations (e.g., multiple entities in the question). Finally the highest scored query from the above steps is used to query the KB for answers.</p><p>Our main contributions include: (i) An im- proved relation detection model by hierarchical matching between questions and relations with residual learning; (ii) We demonstrate that the im- proved relation detector enables our simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features ( <ref type="bibr" target="#b32">Zhou et al., 2005;</ref><ref type="bibr" target="#b15">Rink and Harabagiu, 2010;</ref><ref type="bibr" target="#b17">Sun et al., 2011</ref>). Recent re- search benefits a lot from the advancement of deep learning: from word embeddings <ref type="bibr" target="#b13">(Nguyen and Grishman, 2014;</ref><ref type="bibr" target="#b9">Gormley et al., 2015</ref>) to deep net- works like CNNs and LSTMs ( <ref type="bibr" target="#b31">Zeng et al., 2014;</ref><ref type="bibr" target="#b6">dos Santos et al., 2015;</ref><ref type="bibr" target="#b18">Vu et al., 2016</ref>) and atten- tion models ( .</p><p>The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained rela- tions; SemEval2010 Task8 has 19 relations; TAC-KBP2015 has 74 relations although it considers open-domain Wikipedia relations. All are much fewer than thousands of relations in KBQA. As a result, few work in this field focuses on dealing with large number of relations or unseen relations. <ref type="bibr" target="#b16">Yu et al. (2016)</ref> proposed to use relation embed- dings in a low-rank tensor method. However their relation embeddings are still trained in supervised way and the number of relations is not large in the experiments.  <ref type="bibr" target="#b8">Golub and He (2016)</ref> propose a gener- ative framework for single-relation KBQA which predicts relation with a character-level sequence- to-sequence model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation</head><p>Another difference between relation detection in KBQA and general RE is that general RE re- search assumes that the two argument entities are both available. Thus it usually benefits from features ( <ref type="bibr" target="#b13">Nguyen and Grishman, 2014;</ref><ref type="bibr" target="#b9">Gormley et al., 2015</ref>) or attention mechanisms ( ) based on the entity information (e.g. entity types or entity embeddings). For relation detec- tion in KBQA, such information is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one KB entity could have multiple types (type vocabulary size larger than 1,500). This makes KB entity typing itself a difficult problem so no previous used en- tity information in the relation detection model. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background: Different Granularity in KB Relations</head><p>Previous research <ref type="bibr" target="#b26">(Yih et al., 2015;</ref><ref type="bibr" target="#b29">Yin et al., 2016</ref>) formulates KB relation detection as a se- quence matching problem. However, while the questions are natural word sequences, how to rep- resent relations as sequences remains a challeng- ing problem. Here we give an overview of two types of relation sequence representations com- monly used in previous work.</p><p>(1) Relation Name as a Single Token (relation- level). In this case, each relation name is treated as a unique token. The problem with this ap- proach is that it suffers from the low relation cov- erage due to limited amount of training data, thus cannot generalize well to large number of open- domain relations. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, when treating relation names as single tokens, it will be difficult to match the questions to relation names "episodes written" and "starring roles" if these names do not appear in training data -their rela- tion embeddings h r s will be random vectors thus are not comparable to question embeddings h q s.</p><p>(2) Relation as Word Sequence (word-level). In this case, the relation is treated as a sequence of words from the tokenized relation name. It has better generalization, but suffers from the lack of global information from the original relation names. For example in <ref type="figure" target="#fig_0">Figure 1</ref>(b), when doing only word-level matching, it is difficult to rank the target relation "starring roles" higher compared to the incorrect relation "plays produced". This is because the incorrect relation contains word "plays", which is more similar to the question</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Token</head><p>Question 1 Question 2 what tv episodes were &lt;e&gt; the writer of what episode was written by &lt;e&gt; relation-level episodes written tv episodes were &lt;e&gt; the writer of episode was written by &lt;e&gt; word-level episodes tv episodes episode written the writer of written <ref type="table" target="#tab_1">Table 1</ref>: An example of KB relation (episodes written) with two types of relation tokens (relation names and words), and two questions asking this relation. The topic entity is replaced with token &lt;e&gt; which could give the position information to the deep networks. The italics show the evidence phrase for each relation token in the question.</p><p>(containing word "play") in the embedding space.</p><p>On the other hand, if the target relation co-occurs with questions related to "tv appearance" in train- ing, by treating the whole relation as a token (i.e. relation id), we could better learn the correspon- dence between this token and phrases like "tv show" and "play on".</p><p>The two types of relation representation con- tain different levels of abstraction. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the word-level focuses more on lo- cal information (words and short phrases), and the relation-level focus more on global informa- tion (long phrases and skip-grams) but suffer from data sparsity. Since both these levels of granu- larity have their own pros and cons, we propose a hierarchical matching approach for KB relation detection: for a candidate relation, our approach matches the input question to both word-level and relation-level representations to get the final rank- ing score. Section 4 gives the details of our pro- posed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Improved KB Relation Detection</head><p>This section describes our hierarchical sequence matching with residual learning approach for rela- tion detection. In order to match the question to different aspects of a relation (with different ab- straction levels), we deal with three problems as follows on learning question/relation representa- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relation Representations from Different Granularity</head><p>We provide our model with both types of re- lation representation: word-level and relation- level. Therefore, the input relation becomes r = {r word</p><formula xml:id="formula_0">1 , · · · , r word M 1 } [ {r rel 1 , · · · , r rel M 2</formula><p>}, where the first M 1 tokens are words (e.g. {episode, writ- ten}), and the last M 2 tokens are relation names, e.g., {episode written} or {starring roles, series} (when the target is a chain like in <ref type="figure" target="#fig_0">Figure 1(b)</ref>). We transform each token above to its word embed- ding then use two BiLSTMs (with shared parame- ters) to get their hidden representations [B word</p><formula xml:id="formula_1">1:M 1 : B rel 1:M 2</formula><p>] (each row vector i is the concatena- tion between forward/backward representations at i). We initialize the relation sequence LSTMs with the final state representations of the word se- quence, as a back-off for unseen relations. We ap- ply one max-pooling on these two sets of vectors and get the final relation representation h r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Different Abstractions of Questions Representations</head><p>From As a result, we hope the question representa- tions could also comprise vectors that summa- rize various lengths of phrase information (differ- ent levels of abstraction), in order to match rela- tion representations of different granularity. We deal with this problem by applying deep BiL- STMs on questions. The first-layer of BiLSTM works on the word embeddings of question words q = {q 1 , · · · , q N } and gets hidden representations</p><formula xml:id="formula_2">(1) 1:N = [ (1) 1 ; · · · ; (1) N ].</formula><p>The second-layer BiL- STM works on <ref type="bibr">(1)</ref> 1:N to get the second set of hid- den representations <ref type="bibr">(2)</ref> 1:N . Since the second BiL- STM starts with the hidden vectors from the first layer, intuitively it could learn more general and abstract information compared to the first layer.</p><p>Note that the first(second)-layer of question rep- resentations does not necessarily correspond to the word(relation)-level relation representations, in- stead either layer of question representations could potentially match to either level of relation repre- sentations. This raises the difficulty of matching between different levels of relation/question rep- resentations; the following section gives our pro- posal to deal with such problem. ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>…</head><p>...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Relation</head><p>Figure 2: The proposed Hierarchical Residual BiLSTM (HR-BiLSTM) model for relation detection.</p><p>Note that without the dotted arrows of shortcut connections between two layers, the model will only compute the similarity between the second-layer of questions representations and the relation, thus is not doing hierarchical matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hierarchical Matching between Relation and Question</head><p>Now we have question contexts of different lengths encoded in</p><p>1:N and</p><p>1:N . Unlike the standard usage of deep BiLSTMs that employs the representations in the final layer for prediction, here we expect that two layers of question repre- sentations can be complementary to each other and both should be compared to the relation represen- tation space (Hierarchical Matching). This is im- portant for our task since each relation token can correspond to phrases of different lengths, mainly because of syntactic variations. For example in Ta- ble 1, the relation word written could be matched to either the same single word in the question or a much longer phrase be the writer of.</p><p>We could perform the above hierarchical match- ing by computing the similarity between each layer of and h r separately and doing the (weighted) sum between the two scores. How- ever this does not give significant improvement (see <ref type="table" target="#tab_2">Table 2</ref>). Our analysis in Section 6.2 shows that this naive method suffers from the training difficulty, evidenced by that the converged train- ing loss of this model is much higher than that of a single-layer baseline model. This is mainly because (1) Deep BiLSTMs do not guarantee that the two-levels of question hidden representations are comparable, the training usually falls to local optima where one layer has good matching scores and the other always has weight close to 0. <ref type="formula" target="#formula_4">(2)</ref> The training of deeper architectures itself is more difficult.</p><p>To overcome the above difficulties, we adopt the idea from Residual Networks (He et al., 2016) for hierarchical matching by adding shortcut connec- tions between two BiLSTM layers. We proposed two ways of such Hierarchical Residual Match- ing: (1) Connecting each max . Fi- nally we compute the matching score of r given q as s rel (r; q) = cos(h r , h q ).</p><p>Intuitively, the proposed method should benefit from hierarchical training since the second layer is fitting the residues from the first layer of matching, so the two layers of representations are more likely to be complementary to each other. This also en- sures the vector spaces of two layers are compara- ble and makes the second-layer training easier.</p><p>During training we adopt a ranking loss to max- imizing the margin between the gold relation r + and other relations r in the candidate pool R. l rel = max{0, s rel (r + ; q) + s rel (r ; q)} where is a constant parameter. <ref type="figure">Fig 2 sum</ref>- marizes the above Hierarchical Residual BiLSTM (HR-BiLSTM) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark: Another way of hierarchical matching</head><p>consists in relying on attention mechanism, e.g. ( <ref type="bibr" target="#b14">Parikh et al., 2016)</ref>, to find the correspondence between different levels of representations. This performs below the HR-BiLSTM (see <ref type="table" target="#tab_2">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">KBQA Enhanced by Relation Detection</head><p>This section describes our KBQA pipeline system. We make minimal efforts beyond the training of the relation detection model, making the whole system easy to build. Following previous work <ref type="bibr" target="#b26">(Yih et al., 2015;</ref>), our KBQA system takes an existing entity linker to produce the top-K linked entities, EL K (q), for a question q ("initial entity linking"). Then we generate the KB queries for q following the four steps illustrated in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: KBQA with two-step relation detection</head><p>Input : Question q, Knowledge Base KB, the initial top-K entity candidates ELK (q) Output: Top query tuple (ˆ e, ˆ r, {(c, rc)})</p><p>1 Entity Re-Ranking (first-step relation detection): Use the raw question text as input for a relation detector to score all relations in the KB that are associated to the entities in ELK (q); use the relation scores to re-rank ELK (q) and generate a shorter list EL 0 K 0 (q) containing the top-K 0 entity candidates (Section 5.1) 2 Relation Detection: Detect relation(s) using the reformatted question text in which the topic entity is replaced by a special token &lt;e&gt; (Section 5.2) 3 Query Generation: Combine the scores from step 1 and 2, and select the top pair (ˆ e, ˆ r) (Section 5.3) 4 Constraint Detection (optional): Compute similarity between q and any neighbor entity c of the entities alongˆralongˆ alongˆr (connecting by a relation rc) , add the high scoring c and rc to the query (Section 5.4).</p><p>Compared to previous approaches, the main dif- ference is that we have an additional entity re- ranking step after the initial entity linking. We have this step because we have observed that entity linking sometimes becomes a bottleneck in KBQA systems. For example, on SimpleQuestions the best reported linker could only get 72.7% top-1 accuracy on identifying topic entities. This is usu- ally due to the ambiguities of entity names, e.g. in <ref type="figure" target="#fig_0">Fig 1(a)</ref>, there are TV writer and baseball player "Mike Kelley", which is impossible to distinguish with only entity name matching.</p><p>Having observed that different entity candidates usually connect to different relations, here we pro- pose to help entity disambiguation in the initial en- tity linking with relations detected in questions.</p><p>Sections 5.1 and 5.2 elaborate how our relation detection help to re-rank entities in the initial en- tity linking, and then those re-ranked entities en- able more accurate relation detection. The KBQA end task, as a result, benefits from this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Entity Re-Ranking</head><p>In this step, we use the raw question text as input for a relation detector to score all relations in the KB with connections to at least one of the entity candidates in EL K (q). We call this step relation detection on entity set since it does not work on a single topic entity as the usual settings. We use the HR-BiLSTM as described in Sec. 4. For each question q, after generating a score s rel (r; q) for each relation using HR-BiLSTM, we use the top l best scoring relations (R l q ) to re-rank the origi- nal entity candidates. Concretely, for each entity e and its associated relations R e , given the origi- nal entity linker score s linker , and the score of the most confident relation r 2 R l q \R e , we sum these two scores to re-rank the entities:</p><formula xml:id="formula_5">s rerank (e; q) =↵ · s linker (e; q) +(1 ↵) · max r2R l q \Re s rel (r; q).</formula><p>Finally, we select top K 0 &lt; K entities according to score s rerank to form the re-ranked list EL 0 K 0 (q). We use the same example in <ref type="figure" target="#fig_0">Fig 1(a)</ref> to illustrate the idea. Given the input question in the exam- ple, a relation detector is very likely to assign high scores to relations such as "episodes written", "author of " and "profession". Then, according to the connections of entity candidates in KB, we find that the TV writer "Mike Kelley" will be scored higher than the baseball player "Mike Kelley", because the former has the relations "episodes written" and "profession". This method can be viewed as exploiting entity-relation collo- cation for entity linking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relation Detection</head><p>In this step, for each candidate entity e 2 EL 0 K (q), we use the question text as the input to a relation detector to score all the relations r 2 R e that are associated to the entity e in the KB. <ref type="bibr">4</ref> Be- cause we have a single topic entity input in this step, we do the following question reformatting: we replace the the candidate e's entity mention in q with a token "&lt;e&gt;". This helps the model bet- ter distinguish the relative position of each word compared to the entity. We use the HR-BiLSTM model to predict the score of each relation r 2 R e : s rel (r; e, q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Query Generation</head><p>Finally, the system outputs the &lt;entity, relation (or core-chain)&gt; pair (ˆ e, ˆ r) according to:</p><formula xml:id="formula_6">s(ˆ e, ˆ r; q) = max e2EL 0 K 0 (q),r2Re</formula><p>( · s rerank (e; q)</p><formula xml:id="formula_7">+(1 ) · s rel (r; e, q))</formula><p>, where is a hyperparameter to be tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Constraint Detection</head><p>Similar to <ref type="bibr" target="#b26">(Yih et al., 2015)</ref>, we adopt an ad- ditional constraint detection step based on text matching. Our method can be viewed as entity- linking on a KB sub-graph. It contains two steps: (1) Sub-graph generation: given the top scored query generated by the previous 3 steps 5 , for each node v (answer node or the CVT node like in <ref type="figure" target="#fig_0">Fig- ure 1(b)</ref>), we collect all the nodes c connecting to v (with relation r c ) with any relation, and generate a sub-graph associated to the original query. (2) Entity-linking on sub-graph nodes: we compute a matching score between each n-gram in the input question (without overlapping the topic entity) and entity name of c (except for the node in the orig- inal query) by taking into account the maximum overlapping sequence of characters between them (see Appendix A for details and B for special rules dealing with date/answer type constraints). If the matching score is larger than a threshold ✓ (tuned on training set), we will add the constraint entity c (and r c ) to the query by attaching it to the corre- sponding node v on the core-chain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Task Introduction &amp; Settings</head><p>We use the SimpleQuestions ( <ref type="bibr" target="#b3">Bordes et al., 2015)</ref> and <ref type="bibr">WebQSP (Yih et al., 2016</ref>) datasets. Each question in these datasets is labeled with the gold semantic parse. Hence we can directly evaluate relation detection performance independently as well as evaluate on the KBQA end task.</p><p>SimpleQuestions (SQ): It is a single-relation KBQA task. The KB we use consists of a Freebase subset with 2M entities (FB2M) ( <ref type="bibr" target="#b3">Bordes et al., 2015)</ref>, in order to compare with previous research. <ref type="bibr" target="#b29">Yin et al. (2016)</ref> also evaluated their relation ex- tractor on this data set and released their proposed question-relation pairs, so we run our relation de- tection model on their data set. For the KBQA evaluation, we also start with their entity linking results <ref type="bibr">6</ref> . Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following <ref type="bibr" target="#b28">Yih et al. (2016)</ref>, we use S-MART (Yang and Chang, 2015) entity-linking outputs. <ref type="bibr">7</ref> In order to evaluate the relation detec- tion models, we create a new relation detection task from the WebQSP data set. 8 For each ques- tion and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length  2) connected to the topic entity, and set the core- chain labeled in the parse as the positive label and all the others as the negative examples.</p><p>We tune the following hyper-parameters on de- velopment sets: (1) the size of hidden states for LSTMs ({50, 100, 200, 400}) 9 ; (2) learning rate ({0.1, 0.5, 1.0, 2.0}); (3) whether the shortcut connections are between hidden states or between max-pooling results (see Section 4.3); and (4) the number of training epochs.</p><p>For both the relation detection experiments and the second-step relation detection in KBQA, we have entity replacement first (see Section 5.2 and <ref type="figure" target="#fig_0">Figure 1</ref>). All word vectors are initialized with 300-d pretrained word embeddings ( <ref type="bibr" target="#b12">Mikolov et al., 2013)</ref>. The embeddings of relation names are randomly initialized, since existing pre-trained relation embeddings (e.g. TransE) usually support limited sets of relation names. We leave the usage of pre-trained relation embeddings to future work.  ods. We re-implemented the BiCNN model from <ref type="bibr" target="#b26">(Yih et al., 2015)</ref>, where both questions and rela- tions are represented with the word hash trick on character tri-grams. The baseline BiLSTM with relation word sequence appears to be the best base- line on WebQSP and is close to the previous best result of AMPCNN on SimpleQuestions. Our pro- posed HR-BiLSTM outperformed the best base- lines on both tasks by margins of 2-3% (p &lt; 0.001 and 0.01 compared to the best baseline BiLSTM w/ words on SQ and WQ respectively). Note that using only relation names instead of words results in a weaker baseline BiLSTM model. The model yields a significant per- formance drop on SimpleQuestions (91.2% to 88.9%). However, the drop is much smaller on WebQSP, and it suggests that unseen relations have a much bigger impact on SimpleQuestions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Relation Detection Results</head><p>Ablation Test: The bottom of <ref type="table" target="#tab_2">Table 2</ref> shows ab- lation results of the proposed HR-BiLSTM. First, hierarchical matching between questions and both relation names and relation words yields improve- ment on both datasets, especially for SimpleQues- tions (93.3% vs. 91.2/88.8%). Second, residual learning helps hierarchical matching compared to weighted-sum and attention-based baselines (see Section 4.3). For the attention-based baseline, we tried the model from ( <ref type="bibr" target="#b14">Parikh et al., 2016)</ref> and its one-way variations, where the one-way model gives better results <ref type="bibr">10</ref> . Note that residual learn- ing significantly helps on WebQSP (80.65% to <ref type="bibr">10</ref> We also tried to apply the same attention method on deep BiLSTM with residual connections, but it does not lead to better results compared to HR-BiLSTM. We hypothesize that the idea of hierarchical matching with attention mechanism may work better for long sequences, and the new advanced attention mechanisms ( <ref type="bibr" target="#b20">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b21">Wang et al., 2017</ref>) might help hierarchical matching. We leave the above directions to future work. 82.53%), while it does not help as much on Sim- pleQuestions. On SimpleQuestions, even remov- ing the deep layers only causes a small drop in per- formance. WebQSP benefits more from residual and deeper architecture, possibly because in this dataset it is more important to handle larger scope of context matching.</p><p>Finally, on WebQSP, replacing BiLSTM with CNN in our hierarchical matching framework re- sults in a large performance drop. Yet on Sim- pleQuestions the gap is much smaller. We believe this is because the LSTM relation encoder can bet- ter learn the composition of chains of relations in WebQSP, as it is better at dealing with longer de- pendencies.</p><p>Analysis Next, we present empirical evidences, which show why our HR-BiLSTM model achieves the best scores. We use WebQSP for the analy- sis purposes. First, we have the hypothesis that training of the weighted-sum model usually falls to local optima, since deep BiLSTMs do not guar- antee that the two-levels of question hidden rep- resentations are comparable. This is evidenced by that during training one layer usually gets a weight close to 0 thus is ignored. For exam- ple, one run gives us weights of -75.39/0.14 for the two layers (we take exponential for the final weighted sum). It also gives much lower train- ing accuracy (91.94%) compared to HR-BiLSTM (95.67%), suffering from training difficulty.</p><p>Second, compared to our deep BiLSTM with shortcut connections, we have the hypothesis that for KB relation detection, training deep BiLSTMs is more difficult without shortcut connections. Our experiments suggest that deeper BiLSTM does not always result in lower training accuracy. In the experiments a two-layer BiLSTM converges to 94.99%, even lower than the 95.25% achieved by a single-layer BiLSTM. Under our setting the two- layer model captures the single-layer model as a special case (so it could potentially better fit the training data), this result suggests that the deep BiLSTM without shortcut connections might suf- fers more from training difficulty.</p><p>Finally, we hypothesize that HR-BiLSTM is more than combination of two BiLSTMs with residual connections, because it encourages the hierarchical architecture to learn different levels of abstraction. To verify this, we replace the deep BiLSTM question encoder with two single-layer BiLSTMs (both on words) with shortcut connec- tions between their hidden states. This decreases test accuracy to 76.11%. It gives similar training accuracy compared to HR-BiLSTM, indicating a more serious over-fitting problem. This proves that the residual and deep structures both con- tribute to the good performance of HR-BiLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">KBQA End-Task Results</head><p>Table 3 compares our system with two published baselines (1) STAGG ( <ref type="bibr" target="#b26">Yih et al., 2015)</ref>, the state- of-the-art on WebQSP 11 and (2) AMPCNN ( <ref type="bibr" target="#b29">Yin et al., 2016)</ref>, the state-of-the-art on SimpleQues- tions. Since these two baselines are specially de- signed/tuned for one particular dataset, they do not generalize well when applied to the other dataset. In order to highlight the effect of different rela- tion detection models on the KBQA end-task, we also implemented another baseline that uses our KBQA system but replaces HR-BiLSTM with our implementation of AMPCNN (for SimpleQues- tions) or the char-3-gram BiCNN (for WebQSP) relation detectors (second block in <ref type="table">Table 3</ref>).</p><p>Compared to the baseline relation detector (3rd row of results), our method, which includes an im- proved relation detector (HR-BiLSTM), improves the KBQA end task by 2-3% (4th row). Note that in contrast to previous KBQA systems, our sys- tem does not use joint-inference or feature-based re-ranking step, nevertheless it still achieves better or comparable results to the state-of-the-art.</p><p>The third block of the table details two ablation tests for the proposed components in our KBQA systems: (1) Removing the entity re-ranking step significantly decreases the scores. Since the re- ranking step relies on the relation detection mod- els, this shows that our HR-BiLSTM model con- tributes to the good performance in multiple ways. <ref type="bibr">11</ref> The STAGG score on SQ is from ( <ref type="bibr" target="#b0">Bao et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>SQ WQ STAGG 72.8 63.9 AMPCNN ( <ref type="bibr" target="#b29">Yin et al., 2016)</ref> 76.4 - Baseline: Our Method w/ 75.1 60.0 baseline relation detector Our Method 77.0 63.0 w/o entity re-ranking 74.9 60.6 w/o constraints - 58.0 Our Method (multi-detectors) 78.7 63.9 <ref type="table">Table 3</ref>: KBQA results on SimpleQuestions (SQ) and WebQSP (WQ) test sets. The numbers in green color are directly comparable to our results since we start with the same entity linking results.</p><p>Appendix C gives the detailed performance of the re-ranking step. (2) In contrast to the conclusion in ( <ref type="bibr" target="#b26">Yih et al., 2015)</ref>, constraint detection is crucial for our system <ref type="bibr">12</ref> . This is probably because our joint performance on topic entity and core-chain detection is more accurate (77.5% top-1 accuracy), leaving a huge potential (77.5% vs. 58.0%) for the constraint detection module to improve.</p><p>Finally, like STAGG, which uses multiple rela- tion detectors (see <ref type="bibr" target="#b26">Yih et al. (2015)</ref> for the three models used), we also try to use the top-3 rela- tion detectors from Section 6.2. As shown on the last row of <ref type="table">Table 3</ref>, this gives a significant perfor- mance boost, resulting in a new state-of-the-art re- sult on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>KB relation detection is a key step in KBQA and is significantly different from general relation ex- traction tasks. We propose a novel KB relation detection model, HR-BiLSTM, that performs hier- archical matching between questions and KB rela- tions. Our model outperforms the previous meth- ods on KB relation detection tasks and allows our KBQA system to achieve state-of-the-arts. For fu- ture work, we will investigate the integration of our HR-BiLSTM into end-to-end systems. For ex- ample, our model could be integrated into the de- coder in ( <ref type="bibr" target="#b11">Liang et al., 2016)</ref>, to provide better se- quence prediction. We will also investigate new emerging datasets like GraphQuestions ( <ref type="bibr" target="#b16">Su et al., 2016</ref>) and ComplexQuestions ( <ref type="bibr" target="#b0">Bao et al., 2016)</ref> to handle more characteristics of general QA.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: KBQA examples and its three key components. (a) A single relation example. We first identify the topic entity with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Detection in KBQA Systems Rela- tion detection for KBQA also starts with feature- rich approaches (Yao and Van Durme, 2014; Bast and Haussmann, 2015) towards usages of deep networks (Yih et al., 2015; Xu et al., 2016; Dai et al., 2016) and attention models (Yin et al., 2016; Golub and He, 2016). Many of the above re- lation detection research could naturally support large relation vocabulary and open relation sets (especially for QA with OpenIE KB like ParaLex (Fader et al., 2013)), in order to fit the goal of open-domain question answering. Different KBQA data sets have different levels of requirement about the above open-domain ca- pacity. For example, most of the gold test relations in WebQuestions can be observed during train- ing, thus some prior work on this task adopted the close domain assumption like in the general RE re- search. While for data sets like SimpleQuestions and ParaLex, the capacity to support large relation sets and unseen relations becomes more necessary. To the end, there are two main solutions: (1) use pre-trained relation embeddings (e.g. from TransE (Bordes et al., 2013)), like (Dai et al., 2016); (2) factorize the relation names to sequences and for- mulate relation detection as a sequence match- ing and ranking task. Such factorization works because that the relation names usually comprise meaningful word sequences. For example, Yin et al. (2016) split relations to word sequences for single-relation detection. Liang et al. (2016) also achieve good performance on WebQSP with word- level relation representation in an end-to-end neu- ral programmer model. Yih et al. (2015) use char- acter tri-grams as inputs on both question and rela- tion sides.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>i</head><label></label><figDesc>for each position i. Then the final question representation h q becomes a max- pooling over all 0 i s, 1iN . (2) Applying max- pooling on (1) 1:N and (2) 1:N to get h (1) max and h (2) max , respectively, then setting h q = h (1) max + h (2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Question : what episode was mike kelley the writer of</head><label>Question</label><figDesc></figDesc><table>Knowledge 
Base 

Mike Kelley 

(American television 
writer/producer) 

Mike Kelley 

(American baseball 
player) 

… 

Entity Linking 

Love Will Find a Way 

USA 

… 

First baseman 

… 

episodes_written 

position_played 

Relation Detection 

(a) 
(b) 

Question: what tv show did grant show play on in 2008 

Mike Kelley 
? 

episodes_written 

Entity Linking 
Relation 
Detection 

Grant Show 

? 

starring_roles 

series 

(date) from 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 ,</head><label>1</label><figDesc>we can see that different parts of a relation could match different contexts of question texts. Usually relation names could match longer phrases in the question and relation words could match short phrases. Yet different words might match phrases of different lengths.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>the results on two relation detec-
tion tasks. The AMPCNN result is from (Yin 
et al., 2016), which yielded state-of-the-art scores 
by outperforming several attention-based meth-

6 The two resources have been downloaded from https: 

//github.com/Gorov/SimpleQuestions-EntityLinking 

7 https://github.com/scottyih/STAGG 
8 The dataset is available at https://github.com/Gorov/ 

SimpleQuestions-EntityLinking. 

9 For CNNs we double the size for fair comparison. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Accuracy on the SimpleQuestions and WebQSP relation detection tasks (test sets). The top 
shows performance of baselines. On the bottom we give the results of our proposed model together with 
the ablation tests. 

</table></figure>

			<note place="foot" n="1"> In the information extraction field such tasks are usually called relation extraction or relation classification.</note>

			<note place="foot" n="2"> Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node.</note>

			<note place="foot" n="3"> Such entity information has been used in KBQA systems as features for the final answer re-rankers.</note>

			<note place="foot" n="4"> Note that the number of entities and the number of relation candidates will be much smaller than those in the previous step.</note>

			<note place="foot" n="5"> Starting with the top-1 query suffers more from error propagation. However we still achieve state-of-the-art on WebQSP in Sec.6, showing the advantage of our relation detection model. We leave in future work beam-search and feature extraction on beam for final answer re-ranking like in previous research.</note>

			<note place="foot" n="12"> Note that another reason is that we are evaluating on accuracy here. When evaluating on F1 the gap will be smaller.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Constraint-based question answering with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2503" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">More accurate question answering on freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Bast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmar</forename><surname>Haussmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1431" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cfo: Conditional focused neural question answering with largescale knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="800" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1). Citeseer</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1608" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Character-level question answering with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00727</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved relation extraction with feature-rich compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Kenneth D Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00020</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Employing word representations and regularization for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="68" to="74" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Utd: Classifying semantic relations by combining lexical and semantic resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics</title>
		<meeting>the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="256" to="259" />
		</imprint>
	</monogr>
	<note>Sweden</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On generating characteristic-rich question sets for qa evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mudhakar</forename><surname>Srivatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1054" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="562" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised relation extraction with large-scale word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="521" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining recurrent and convolutional neural networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1298" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning natural language inference with lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1442" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Bilateral multi-perspective matching for natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03814</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Question answering on freebase via relation extraction and textual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2326" to="2336" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">S-mart: Novel tree-based structured learning algorithms applied to tweet entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Freebase qa: Information extraction or semantic parsing? ACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">82</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1). Citeseer</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semantic parsing for single-relation question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="643" to="648" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The value of semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingwei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simple question answering by attentive convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. The COLING 2016 Organizing Committee<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1746" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Embedding lexical features via lowrank tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1117" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City University and Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
