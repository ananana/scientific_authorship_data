<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-Linear Text Regression with a Deep Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 26-31, 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Bitvai</surname></persName>
							<email>z.bitvai@shef.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Sheffield</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<email>t.cohn@unimelb.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-Linear Text Regression with a Deep Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="180" to="185"/>
							<date type="published">July 26-31, 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Text regression has traditionally been tackled using linear models. Here we present a non-linear method based on a deep convolutional neural network. We show that despite having millions of parameters , this model can be trained on only a thousand documents, resulting in a 40% relative improvement over sparse linear models, the previous state of the art. Further, this method is flexible allowing for easy incorporation of side information such as document meta-data. Finally we present a novel technique for interpreting the effect of different text inputs on this complex non-linear model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text regression involves predicting a real world phenomenon from textual inputs, and has been shown to be effective in domains including elec- tion results ( <ref type="bibr" target="#b9">Lampos et al., 2013)</ref>, financial risk ( <ref type="bibr" target="#b6">Kogan et al., 2009)</ref> and public health <ref type="bibr" target="#b8">(Lampos and Cristianini, 2010)</ref>. Almost universally, the text regression problem has been framed as lin- ear regression, with the modelling innovation fo- cussed on effective regression, e.g., using Lasso penalties to promote feature sparsity <ref type="bibr" target="#b13">(Tibshirani, 1996)</ref>. <ref type="bibr">1</ref> Despite their successes, linear models are limiting: text regression problems will often in- volve complex interactions between textual inputs, thus requiring a non-linear approach to properly capture such phenomena. For instance, in mod- elling movie revenue conjunctions of features are likely to be important, e.g., a movie described as 'scary' is likely to have different effects for chil- dren's versus adult movies. While these kinds of features can be captured using explicit feature en- gineering, this process is tedious, limited in scope (e.g., to conjunctions) and -as we show here - can be dramatically improved by representational learning as part of a non-linear model.</p><p>In this paper, we propose an artificial neu- ral network (ANN) for modelling text regression. In language processing, ANNs were first pro- posed for probabilistic language modelling <ref type="bibr" target="#b0">(Bengio et al., 2003)</ref>, followed by models of sentences ( <ref type="bibr" target="#b4">Kalchbrenner et al., 2014</ref>) and parsing <ref type="bibr" target="#b12">(Socher et al., 2013</ref>) inter alia. These approaches have shown strong results through automatic learning dense low-dimensional distributed representations for words and other linguistic units, which have been shown to encode important aspects of lan- guage syntax and semantics. In this paper we develop a convolutional neural network, inspired by their breakthrough results in image process- ing ( <ref type="bibr" target="#b7">Krizhevsky et al., 2012</ref>) and recent applica- tions to language processing ( <ref type="bibr" target="#b4">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b5">Kim, 2014)</ref>. These works have mainly fo- cused on 'big data' problems with plentiful train- ing examples. Given their large numbers of pa- rameters, often in the millions, one would expect that such models can only be effectively learned on very large datasets. However we show here that a complex deep convolution network can be trained on about a thousand training examples, al- though careful model design and regularisation is paramount.</p><p>We consider the problem of predicting the fu- ture box-office takings of movies based on reviews by movie critics and movie attributes. Our ap- proach is based on the method and dataset of <ref type="bibr" target="#b3">Joshi et al. (2010)</ref>, who presented a linear regression model over uni-, bi-, and tri-gram term frequency counts extracted from reviews, as well as movie and reviewer metadata. This problem is especially interesting, as comparatively few instances are available for training (see <ref type="table" target="#tab_1">Table 1) while each in-train   dev  test  total   # movies  1147  317  254  1718  # reviews per movie  4.2  4.0  4.1  4.1  # sentences per movie  95  84  82  91  # words per movie  2879 2640 2605 2794   Table 1</ref>: Movie review dataset ( <ref type="bibr" target="#b3">Joshi et al., 2010)</ref>. stance (movie) includes a rich array of data includ- ing the text of several critic reviews from various review sites, as well as structured data (genre, rat- ing, actors, etc.) Joshi et al. found that regression purely from movie meta-data gave strong predic- tive accuracy, while text had a weaker but comple- mentary signal. Their best results were achieved by domain adaptation whereby text features were conjoined with a review site identifier. Inspired by <ref type="bibr" target="#b3">Joshi et al. (2010)</ref> our model also operates over n- grams, 1 ≤ n ≤ 3, and movie metadata, albeit using an ANN in place of their linear model. We use word embeddings to represent words in a low dimensional space, a convolutional network with max-pooling to represent documents in terms of n-grams, and several fully connected hidden lay- ers to allow for learning of complex non-linear in- teractions. We show that including non-linearities in the model is crucial for accurate modelling, pro- viding a relative error reduction of 40% (MAE) over their best linear model. Our final contribu- tion is a novel means of model interpretation. Al- though it is notoriously difficult to interpret the pa- rameters of an ANN, we show a simple method of quantifying the effect of text n-grams on the pre- diction output. This allows for identification of the most important textual inputs, and investigation of non-linear interactions between these words and phrases in different data instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>The outline of the convolutional network is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We have n training examples of the</p><formula xml:id="formula_0">form {b i , r i , y i } n i=1</formula><p>, where b i is the meta data as- sociated with movie i, y i is the target gross week- end revenue, and r i is a collection of u i number of reviews, r i = {x j , t j } u i j=1 where each review has review text x j and a site id t j . We concatenate all the review texts d i = (x 1 , x 2 , ..., x u ) to form our text input (see part I of <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>To acquire a distributed representation of the text, we look up the input tokens in a pretrained word embedding matrix E with size |V |×e, where</p><formula xml:id="formula_1">Embeddings, E • • •</formula><p>• This movie is the best one of the century, the actors are exceptional ... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boston Globe, r 1 = {x</head><formula xml:id="formula_2">(IV) (V)</formula><p>uni-grams</p><formula xml:id="formula_3">C (1)</formula><p>bi-grams tri-grams</p><formula xml:id="formula_4">C (2) C (3) • • • • • • • • ... • • • • • • • • • • • • • • • • fully connected</formula><p>fully connected fully connected |V | is the size of our vocabulary and e is the em- bedding dimensionality. This gives us a dense document matrix D i,j = E d i ,j with dimensions m × e where m is the number of tokens in the document.</p><p>Since the length of text documents vary, in part II we apply convolutions with width one, two and three over the document matrix to obtain a fixed length representation of the text. The n-gram con- volutions help identify local context and map that to a new higher level feature space. For each fea- ture map, the convolution takes adjacent word em- beddings and performs a feed forward computa- tion with shared weights over the convolution win- dow. For a convolution with width 1 ≤ q ≤ m this is</p><formula xml:id="formula_5">S (q) i,· = (D i,· , D i+1,· , ..., D i+q−1,· ) C (q) i,· = S (q) i,· , W (q)</formula><p>where S (q) i,· is q adjacent word embeddings con- catenated, and C (q) is the convolution output ma- trix with (m−q+1) rows after a linear transforma- tion with weights W (q) . To allow for a non-linear transformation, we make use of rectified linear ac- tivation units, H (q) = max(C (q) , 0), which are universal function approximators. Finally, to com- press the representation of text to a fixed dimen- sional vector while ensuring that important infor- mation is preserved and propagated throughout the network, we apply max pooling over time, i.e. the sequence of words, for each dimension, as shown in part III,</p><formula xml:id="formula_6">p (q) j = max H (q) ·,j where p (q)</formula><p>j is dimension j of the pooling layer for convolution q, and p is the concatenation of all pooling layers, p = (p (1) , p (2) , ..., p (q) ).</p><p>Next, we perform a series of non-linear trans- formations to the document vector in order to pro- gressively acquire higher level representations of the text and approximate a linear relationship in the final output prediction layer. Applying multi- ple hidden layers in succession can require expo- nentially less data than mapping through a single hidden layer <ref type="bibr" target="#b1">(Bengio, 2009)</ref>. Therefore, in part IV, we apply densely connected neural net layers of the form o k = h(g(a k , W k )) where a k is the input and o k = a k+1 is the output vector for layer k, g is a linear transformation function a k , W k , and h is the activation function, i.e. rectified linear seen above. l = 3 hidden layers are applied be- fore the final regression layer to produce the out- put f = g(o l , w l+1 ) in part V.</p><p>The mean absolute error is measured between the predictions f and the targets y, which is more permissible to outliers than the squared error. The cost J is defined as</p><formula xml:id="formula_7">J = 1 n n v=1 |f v − y v |.</formula><p>The network is trained with stochastic gradient de- scent and the Ada Delta (Zeiler, 2012) update rule using random restarts. Stochastic gradient descent is noisier than batch training due to a local esti- mation of the gradient, but it can start converging much faster. Ada Delta keeps an exponentially de- caying history of gradients and updates in order to adapt the learning rate for each parameter, which partially smooths out the training noise. Regulari- sation and hyperparmeter selection are performed by early stopping on the development set. The size of the vocabulary is 90K words. Note that 10% of our lexicon is not found in the embeddings i,· , t) in part II, where t z = 1 z indicates whether domain z has reviewed the movie. <ref type="bibr">2</ref> This helps the network bias the convolutions, and thus change which features get propagated in the pool- ing layer.</p><note type="other">Model Description MAE($M) Baseline mean 11.7 Linear Text 8.0 Linear Text+Domain+POS 7.4 Linear Meta 6.0 Linear Text+Meta 5.9 Linear Text+Meta+Domain+Deps 5.7 ANN Text 6.3 ANN Text+Domain 6.0 ANN Meta 3.9 ANN Text+Meta 3.4 ANN Text+Meta+Domain 3.4</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The results in <ref type="table" target="#tab_1">Table 2</ref> show that the neural net- work performs very well, with around 40% im- provement over the previous best results ( <ref type="bibr" target="#b3">Joshi et al., 2010</ref>  casing, and discarding feature instances that oc- curred in fewer than five reviews. In contrast, we did not perform any processing of the text or fea- ture engineering, apart from tokenization, instead learning this automatically. <ref type="bibr">3</ref> We find that both text and meta data con- tain complementary signals with some informa- tion overlap between them. This confirms the find- ing of <ref type="bibr" target="#b2">Bitvai and Cohn (2015)</ref> on another text re- gression problem. The meta features alone almost achieve the best results whereas text alone per- forms worse but still well above the baseline. For the combined model, the performance improves slightly. In <ref type="table" target="#tab_3">Table 3</ref> we can see that contrary to ex- pectations, fine tuning the word embeddings does not help significantly compared to keeping them fixed. Moreover, randomly initialising the embed- dings and fixing them performs quite well. Fine tuning may be a challenging optimisation due to the high dimensional embedding space and the loss of monolingual information. This is further exacerbated due to the limited supervision signal.</p><p>One of the main sources of improvement ap- pears to come from the non-linearity applied to the neural network activations. To test this, we try us- ing linear activation units in parts II and IV of the network. Composition of linear functions yields a linear function, and therefore we recover the lin- ear model results. This is much worse than the model with non-linear activations. Changing the network depth, we find that the model performs much better with a single hidden layer than with-out any, while three hidden layers are optimal. For the weight dimensions we find square 1058 dimen- sional weights to perform the best. The ideal num- ber of convolutions are three with uni, bi and tri- grams, but unigrams alone perform only slightly worse, while taking a larger n-gram window n &gt; 3 does not help. Average and sum pooling perform comparatively well, while max pooling achieves the best result. Note that sum pooling recovers a non-linear bag-of-words model. With respect to activation functions, both ReLU and sigmoid work well.</p><p>Model extensions Multi task learning with task identifiers, ANN Text+Domain, does improve the ANN Text model. This suggests that the tendency by certain sites to review specific movies is in it- self indicative of the revenue. However this im- provement is more difficult to discern with the ANN Text+Meta+Domain model, possibly due to redundancy with the meta data. An alternative ap- proach for multi-task learning is to have a sepa- rate convolutional weight matrix for each review site, which can learn site specific characteristics of the text. This can also be achieved with site specific word embedding dimensions. However neither of these methods resulted in performance improvements. In addition, we experimented with applying a hierarchical convolution over reviews in two steps with k-max pooling <ref type="bibr" target="#b4">(Kalchbrenner et al., 2014</ref>), as well as parsing sentences recursively <ref type="bibr" target="#b12">(Socher et al., 2013</ref>), but did not observe any im- provements.</p><p>For optimisation, both Ada Grad and Steepest Gradient Descent had occasional problems with local minima, which Ada Delta was able to es- cape more often. In contrast to earlier work <ref type="bibr" target="#b5">(Kim, 2014)</ref>, applying dropout on the final layer did not improve the validation error. The optimiser mostly found good parameters after around 40 epochs which took around 30 minutes on a NVidia Kepler Tesla K40m GPU.</p><p>Model interpretation Next we perform anal- ysis to determine which words and phrases in- fluenced the output the most in the ANN Text model. To do so, we set each phrase input to zeros in turn and measure the prediction differ- ence for each movie across the test set. We re- port the min/max/average/count values in <ref type="table">Table  4</ref>. We isolate the effect of each n-gram by mak- ing sure the uni, bi and trigrams are independent, i.e. we process "Hong Kong" without zeroing "Hong" or "Kong". About 95% of phrases re- sult in no output change, including common sen- timent words, which shows that text regression is a different problem to sentiment analysis. We see that words related to series "# 2", effects, awards "praise", positive sentiment "intense", locations, references "Batman", body parts "chest", and oth- ers such as "plot twist", "evil", and "cameo" re- sult in increased revenue by up to $5 million.</p><p>On the other hand, words related to independent films "vérité", documentaries "the period", for- eign film "English subtitles" and negative senti- ment decrease revenue. Note that the model has identified structured data in the unstructured text, such as related to revenue of prequels "39 mil- lion", crew members, duration "15 minutes in", genre "[sci] fi", ratings, sexuality, profanity, re- lease periods "late 2008 release", availability "In selected theaters" and themes. Phrases can be composed, such as "action unfolds" amplifies "ac- tion", and "cautioned" is amplified by "strongly cautioned". "functional" is neutral, but "func- tional at best" is strongly negative. Some words exhibit both positive and negative impacts depend- ing on the context. This highlights the limitation of a linear model which is unable to discover these non-linear relationships. "13 -year <ref type="bibr">[old]</ref>" is posi- tive in New in Town, a romantic comedy and nega- tive in Zombieland, a horror. The character strings "k /" (mannerism of reviewer), "they're" (unique apostrophe), "&amp;#39" (encoding error) are high im- pact and unique to specific review sites, showing that the model indirectly uncovers domain infor- mation. This can explain the limited gain that can be achieved via multi task learning. Last, we have <ref type="table" target="#tab_1">Top 5 positive phrases  min  max  avg  #   sequel  20  4400  2300  28  flick  0  3700  1600  22  k /  1500  3600  2200  3  product  10  3400  1800  27  predecessor  22  3400  1400</ref>   <ref type="table">Table 4</ref>: Selected phrase impacts on the predic- tions in $ USD(K) in the test set, showing min, max and avg change in prediction value and num- ber of occurrences (denoted #). Periods denote ab- breviations (language, accompanying).</p><p>plotted the last hidden layer of each test set movie with t-SNE (Van der Maaten and Hinton, 2008). This gives a high level representation of a movie. In <ref type="figure" target="#fig_1">Figure 2</ref> it is visible that the test set movies can be discriminated into high and low revenue groups and this also correlates closely with their produc- tion budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we have shown that convolutional neural networks with deep architectures greatly outperform linear models even with very little su- pervision, and they can identify key textual and numerical characteristics of data with respect to predicting a real world phenomenon. In addition, we have demonstrated a way to intuitively inter- pret the model. In the future, we will investi- gate ways for automatically optimising the hyper- parameters of the network <ref type="bibr" target="#b11">(Snoek et al., 2012)</ref> and various extensions to recursive or hierarchical con- volutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Outline of the network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Projection of the last hidden layer of test movies using t-SNE. Red means high and blue means low revenue. The cross vs dot symbols indicate a production budget above or below $15M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experiment results on test set. Linear 
models by (Joshi et al., 2010). 

pretrained on Google News. Those terms are ini-
tialised with random small weights. The model 
has around 4 million weights plus 27 million tun-
able word embedding parameters. 

Structured data Besides text, injecting meta 
data and domain information into the model likely 
provides additional predictive power. Combin-
ing text with structured data early in the network 
fosters joint non-linear interaction in subsequent 
hidden layers. Hence, if meta data b is present, 
we concatenate that with the max pooling layer 
a 1 = (p, b) in part III. Domain specific infor-
mation t is appended to each n-gram convolution 
input (S 

(q) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>). Our dataset splits are identical, and we have accurately reproduced the results of their lin- ear model. Non-linearities are clearly helpful as evidenced by the ANN Text model beating the bag of words Linear Text model with a mean absolute test error of 6.0 vs 8.0. Moreover, simply using structured data in the ANN Meta beats all the Lin- ear models by a sizeable margin. Further improve- ments are realised through the inclusion of text, giving the lowest error of 3.4. Note that Joshi et al. (2010) preprocessed the text by stemming, down-</figDesc><table>Model Description 

MAE($M) 

fixed word2vec embeddings 
3.4* 
tuned word2vec embeddings 
3.6 
fixed random embeddings 
3.6 
tuned random embeddings 
3.8 

uni-grams 
3.6 
uni+bi-grams 
3.5 
uni+bi+tri-grams 
3.4* 
uni+bi+tri+four-grams 
3.6 

0 hidden layer 
6.3 
1 hidden layer 
3.9 
2 hidden layers 
3.5 
3 hidden layers 
3.4* 
4 hidden layers 
3.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Various alternative configurations, based 
on the ANN Text+Meta model. The asterisk ( * ) 
denotes the settings in the ANN Text+Meta model. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>13</head><label>13</label><figDesc></figDesc><table>Top 5 negative phrases 
min 
max 
avg 
# 

Mildly raunchy lang. 
-3100 -3100 -3100 
1 
( Under 17 
-2500 
1 
-570 
75 
Lars von 
-2400 
-900 -1500 
3 
talk the language 
-2200 -2200 -2200 
1 
. their English 
-2200 -2200 -2200 
1 

Selected phrases 
min 
max 
avg 
# 

CGI 
145 
3000 
1700 
28 
action 
-7 
1500 
700 105 
summer 
3 
1200 
560 
42 
they're 
3 
1300 
530 
68 
1950s 
10 
1600 
500 
17 
hit 
8 
950 
440 
72 
fi 
-15 
340 
160 
26 
Cage 
7 
95 
45 
28 
Hong Kong 
-440 
40 
-85 
11 
requires acc. parent 
-780 
1 
-180 
77 
English 
-850 
6 
-180 
41 
Sundance Film Festival 
-790 
3 
-180 
10 
written and directed 
-750 
-3 
-220 
19 
independent 
-990 
-2 
-320 
12 
some strong language 
-1600 
6 
-520 
13 

</table></figure>

			<note place="foot" n="1"> Some preliminary work has shown strong results for nonlinear text regression using Gaussian Process models (Lampos et al., 2014), however this approach has not been shown to scale to high dimensional inputs.</note>

			<note place="foot" n="2"> Alternatively, site information can be encoded with onehot categorical variables.</note>

			<note place="foot" n="3"> Although we do make use of pretrained word embeddings in our text features.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predicting peerto-peer loan rates using Bayesian non-linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Bitvai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI conference on Artificial Intelligence</title>
		<meeting>the 29th AAAI conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2203" to="2210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Movie reviews and revenues: An experiment in text regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="293" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting risk from financial reports with regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Kogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitry</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><forename type="middle">S</forename><surname>Bryan R Routledge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Sagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="272" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tracking the flu pandemic by monitoring the social web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Lampos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive Information Processing</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="411" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A user-centric model of voting intention from social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Lampos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preotiuc-Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="993" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting and characterising user impact on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Lampos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Preot¸iucpreot¸iuc-Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="405" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Practical Bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
