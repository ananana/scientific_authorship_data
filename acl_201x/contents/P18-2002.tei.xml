<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Salle</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Federal University of Rio Grande do Sul</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
							<email>avillavicencio@inf.ufrgs.br</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Federal University of Rio Grande do Sul</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Electronic Engineering</orgName>
								<orgName type="institution">University of Essex</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="8" to="13"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>8</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Increasing the capacity of recurrent neu-ral networks (RNN) usually involves augmenting the size of the hidden layer, with significant increase of computational cost. Recurrent neural tensor networks (RNTN) increase capacity using distinct hidden layer weights for each word, but with greater costs in memory usage. In this paper , we introduce restricted recurrent neu-ral tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve language model performance over RNNs using only a small fraction of the parameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated Recurrent Units and Long Short-Term Memory.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks (RNN), which com- pute their next output conditioned on a previously stored hidden state, are a natural solution to se- quence modeling. <ref type="bibr" target="#b13">Mikolov et al. (2010)</ref> applied RNNs to word-level language modeling (we refer to this model as s-RNN), outperforming traditional n-gram methods. However, increasing capacity (number of tunable parameters) by augmenting the size H of the hidden (or recurrent) layer -to model more complex distributions -results in a significant increase in computational cost, which is O(H 2 ).</p><p>Sutskever et al. (2011) increased the perfor- mance of a character-level language model with a multiplicative RNN (m-RNN), the factored ap- proximation of a recurrent neural tensor network (RNTN), which maps each symbol to separate hid- den layer weights (referred to as recurrence matri- ces from hereon). Besides increasing model ca- pacity while keeping computation constant, this approach has another motivation: viewing the RNN's hidden state as being transformed by each new symbol in the sequence, it is intuitive that dif- ferent symbols will transform the network's hid- den state in different ways <ref type="bibr" target="#b17">(Sutskever et al., 2011</ref>). Various studies on compositionality similarly ar- gue that some words are better modeled by matri- ces than by vectors ( <ref type="bibr" target="#b0">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b15">Socher et al., 2012)</ref>. Unfortunately, having sepa- rate recurrence matrices for each symbol requires memory that is linear in the symbol vocabulary size (|V |). This is not an issue for character-level models, which have small vocabularies, but is pro- hibitive for word-level models which can have vo- cabulary size in the millions if we consider surface forms.</p><p>In this paper, we propose the Restricted RNTN (r-RNTN) which uses only K &lt; |V | recurrence matrices. Given that |V | words must be assigned K matrices, we map the most frequent K − 1 words to the first K − 1 matrices, and share the K-th matrix among the remaining words. This mapping is driven by the statistical intuition that frequent words are more likely to appear in di- verse contexts and so require richer modeling, and by the greater presence of predicates and function words among the most frequent words in standard corpora like COCA <ref type="bibr" target="#b5">(Davies, 2009)</ref>. As a result, adding K matrices to the s-RNN both increases model capacity and satisfies the idea that some words are better represented by matrices. Re- sults show that r-RNTNs improve language model performance over s-RNNs even for small K with no computational overhead, and even for small K approximate the performance of RNTNs us- ing a fraction of the parameters. We also exper-iment with r-RNTNs using Gated Recurrent Units (GRU) (  and Long Short-Term Memory (LSTM) <ref type="bibr" target="#b6">(Hochreiter and Schmidhuber, 1997)</ref>, obtaining lower perplexity for fixed hidden layer sizes. This paper discusses related work ( §2), and presents r-RNTNs ( §3) along with the evalua- tion method ( §4). We conclude with results ( §5), and suggestions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We focus on related work that addresses lan- guage modeling via RNNs, word representation, and conditional computation.</p><p>Given a sequence of words (x 1 , ..., x T ), a lan- guage model gives the probability P (x t |x 1...t−1 ) for t ∈ <ref type="bibr">[1, T ]</ref>. Using a RNN, <ref type="bibr" target="#b13">Mikolov et al. (2010)</ref> created the s-RNN language model given by:</p><formula xml:id="formula_0">h t = σ(W h x t + U h h t−1 + b h ) (1) P (x t |x 1...t−1 ) = x T t Sof tmax(W o h t + b o ) (2)</formula><p>where h t is the hidden state represented by a vec- </p><formula xml:id="formula_1">tor of dimension H, σ(z) is the pointwise logistic function, W h is the H × V embedding matrix, U h is the H × H recurrence matrix,</formula><formula xml:id="formula_2">h t = σ(W h x t + U i(xt) h h t−1 + b h )<label>(3)</label></formula><p>where i(z) maps a hot-one encoded vector to its integer representation. Thus the U h tensor is composed of |V | recurrence matrices, and at each step of sequence processing the matrix cor- responding to the current input is used to trans- form the hidden state. The authors also proposed m-RNN, a factorization of the U i(xt) h matrix into U lh diag(v xt )U rh to reduce the number of param- eters, where v xt is a factor vector of the current input x t , but like the RNTN, memory still grows linearly with |V |. The RNTN has the property that input symbols have both a vector representation given by the embedding and a matrix representa- tion given by the recurrence matrix, unlike the s- RNN where symbols are limited to vector repre- sentations.</p><p>The integration of both vector and matrix rep- resentations has been discussed but with a focus on representation learning and not sequence mod- eling ( <ref type="bibr" target="#b0">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b15">Socher et al., 2012</ref>). For instance, <ref type="bibr" target="#b0">Baroni and Zamparelli (2010)</ref> argue for nouns to be represented as vectors and adjectives as matrices. <ref type="bibr" target="#b7">Irsoy and Cardie (2014)</ref> used m-RNNs for the task of sentiment classification and obtained equal or better performance than s-RNNs. Methods that use conditional computation <ref type="bibr" target="#b1">Bengio et al., 2015;</ref><ref type="bibr" target="#b14">Shazeer et al., 2017</ref>) are similar to RNTNs and r-RNTNs, but rather than use a static mapping, these methods train gating functions which do the mapping. Although these methods can potentially learn better policies than our method, they are significantly more complex, requiring the use of reinforcement learning <ref type="bibr" target="#b1">Bengio et al., 2015</ref>) or addi- tional loss functions <ref type="bibr" target="#b14">(Shazeer et al., 2017)</ref>, and more linguistically opaque (one must learn to in- terpret the mapping performed by the gating func- tions).</p><p>Whereas our work is concerned with updating the network's hidden state, <ref type="bibr" target="#b2">Chen et al. (2015)</ref> introduce a technique that better approximates the output layer's Softmax function by allocating more parameters to frequent words.</p><formula xml:id="formula_3">h t−1 + b f (i(xt)) h ) (4)</formula><p>where U h is a tensor of K &lt; |V | matrices of size H × H, b h is a H × K bias matrix with columns indexed by f . The function f (w) maps each vo- cabulary word to an integer between 1 and K.</p><p>We use the following definition for f :</p><formula xml:id="formula_4">f (w) = min(rank(w), K)<label>(5)</label></formula><p>where rank(w) is the rank of word w when the vocabulary is sorted by decreasing order of uni- gram frequency. This is an intuitive choice because words which appear more often in the corpus tend to have more Test PPL variable contexts, so it makes sense to dedicate a large part of model capacity to them. A second ar- gument is that frequent words tend to be predicates and function words. We can imagine that predi- cates and function words transform the meaning of the current hidden state of the RNN through matrix multiplication, whereas nouns, for exam- ple, add meaning through vector addition, follow- ing <ref type="bibr" target="#b0">Baroni and Zamparelli (2010)</ref>. We also perform initial experiments with r- RNTNs using LSTM and GRUs. A GRU is de- scribed by</p><note type="other">r-RNTN f r-RNTN f mod RNTN s-RNN H = 100 s-RNN H = 150 10 1 10 2 10 3 10</note><formula xml:id="formula_5">r t = σ(W r h x t + U r h h t−1 + b r h ) (6) z t = σ(W z h x t + U z h h t−1 + b z h ) (7) ˜ h t = tanh(W h h x t + U h h (r t h t−1 ) + b h h ) (8) h t = z t h t−1 + (1 − z t ) ˜ h t<label>(9)</label></formula><p>and an LSTM by</p><formula xml:id="formula_6">f t = σ(W f h x t + U f h h t−1 + b f h )<label>(10)</label></formula><formula xml:id="formula_7">i t = σ(W i h x t + U i h h t−1 + b i h ) (11) o t = σ(W o h x t + U o h h t−1 + b o h )<label>(12)</label></formula><formula xml:id="formula_8">˜ c t = tanh(W c h x t + U c h h t−1 + b c h ) (13) c t = i t ˜ c t + f t c t−1 (14) h t = o t tanh(c t )<label>(15)</label></formula><p>We create r-RNTN GRUs (r-GRU) by making U h h and b h h input-specific (as done in eq. <ref type="formula">(4)</ref>). For r- RNTN LSTMs (r-LSTM), we do the same for U c h and b c h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Materials</head><p>We evaluate s-RNNs, RNTNs, and r-RNTNs by training and measuring model perplexity (PPL) on the Penn Treebank (PTB) corpus <ref type="bibr" target="#b9">(Marcus et al., 1994</ref>) using the same preprocessing as <ref type="bibr" target="#b12">Mikolov et al. (2011)</ref>. Vocabulary size is 10000.</p><p>For an r-RNTN with H = 100, we vary the ten- sor size K from 1, which corresponds to the s- RNN, all the way up to 10000, which corresponds to the unrestricted RNTN. As a simple way to eval- uate our choice of rank-based mapping function f , we compare it to a pseudo-random variant:</p><formula xml:id="formula_9">f mod (w) = rank(w) mod K<label>(16)</label></formula><p>We also compare results to 1) an s-RNN with H = 150, which has the same number of parame- ters as an r-RNTN with H = 100 and K = 100. 2) An m-RNN with H = 100 with the size of factor vectors set to 100 to match this same num- ber of parameters. 3) An additional r-RNTN with H = 150 is trained to show that performance scales with H as well.</p><p>We split each sentence into 20 word sub- sequences and run stochastic gradient descent via backpropagation through time for 20 steps with- out mini-batching, only reseting the RNN's hid- den state between sentences. Initial learning rate (LR) is 0.1 and halved when the ratio of the valida- tion perplexity between successive epochs is less than 1.003, stopping training when validation im- provement drops below this ratio for 5 consecu- tive epochs. We use Dropout ( <ref type="bibr" target="#b16">Srivastava et al., 2014</ref>) with p = .5 on the softmax input to reduce overfitting. Weights are drawn from N (0, .001); gradients are not clipped. To validate our pro- posed method, we also evaluate r-RNTNs using the much larger text8 1 corpus with a 90MB-5MB- 5MB train-validation-test split, mapping words which appear less than 10 times to unk for a to- tal vocabulary size of 37751.</p><p>Finally, we train GRUs, LSTMs, and their r- RNTN variants using the PTB corpus and parame- ters similar to those used by <ref type="bibr">Zaremba et al. (2014)</ref>. All networks use embeddings of size 650 and a single hidden layer. Targeting K = 100, we set H = 244 for the r-GRU and compare with a GRU with H = 650 which has the same num- ber of parameters. The r-LSTM has H = 254 to match the number of parameters of an LSTM  <ref type="table">Table 1</ref>: Comparison of validation and test set perplexity for r-RNTNs with f mapping (K = 100 for PTB, K = 376 for text8) versus s-RNNs and m-RNN. r-RNTNs with the same H as corresponding s-RNNs significantly increase model capacity and performance with no computational cost. The RNTN was not run on text8 due to the number of parameters required.</p><note type="other">s-RNN 150 3M 133.7 11.4M 207.9 LSTM 650 16.4M 84.6 r-RNTN f 150 5.3M 126.4 19.8M 171.7 r-LSTM f 254 16.4M 87.1</note><p>with H = 650. The training procedure is the same as above but with mini-batches of size 20, 35 timestep sequences without state resets, initial LR of 1, Dropout on all non-recurrent connections, weights drawn from U(−.05, .05), and gradients norm-clipped to 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Results are shown in <ref type="figure" target="#fig_1">fig. 1</ref> and table 1.</p><p>Comparing the r-RNTN to the baseline s-RNN with H = 100 ( <ref type="figure" target="#fig_1">fig. 1)</ref>, as model capacity grows with K, test set perplexity drops, showing that r- RNTN is an effective way to increase model ca- pacity with no additional computational cost. As expected, the f mapping outperforms the baseline f mod mapping at smaller K. As K increases, we see a convergence of both mappings. This may be due to matrix sharing at large K between frequent and infrequent words because of the modulus op- eration in eq. (16). As infrequent words are rarely observed, frequent words dominate the matrix up- dates and approximate having distinct matrices, as they would have with the f mapping.</p><p>It is remarkable that even with K as small as 100, the r-RNTN approaches the performance of the RNTN with a small fraction of the parameters. This reinforces our hypothesis that complex trans- formation modeling afforded by distinct matrices is needed for frequent words, but not so much for infrequent words which can be well represented by a shared matrix and a distinct vector embed- ding. As shown in table 1, with an equal number of parameters, the r-RNTN with f mapping out- performs the s-RNN with a bigger hidden layer. It appears that heuristically allocating increased model capacity as done by the f based r-RNTN is a better way to increase performance than sim- ply increasing hidden layer size, which also incurs a computational penalty.</p><p>Although m-RNNs have been successfully em- ployed in character-level language models with small vocabularies, they are seldom used in word- level models. The poor results shown in table 1 could explain why. <ref type="bibr">2</ref> For fixed hidden layer sizes, r-RNTNs yield significant improvements to s-RNNs, GRUs, and LSTMs, confirming the advantages of distinct rep- resentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we proposed restricted recurrent neural tensor networks, a model that restricts the size of recurrent neural tensor networks by map- ping frequent words to distinct matrices and infre- quent words to shared matrices. r-RNTNs were motivated by the need to increase RNN model capacity without increasing computational costs, while also satisfying the ideas that some words are better modeled by matrices rather than vec- tors ( <ref type="bibr" target="#b0">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b15">Socher et al., 2012</ref>). We achieved both goals by pruning the size of the recurrent neural tensor network described by <ref type="bibr" target="#b17">Sutskever et al. (2011)</ref> via sensible word-to- matrix mapping. Results validated our hypothesis that frequent words benefit from richer, dedicated modeling as reflected in large perplexity improve- ments for low values of K.</p><p>Interestingly, results for s-RNNs and r-GRUs suggest that given the same number of parame- ters, it is possible to obtain higher performance by increasing K and reducing H. This is not the case with r-LSTMs, perhaps to due to our choice of which of the recurrence matrices to make input- specific. We will further investigate both of these phenomena in future work, experimenting with different combinations of word-specific matrices for r-GRUs and r-LSTMs (rather than only U h h and U c h ), and combining our method with recent im- provements to gated networks in language model- ing ( <ref type="bibr" target="#b8">Jozefowicz et al., 2016;</ref><ref type="bibr" target="#b11">Merity et al., 2018;</ref><ref type="bibr" target="#b10">Melis et al., 2018</ref>) which we believe are orthogo- nal and hopefully complementary to our own.</p><p>Finally, we plan to compare frequency-based and additional, linguistically motivated f map- pings (for example different inflections of a verb sharing a single matrix) with mappings learned via conditional computing to measure how exter- nal linguistic knowledge contrasts with knowledge automatically inferred from training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PTB test PPL as K varies from 1 to 10000 (100 for gated networks). At K = 100, the r-RNTN with f mapping already closely approximates the much bigger RNTN, with little gain for bigger K, showing that dedicated matrices should be reserved for frequent words as hypothesized.</figDesc></figure>

			<note place="foot" n="3"> Restricted Recurrent Neural Tensor Networks To balance expressiveness and computational cost, we propose restricting the size of the recurrence tensor in the RNTN such that memory does not grow linearly with vocabulary size, while still keeping dedicated matrix representations for a subset of words in the vocabulary. We call these Restricted Recurrent Neural Tensor Networks (rRNTN), which modify eq. (3) as follows: h t = σ(W h x t + U f (i(xt)) h</note>

			<note place="foot" n="1"> http://mattmahoney.net/dc/textdata.html</note>

			<note place="foot" n="2"> It should be noted that Sutskever et al. (2011) suggest m-RNNs would be better optimized using second-order gradient descent methods, whereas we employed only first-order gradients in all models we trained to make a fair comparison.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partly supported by <ref type="bibr">CAPES and CNPq (projects 312114/2015</ref><ref type="bibr">-0, 423843/2016</ref><ref type="bibr">-8, and 140402/2018</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06297</idno>
		<title level="m">Conditional computation in neural networks for faster models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Welin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04906</idno>
		<title level="m">Strategies for training large vocabulary neural language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Exponentially increasing the capacity-to-computation ratio for conditional computation in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.7362</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Gülehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The 385+ million word corpus of contemporary american english (1990-2008+): Design, architecture, and linguistic insights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of corpus linguistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="190" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="80" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6577</idno>
		<title level="m">Modeling compositionality with multiplicative recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">A</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gbor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Empirical evaluation and combination of advanced language modeling techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernock´ycernock´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="605" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
