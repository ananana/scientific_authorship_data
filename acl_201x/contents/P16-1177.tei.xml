<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Text Pair Similarity with Context-sensitive Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Amiri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Colorado</orgName>
								<address>
									<settlement>Boulder</settlement>
									<region>CO</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><forename type="middle">Daumé</forename><surname>Iii</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Advanced Computer Studies</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Text Pair Similarity with Context-sensitive Autoencoders</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1882" to="1892"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a pairwise context-sensitive Autoencoder for computing text pair similarity. Our model encodes input text into context-sensitive representations and uses them to compute similarity between text pairs. Our model outperforms the state-of-the-art models in two semantic retrieval tasks and a contextual word similarity task. For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outper-forms them.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representation learning algorithms learn repre- sentations that reveal intrinsic low-dimensional structure in data ( <ref type="bibr" target="#b1">Bengio et al., 2013)</ref>. Such rep- resentations can be used to induce similarity be- tween textual contents by computing similarity be- tween their respective vectors ( <ref type="bibr" target="#b9">Huang et al., 2012;</ref><ref type="bibr" target="#b28">Silberer and Lapata, 2014)</ref>.</p><p>Recent research has made substantial progress on semantic similarity using neural networks ( <ref type="bibr" target="#b25">Rothe and Schütze, 2015;</ref><ref type="bibr" target="#b6">Dos Santos et al., 2015;</ref><ref type="bibr" target="#b27">Severyn and Moschitti, 2015)</ref>. In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders.</p><p>In representation learning, context may appear in various forms. For example, the context of a current sentence in a document could be ei- ther its neighboring sentences ( <ref type="bibr" target="#b15">Lin et al., 2015</ref>; <ref type="bibr" target="#b33">Wang and Cho, 2015)</ref>, topics associated with the sentence <ref type="bibr" target="#b18">(Mikolov and Zweig, 2012;</ref><ref type="bibr" target="#b13">Le and Mikolov, 2014)</ref>, the document that contains the sentence ( <ref type="bibr" target="#b9">Huang et al., 2012)</ref>, as well as their com- binations ( <ref type="bibr" target="#b11">Ji et al., 2016)</ref>. It is important to inte- grate context into neural networks because these models are often trained with only local informa- tion about their individual inputs. For example, recurrent and recursive neural networks only use local information about previously seen words in a sentence to predict the next word or composition. <ref type="bibr">1</ref> On the other hand, context information (such as topical information) often capture global informa- tion that can guide neural networks to generate more accurate representations.</p><p>We investigate the utility of context informa- tion in three semantic similarity tasks: contextual word sense similarity in which we aim to predict semantic similarity between given word pairs in their sentential context <ref type="bibr" target="#b9">(Huang et al., 2012;</ref><ref type="bibr" target="#b25">Rothe and Schütze, 2015)</ref>, question ranking in which we aim to retrieve semantically equivalent questions with respect to a given test question <ref type="bibr" target="#b6">(Dos Santos et al., 2015</ref>), and answer ranking in which we aim to rank single-sentence answers with respect to a given question <ref type="bibr" target="#b27">(Severyn and Moschitti, 2015)</ref>.</p><p>The contributions of this paper are as follows: (1) integrating context information into deep au- toencoders and <ref type="bibr">(2)</ref> showing that such integra- tion improves the representation performance of deep autoencoders across several different seman- tic similarity tasks.</p><p>Our model outperforms the state-of-the-art su-pervised baselines in three semantic similarity tasks. Furthermore, the unsupervised version of our autoencoder show comparable performance with the supervised baseline models and in some cases outperforms them.</p><p>2 Context-sensitive Autoencoders</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic Autoencoders</head><p>We first provide a brief description of basic au- toencoders and extend them to context-sensitive ones in the next Section. Autoencoders are trained using a local unsupervised criterion <ref type="bibr" target="#b32">(Vincent et al., 2010;</ref><ref type="bibr" target="#b8">Hinton and Salakhutdinov, 2006;</ref><ref type="bibr" target="#b31">Vincent et al., 2008)</ref>. Specifically, the basic autoencoder in <ref type="figure" target="#fig_3">Figure 1</ref>(a) locally optimizes the hidden represen- tation h of its input x such that h can be used to accurately reconstruct x,</p><formula xml:id="formula_0">h = g(Wx + b h )<label>(1)</label></formula><formula xml:id="formula_1">ˆ x = g(W h + b ˆ x ),<label>(2)</label></formula><p>wherê x is the reconstruction of x, the learning pa- rameters W ∈ R d ×d and W ∈ R d×d are weight matrices, b h ∈ R d and b ˆ x ∈ R d are bias vectors for the hidden and output layers respectively, and g is a nonlinear function such as tanh(.). 2 Equa- tion (1) encodes the input into an intermediate rep- resentation and Equation (2) decodes the resulting representation.</p><p>Training a single-layer autoencoder corre- sponds to optimizing the learning parameters to minimize the overall loss between inputs and their reconstructions. For real-valued x, squared loss is often used, l(x) = ||x − ˆ x|| 2 , ( <ref type="bibr" target="#b32">Vincent et al., 2010)</ref>:</p><formula xml:id="formula_2">min Θ n i=1 l(x (i) ) Θ = {W, W , b h , b ˆ x }.<label>(3)</label></formula><p>This can be achieved using mini-batch stochastic gradient descent (Zeiler, 2012).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Integrating Context into Autoencoders</head><p>We extend the above basic autoencoder to inte- grate context information about inputs. We as- sume that-for each training example x ∈ R d - we have a context vector c x ∈ R k that contains contextual information about the input. <ref type="bibr">3</ref> The na- 2 If the squared loss is used for optimization, as in Equa- tion (3), nonlinearity is often not used in Equation <ref type="formula" target="#formula_1">(2)</ref>   <ref type="figure" target="#fig_3">Figure 1</ref>: Schematic representation of basic and context-sensitive autoencoders: (a) Basic autoen- coder maps its input x into the representation h such that it can reconstruct x with minimum loss, and (b) Context-sensitive autoencoder maps its in- puts x and h c into a context-sensitive representa- tion h (h c is the representation of the context in- formation associated to x).</p><p>ture of this context vector depends on the input and target task. For example, neighboring words can be considered as the context of a target word in contextual word similarity task.</p><p>We first learn the hidden representation h c ∈ R d for the given context vector c x . For this, we use the same process as discussed above for the basic autoencoder where we use c x as the input in Equations <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref> to obtain h c . We then use h c to develop our context-sensitive autoencoder as depicted in <ref type="figure" target="#fig_3">Figure 1(b)</ref>. This autoencoder maps its inputs x and h c into a context-sensitive represen- tation h as follows:</p><formula xml:id="formula_3">h = g(Wx + Vh c + b h )<label>(4)</label></formula><formula xml:id="formula_4">ˆ x = g(W h + b ˆ x ) (5) ˆ h c = g(V h + b ˆ hc ).<label>(6)</label></formula><p>Our intuition is that if h leads to a good recon- struction of its inputs, it has retained information available in the input. Therefore, it is a context- sensitive representation.</p><p>The loss function must then compute the loss between the input pair (x, h c ) and its reconstruc- tion (ˆ x, ˆ h c ). For optimization, we can still use squared loss with a different set of parameters to minimize the overall loss on the training examples:   where λ ∈ [0, 1] is a weight parameter that con- trols the effect of context information in the re- construction process.</p><formula xml:id="formula_5">l(x, h c ) = ||x − ˆ x|| 2 + λ||h c − ˆ h c || 2 min Θ n i=1 l(x (i) , h (i) c ) Θ = {W, W , V, V , b h , b ˆ x , b ˆ hc },<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Denoising</head><p>Denoising autoencoders (DAEs) reconstruct an in- put from a corrupted version of it for more effec- tive learning <ref type="bibr" target="#b32">(Vincent et al., 2010</ref>). The corrupted input is then mapped to a hidden representation from which we obtain the reconstruction. How- ever, the reconstruction loss is still computed with respect to the uncorrupted version of the input as before. Denoising autoencoders effectively learn representations by reversing the effect of the cor- ruption process. We use masking noise to corrupt the inputs where a fraction η of input units are randomly selected and set to zero <ref type="bibr" target="#b31">(Vincent et al., 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Deep Context-Sensitive Autoencoders</head><p>Autoencoders can be stacked to create deep net- works. A deep autoencoder is composed of mul- tiple hidden layers that are stacked together. The initial weights in such networks need to be prop- erly initialized through a greedy layer-wise train- ing approach. Random initialization does not work because deep autoencoders converge to poor local minima with large initial weights and result in tiny gradients in the early layers with small ini- tial weights <ref type="bibr" target="#b8">(Hinton and Salakhutdinov, 2006</ref>). Our deep context-sensitive autoencoder is com- posed of a stacked set of DAEs. As discussed above, we first need to properly initialize the learn- ing parameters (weights and biases) associated to each DAE. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(a), we first train DAE-0, which initializes parameters associated to the context layer. The training procedure is exactly the same as training a basic autoencoder (Sec- tion 2.1 and <ref type="figure" target="#fig_3">Figure 1</ref>(a)). <ref type="bibr">4</ref> We then treat h c and x as "inputs" for DAE-1 and use the same approach as in training a context-sensitive autoencoder to initialize the parameters of DAE-1 (Section 2.2 and <ref type="figure" target="#fig_3">Figure 1</ref>(b)). Similarly, the i th DAE is built on the output of the (i − 1) th DAE and so on until the desired number of layers (e.g. n layers) are ini- tialized. For denoising, the corruption is only ap- plied on "inputs" of individual autoencoders. For example, when we are training DAE-i, h i−1 and h c are first obtained from the original inputs of the network (x and c x ) through a single forward pass and then their corrupted versions are computed to train DAE-i. <ref type="figure" target="#fig_1">Figure 2</ref>(b) shows that the n properly initial- ized DAEs can be stacked to form a deep context- sensitive autoencoder. We unroll this network to fully optimize its weights through gradient descent and backpropagation <ref type="bibr" target="#b32">(Vincent et al., 2010;</ref><ref type="bibr" target="#b8">Hinton and Salakhutdinov, 2006</ref>) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Unrolling and Fine-tuning</head><p>We optimize the learning parameters of our ini- tialized context-sensitive deep autoencoder by un- folding its n layers and making a 2n − 1 layer net-work whose lower layers form an "encoder" net- work and whose upper layers form a "decoder" network ( <ref type="figure" target="#fig_1">Figure 2</ref>(c)). A global fine-tuning stage backpropagates through the entire network to fine- tune the weights for optimal reconstruction. In this stage, we update the network parameters again by training the network to minimize the loss be- tween original inputs and their actual reconstruc- tion. We backpropagate the error derivatives first through the decoder network and then through the encoder network. Each decoder layer tries to re- cover the input of its corresponding encoder layer. As such, the weights are initially symmetric and the decoder weights do need to be learned.</p><p>After the training is complete, the hidden layer h n contains a context-sensitive representation of the inputs x and c x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Context Information</head><p>Context is task and data dependent. For example, a sentence or document that contains a target word forms the word's context.</p><p>When context information is not readily avail- able, we use topic models to determine such con- text for individual inputs ( <ref type="bibr" target="#b2">Blei et al., 2003;</ref><ref type="bibr" target="#b30">Stevens et al., 2012</ref>). In particular, we use Non-Negative Matrix Factorization (NMF) <ref type="bibr" target="#b16">(Lin, 2007)</ref>: Given a training set with n instances, i.e., X ∈ R v×n , where v is the size of a global vocabulary and the scalar k is the number of topics in the dataset, we learn the topic matrix D ∈ R v×k and context ma- trix C ∈ R k×n using the following sparse coding algorithm:</p><formula xml:id="formula_6">min D,C X − DC 2 F + µC 1 ,<label>(8)</label></formula><formula xml:id="formula_7">s.t. D ≥ 0, C ≥ 0,</formula><p>where each column in C is a sparse representa- tion of an input over all topics and will be used as global context information in our model. We obtain context vectors for test instances by trans- forming them according to the fitted NMF model on training data. We also note that advanced topic modeling approaches, such as syntactic topic models (Boyd-Graber and Blei, 2009), can be more effective here as they generate linguistically rich context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Text Pair Similarity</head><p>We present unsupervised and supervised ap- proaches for predicting semantic similarity scores Additional features SoftMax í µí±´1µí±´1 í µí±´2µí±´2 í µí±´0µí±´0 for input texts (e.g., a pair of words) each with its corresponding context information. These scores will then be used to rank "documents" against "queries" (in retrieval tasks) or evaluate how pre- dictions of a model correlate with human judg- ments (in contextual word sense similarity task).</p><p>In unsupervised settings, given a pair of in- put texts with their corresponding context vectors, (x 1 ,c x 1 ) and (x 2 ,c x 2 ), we determine their seman- tic similarity score by computing the cosine simi- larity between their hidden representations h 1 n and h 2 n respectively.</p><p>In supervised settings, we use a copy of our context-sensitive autoencoder to make a pairwise architecture as depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. Given (x 1 ,c x 1 ), (x 2 ,c x 2 ), and their binary relevance score, we use h 1 n and h 2 n as well as additional fea- tures (see below) to train our pairwise network (i.e. further fine-tune the weights) to predict a similar- ity score for the input pair as follows:</p><formula xml:id="formula_8">rel(x 1 , x 2 ) = sof tmax(M 0 a+M 1 h 1 n +M 2 h 2 n +b)<label>(9)</label></formula><p>where a carries additional features, Ms are weight matrices, and b is the bias. We use the difference and similarity between the context-sensitive rep- resentations of inputs, h 1 n and h 2 n , as additional features:</p><formula xml:id="formula_9">h sub = |h 1 n − h 2 n | h dot = h 1 n h 2 n ,<label>(10)</label></formula><p>where h sub and h dot capture the element-wise dif- ference and similarity (in terms of the sign of ele- ments in each dimension) between h 1 n and h 2 n , re- spectively. We expect elements in h sub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in h dot to be positive for relevant inputs and negative otherwise.</p><p>We can use any task-specific feature as addi- tional features. This includes features from the minimal edit sequences between parse trees of the input pairs <ref type="bibr" target="#b7">(Heilman and Smith, 2010;</ref><ref type="bibr" target="#b35">Yao et al., 2013)</ref>, lexical semantic features extracted from re- sources such as WordNet ( <ref type="bibr" target="#b36">Yih et al., 2013)</ref>, or other features such as word overlap features <ref type="bibr" target="#b27">(Severyn and Moschitti, 2015;</ref><ref type="bibr" target="#b26">Severyn and Moschitti, 2013)</ref>. We can also use additional features (Equa- tion 10), computed for BOW representations of the inputs x 1 and x 2 . Such additional features im- prove the performance of our and baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this Section, we use t-test for significant test- ing and asterisk mark (*) to indicate significance at α = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Context Information</head><p>We use three datasets: "SCWS" a word similar- ity dataset with ground-truth labels on similar- ity of pairs of target words in sentential context from <ref type="bibr" target="#b9">Huang et al. (2012)</ref>; "qAns" a TREC QA dataset with ground-truth labels for semantically relevant questions and (single-sentence) answers from <ref type="bibr" target="#b34">Wang et al. (2007)</ref>; and "qSim" a commu- nity QA dataset crawled from Stack Exchange with ground-truth labels for semantically equiva- lent questions from Dos <ref type="bibr" target="#b6">Santos et al. (2015)</ref>. Ta- ble 1 shows statistics of these datasets. To enable direct comparison with previous work, we use the same training, development, and test data provided by Dos <ref type="bibr" target="#b6">Santos et al. (2015)</ref> and <ref type="bibr" target="#b34">Wang et al. (2007)</ref> for qSim and qAns respectively and the entire data of SCWS (in unsupervised setting).</p><p>We consider local and global context for tar- get words in SCWS. The local context of a target word is its ten neighboring words (five before and five after) ( <ref type="bibr" target="#b9">Huang et al., 2012)</ref>, and its global con- text is a short paragraph that contains the target word (surrounding sentences). We compute aver- age word embeddings to create context vectors for target words.</p><p>Also, we consider question title and body and answer text as input in qSim and qAns and use NMF to create global context vectors for questions and answers (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter Setting</head><p>We use pre-trained word vectors from GloVe ( <ref type="bibr" target="#b24">Pennington et al., 2014</ref>). However, because qSim questions are about specific technical topics, we only use GloVe as initialization.   In this unsupervised setting, we set the weight pa- rameter λ = .5, masking noise η = 0, depth of our model n = 3. Tuning these parameters will further improve the performance of our model.</p><p>For qSim and qAns, we use 300-dimensional word embeddings, d = 300, with hidden layers of size d = 200. We set the size of context vec- tors k (number of topics) using the reconstruction error of NMF on training data for different values of k. This leads to k = 200 for qAns and k = 300 for qSim. We tune the other hyper-parameters (η, n, and λ) using development data.</p><p>We set each input x (target words in SCWS, question titles and bodies in qSim, and question titles and single-sentence answers in qAns) to the average of word embeddings in the input. Input vectors could be initialized through more accurate approaches ( <ref type="bibr" target="#b21">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b14">Li and Hovy, 2014)</ref>; however, averaging leads to reasonable rep- resentations and is often used to initialize neural networks <ref type="bibr" target="#b5">(Clinchant and Perronnin, 2013;</ref><ref type="bibr" target="#b10">Iyyer et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Contextual Word Similarity</head><p>We first consider the contextual word similarity task in which a model should predict the semantic similarity between words in their sentential con- text. For this evaluation, we compute Spearman's ρ correlation ( <ref type="bibr" target="#b12">Kokoska and Zwillinger, 2000</ref>) be- tween the "relevance scores" predicted by differ- ent models and human judgments (Section 3).</p><p>The state-of-the-art model for this task is a semi-supervised approach <ref type="bibr" target="#b25">(Rothe and Schütze, 2015)</ref>. This model use resources like WordNet to compute embeddings for different senses of words. Given a pair of target words and their context (neighboring words and sentences), this model represents each target word as the average of its sense embeddings weighted by cosine simi- larity to the context. The cosine similarity between the representations of words in a pair is then used to determine their semantic similarity. Also, the Skip-gram model ( <ref type="bibr" target="#b20">Mikolov et al., 2013a</ref>) is ex- tended in <ref type="bibr" target="#b22">(Neelakantan et al., 2014;</ref><ref type="bibr" target="#b4">Chen et al., 2014</ref>) to learn contextual word pair similarity in an unsupervised way. <ref type="table" target="#tab_5">Table 2</ref> shows the performance of different models on the SCWS dataset. SAE, CSAE-LC, CSAE-LGC show the performance of our pairwise autoencoders without context, with local context, and with local and global context, respectively. In case of CSAE-LGC, we concatenate local and global context to create context vectors. CSAE- LGC performs significantly better than the base- lines, including the semi-supervised approach in <ref type="bibr" target="#b25">Rothe and Schütze (2015)</ref>. It is also interesting that SAE (without any context information) out- performs the pre-trained word embeddings (Pre- trained embeds.).</p><p>Comparing the performance of CSAE-LC and CSAE-LGC indicates that global context is use- ful for accurate prediction of semantic similarity between word pairs. We further investigate these models to understand why global context is useful. <ref type="table">Table 3</ref> shows an example in which global con- text (words in neighboring sentences) effectively help to judge the semantic similarity between "Air- port" and "Airfield." This is while local context (ten neighboring words) are less effective in help- ing the models to relate the two words.</p><p>Furthermore, we study the effect of global con- text in different POS tag categories. As <ref type="figure">Figure 4</ref> shows global context has greater impact on A-A and N-N categories. We expect high improve- ment in the N-N category as noun senses are fairly self-contained and often refer to concrete things. Thus broader (not only local) context is needed to judge their semantic similarity. However, we don't know the reason for improvement on the A-A cat- egory as, in context, adjective interpretation is of- ten affected by local context (e.g., the nouns that adjectives modify). One reason for improvement could be because adjectives are often interchange- able and this characteristic makes their meaning to be less sensitive to local context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Context ρ×100 Huang et al. <ref type="formula" target="#formula_0">(2012)</ref> LGC 65.7 <ref type="bibr" target="#b4">Chen et al. (2014)</ref> LGC 65.4 <ref type="bibr" target="#b22">Neelakantan et al. (2014)</ref> LGC 69.3 <ref type="bibr" target="#b25">Rothe and Schütze (2015)</ref> LGC 69.8 Pre-trained embeds. <ref type="table" target="#tab_5">(GloVe)  - 60.2  SAE  - 61.1  CSAE  LC  66.4  CSAE</ref> LGC 70.9* Without any soldiers to defend the airfield it was aban- doned under heavy fire. In a little under an hour, 108 Ju-52s and 16 Ju-86s took off for Novocherkassk -leav- ing 72 Ju-52s and many other aircraft burning on the ground. A new base was established. . . <ref type="table">Table 3</ref>: The importance of global context (neigh- boring sentences) in predicting the semantically similar words (Airport, Airfield).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Answer Ranking Performance</head><p>We evaluate the performance of our model in the answer ranking task in which a model should re- trieve correct answers from a set of candidates for test questions. For this evaluation, we rank an- swers with respect to each test question accord- ing to the "relevance score" between question and each answer (Section 3). The state-of-the-art model for answer ranking on qAns is a pairwise convolutional neural net- work (PCNN) presented in <ref type="bibr" target="#b27">(Severyn and Moschitti, 2015)</ref>. PCNN is a supervised model that first maps input question-answer pairs to hidden representations through a standard convolutional neural network (CNN) and then utilizes these rep- resentations in a pairwise CNN to compute a rele- vance score for each pair. This model also utilizes external word overlap features for each question- answer pair. 5 PCNN outperforms other competing CNN models ( <ref type="bibr" target="#b37">Yu et al., 2014</ref>) and models that use  syntax and semantic features <ref type="bibr" target="#b7">(Heilman and Smith, 2010;</ref><ref type="bibr" target="#b35">Yao et al., 2013)</ref>. <ref type="table" target="#tab_7">Tables 4 and 5</ref> show the performance of dif- ferent models in terms of Mean Average Preci- sion (MAP) and Mean Reciprocal Rank (MRR) in supervised and unsupervised settings. PCNN- WO and PCNN show the baseline performance with and without word overlap features. SAE and CSAE show the performance of our pairwise au- toencoders without and with context information respectively. Their "X-DST" versions show their performance when additional features (Equation 10) are used. These features are computed for the hidden and BOW representations of question- answer pairs. We also include word overlap fea- tures as additional features. <ref type="table" target="#tab_7">Table 4</ref> shows that SAE and CSAE consistently outperform PCNN, and SAE-DST and CSAE- DST outperform PCNN-WO when the models are trained on the larger training dataset, "Train-All." But PCNN shows slightly better performance than our model on "Train," the smaller training dataset. We conjecture this is because PCNN's convolu- tion filter is wider (n-grams, n &gt; 2) <ref type="bibr" target="#b27">(Severyn and Moschitti, 2015)</ref>. <ref type="table" target="#tab_8">Table 5</ref> shows that the performance of unsuper- vised SAE and CSAE are comparable and in some cases better than the performance of the super- vised PCNN model. We attribute the high perfor- mance of our models to context information that leads to richer representations of inputs.</p><p>Furthermore, comparing the performance of CSAE and SAE in both supervised and unsuper- vised settings in <ref type="table" target="#tab_7">Tables 4 and 5</ref> shows that context information consistently improves the MAP and MRR performance at all settings except for MRR on "Train" (supervised setting) that leads to a com-   parable performance. Context-sensitive represen- tations significantly improve the performance of our model and often lead to higher MAP than the models that ignore context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Question Ranking Performance</head><p>In the question ranking task, given a test ques- tion, a model should retrieve top-K questions that are semantically equivalent to the test question for K = {1, 5, 10}. We use qSim for this evaluation. We compare our autoencoders against PCNN and PBOW-PCNN models presented in Dos <ref type="bibr" target="#b6">Santos et al. (2015)</ref>. PCNN is a pairwise convolu- tional neural network and PBOW-PCNN is a joint model that combines vector representations ob- tained from a pairwise bag-of-words (PBOW) net- work and a pairwise convolutional neural network (PCNN). Both models are supervised as they re- quire similarity scores to train the network. <ref type="table" target="#tab_11">Table 6</ref> shows the performance of differ- ent models in terms of Precision at Rank K, P@K. CSAE is more precise than the baseline; CSAE and CSAE-DST models consistently out- perform the baselines on P@1, an important met- ric in search applications (CSAE also outperforms PCNN on P@5). Although context-sensitive mod- els are more precise than the baselines at higher ranks, the PCNN and PBOW-PCNN models re- main the best model for P@10. <ref type="table" target="#tab_11">Tables 6 and 7</ref> show that context information consistently improves the results at all ranks in both supervised and unsupervised settings.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance Analysis and Discussion</head><p>We investigate the effect of context information in reconstructing inputs and try to understand rea- sons for improvement in reconstruction error. We compute the average reconstruction error of SAE and CSAE (Equations (3) and <ref type="formula" target="#formula_5">(7)</ref>). For these ex- periments, we set λ = 0 in Equation <ref type="formula" target="#formula_5">(7)</ref> so that we can directly compare the resulting loss of the two models. CSAE will still use context informa- tion with λ = 0 but it does not backpropagate the reconstruction loss of context information.</p><p>Figures 5(a) and 5(b) show the average recon- struction error of SAE and CSAE on qSim and qAns datasets. Context information conistently improves reconstruction. The improvement is greater on qSim which contains smaller number of words per question as compared to qAns. Also, both models generate smaller reconstruction errors than NMF (Section 2.3). The lower performance of NMF is because it reconstructs inputs merely using global topics identified in datasets, while our models utilize both local and global information to reconstruct inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis of Context information</head><p>The improvement in reconstruction error mainly stems from areas in data where "topic density" is lower. We define topic density for a topic as the number of documents that are assigned to the topic by our topic model. We compute the average im- provement in reconstruction error for each topic T j using the loss functions for the basic and context- sensitive autoencoders:</p><formula xml:id="formula_10">∆ j = 1 |T j | x∈T j l(x) − l(x, h x )</formula><p>where we set λ = 0. <ref type="figure" target="#fig_6">Figure 5</ref>(c) shows improve- ment of reconstruction error versus topic density on qSim. Lower topic densities have greater im- provement. This is because they have insufficient training data to train the networks. However, in- jecting context information improves the recon- struction power of our model by providing more information. The improvements in denser areas are smaller because neural networks can train ef- fectively in these areas. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of Depth</head><p>The intuition behind deep autoencoders (and, gen- erally, deep neural networks) is that each layer learns a more abstract representation of the in- put than the previous one <ref type="bibr" target="#b8">(Hinton and Salakhutdinov, 2006;</ref><ref type="bibr" target="#b1">Bengio et al., 2013</ref>  <ref type="figure">Figure 6</ref>: Effect of depth in contextual word simi- larity. Three hidden layers is optimal for this task.</p><p>if adding depth to our context-sensitive autoen- coder will improve its performance in the contex- tual word similarity task. <ref type="figure">Figure 6</ref> shows that as we increase the depth of our autoencoders, their performances initially im- prove. The CSAE-LGC model that uses both lo- cal and global context benefits more from greater number of hidden layers than CSAE-LC that only uses local context. We attribute this to the use of global context in CSAE-LGC that leads to more accurate representations of words in their context. We also note that with just a single hidden layer, CSAE-LGC largely improves the performance as compared to CSAE-LC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Representation learning models have been ef- fective in many tasks such as language model- ing ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b21">Mikolov et al., 2013b</ref>), topic modeling ( <ref type="bibr" target="#b23">Nguyen et al., 2015)</ref>, paraphrase detection <ref type="bibr" target="#b29">(Socher et al., 2011)</ref>, and ranking tasks ( <ref type="bibr" target="#b36">Yih et al., 2013)</ref>. We briefly review works that use context information for text representation. <ref type="bibr" target="#b9">Huang et al. (2012)</ref> presented an RNN model that uses document-level context information to construct more accurate word representations. In particular, given a sequence of words, the ap- proach uses other words in the document as exter- nal (global) knowledge to predict the next word in the sequence. Other approaches have also mod- eled context at the document level ( <ref type="bibr" target="#b15">Lin et al., 2015;</ref><ref type="bibr" target="#b33">Wang and Cho, 2015;</ref><ref type="bibr" target="#b11">Ji et al., 2016)</ref>. <ref type="bibr" target="#b11">Ji et al. (2016)</ref> presented a context-sensitive RNN-based language model that integrates repre- sentations of previous sentences into the language model of the current sentence. They showed that this approach outperforms several RNN language models on a text coherence task. <ref type="bibr" target="#b15">Liu et al. (2015)</ref> proposed a context-sensitive RNN model that uses Latent Dirichlet Alloca- tion ( <ref type="bibr" target="#b2">Blei et al., 2003</ref>) to extract topic-specific word embeddings. Their best-performing model regards each topic that is associated to a word in a sentence as a pseudo word, learns topic and word embeddings, and then concatenates the embed- dings to obtain topic-specific word embeddings.</p><p>Mikolov and Zweig (2012) extended a basic RNN language model ( <ref type="bibr" target="#b19">Mikolov et al., 2010)</ref> by an additional feature layer to integrate external in- formation (such as topic information) about inputs into the model. They showed that such informa- tion improves the perplexity of language models.</p><p>In contrast to previous research, we integrate context into deep autoencoders. To the best of our knowledge, this is the first work to do so. Also, in this paper, we depart from most previ- ous approaches by demonstrating the value of con- text information in sentence-level semantic simi- larity and ranking tasks such as QA ranking tasks. Our approach to the ranking problems, both for Answer Ranking and Question Ranking, is dif- ferent from previous approaches in the sense that we judge the relevance between inputs based on their context information. We showed that adding sentential or document context information about questions (or answers) leads to better rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We introduce an effective approach to integrate sentential or document context into deep autoen- coders and show that such integration is impor- tant in semantic similarity tasks. In the future, we aim to investigate other types of linguistic context (such as POS tag and word dependency informa- tion, word sense, and discourse relations) and de- velop a unified representation learning framework that integrates such linguistic context with repre- sentation learning models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Proposed framework for integrating context into deep autoencoders. Context layer (c x and h c ) and context-sensitive representation of input (h n ) are shown in light red and gray respectively. (a) Pretraining properly initializes a stack of context-sensitive denoising autoencoders (DAE), (b) A contextsensitive deep autoencoder is created from properly initialized DAEs, (c) The network in (b) is unrolled and its parameters are fine-tuned for optimal reconstruction.</figDesc><graphic url="image-11.png" coords="3,92.50,160.77,58.05,58.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pairwise context-sensitive autoencoder for computing text pair similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 :</head><label>1</label><figDesc>Data statistics. (#Pairs: number of word- word pairs in SCWS, question-answer pairs in qAns, and question-question pairs in qSim; %Rel: percentage of positive pairs.) For the unsupervised SCWS task, following Huang et al. (2012), we use 100-dimensional word embeddings, d = 100, with hidden layers and con- text vectors of the same size, d = 100, k = 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 4: Effect of global context on contextual word similarity in different parts of speech (N: noun, V: verb, A: adjective). We only consider frequent categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Reconstruction Error and Improvement: (a) and (b) reconstruction error on qSim and qAns respectively. err N M F shows the reconstruction error of NMF. Smaller error is better, (c) improvement in reconstruction error vs. topic density: greater improvement is obtained in topics with lower density.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Spearman's ρ correlation between model 
predictions and human judgments in contextual 
word similarity. (LC: local context only, LGC: lo-
cal and global context.) 

. . . No cases in Gibraltar were reported. The airport 
is built on the isthmus which the Spanish Government 
claim not to have been ceded in the Treaty of Utrecht. 
Thus the integration of Gibraltar Airport in the Single 
European Sky system has been blocked by Spain. The 
1987 agreement for joint control of the airport with. . . 
. . . called "Tazi" by the German pilots. On 23 Dec 
1942, the Soviet 24th Tank Corps reached nearby Skas-
sirskaya and on 24 Dec, the tanks reached Tatsinskaya. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 4 : Answer ranking in supervised setting</head><label>4</label><figDesc></figDesc><table>Model 
Train 
Train-All 
MAP 
MRR 
MAP 
MRR 
SAE 
63.81 
69.30 
66.37 
71.71 
CSAE 
64.86* 69.93* 
66.76* 73.79* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 : Answer ranking in unsupervised setting.</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head></head><label></label><figDesc>The performance of the unsupervised SAE and CSAE models are comparable with the supervised PCNN model in higher ranks.</figDesc><table>1888 

5 

10 
15 
20 
25 
30 
35 
40 
45 

3.8 

4 

4.2 

4.4 

4.6 

4.8 

5 

5.2 

5.4 

5.6 

5.8 

#epoch 

reconstruction error 

err NMF = 10.79 

sAE 
context−sAE 

(a) qSim Dataset 

5 
10 
15 
20 
25 
30 
35 
40 
45 

3 

3.5 

4 

4.5 

5 

5.5 

6 

#epoch 

reconstruction error 

err NMF = 9.52 

sAE 
context−sAE 

(b) qAns Dataset 

2 
2.5 
3 
3.5 
4 
4.5 

x 10 

4 

0.06 

0.07 

0.08 

0.09 

0.1 

0.11 

0.12 

topic density 
reconstruction improvement 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 6 : Question ranking in supervised setting</head><label>6</label><figDesc></figDesc><table>Model 
P@1 
P@5 
P@10 
SAE 
17.3 
32.4 
32.8 
CSAE 
18.6 
33.2 
34.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 7 : Question ranking in unsupervised setting</head><label>7</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> For example, RNNs can predict the word &quot;sky&quot; given the sentence &quot;clouds are in the ,&quot; but they are less accurate when longer history or global context is required, e.g. predicting the word &quot;french&quot; given the paragraph &quot;I grew up in France.. .. I speak fluent .&quot;</note>

			<note place="foot" n="4"> Figure 2(a) shows compact schematic diagrams of autoencoders used in Figures 1(a) and 1(b)</note>

			<note place="foot" n="5"> Word overlap and IDF-weighted word overlap computed for (a): all words, and (b): only non-stop words for each question-answer pair (Severyn and Moschitti, 2015).</note>

			<note place="foot" n="6"> We observed the same pattern in qAns.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank anonymous reviewers for their thought-ful comments. This paper is based upon work sup-ported, in whole or in part, with funding from the United States Government. Boyd-Graber is sup-ported by NSF grants IIS/1320538, IIS/1409287, and NCSE/1422492. Any opinions, findings, con-clusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Syntactic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Jordan L Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<title level="m">Aggregating continuous word embeddings for information retrieval. the Workshop on Continuous Vector Space Models and their Compositionality, ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning hybrid representations to retrieve semantically equivalent questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Dasha Bogdanova, and Bianca Zadrozny</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tree edit models for recognizing textual entailments, paraphrases, and answers to questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Document context language models. ICLR (Workshop track</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kokoska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zwillinger</surname></persName>
		</author>
		<title level="m">CRC Standard Probability and Statistics Tables and Formulae</title>
		<imprint>
			<publisher>Taylor &amp; Francis</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for document modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Projected gradient methods for nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Bi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2756" to="2779" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Maosong Sun. 2015. Topical word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technologies</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Cernocky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient nonparametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving topic models with latent feature word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Billingsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="299" to="313" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Autoextend: Extending word embeddings to embeddings for synsets and lexemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJNLP</title>
		<meeting>ACLIJNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic feature engineering for answer selection and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning grounded meaning representations with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carina</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring topic coherence over many models and many topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Kegelmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andrzejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Buttler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CNNL</title>
		<meeting>EMNLP-CNNL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Larger-context language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1511.03729</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What is the Jeopardy model? a quasisynchronous grammar for QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Answer extraction as sequence tagging with tree edit distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Question answering using enhanced lexical semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrzej</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pastusiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning for answer sentence selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Pulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS, Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
