<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Role of Seed Lexicons in Learning Bilingual Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab DTAL</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Language Technology Lab DTAL</orgName>
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On the Role of Seed Lexicons in Learning Bilingual Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="247" to="257"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A shared bilingual word embedding space (SBWES) is an indispensable resource in a variety of cross-language NLP and IR tasks. A common approach to the SB-WES induction is to learn a mapping function between monolingual semantic spaces, where the mapping critically relies on a seed word lexicon used in the learning process. In this work, we analyze the importance and properties of seed lexicons for the SBWES induction across different dimensions (i.e., lexicon source, lexicon size, translation method, translation pair reliability). On the basis of our analysis, we propose a simple but effective hybrid bilingual word embedding (BWE) model. This model (HYBWE) learns the mapping between two monolingual embedding spaces using only highly reliable symmetric translation pairs from a seed document-level embedding space. We perform bilingual lexicon learning (BLL) with 3 language pairs and show that by carefully selecting reliable translation pairs our new HYBWE model outperforms benchmarking BWE learning models, all of which use more expensive bilingual signals. Effectively, we demonstrate that a SBWES may be induced by leveraging only a very weak bilingual signal (document alignments) along with monolingual data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dense real-valued vector representations of words or word embeddings (WEs) have recently gained increasing popularity in natural language process- ing (NLP), serving as invaluable features in a broad range of NLP tasks, e.g., <ref type="bibr" target="#b44">(Turian et al., 2010;</ref><ref type="bibr" target="#b7">Collobert et al., 2011;</ref><ref type="bibr" target="#b6">Chen and Manning, 2014)</ref>. Sev- eral studies have showcased a direct link and com- parable performance to "more traditional" distribu- tional models <ref type="bibr" target="#b45">(Turney and Pantel, 2010</ref>). Yet the widely used skip-gram model with negative sam- pling (SGNS) ( <ref type="bibr" target="#b33">Mikolov et al., 2013b</ref>) is considered as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks ( <ref type="bibr" target="#b3">Baroni et al., 2014;</ref><ref type="bibr" target="#b26">Levy and Goldberg, 2014b;</ref>.</p><p>Research interest has recently extended to bilin- gual word embeddings <ref type="bibr">(BWEs)</ref>. BWE learning models focus on the induction of a shared bilingual word embedding space (SBWES) where words from both languages are represented in a uniform language-independent manner such that similar words (regardless of the actual language) have sim- ilar representations (see <ref type="figure" target="#fig_0">Fig. 1</ref>). A variety of BWE learning models have been proposed, differing in the essential requirement of a bilingual signal nec- essary to construct such a SBWES (discussed later in Sect. 2). SBWES may be used to support many tasks, e.g., computing cross-lingual/multilingual semantic word similarity <ref type="bibr" target="#b10">(Faruqui and Dyer, 2014)</ref>, learning bilingual word lexicons ( <ref type="bibr" target="#b32">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b12">Gouws et al., 2015;</ref>, cross-lingual entity linking <ref type="bibr" target="#b42">(Tsai and Roth, 2016</ref>), parsing ( <ref type="bibr" target="#b13">Guo et al., 2015;</ref><ref type="bibr" target="#b18">Johannsen et al., 2015)</ref>, machine translation ( <ref type="bibr" target="#b54">Zou et al., 2013)</ref>, or cross- lingual information retrieval <ref type="bibr" target="#b49">(Vuli´cVuli´c and Moens, 2015;</ref><ref type="bibr" target="#b36">Mitra et al., 2016)</ref>. BWE models should have two desirable prop- erties: (P1) leverage (large) monolingual training sets tied together through a bilingual signal, (P2) use as inexpensive bilingual signal as possible in order to learn a SBWES in a scalable and widely applicable manner across languages and domains.</p><p>While we provide a classification of related work, that is, different BWE models according to these properties in Sect. 2.1, the focus of this work is on a popular class of models labeled Post-Hoc Mapping with Seed Lexicons. These models op- erate as follows ( <ref type="bibr" target="#b32">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b2">Ammar et al., 2016)</ref>: (1) two separate non-aligned monolingual embed- ding spaces are induced using any monolingual WE learning model (SGNS is the typical choice), <ref type="formula">(2)</ref> given a seed lexicon of word translation pairs as the bilingual signal for training, a mapping function is learned which ties the two monolingual spaces together into a SBWES.</p><p>All existing work on this class of models as- sumes that high-quality training seed lexicons are readily available. In reality, little is understood regarding what constitutes a high quality seed lexi- con, even with "traditional" distributional models ( <ref type="bibr" target="#b11">Gaussier et al., 2004;</ref><ref type="bibr" target="#b16">Holmlund et al., 2005;</ref><ref type="bibr" target="#b47">Vuli´cVuli´c and Moens, 2013)</ref>. Therefore, in this work we ask whether BWE learning could be improved by mak- ing more intelligent choices when deciding over seed lexicon entries. In order to do this we delve deeper into the cross-lingual mapping problem by analyzing a spectrum of seed lexicons with respect to controllable parameters such as lexicon source, its size, translation method, and translation pair reliability.</p><p>The contributions of this paper are as follows: (C1) We present a systematic study on the impor- tance of seed lexicons for learning mapping func- tions between monolingual WE spaces. (C2) Given the insights gained, we propose a sim- ple yet effective hybrid BWE model HYBWE that removes the need for readily available seed lexi- cons, and satisfies properties P1 and P2. HYBWE relies on an inexpensive seed lexicon of highly reli- able word translation pairs obtained by a document- level BWE model ) from document-aligned comparable data.</p><p>(C3) Using a careful pair selection process when constructing a seed lexicon, we show that in the BLL task HYBWE outperforms a BWE model of <ref type="bibr" target="#b32">Mikolov et al. (2013a)</ref> which relies on readily available seed lexicons. HYBWE also outperforms state-of-the-art models of ( <ref type="bibr">Hermann and Blunsom, 2014b;</ref><ref type="bibr" target="#b12">Gouws et al., 2015</ref>) which require sentence- aligned parallel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning SBWES using Seed Lexicons</head><p>Given source and target language vocabularies V S and V T , all BWE models learn a representation of each word w ∈ V S V T in a SBWES as a real- valued vector:</p><formula xml:id="formula_0">w = [f 1 , . . . , f d ],</formula><p>where f k ∈ R denotes the value for the k-th cross-lingual fea- ture for w within a d-dimensional SBWES. Se- mantic similarity sim(w, v) between two words w, v ∈ V S V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the SBWES: sim(w, v) = SF (w, v) = cos(w, v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work: BWE Models and</head><p>Bilingual Signals BWE models may be clustered into four different types according to bilingual signals used in train- ing, and properties P1 and P2 (see Sect. 1). Upad- hyay et al. (2016) provide a similar overview of recent bilingual embedding learning architectures regarding different bilingual signals required for the embedding induction.</p><p>(Type 1) Parallel-Only: This group of BWE mod- els relies on sentence-aligned and/or word-aligned parallel data as the only data source ( <ref type="bibr" target="#b54">Zou et al., 2013;</ref><ref type="bibr">Hermann and Blunsom, 2014a;</ref><ref type="bibr" target="#b22">Kočiský et al., 2014;</ref><ref type="bibr">Hermann and Blunsom, 2014b;</ref><ref type="bibr" target="#b5">Chandar et al., 2014</ref>). In addition to an expensive bilingual signal (colliding with P2), these models do not leverage larger monolingual datasets for training (not satisfying P1). embeddings and aim to capture the semantic struc- ture of each language, whereas the cross-lingual objective Bi ensures that similar words across lan- guages are assigned similar embeddings. It ties the two monolingual spaces together into a SBWES (thus satisfying P1). Parameters γ and δ govern the influence of the monolingual and bilingual compo- nents. <ref type="bibr">1</ref> The main disadvantage of Type 2 models is the costly parallel data needed for the bilingual signal (thus colliding with P2).</p><p>(Type 3) Pseudo-Bilingual Training: This set of models requires document alignments as bilin- gual signal to induce a SBWES. Vuli´cVuli´c and Moens (2016) create a collection of pseudo-bilingual docu- ments by merging every pair of aligned documents in training data, in a way that preserves impor- tant local information: words that appeared next to other words within the same language and those that appeared in the same region of the document across different languages. This collection is then used to train word embeddings with monolingual SGNS from word2vec.</p><p>With pseudo-bilingual documents, the "context" of a word is redefined as a mixture of neighbouring words (in the original language) and words that appeared in the same region of the document (in the "foreign" language). The bilingual contexts for each word in each document steer the final model towards constructing a SBWES. The advan- tage over other BWE model types lies in exploiting weaker document-level bilingual signals (satisfying P2), but these models are unable to exploit mono- lingual corpora during training (unlike Type 2 or Type 4; thus colliding with P1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Type 4) Post-Hoc Mapping with Seed Lexicons:</head><p>These models learn post-hoc mapping functions be- tween monolingual WE spaces induced separately for two different languages (e.g., by SGNS). All Type 4 models ( <ref type="bibr" target="#b32">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b10">Faruqui and Dyer, 2014;</ref>) rely on readily available seed lexicons of highly frequent words obtained by e.g. Google Translate (GT) to learn the mapping (again collid- ing with P2), but they are able to satisfy P1.</p><p>1 Type 1 models may be considered a special case of Type 2 models: Setting γ = 0 reduces Type 2 models to Type 1 models trained solely on parallel data, e.g., <ref type="bibr">(Hermann and Blunsom, 2014b;</ref><ref type="bibr" target="#b5">Chandar et al., 2014</ref>). γ = 1 results in the models from ( <ref type="bibr" target="#b21">Klementiev et al., 2012;</ref><ref type="bibr" target="#b12">Gouws et al., 2015;</ref><ref type="bibr" target="#b40">Soyer et al., 2015;</ref><ref type="bibr" target="#b8">Coulmance et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Post-Hoc Mapping with Seed Lexicons: Methodology and Lexicons</head><p>Key Intuition One may infer that a type-hybrid procedure which would retain only highly reliable translation pairs obtained by a Type 3 model as a seed lexicon for Type 4 models effectively satisfies both requirements: (P1) unlike Type 1 and Type 3, it can learn from monolingual data and tie two monolingual spaces using the highly reliable trans- lation pairs, (P2) unlike Type 1 and Type 2, it does not require parallel data; unlike Type 4, it does not require external lexicons and translation systems. The only bilingual signal required are document alignments. Therefore, our focus is on novel less expensive Type 4 models.</p><p>Overview The standard learning setup we use is as follows: First, two monolingual embedding spaces, R d S and R d T , are induced separately in each of the two languages using a standard mono- lingual WE model such as CBOW or SGNS. d S and d T denote the dimensionality of monolingual WE spaces. The bilingual signal is a seed lexicon, i.e., a list of word translation pairs (x i , y i ), where</p><formula xml:id="formula_1">x i ∈ V S , y i ∈ V T , and x i ∈ R d S , y i ∈ R d T .</formula><p>Learning Objectives Training is cast as a mul- tivariate regression problem: it implies learning a function that maps the source language vectors from the training data to their corresponding target language vectors. A standard approach (Mikolov et al., 2013a; ) is to assume a lin- ear map W ∈ R d S ×d T , where a L 2 -regularized least-squares error objective (i.e., ridge regression) is used to learn the map W. The map is learned by solving the following optimization problem (typi- cally by stochastic gradient descent (SGD)):</p><formula xml:id="formula_2">min W∈R d S ×d T ||XW − Y|| 2 F + λ||W|| 2 F<label>(1)</label></formula><p>X and Y are matrices obtained through the re- spective concatenation of source language and tar- get language vectors from training pairs. Once the linear map W is estimated, any previously unseen source language word vector x u may be straightfor- wardly mapped into the target language embedding space R d T as Wx u . After mapping all vectors x, x ∈ V S , the target embedding space R d T in fact serves as SBWES. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seed Lexicon Source and Translation Method</head><p>Prior work on post-hoc mapping with seed lexi- cons used a translation system (i.e., GT) to translate highly frequent English words to other languages such as Czech, <ref type="bibr">Spanish (Mikolov et al., 2013a;</ref><ref type="bibr" target="#b12">Gouws et al., 2015</ref>) or Italian ( . This method presupposes the availability and high quality of such an exter- nal translation system. To simulate this setup, we take as a starting point the BNC word frequency list from Kilgarriff (1997) containing 6, 318 most frequent English lemmas. The list is then translated to other languages via GT. We call the BNC-based lexicons obtained by employing Google Translate BNC+GT.</p><p>In this paper, we propose another option: first, we learn the "first" SBWES (i.e., SBWES-1) us- ing another BWE model (see Sect. 2.1), and then translate the BNC list through SBWES-1 by re- taining the nearest cross-lingual neighbor y i ∈ V T for each x i in the BNC list which is represented in SBWES-1. The pairs (x i , y i ) constitute the seed lexicon needed for learning the mapping between monolingual spaces, that is, to induce the final SBWES-2.</p><p>Although in theory any BWE induction model may be used to induce SBWES-1, we rely on a document-level Type 3 BWE induction model from , since it requires only document alignments as (weak) bilingual sig- nal. The resulting hybrid BWE induction model (HYBWE) combines the output of a Type 3 model (SBWES-1) and a Type 4 model (SBWES-2). This seed lexicon and BWE learning variant is called BNC+HYB.</p><p>Our new hybrid model allows us to also use source language words occurring in SBWES-1 sorted by frequency as seed lexicon source, again leaning on the intuition that higher frequency phe- nomena are more reliably translated using statisti- cal models. Their translations can also be found through SBWES-1 to obtain seed lexicon pairs (x i , y i ). This variant is called HFQ+HYB.</p><p>Another possibility, recently introduced by <ref type="bibr">Kiros et al. (2015)</ref> for vocabulary expansion in monolin- gual settings, relies on all words shared between two vocabularies to learn the mapping. In this work, we test the ability and limits of such orthographic evidence in cross-lingual settings: seed lexicon pairs are (x i , x i ), where x i ∈ V S and x i ∈ V T . This seed lexicon variant is called ORTHO.</p><p>Seed Lexicon Size While all prior reported only results with restricted seed lexicon sizes only (i.e., 1K, 2K and 5K lexicon pairs are used as standard), in this work we provide a full-fledged analysis of the influence of seed lexicon size on the SBWES performance in cross-lingual tasks. More extreme settings are also investigated, in the attempt to an- swer two important questions: (1) Can a Type 4 SBWES be induced in a limited setting with only a few hundred lexicon pairs available (e.g., 100- 500)? (2) Can the Type 4 models profit from the inclusion of more seed lexicon pairs (e.g., more than 5K, even up to 40K-50K lexicon pairs)?</p><p>Translation Pair Reliability When building seed lexicons through SBWES-1 (i.e., BNC+HYB and HFQ+HYB methods), it is possible to con- trol for the reliability of translation pairs to be in- cluded in the final lexicon, with the idea that the use of only highly reliable pairs can potentially lead to an improved SBWES-2. A simple yet effective reliability reliability feature for transla- tion pairs is the symmetry constraint <ref type="bibr" target="#b37">(Peirsman and Padó, 2010;</ref><ref type="bibr" target="#b47">Vuli´cVuli´c and Moens, 2013)</ref> : two words x i ∈ V S and y i ∈ V S are used as seed lexicon pairs only if they are mutual nearest neighbours given their representations in SBWES-1. The two variants of seed lexicons with only symmetric pairs are BNC+HYB+SYM and HFREQ+HYB+SYM. We also test the variants without the sym- metry constraint (i.e., BNC+HYB+ASYM and HFQ+HYB+ASYM).</p><p>Even more conservative reliability measures may be applied by exploiting the scores in the lists of translation candidates ranked by their similarity to the cue word x i . We investigate a symmetry constraint with a threshold: two words x i ∈ V S and y i ∈ V S are included as seed lexicon pair (x i , y i ) iff they are mutual nearest neighbours in SBWES-1 and it holds:</p><formula xml:id="formula_3">sim(xi, yi) − sim(xi, zi) &gt; T HR (2) sim(yi, xi) − sim(yi, wi) &gt; T HR<label>(3)</label></formula><p>where z i ∈ V T is the second best translation can- didate for x i , and w i ∈ V S for y i . THR is a param- eter which specifies the margin between the two best translation candidates. The intuition is that highly unambiguous and monosemous translation pairs (which is reflected in higher score margins) are also highly reliable. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>Task: Bilingual Lexicon Learning (BLL) Af- ter the final SBWES is induced, given a list of n source language words x u1 , . . . , x un , the task is to find a target language word t for each x u in the list using the SBWES. t is the target language word closest to the source language word x u in the in- duced SBWES, also known as the cross-lingual nearest neighbor. The set of learned n (x u , t) pairs is then run against a gold standard BLL test set. Following the standard practice ( <ref type="bibr" target="#b32">Mikolov et al., 2013a;</ref>, for all Type 4 models, all pairs containing any of the test words x u1 , . . . , x un are removed from training seed lexicons. Baseline Models To induce SBWES-1, we re- sort to document-level embeddings of Vuli´cVuli´c and Moens (2016) (Type 3). We also compare to re- sults obtained directly by their model (BWESG) to measure the performance gains with HYBWE.</p><p>To compare with a representative Type 2 model, we opt for the BilBOWA model of <ref type="bibr" target="#b12">Gouws et al. (2015)</ref> due to its solid performance and robustness in the BLL task when trained on general-domain corpora such as Wikipedia ( <ref type="bibr" target="#b28">Luong et al., 2015)</ref>, its reduced complexity reflected in fast computations on massive datasets, as well as its public availabil- literature ( <ref type="bibr" target="#b39">Smith and Eisner, 2007;</ref><ref type="bibr" target="#b43">Tu and Honavar, 2012;</ref><ref type="bibr" target="#b47">Vuli´cVuli´c and Moens, 2013</ref>), but we do not observe any significant gains when resorting to the more complex reliability estimates. <ref type="bibr">4</ref> http://people.cs.kuleuven.be/~ivan.vulic/ 5 Similar trends are observed within a more lenient setting with Acc5 and Acc10 scores, but we omit these results for clarity and the fact that the actual BLL performance is best reflected in Acc1 scores (i.e., best translation only). ity. 6 In short, BilBOWA combines the adapted SGNS for monolingual objectives together with a cross-lingual objective that minimizes the L 2 -loss between the bag-of-word vectors of parallel sen- tences. BilBOWA uses the same training setup as HYBWE (monolingual datasets plus a bilingual signal), but relies on a stronger bilingual signal (sentence alignments as opposed to HYBWE's doc- ument alignments).</p><p>We also compare with a benchmarking Type 1 model from sentence-aligned parallel data called BiCVM ( <ref type="bibr">Hermann and Blunsom, 2014b</ref>). Finally, a SGNS-based BWE model with the BNC+GT seed lexicon is taken as a baseline Type 4 model ( <ref type="bibr" target="#b32">Mikolov et al., 2013a</ref>). <ref type="bibr">7</ref> Training Data and Setup We use standard train- ing data and suggested settings to obtain BWEs for all models involved in comparison. We retain the 100K most frequent words in each language for all models. To induce monolingual WE spaces, two monolingual SGNS models were trained on the cleaned and tokenized Wikipedias from the Poly- glot website (Al-Rfou et al., 2013) using SGD with a global learning rate of 0.025. For BilBOWA, as in the original work ( <ref type="bibr" target="#b12">Gouws et al., 2015</ref>), the bilingual signal for the cross-lingual regularization is provided by the first 500K sentences from Eu- roparl.v7 <ref type="bibr" target="#b41">(Tiedemann, 2012)</ref>. We use SGD with a global rate of 0.15. <ref type="bibr">8</ref> The window size is varied from 2 to 16 in steps of 2, and the best scoring model is always reported in all comparisons.</p><p>BWESG was trained on the cleaned and tok- enized document-aligned Wikipedias available on- line 9 , SGD on pseudo-bilingual documents with a global rate 0.025. For BiCVM, we use the tool released by its authors 10 and train on the whole Europarl.v7 for each language pair: we train an additive model, with hinge loss margin set to d (i.e., dimensionality) as in the original paper, batch size of 50, and noise parameter of 10. All BiCVM models are trained with 200 iterations.</p><p>For all models, we obtain BWEs with d = 40, 64, 300, 500, but we report only results with 300-dimensional BWEs as similar trends were ob- served with other d-s. Other parameters are: 15 epochs, 15 negatives, subsampling rate 1e − 4. <ref type="table">casamiento  casamiento  casamiento  casamiento  casamiento  casamiento   marriage  marry  marriage  marriage  marriage  maría  marry  marriage  marry  marry  marry  señor  marrying  marrying  marrying  betrothal  betrothal  doña  betrothal  wed  wedding  marrying  marrying  juana  wedding  wedding  betrothal  wedding  wedding  noche  wed  betrothal  wed  daughter  wed  amor  elopement</ref> remarry marriages betrothed elopement guerra  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BNC+GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BNC+HYB+ASYM BNC+HYB+SYM HFQ+HYB+ASYM HFQ+HYB+SYM ORTHO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Exp. I: Standard BLL Setting First, we replicate the previous BLL setups with Type 4 models from <ref type="bibr" target="#b32">(Mikolov et al., 2013a;</ref>) by relying on seed lexicons of exactly 5K word pairs (except for BNC+HYB+SYM which exhausts all possible pairs before the 5K limit) sorted by frequency of the source language word. Results with different lexicons for the three language pairs are summa- rized in <ref type="table" target="#tab_1">Table 2, while Table 1</ref> shows examples of nearest neighbour words for a Spanish word not present in any of the training lexicons. <ref type="table" target="#tab_1">Table 1</ref> provides evidence for our first insight: Type 4 models do not necessarily require external lexicons (such as the BNC+GT model) to learn a semantically plausible SBWES (i.e., the lists of nearest neighbours are similar for all lexicons ex- cluding ORTHO). <ref type="table" target="#tab_1">Table 1</ref> also suggests that the choice of seed lexicon pairs may strongly influence the properties of the resulting SBWES. Due to its design, ORTHO finds a mapping which naturally brings foreign words appearing in the English vo- cabulary closer in the induced SBWES.</p><p>This first batch of quantitative results already shows that Type 4 models with inexpensive auto- matically induced lexicons (i.e., HYBWE) are on a par with or even better than Type 4 models relying on external resources or translation systems. In ad- dition, the best reported scores using the more con- strained symmetric BNC/HFQ+HYB+SYM lexi- con variants are higher than those for three baseline models (of Type 1, Type 2, and Type 3) that pre- viously held highest scores on the BLL test sets . These improvements over the baseline models and BNC+GT are sta- tistically significant (using McNemar's statistical significance test, p &lt; 0.05). <ref type="table" target="#tab_2">Table 2</ref> also suggests that a careful selection of reliable pairs can lead to peak performances even with a lower number of pairs, i.e., see the results of BNC+HYB+SYM.</p><p>Exp. II: Lexicon Size BLL results for ES-EN and NL-EN obtained by varying the seed lexicon sizes are displayed in <ref type="figure" target="#fig_1">Fig. 2(a) and 2(b)</ref>. Results for IT-EN closely follow the patterns observed with ES- EN. BNC+HYB+SYM and HFQ+HYB+ASYM -the two models that do not blindly use all po- tential training pairs, but rely on sets of symmet- ric pairs (i.e., they include the simple measure of translation pair reliability) -display the best per- formance across all lexicon sizes. The finding con- firms the intuition that a more intelligent pair selec- tion strategy is essential for Type 4 BWE models. HFQ+HYB+SYM -a simple hybrid BWE model (HYBWE) combining a document-level Type 3 model with a Type 4 model and translation reliabil- ity detection -is the strongest BWE model overall (see also <ref type="table" target="#tab_2">Table 2</ref> again).</p><p>HYBWE-based models which do not perform any pair selection (i.e., BNC/HFQ+HYB+ASYM) closely follow the behaviour of the GT-based model. This demonstrates that an external lexi- con or translation system may be safely replaced by a document-level embedding model without any significant performance loss in the BLL task. The ORTHO-based model falls short of its competitors. However, we observe that even this model with the learning setting relying on the cheapest bilingual signal may lead to reasonable BLL scores, espe- cially for the more related NL-EN pair.</p><p>The two models with the symmetry constraint display a particularly strong performance with set- tings relying on scarce resources (i.e., only a small portion of training pairs is available). For instance, HFQ+HYB+SYM scores 0.129 for ES-EN with only 200 training pairs (vs 0.002 with BNC+GT), and 0.529 with 500 pairs (vs 0.145 with BNC+GT). On the other hand, adding more pairs does not lead to an improved BLL performance. In fact, we observe a slow and steady decrease in perfor- mance with lexicons containing 10, 000 and more training pairs for all HYBWE variants. The phe- nomenon may be attributed to the fact that highly frequent words receive more accurate representa- tions in SBWES-1, and adding less frequent and, consequently, less accurate training pairs to the SBWES-2 learning process brings in additional noise. In plain language, when it comes to seed lex- icons Type 4 models prefer quality over quantity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exp. III: Translation Pair Reliability</head><p>In the next experiment, we vary the threshold value THR (see sect. 2.2) in the HFQ+HYB+SYM variant with the following values in comparison: 0.0 (None), 0.01, 0.025, 0.05, 0.075, 0.1. We in- vestigate whether retaining only highly unambigu- ous pairs would lead to even better BLL perfor- mance. The results for all three language pairs are summarized in <ref type="figure">Fig. 3(a)-3(c)</ref>. The results for all variant models again decrease when employ- ing larger lexicons (due to the usage of less fre- quent word pairs in training). We observe that a slightly stricter selection criterion (i.e., THR = 0.01, 0.025) also leads to slightly improved peak BLL scores for ES-EN and IT-EN around the 5K region. The improvements, however, are not statis- tically significant. On the other hand, a too conser- vative pair selection criterion with higher threshold values significantly deteriorates the overall perfor- mance of HYBWE with HFQ+HYB+SYM. The conservative criteria discard plenty of potentially useful training pairs. Therefore, as one line of future research, we plan to investigate more sophis- ticated models for the selection of reliable seed lexicon pairs that will lead to a better trade-off be- tween the lexicon size and reliability of the pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exp. IV: Another Task -Suggesting Word</head><p>Translations in Context (SWTC) In the final experiment, we test whether the findings originat- ing from the BLL task generalize to another cross- lingual semantic task: suggesting word translations in context (SWTC) recently proposed by <ref type="bibr" target="#b48">Vuli´cVuli´c and Moens (2014)</ref>. Given an occurrence of a polyse- mous word w ∈ V S , the SWTC task is to choose the correct translation in the target language of that particular occurrence of w from the given set T C(w) = {t 1 , . . . , t tq }, T C(w) ⊆ V T , of its tq possible translations/meanings. Whereas in the BLL task the candidate search is performed over the entire vocabulary V T , the set T C(w) typically comprises only a few pre-selected words/senses. One may refer to T C(w) as an inventory of transla- tion candidates for w. The best scoring translation candidate in the ranked list is then the correct trans- lation for that particular occurrence of w observing its local context Con(w). SWTC is an extended  <ref type="table">Table 3</ref>: Acc 1 scores in the SWTC task. All seed lexicons contain 6K translation pairs, except for BNC+HYB+SYM (its sizes provided in parenthe- ses). * denotes a statistically significant improve- ment over baselines and BNC+GT using McNe- mar's statistical significance test with the Bonfer- roni correction, p &lt; 0.05.</p><p>cross-lingual variant of the task proposed by <ref type="bibr" target="#b17">Huang et al. (2012)</ref> which evaluates monolingual context- sensitive semantic similarity of words in sentential context, and it is also very related to cross-lingual lexical substitution ( <ref type="bibr" target="#b31">Mihalcea et al., 2010</ref>).</p><p>To isolate the performance of each BWE induc- tion model from the details of the SWTC setup, we use the same approach with all models: we opt for the SWTC framework proven to yield excellent results with BWEs in the SWTC task . In short, the context bag Con(w) = {cw 1 , . . . , cw r } is obtained by harvest- ing all r words that occur with w in the sentence.</p><p>The vector representation of Con(w) is the d- dimensional embedding computed by aggregating over all word embeddings for each cw j ∈ Con(w) using standard addition as the compositional opera- tor ( <ref type="bibr" target="#b35">Mitchell and Lapata, 2008)</ref> which was proven a robust choice ( <ref type="bibr" target="#b34">Milajevs et al., 2014)</ref>:</p><formula xml:id="formula_4">Con(w) = cw 1 + cw 2 + . . . + cw r (4)</formula><p>where cw j is the embedding of the j-th con- text word, and Con(w) is the resulting embed- ding of the context bag Con(w). Finally, for each t j ∈ T C(w), the context-sensitive similar- ity with w is computed as: sim(w, t j , Con(w)) = cos(Con(w), t j ), where Con(w) and t j are rep- resentations of the (sentential) context bag and the candidate translation t j in the same SBWES. <ref type="bibr">11</ref> The evaluation set consists of 360 sentences for 15 polysemous nouns (24 sentences for each noun) in each of the three languages: Spanish, Dutch, Ital- ian, along with the single gold standard single word English translation given the sentential context. 12 <ref type="table">Table 3</ref> summarizes the results (Acc 1 scores) in the SWTC task. NO-CONTEXT refers to the context- insensitive majority baseline obtained by BNC+GT (i.e., it always chooses the most semantically sim- ilar translation candidate at the word type level). We also report the results of the best SWTC model from Vuli´c <ref type="bibr" target="#b48">Vuli´c and Moens (2014)</ref>.</p><p>The results largely support the claims estab- lished with the BLL evaluation. An exter-nal seed lexicon of BNC+GT may be safely replaced by an automatically induced inex- pensive seed lexicon (as in HYBWE with BNC+HYB+SYM/ASYM). The best perform- ing models are again BNC+HYB+SYM and HFQ+HYB+SYM. The comparison of ASYM and SYM lexicon variants further suggests that filter- ing translation pairs using the symmetry constraint again leads to consistent improvements, but stricter selection criteria with higher thresholds do not lead to significant performance boosts, and may even hurt the performance (see the results for NL-EN). Various HYBWE variants significantly improve over baseline BWE models <ref type="figure" target="#fig_0">(Types 1-4)</ref>, also out- performing previous best SWTC results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We presented a detailed analysis of the importance and properties of seed bilingual lexicons in learn- ing bilingual word embeddings (BWEs) which are valuable for many cross-lingual/multilingual NLP tasks. On the basis of the analysis, we proposed a simple yet effective hybrid bilingual word embed- ding model called HYBWE. It learns the mapping between two monolingual embedding spaces us- ing only highly reliable symmetric translation pairs from an inexpensive seed document-level embed- ding space. The results in the tasks of (1) bilingual lexicon learning and (2) suggesting word transla- tions in context demonstrate that -due to its careful selection of reliable translation pairs for seed lexi- cons -HYBWE outperforms benchmarking BWE induction models, all of which use more expensive bilingual signals for training.</p><p>In future work, we plan to investigate other meth- ods for seed pairs selection, settings with scarce resources <ref type="bibr" target="#b0">(Agi´cAgi´c et al., 2015;</ref><ref type="bibr" target="#b53">Zhang et al., 2016)</ref>, other context types inspired by recent work in the monolingual settings ( <ref type="bibr" target="#b25">Levy and Goldberg, 2014a;</ref><ref type="bibr" target="#b30">Melamud et al., 2016)</ref>, as well as model adapta- tions that can work with multi-word expressions. Encouraged by the excellent results, we also plan to test the portability of the approach to more lan- guage pairs, and other tasks and applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A toy example of a 3-dimensional monolingual vs shared bilingual word embedding space (further SBWES) from Gouws et al. (2015).</figDesc><graphic url="image-2.png" coords="1,312.27,222.54,99.71,68.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(Type 2 )</head><label>2</label><figDesc>Joint Bilingual Training: These models jointly optimize two monolingual objectives, with the cross-lingual objective acting as a cross-lingual regularizer during training (Klementiev et al., 2012; Gouws et al., 2015; Soyer et al., 2015; Shi et al., 2015; Coulmance et al., 2015). The idea may be summarized by the simplified formulation (Luong et al., 2015): γ(Mono S +Mono T )+δBi. The mono- lingual objectives M ono S and M ono T ensure that similar words in each language are assigned similar</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BLL results (Acc 1 ) across different seed lexicon sizes for all lexicons. x axes are in log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Nearest EN neighbours of the Spanish word casamiento (marriage) with different seed lexicons. 

Model 
ES-EN NL-EN IT-EN 

BICVM (TYPE 1) 
0.532 
0.583 
0.569 
BILBOWA (TYPE 2) 
0.632 
0.636 
0.647 
BWESG (TYPE 3) 
0.676 
0.626 
0.643 

BNC+GT (Type 4) 
0.677 
0.641 
0.646 

ORTHO 
0.233 
0.506 
0.224 
BNC+HYB+ASYM 
0.673 
0.626 
0.644 
BNC+HYB+SYM 
0.681 
0.658* 
0.663* 
(3388; 2738; 3145) 
HFQ+HYB+ASYM 
0.673 
0.596 
0.635 
HFQ+HYB+SYM 
0.695* 
0.657* 
0.667* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Acc 1 scores in a standard BLL setup 
(for Type 4 models): all seed lexicons contain 5K 
translation pairs, except for BNC+HYB+SYM (its 
sizes provided in parentheses). * denotes a statisti-
cally significant improvement over baselines and 
BNC+GT using McNemar's statistical significance 
test with the Bonferroni correction, p &lt; 0.05. 

</table></figure>

			<note place="foot" n="2"> Another possible objective (found in the zero-shot learning literature) is a margin-based ranking loss (Weston et al., 2011; Lazaridou et al., 2015). We omit the results with this objective for brevity, and due to the fact that similar trends are observed as with (more standard) linear maps.</note>

			<note place="foot" n="3"> Other (more elaborate) reliability measures exist in the</note>

			<note place="foot" n="6"> https://github.com/gouwsmeister/bilbowa 7 For details concerning all baseline models, the reader is encouraged to check the relevant literature. 8 Suggested by the authors (personal correspondence). 9 http://linguatools.org/tools/corpora/ 10 https://github.com/karlmoritz/bicvm</note>

			<note place="foot" n="11"> The same ranking of different models (with lower absolute scores) is observed when adapting the monolingual lexical substitution framework of Melamud et al. (2015) to the SWTC task as done by Vuli´cVuli´c and Moens (2016). 12 The SWTC evaluation set is available online at: http://aclweb.org/anthology/attachments/D/D14/D141040.Attachment.zip</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). The authors are grateful to Roi Re-ichart and the anonymous reviewers for their help-ful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Željko</forename><surname>Agi´cagi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<title level="m">If all you have is a bit of the Bible: Learning POS taggers for truly low-resource languages. In ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="268" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Massively multilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1602.01925</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons using the visual similarity of labeled web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1764" to="1769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Sarath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Trans-gram, fast cross-lingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Coulmance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1109" to="1113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Papers</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A geometric view on bilingual lexicon extraction from comparable corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Déjean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="526" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BilBOWA: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing based on distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilingual distributed representations without word alignment</title>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilingual models for compositional distributed semantics</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Creating bilingual lexica using reference wordlists for alignment of monolingual semantic vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Holmlund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NODALIDA</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="71" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Any-language frame-semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Héctor Martínez Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2062" to="2066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Putting frequencies in the dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="155" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning bilingual word representations by marginalizing alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočiský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="224" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hubness and pollution: Delving into cross-space mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Judgment language matters: Multilingual vector space models for judgment language aware lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<idno>abs/1508.00106</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A simple word embedding model for lexical substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The role of context types and dimensionality in learning word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 2: Cross-lingual lexical substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SEMEVAL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluating neural word representations in tensor-based compositional settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrijs</forename><surname>Milajevs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Kartsaklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="708" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A dual embedding space model for document ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">T</forename><surname>Nalisnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<idno>abs/1602.01137</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Crosslingual induction of selectional preferences with bilingual vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="921" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning cross-lingual word embeddings via matrix co-factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bootstrapping feature-rich dependency parsers with entropic priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="667" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Leveraging monolingual data for crosslingual compositional word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in OPUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2214" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cross-lingual wikification using multilingual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unambiguity regularization for unsupervised learning of probabilistic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasant</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1324" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From frequency to meaning: vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artifical Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-lingual models of word embeddings: An empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1613" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Probabilistic models of cross-lingual semantic similarity in context based on latent cross-lingual concepts induced from comparable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="349" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="363" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bilingual distributed word representations from document-aligned comparable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="953" to="994" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multi-modal representations for improved bilingual lexicon learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">WSABIE: scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ten pairs to tag-Multilingual POS tagging via coarse mapping between embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
