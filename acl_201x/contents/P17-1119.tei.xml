<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Adaptation of Synthetic or Stale Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchan</forename><surname>Kim</surname></persName>
						</author>
						<title level="a" type="main">Adversarial Adaptation of Synthetic or Stale Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1297" to="1307"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1119</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Two types of data shift common in practice are 1. transferring from synthetic data to live user data (a deployment shift), and 2. transferring from stale data to current data (a temporal shift). Both cause a distribution mismatch between training and evaluation, leading to a model that over-fits the flawed training data and performs poorly on the test data. We propose a solution to this mismatch problem by framing it as domain adaptation, treating the flawed training dataset as a source domain and the evaluation dataset as a target domain. To this end, we use and build on several recent advances in neural domain adaptation such as adversarial training (Ganin et al., 2016) and domain separation network (Bousmalis et al., 2016), proposing a new effective adversarial training scheme. In both supervised and unsupervised adaptation scenarios, our approach yields clear improvement over strong baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Two types of data shift common in prac- tice are 1. transferring from synthetic data to live user data (a deployment shift), and 2. transferring from stale data to current data (a temporal shift). Both cause a dis- tribution mismatch between training and evaluation, leading to a model that over- fits the flawed training data and performs poorly on the test data. We propose a solu- tion to this mismatch problem by framing it as domain adaptation, treating the flawed training dataset as a source domain and the evaluation dataset as a target domain.</p><p>To this end, we use and build on several recent advances in neural domain adap- tation such as adversarial training ( <ref type="bibr" target="#b10">Ganin et al., 2016</ref>) and domain separation net- work ( <ref type="bibr" target="#b3">Bousmalis et al., 2016)</ref>, proposing a new effective adversarial training scheme. In both supervised and unsupervised adap- tation scenarios, our approach yields clear improvement over strong baselines. We are interested in addressing two types of data shift common in SLU applications. The first data shift problem happens when we trans- fer from synthetic data to live user data (a deploy- ment shift). This is also known as the "cold-start" problem; a model cannot be trained on the real usage data prior to deployment simply because it does not exist. A common practice is to gener- ate a large quantity of synthetic training data that mimics the expected user behavior. Such synthetic data is crafted using domain-specific knowledge and can be time-consuming. It is also flawed in that it typically does not match the live user data generated by actual users; the real queries submit- ted to these systems are different from what the model designers expect to see.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The second data shift problem happens when we transfer from stale data to current data (a tem- poral shift). In our use case, we have one set of training data from 2013 and wish to handle data from <ref type="bibr">[2014]</ref><ref type="bibr">[2015]</ref><ref type="bibr">[2016]</ref>. This is problematic since the content of the user queries changes over time (e.g., new restaurant or movie names may be added). Consequently, the model performance degrades over time.</p><p>Both shifts cause a distribution mismatch be- tween training and evaluation, leading to a model that overfits the flawed training data and performs poorly on the test data. We propose a solution to this mismatch problem by framing it as domain adaptation, treating the flawed training dataset as a source domain and the evaluation dataset as a target domain. To this end, we use and build on several recent advances in neural domain adap- tation such as adversarial training ( <ref type="bibr" target="#b10">Ganin et al., 2016)</ref> and domain separation network ( <ref type="bibr" target="#b3">Bousmalis et al., 2016)</ref>, proposing a new adversarial training scheme based on randomized predictions.</p><p>We consider both supervised and unsupervised adaptation scenarios (i.e., absence/presence of la- beled data in the target domain). We find that un- supervised DA can greatly improve performance without requiring additional annotation. Super-vised DA with a small amount of labeled data gives further improvement on top of unsuper- vised DA. In experiments, we show clear gains in both deployment and temporal shifts across 5 test domains, yielding average error reductions of 74.04% and 41.46% for intent classification and 70.33% and 32.0% for slot tagging compared to baselines without adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work 2.1 Domain Adaptation</head><p>Our work builds on the recent success of DA in the neural network framework. Notably, <ref type="bibr" target="#b10">Ganin et al. (2016)</ref> propose an adversarial training method for unsupervised DA. They partition the model pa- rameters into two parts: one inducing domain- specific (or private) features and the other domain- invariant (or shared) features.</p><p>The domain- invariant parameters are adversarially trained us- ing a gradient reversal layer to be poor at domain classification; as a consequence, they produce rep- resentations that are domain agnostic. This ap- proach is motivated by a rich literature on the the- ory of DA pioneered by <ref type="bibr" target="#b2">Ben-David et al. (2007)</ref>. We describe our use of adversarial training in Sec- tion 3.2.3. A special case of <ref type="bibr" target="#b10">Ganin et al. (2016)</ref> is developed independently by <ref type="bibr" target="#b23">Kim et al. (2016c)</ref> who motivate the method as a generalization of the feature augmentation method of <ref type="bibr" target="#b7">Daum√© III (2009)</ref>. <ref type="bibr" target="#b3">Bousmalis et al. (2016)</ref> extend the framework of <ref type="bibr" target="#b10">Ganin et al. (2016)</ref> by additionally encourag- ing the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and si- multaneously reconstructing the input (for all do- mains) from the features induced by these param- eters.</p><p>Both <ref type="bibr" target="#b10">Ganin et al. (2016)</ref> and <ref type="bibr" target="#b3">Bousmalis et al. (2016)</ref> discuss applications in computer vision. <ref type="bibr" target="#b41">Zhang et al. (2017)</ref> apply the method of <ref type="bibr" target="#b3">Bousmalis et al. (2016)</ref> to tackle transfer learning in NLP. They focus on transfer learning between classifi- cation tasks over the same domain ("aspect trans- fer"). They assume a set of keywords associated with each aspect and use these keywords to inform the learner of the relevance of each sentence for that aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Spoken Language Understanding</head><p>Recently, there has been much investment on the personal digital assistant (PDA) technology in in- dustry <ref type="bibr" target="#b33">(Sarikaya, 2015;</ref>. Ap- ples Siri, Google Now, Microsofts Cortana, and Amazons Alexa are some examples of personal digital assistants. Spoken language understanding (SLU) is an important component of these exam- ples that allows natural communication between the user and the agent <ref type="bibr" target="#b38">(Tur, 2006;</ref><ref type="bibr" target="#b8">El-Kahky et al., 2014</ref>). PDAs support a number of scenarios in- cluding creating reminders, setting up alarms, note taking, scheduling meetings, finding and consum- ing entertainment (i.e. movie, music, games), find- ing places of interest and getting driving directions to them <ref type="bibr" target="#b18">(Kim et al., 2016a</ref>).</p><p>Naturally, there has been an extensive line of prior studies for domain scaling problems to eas- ily scale to a larger number of domains: pre- training ( <ref type="bibr" target="#b21">Kim et al., 2015c</ref>), transfer learning ( <ref type="bibr" target="#b25">Kim et al., 2015d</ref>), constrained decoding with a sin- gle model <ref type="bibr" target="#b18">(Kim et al., 2016a)</ref>, multi-task learn- ing ( <ref type="bibr" target="#b15">Jaech et al., 2016)</ref>, neural domain adapta- tion ( <ref type="bibr" target="#b23">Kim et al., 2016c</ref>), domainless adaptation ( <ref type="bibr" target="#b22">Kim et al., 2016b</ref>), a sequence-to-sequence model , domain attention ( <ref type="bibr" target="#b19">Kim et al., 2017</ref>) and zero-shot learning <ref type="bibr" target="#b9">Ferreira et al., 2015)</ref>.</p><p>There are also a line of prior works on enhanc- ing model capability and features: jointly mod- eling intent and slot predictions <ref type="bibr" target="#b16">(Jeong and Lee, 2008;</ref><ref type="bibr" target="#b39">Xu and Sarikaya, 2013;</ref><ref type="bibr" target="#b12">Guo et al., 2014;</ref><ref type="bibr" target="#b40">Zhang and Wang, 2016;</ref><ref type="bibr">Liu and Lane, 2016a,b)</ref>, modeling SLU models with web search click logs ( <ref type="bibr" target="#b28">Li et al., 2009;</ref><ref type="bibr" target="#b17">Kim et al., 2015a</ref>) and enhancing features, including representations ( <ref type="bibr" target="#b0">Anastasakos et al., 2014;</ref><ref type="bibr" target="#b5">Celikyilmaz et al., , 2010</ref><ref type="bibr" target="#b24">Kim et al., 2016d</ref>) and lexicon ( <ref type="bibr" target="#b20">Kim et al., 2015b</ref>).</p><p>All the above works assume that there are no any data shift issues which our work try to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BiLSTM Encoder</head><p>We use an LSTM simply as a mapping œÜ : R d √ó R d ‚Üí R d that takes an input vector x and a state vector h to output a new state vector h = œÜ(x, h). See Hochreiter and Schmidhuber (1997) for a de- tailed description.</p><p>Let C denote the set of character types and W the set of word types. Let ‚äï denote the vec- tor concatenation operation. We encode an utter- ance using the wildly successful architecture given by bidirectional LSTMs (BiLSTMs) <ref type="bibr" target="#b36">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b11">Graves, 2012)</ref>. The model parame- ters Œò associated with this BiLSTM layer are</p><formula xml:id="formula_0">‚Ä¢ Character embedding e c ‚àà R 25 for each c ‚àà C ‚Ä¢ Character LSTMs œÜ C f , œÜ C b : R 25 √ó R 25 ‚Üí R 25</formula><p>‚Ä¢ Word embedding e w ‚àà R 100 for each w ‚àà W</p><p>‚Ä¢ Word LSTMs œÜ W f , œÜ W b : R 150 √óR 100 ‚Üí R 100 Let w 1 . . . w n ‚àà W denote a word sequence where word w i has character w i (j) ‚àà C at position j. First, the model computes a character-sensitive word representation v i ‚àà R 150 as</p><formula xml:id="formula_1">f C j = œÜ C f e w i (j) , f C j‚àí1 ‚àÄj = 1 . . . |w i | b C j = œÜ C b e w i (j) , b C j+1 ‚àÄj = |w i | . . . 1 v i = f C |w i | ‚äï b C 1 ‚äï e w i</formula><p>for each i = 1 . . . n. 1 Next, the model computes</p><formula xml:id="formula_2">f W i = œÜ W f v i , f W i‚àí1 ‚àÄi = 1 . . . n b W i = œÜ W b v i , b W i+1 ‚àÄi = n . . . 1</formula><p>and induces a character-and context-sensitive word representation h i ‚àà R 200 as</p><formula xml:id="formula_3">h i = f W i ‚äï b W i (1)</formula><p>for each i = 1 . . . n. For convenience, we write the entire operation as a mapping BiLSTM Œò :</p><formula xml:id="formula_4">(h 1 . . . h n ) ‚Üê BiLSTM Œò (w 1 . . . w n )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised DA</head><p>In unsupervised domain adaptation, we assume la- beled data for the source domain but not the target domain. Our approach closely follows the previ- ous work on unsupervised neural domain adapta- tion by <ref type="bibr" target="#b10">Ganin et al. (2016)</ref> and <ref type="bibr" target="#b3">Bousmalis et al. (2016)</ref>. We have three BiLSTM encoders de- scribed in Section 3.1:</p><p>1. Œò src : induces source-specific features 2. Œò tgt : induces target-specific features 3. Œò shd : induces domain-invariant features</p><p>We now define a series of loss functions defined by these encoders.</p><p>1 For simplicity, we assume some random initial state vec- tors such as f C 0 and b C |w i |+1 when we describe LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Source Side Tagging Loss</head><p>The most obvious objective is to minimize the model's error on labeled training data for the source domain. Let w 1 . . . w n ‚àà W be an utter- ance in the source domain annotated with labels y 1 . . . y n ‚àà L. We induce</p><formula xml:id="formula_5">(h src 1 . . . h src n ) ‚Üê BiLSTM Œò src (w 1 . . . w n ) (h shd 1 . . . h shd n ) ‚Üê BiLSTM Œò shd (w 1 . . . w n )</formula><p>Then we define the probability of tag y ‚àà L for the i-th word as</p><formula xml:id="formula_6">z i = W 2 tag tanh W 1 tag ¬Ø h i + b 1 tag + b 2 tag p(y|h i ) ‚àù exp ([z i ] y )</formula><p>where</p><formula xml:id="formula_7">¬Ø h i = h src i ‚äï h shd i and Œò tag = {W 1 tag , W 2 tag , b 1 tag , b 2 tag } denotes additional feed- foward parameters.</formula><p>The tagging loss is given by the negative log likelihood</p><formula xml:id="formula_8">L tag (Œò src , Œò shd , Œò tag ) = ‚àí i log p y i | ¬Ø h i</formula><p>where we iterate over annotated words (w i , y i ) on the source side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Reconstruction Loss</head><p>Following previous works, we ground feature learning by reconstructing encoded utterances. Both <ref type="bibr" target="#b3">Bousmalis et al. (2016)</ref> and <ref type="bibr" target="#b41">Zhang et al. (2017)</ref> use mean squared errors for reconstruc- tion, the former of image pixels and the latter of words in a context window. In contrast, we use an attention-based LSTM that fully re-generates the input utterance and use its log loss. More specifically, let w 1 . . . w n ‚àà W be an ut- terance in domain d ‚àà {src, tgt}. We first use the relevant encoders as before</p><formula xml:id="formula_9">(h d 1 . . . h d n ) ‚Üê BiLSTM Œò d (w 1 . . . w n ) (h shd 1 . . . h shd n ) ‚Üê BiLSTM Œò shd (w 1 . . . w n ) The concatenated vectors ¬Ø h i = h d i ‚äï h shd i</formula><p>are fed into the standard attention-based decoder <ref type="bibr" target="#b1">(Bahdanau et al., 2014</ref>) to define the probability of word w at each position i with state vector ¬µ i‚àí1 (where ¬µ 0 = ¬Ø h n ):</p><formula xml:id="formula_10">Œ± j ‚àù exp ¬µ i‚àí1 ¬Ø h j ‚àÄj ‚àà {1 . . . n}Àúh n}Àú n}Àúh i = n j=1 Œ± j ¬Ø h j ¬µ i = œÜ R (¬µ i‚àí1 ‚äï Àú h i , ¬µ i‚àí1 ) p(w|¬µ i ) ‚àù exp [W 1 rec ¬µ i + b 1 rec ] wwhere Œò rec = {œÜ R , W 1 rec , b 1 rec } denotes additional parameters.</formula><p>The reconstruction loss is given by the negative log likelihood</p><formula xml:id="formula_11">L rec (Œò src , Œò tgt , Œò shd , Œò rec ) = ‚àí i log p (w i |¬µ i )</formula><p>where we iterate over words w i in both the source and target utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Adversarial Domain Classification Loss</head><p>Ganin et al. <ref type="formula" target="#formula_12">(2016)</ref> propose introducing an ad- versarial loss to make shared features domain- invariant. This is motivated by a theoretical result of <ref type="bibr" target="#b2">Ben-David et al. (2007)</ref> who show that the gen- eralization error on the target domain depends on how "different" the source and the target domains are. This difference is approximately measured by</p><formula xml:id="formula_12">2 1 ‚àí 2 inf Œò error(Œò)<label>(2)</label></formula><p>where error(Œò) is the domain classification er- ror using model Œò. It is assumed that the source and target domains are balanced so that inf Œò error(Œò) ‚â§ 1/2 and the difference lies in <ref type="bibr">[0,</ref><ref type="bibr">2]</ref>. In other words, we want to make error(Œò) as large as possible in order to generalize well to the target domain. The intuition is that the more domain-invariant our features are, the easier it is to benefit from the source side training when test- ing on the target side. It can also be motivated as a regularization term ( <ref type="bibr" target="#b10">Ganin et al., 2016)</ref>. Let w 1 . . . w n ‚àà W be an utterance in domain d ‚àà {src, tgt}. We first use the shared encoder</p><formula xml:id="formula_13">(h shd 1 . . . h shd n ) ‚Üê BiLSTM Œò shd (w 1 . . . w n )</formula><p>It is important that we only use the shared encoder for this loss. Then we define the probability of domain d for the utterance as</p><formula xml:id="formula_14">z i = W 2 adv tanh W 1 adv n i=1 h shd i + b 1 adv + b 2 adv p(d|h i ) ‚àù exp ([z i ] d )</formula><p>where</p><formula xml:id="formula_15">Œò adv = {W 1 adv , W 2 adv , b 1 adv , b 2</formula><p>adv } denotes addi- tional feedfoward parameters. The adversarial do- main classification loss is given by the positive log likelihood</p><formula xml:id="formula_16">L adv (Œò shd , Œò adv ) = i log p d (i) |w (i)</formula><p>where we iterate over domain-annotated utter-</p><formula xml:id="formula_17">ances (w (i) , d (i) ).</formula><p>Random prediction training While past work only consider using a negative gradient ( <ref type="bibr" target="#b10">Ganin et al., 2016;</ref><ref type="bibr" target="#b3">Bousmalis et al., 2016</ref>) or positive log likelihood ( <ref type="bibr" target="#b41">Zhang et al., 2017</ref>) to perform ad- versarial training, it is unclear whether these ap- proaches are optimal for the purpose of "confus- ing" the domain predictor. For instance, mini- mizing log likelihood can lead to a model accu- rately predicting the opposite domain, compromis- ing the goal of inducing domain-invariant repre- sentations. Thus we propose to instead optimize the shared parameters for random domain predic- tions. Specifically, the above loss is replaced with</p><formula xml:id="formula_18">L adv (Œò shd , Œò adv ) = ‚àí i log p d (i) |w (i)</formula><p>where d (i) is set to be src with probability 0.5 and tgt with probability 0.5. By optimizing for random predictions, we achieve the desired effect: the shared parameters are trained to induce fea- tures that cannot discriminate between the source and the target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Non-Adversarial Domain Classification Loss</head><p>In addition to the adversarial loss for domain- invariant parameters, we also introduce a non- adversarial loss for domain-specific parameters. Given w 1 . . . w n ‚àà W in domain d ‚àà {src, tgt}, we use the private encoder</p><formula xml:id="formula_19">(h d 1 . . . h d n ) ‚Üê BiLSTM Œò d (w 1 . . . w n )</formula><p>It is important that we only use the private encoder for this loss. Then we define the probability of domain d for the utterance as</p><formula xml:id="formula_20">z i = W 2 nadv tanh W 1 nadv n i=1 h d i + b 1 nadv + b 2 nadv p(d|h i ) ‚àù exp ([z i ] d )</formula><p>where</p><formula xml:id="formula_21">Œò nadv = {W 1 nadv , W 2 nadv , b 1 nadv , b 2 nadv } denotes additional feedfoward parameters.</formula><p>The non- adversarial domain classification loss is given by the negative log likelihood</p><formula xml:id="formula_22">L nadv (Œò d , Œò nadv ) = i log p d (i) |w (i)</formula><p>where we iterate over domain-annotated utter- ances (w (i) , d (i) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Orthogonality Loss</head><p>Finally, following <ref type="bibr" target="#b3">Bousmalis et al. (2016)</ref>, we further encourage the domain-specific features to be mutually exclusive with the shared fea- tures by imposing soft orthogonality constraints. This is achieved as follows. Given an utterance w 1 . . . w n ‚àà W in domain d ‚àà {src, tgt}. We compute</p><formula xml:id="formula_23">(h d 1 . . . h d n ) ‚Üê BiLSTM Œò d (w 1 . . . w n ) (h shd 1 . . . h shd n ) ‚Üê BiLSTM Œò shd (w 1 . . . w n )</formula><p>The orthogonality loss for this utterance is given by</p><formula xml:id="formula_24">L orth (Œò src , Œò tgt , Œò shd ) = i (h d i ) h shd i</formula><p>where we iterate over words i in both the source and target utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Joint Objective</head><p>For unsupervised DA, we optimize</p><formula xml:id="formula_25">L unsup (Œò src , Œò tgt , Œò shd , Œò tag , Œò rec , Œò adv ) = L tag (Œò src , Œò shd , Œò tag ) + L rec (Œò src , Œò tgt , Œò shd , Œò rec ) + L adv (Œò shd , Œò adv ) + L nadv (Œò src , Œò nadv ) + L nadv (Œò tgt , Œò nadv ) + L orth (Œò src , Œò tgt , Œò shd )</formula><p>with respect to all model parameters. In an online setting, given an utterance we compute its recon- struction, adversarial, orthogonality, and tagging loss if in the source domain, and take a gradient step on the sum of these losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Supervised DA</head><p>In supervised domain adaptation, we assume la- beled data for both the source domain and the tar- get domain. We can easily incorporate supervision in the target domain by adding L tag (Œò tgt , Œò shd , Œò tag ) to the unsupervised DA objective:</p><formula xml:id="formula_26">L sup (Œò src , Œò tgt , Œò shd , Œò tag , Œò rec , Œò adv ) = L unsup (Œò src , Œò tgt , Œò shd , Œò tag , Œò rec , Œò adv ) + L tag (Œò tgt , Œò shd , Œò tag )<label>(3)</label></formula><p>We mention that the approach by <ref type="bibr" target="#b23">Kim et al. (2016c)</ref> is a special case of this objective; they op-</p><formula xml:id="formula_27">timize L sup2 (Œò src , Œò tgt , Œò shd , Œò tag ) =L tag (Œò src , Œò shd , Œò tag ) + L tag (Œò tgt , Œò shd , Œò tag )<label>(4)</label></formula><p>which is motivated as a neural extension of the fea- ture augmentation method of Daum√© III (2009).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conducted a series of exper- iments to evaluate the proposed techniques on datasets obtained from real usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Test Domains and Tasks</head><p>We test our approach on a suite of 5 Microsoft Cortana domains with 2 separate tasks in spoken language understanding: (1) intent classifica- tion and (2) slot (label) tagging. The intent classification task is a multi-class classification problem with the goal of determining to which one of the n intents a user utterance belongs conditioning on the given domain. The slot tagging task is a sequence labeling problem with the goal of identifying entities and chunking of useful information snippets in a user utterance.</p><p>For example, a user could say reserve a table at joeys grill for thursday at seven pm for five people. Then the goal of the first task would be to classify this utterance as MAKE RESERVATION intent given the domain PLACES, and the goal of the second task would be to tag joeys grill as RESTAURANT, thursday as DATE, seven pm as TIEM, and five as NUMBER PEOPLE.  <ref type="table" target="#tab_1">Table 1</ref>: The number of intents, the number of slots and a short description of the test domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We consider 2 possible domain adaptation (DA) scenarios: (1) adaptation of an engineered dataset to a live user dataset and (2) adaptation of an old dataset to a new dataset. For the first DA scenario, we test whether our approach can effectively make a system adapt from experimental, engineered data to real-world, live data. We use synthetic data which domain experts manually create based on a given domain schema 2 before the system goes live as the engineered data. We use transcribed dataset from users' speech input as the live user data. For the second scenario, we test whether our approach can effectively make a system adapt over time. A large number of users will quickly generate a large amount of data, and the usage pattern could also change. We use annotation data over 1 month in 2013 (more precisely August of 2013) as our old dataset, and use the whole data between 2014 and 2016 as our new dataset regardless of whether the data type is engineered or live user. As we describe in the earlier sections, we con- sider both supervised and unsupervised DA. We apply our DA approach with labeled data in the target domain for the supervised setting and with unlabeled data for the unsupervised one. We give details of the baselines and variants of our ap- proach below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised DA baselines and variants:</head><p>‚Ä¢ SRC: a single LSTM model trained on a source domain without DA techniques</p><p>‚Ä¢ DA W : an unsupervised DA model with a word-level decoder (i.e., re-generate each word independently)</p><p>‚Ä¢ DA S : an unsupervised DA model with a sentence-level decoder described in Section 3.2</p><p>Supervised DA baselines and variants:</p><p>‚Ä¢ SRC: a single LSTM model trained only on a source domain</p><p>‚Ä¢ TGT: a single LSTM model trained only on a target domain</p><p>‚Ä¢ Union: a single LSTM model trained on the union of source and target domains.</p><p>‚Ä¢ DA: a supervised DA model described in Section 3.3</p><p>‚Ä¢ DA A : DA with adversary domain training</p><p>‚Ä¢ DA U : DA with reasonably sufficient unla- beled data</p><p>In our experiments, all the models were imple- mented using Dynet ( <ref type="bibr" target="#b32">Neubig et al., 2017)</ref> and were trained using Stochastic Gradient Descent (SGD) with Adam ( <ref type="bibr" target="#b26">Kingma and Ba, 2015</ref>)-an adaptive learning rate algorithm. We used the ini- tial learning rate of 4 √ó 10 ‚àí4 and left all the other hyper parameters as suggested in <ref type="bibr" target="#b26">Kingma and Ba (2015)</ref>. Each SGD update was computed with- out a minibatch with Intel MKL (Math Kernel Li- brary) <ref type="bibr">3</ref> . We used the dropout regularization <ref type="bibr" target="#b37">(Srivastava et al., 2014</ref>) with the keep probability of 0.4. We encode user utterances with BiLSTMs as described in Section 3.1. We initialize word em- beddings with pre-trained embeddings used by <ref type="bibr" target="#b27">Lample et al. (2016)</ref>. In the following sections, we report intent classification results in accuracy percentage and slot results in F1-score. To com- pute slot F1-score, we used the standard CoNLL evaluation script <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results: Unsupervised DA</head><p>We first show our results in the unsupervised DA setting where we have a labeled dataset in the source domain, but only unlabeled data in the tar- get domain. We assume that the amount of data in both datasets is sufficient. Dataset statistics are shown in <ref type="table">Table 2</ref>.</p><p>The performance of the baselines and our model variants are shown in <ref type="table" target="#tab_3">Table 3</ref>. The left side of the table shows the results of the DA scenario of adapting from engineered data to live user data, and the baseline which trained only on the source domain (SRC) show a poor performance, yield- ing on average 48.5% on the intent classification and 42.7% F1-score on the slot tagging. Using our DA approach with a word-level decoder (DA W ) shows a significant increase in performance in all 5 test domains, yielding on average 82.2% intent ac- curacy and 80.5% slot F1-score. The performance increases further using the DA approach with a sentence-level decoder DA S , yielding on average 85.6% intent accuracy and 83.0% slot F1-score.</p><p>The right side of the table shows the results of the DA scenario of adapting from old to new data, and the baseline trained only on SRC also show <ref type="table" target="#tab_1">Domain  Train  Train*  Dev  Test  Train  Train*  Dev  Test  calendar  16904  50000  1878 10k  13165  13165 1463 10k  communication  32072  50000  3564 10k  12631  12631 1403 10k  places  23410  50000  2341 10k  21901  21901 2433 10k  reminder  19328  50000  1933 10k  16245  16245 1805 10k  weather  20794  50000  2079 10k  15575  15575 1731 10k  AVG  23590  50000  2359 10k  15903  15903 1767 10k   Table 2</ref>: Data statistics for unsupervised domain adaptation; In the first row, the columns are adaptation of engineered dataset to live user dataset, and and adaptation of old dataset to new dataset. In the second row, columns are domain, size of labeled training, unlabeled training, development and test sets. * denotes unlabeled data    intent accuracy and 85.7% slot F1-score. Simi- larly, the performance increases further with the DA S with 90.2% intent accuracy and 89.3% F1- score.</p><note type="other">Engineered ‚Üí Live User Old ‚Üí New</note><formula xml:id="formula_28">Engineered ‚Üí User Live Old ‚Üí New Task Domain SRC DA W DA S SRC DA W DA S Intent</formula><p>Our DA approach variants yield average error reductions of 72.04% and 79.71% for intent clas- sification and 70.33% and 80.99% for slot tag- ging. The results suggest that our DA approach can quickly make a model adapt from synthetic data to real-world data and from old data to new data with the additional use of only 2 to 2.5 more data from the target domain. Aside from the per- formance boost itself, the approach shows even more power since the new data from the target down do not need to be labeled and it only re- quires collecting a little more data from the tar- get domain. We note that the model development sets were created only from the source domain for a fully unsupervised setting. But having the de- velopment set from the target domain shows even more boost in performance although not shown in the results, and labeling only the development set from the target domain is relatively less expensive than labeling the whole dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results: Supervised DA</head><p>Second, we show our results in the supervised DA setting where we have a sufficient amount of labeled data in the source domain but relatively insufficient amount of labeled data in the target domain. Having more labeled data in the target domain would most likely help with the perfor- mance, but we intentionally made the setting more disadvantageous for our DA approach to better simulate real-world scenarios where there is usu- ally lack of resources and time to label a large amount of new data. For each personal assistant test domain, we only used 1000 training utter- ances to simulate scarcity of newly labeled data, and dataset statistics are shown in <ref type="table">Table 2</ref>. Unlike the unsupervised DA scenario, here we used the development sets created from the target domain shown in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>The left side of <ref type="table" target="#tab_5">Table 5</ref> shows the results of the supervised DA approach of adapting from engi- neered data to live user data. The baseline trained only on the source (SRC) shows on average 48.5% intent accuracy and 42.7% slot F1-score. Train- ing only on the target domain (TGT) increases the performance to 71.3% and 65.0%, but training on the union of the source and target domains (Union) again brings the performance down to 48.7% and 42.3%. As shown in the unsupervised setting, us- ing our DA approach (DA) shows significant per- formance increase in all 5 test domains, yielding on average 81.7% intent accuracy and 76.2% slot tagging. The DA approach with adversary domain training (DA A ) shows similar performance com- pared to that of DA, and performance shows more increase when using our DA approach with suf- ficient unlabeled data 5 (DA U ), yielding on aver- age 83.6% and 77.6%. For the second scenario of adapting from old to new dataset, the results show a very similar trend in performance.</p><p>The results show that our supervised DA (DA) approach also achieves a significant performance gain in all 5 test domains, yielding average error reductions of 68.18% and 51.35% for intent clas- sification and 60.90% and 50.09% for slot tagging. The results suggest that an effective domain adap- tation can be done using the supervised DA by having only a handful more data of 1k newly la- beled data points. In addition, having both a small amount of newly labeled data combined with suffi- cient unlabeled data can help the models perform even better. The poor performance of using the union of both source and target domain data might be due to the relatively very small size of the tar- get domain data, overwhelmed by the data in the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results: Adversarial Domain Classification Loss</head><p>Eng.  The impact on the performance of two different adversarial classification losses are shown in <ref type="table" target="#tab_7">Table  6</ref>. RAND represents the unsupervised DA model with sentence-level decoder (DA S ) using random prediction loss. The ADV shows the performance of same model using the adversarial loss of <ref type="bibr" target="#b10">Ganin et al. (2016)</ref> as described in 3.2.3. Unfortunately, in the deployment shift scenario, using the adver- sarial loss fails to provide any improvement on intent classification accuracy and slot tagging F1 score, achieving 82.5% intent accuracy and 79.8% slot F1 score. These results align with our hypoth- esis that the adversarial loss using does not con- fuse the classifier sufficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Proxy A-distance</head><p>Eng.  <ref type="table">Table 7</ref>: Proxy A-distance of resulting models: (1) engineered and live user dataset and (2) old and new dataset.</p><p>The results in shown in <ref type="table">Table 7</ref> show Proxy A- distance( <ref type="bibr" target="#b10">Ganin et al., 2016)</ref> to check if our ad- versary domain training generalize well to the tar- get domain. The distance between two datasets is computed byÀÜd byÀÜ byÀÜd A = 2(1 ‚àí 2 min {Œµ, 1 ‚àí Œµ})</p><p>where Œµ is a generalization error in discriminating between the source and target datasets. The range ofÀÜdofÀÜ ofÀÜd A distance is between 0 and 2.0. 0 is the best case where adversary training successfully fake shared encoder to predict do- mains. In other words, thanks to adversary train- ing our model make the domain-invariant features in shared encoder in order to generalize well to the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Vocabulary distance between engineered data and live user data</head><p>The results in shown in <ref type="table" target="#tab_10">Table 8</ref> show the discrep- ancy between two datasets. We measure the de- gree of overlap between vocabulary V employed Eng.  by the two datasets. We simply take the Jaccard coefficient between the two sets of such vocabu- lary:</p><formula xml:id="formula_30">d V (s, t) = 1 ‚àí JC(V s , V t ),</formula><p>where V s is the set of vocabulary in source s do- main, and V t is the corresponding set for target t domain and JC(A, B) = |A‚à©B| |A‚à™B| is the Jaccard coefficient, measuring the similarity of two sets. The distance d V is the high it means that they are not shared with many words. Overall, the distance between old and new dataset are still far and the number of overlapped are small, but better than live user case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have addressed two types of data shift common in SLU applications: 1. transferring from synthetic data to live user data (a deployment shift), and 2. transferring from stale data to cur- rent data (a temporal shift). Our method is based on domain adaptation, treating the flawed train- ing dataset as a source domain and the evaluation dataset as a target domain. We use and build on several recent advances in neural domain adapta- tion such as adversarial training and domain sep- aration network, proposing a new effective adver- sarial training scheme based on randomized pre- dictions. In both supervised and unsupervised adaptation scenarios, our approach yields clear im- provement over strong baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 gives</head><label>1</label><figDesc>a summary of the 5 test domains. We note that the domains have various levels of label granularity.</figDesc><table>Domain 
Intent Slot 
Description 
calendar 
23 
43 
Set appointments in calendar 
comm. 
38 
45 
Make calls &amp; send messages 
places 
35 
64 
Find locations &amp; directions 
reminder 
14 
35 
Remind tasks in a to-do list 
weather 
13 
19 
Get weather information 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Intent classification accuracy (%) and slot tagging F1-score (%) for the unsupervised domain 
adaptation. The results that perform in each domain are in bold font. 

Engineered ‚Üí Live User 
Old 
‚Üí 
New 
Domain 
Train 
Train* 
Dev Test 
Train 
Train* Dev Test 
calendar 
16904 
1000 
100 10k 
13165 
1000 
100 10k 
communication 
32072 
1000 
100 10k 
12631 
1000 
100 10k 
places 
23410 
1000 
100 10k 
21901 
1000 
100 10k 
reminder 
19328 
1000 
100 10k 
16245 
1000 
100 10k 
weather 
20794 
1000 
100 10k 
15575 
1000 
100 10k 
AVG 
23590 
1000 
100 10k 
15903 
1000 
100 10k 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Data statistics for supervised domain adaptation</head><label>4</label><figDesc></figDesc><table>Engineered ‚Üí User Live 
Old ‚Üí New 
Domain 
SRC TGT Union DA DA A DA U 
SRC TGT Union DA DA A DA U 

I 

calendar 
47.5 69.2 
48.3 
80.7 
80.5 
82.4 
50.7 69.2 
49.9 
74.4 
75.4 
75.8 
comm. 
45.8 
67.4 
47.0 
77.5 
78.0 
79.7 
49.4 65.8 
50.0 
70.2 
70.7 
71.9 
places 
48.5 71.2 
48.5 
82.0 
82.4 
83.2 
51.7 69.6 
52.2 
75.8 
76.4 
77.3 
reminder 50.7 
75.0 
49.9 
83.9 
84.1 
87.3 
53.3 72.3 
53.9 
77.2 
78.0 
78.5 
weather 
50.3 73.8 
49.6 
84.3 
84.7 
85.6 
53.4 71.4 
52.7 
76.9 
78.1 
79.2 
AVG 
48.5 
71.3 
48.7 
81.7 
81.9 
83.6 
51.7 69.7 
51.7 
74.9 
75.7 
76.5 

S 

calendar 
42.4 64.9 
43.0 
76.1 
76.7 
77.1 
42.2 61.8 
41.6 
68.0 
66.9 
69.3 
comm. 
41.1 
62.0 
40.4 
73.3 
72.1 
73.8 
41.5 61.1 
44.9 
67.2 
66.3 
68.4 
places 
40.2 61.8 
39.0 
72.1 
72.0 
72.9 
44.1 64.6 
47.7 
70.1 
68.7 
72.5 
reminder 42.6 
65.1 
42.6 
76.8 
75.7 
80.0 
47.4 70.9 
44.2 
78.4 
76.2 
78.9 
weather 
47.2 71.2 
46.4 
82.6 
83.0 
84.4 
43.2 64.1 
44.7 
71.0 
69.0 
70.2 
AVG 
42.7 
65.0 
42.3 
76.2 
75.9 
77.6 
43.7 64.5 
44.6 
71.0 
69.4 
71.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Intent classification accuracy (%) and slot tagging F1-score (%) for the supervised domain 
adaptation. 

a similar poor performance, yielding on average 
51.7% accuracy and 43.7% F1-score. DA W ap-
proach shows a significant performance increase 
in all 5 test domains, yielding on average 86.9% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Intent classification accuracy (%) and slot 
tagging F1-score (%) for the unsupervised domain 
adaptation with two different adversarial classifi-
cation losses -our claimed random domain pre-
dictions (RAND) and adversarial loss (ADVR) of 
Ganin et al. (2016) as explained in 3.2.3. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Distance between different datasets: (1) 
engineered and live user dataset and (2) old and 
new dataset. 

</table></figure>

			<note place="foot" n="2"> This is a semantic template that defines a set of intents and slots for each domain according to the intended functionality of the system.</note>

			<note place="foot" n="3"> https://software.intel.com/en-us/articles/intelr-mkl-andc-template-libraries 4 http://www.cnts.ua.ac.be/conll2000/chunking/output.html</note>

			<note place="foot" n="5"> This data is used for unsupervised DA experiments (Table 2).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task specific continuous word representations for mono and multi-lingual spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasos</forename><surname>Anastasakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical investigation of word class-based features for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>TASLP)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional neural network based semantic tagging with entity embeddings. genre</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silicon</forename><surname>Valley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkanitur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Zero-shot learning of intent embeddings for expansion by convolutional deep structured semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-T√ºr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno type="arXiv">arXiv:0907.1815</idno>
		<title level="m">Frustratingly easy domain adaptation</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Extending domain coverage of language understanding systems via intent transfer between domains using knowledge graphs and search query click logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>El-Kahky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zero-shot semantic parser for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Jabaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Lef√®vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran√ßois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint semantic utterance classification and slot filling with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-domain joint semantic frame parsing using bi-directional rnn-lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-T√ºr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 17th Annual Meeting of the International Speech Communication Association</title>
		<meeting>The 17th Annual Meeting of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Jaech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00117</idno>
		<title level="m">Domain adaptation of recurrent neural networks for natural language understanding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Triangular-chain conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary Geunbae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Weakly supervised slot tagging with partially labeled sequences from web search click logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL. Association for Computational Linguistics</title>
		<meeting>the NAACL. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural language model reusability for scaling to different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rochette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the Empiricial Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain attention with an ensemble of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchan</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compact lexicon selection with spectral methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pre-training of hidden-unit crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>of Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domainless adaptation by constrained decoding on a schema lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 26th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Frustratingly easy neural domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Computational Linguistics (COLING)</title>
		<meeting>the 26th International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable semi-supervised query classification using matrix sketching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 54th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>page 8</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">New transfer learning techniques for disparate label sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL. Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extracting structured information from user queries with semi-supervised conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 32nd international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint online spoken language understanding and language modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics</title>
		<meeting>the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics<address><addrLine>Los Angeles</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A discriminative model based entity dictionary weighting approach for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="195" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
	</analytic>
	<monogr>
		<title level="m">The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The technology powering personal digital assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Keynote at Interspeech</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shrinkage based features for slot tagging with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anoop Deoras, and Minwoo Jeong</title>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An overview of endto-end language understanding and dialog management for personal digital assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Crook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Philippe</forename><surname>Robichaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rochette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">Zia</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuahu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Spoken Language Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multitask learning for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICASSP</title>
		<meeting>the ICASSP<address><addrLine>Toulouse, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular crf for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A joint model of intent determination and slot filling for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Aspect-augmented adversarial networks for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00188</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
