<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incremental Predictive Parsing with TurboParser</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Köhn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fachbereich Informatik</orgName>
								<orgName type="institution">Universität Hamburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Menzel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fachbereich Informatik</orgName>
								<orgName type="institution">Universität Hamburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Incremental Predictive Parsing with TurboParser</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="803" to="808"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most approaches to incremental parsing either incur a degradation of accuracy or they have to postpone decisions, yielding underspecified intermediate output. We present an incremental predictive dependency parser that is fast, accurate, and largely language independent. By extending a state-of-the-art dependency parser, connected analyses for sentence prefixes are obtained, which even predict properties and the structural embedding of upcoming words. In contrast to other approaches, accuracy for complete sentence analyses does not decrease.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When humans communicate by means of a natu- ral language, utterances are not produced at once but evolve over time. Human interaction benefits from this property by processing yet unfinished utterances and reacting on them. Computational parsing on the other hand is mostly performed on complete sentences, a processing mode which ren- ders a responsive interaction based on incomplete utterances impossible.</p><p>When spoken language is analyzed, a mismatch between speech recognition and parsing occurs: If parsing does not work incrementally, the over- all system loses all the desirable properties made possible by incremental processing. For speech di- alogue systems, this leads to increased reaction times and an unnatural ping-pong style of interac- tion ( <ref type="bibr" target="#b11">Schlangen and Skantze, 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Desirable features of incremental parsers</head><p>Dependency parsing assigns a head and a depen- dency label to each word form of an input sentence and the resulting analysis of the sentence is usually required to form a tree. An incremental dependency parser processes a sentence word by word, building analyses for sentence prefixes (partial dependency analyses, PDA), which are extended and modified in a piecemeal fashion as more words become avail- able.</p><p>A PDA should come with three important (but partly contradictory) properties: beyond being ac- curate, it should also be as stable and informative as possible. Stability can be measured as the amount of structure (attachments and their labels) of a PDA a i which is also part of the analysis a n of the whole sentence. To be maximally informative, at least all available word forms should be integrated into the prefix PDA. Even such a simple requirement cannot easily be met without predicting a structural skele- ton for the word forms in the upcoming part of the sentence(bottom-up prediction). Other predictions merely serve to satisfy completeness conditions (i.e. valency requirements) in an anticipatory way (top-down predictions). In fact, humans are able to derive such predictions and they do so during sen- tence comprehension <ref type="bibr" target="#b12">(Sturt and Lombardo, 2005</ref>).</p><p>Without prediction, the sentence prefix "John drives a" of "John drives a car" can only be parsed as a disconnected structure:</p><p>John drives a SB J</p><p>The determiner remains unconnected to the rest of the sentence, because a possible head is not yet available. However, the determiner could be inte- grated into the PDA if the connection is established by means of a predicted word form, which has not yet been observed. <ref type="bibr" target="#b1">Beuck et al. (2011)</ref> propose to use virtual nodes (VNs) for this purpose. Each VN represents exactly one upcoming word. Its lexical instantiation and its exact position remain unspeci- fied. Using a VN, the prefix "John drives a" could then be parsed as follows, creating a fully con- nected analysis, which also satisfies the valency requirements of the finite verb.</p><p>John drives a <ref type="bibr">[VirtNoun]</ref> S B J DE T OB J This analysis is clearly more informative but still restricted to the existence of a noun filling the ob- ject role of "drives" without predicting its position. Although a VN does not specify the lexical identity of the word form it represents, it can nonetheless carry some information such as a coarse-grained part-of-speech category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related work</head><p>Parsers that produce incremental output are rel- atively rare: PLTag (Demberg-Winterfors, 2010) aims at psycholinguistic plausibility. It makes trade- offs in the field of accuracy and coverage (they report 6.2 percent of unparseable sentences on sen- tences of the Penn Treebank with less than 40 words). Due to its use of beam search, the incre- mental results are non-monotonic. <ref type="bibr" target="#b5">Hassan et al. (2009)</ref> present a CCG-based parser that can parse in an incremental mode. The parser guarantees that every parse of an increment extends the previ- ous parse monotonically. However, using the incre- mental mode without look-ahead, parsing accuracy drops from 86.70% to 59.01%. Obviously, insist- ing on strict monotonicity (a i ⊆ a n ) is too strong a requirement, since it forces the parser to keep attachments that later turn out to be clearly wrong in light of new evidence. Being a transition-based parser, Maltparser ( <ref type="bibr" target="#b10">Nivre et al., 2007</ref>) does incremental parsing by design. It is, however, not able to predict upcom- ing structure and therefore its incremental output is usually fragmented into several trees. In addition, Maltparser needs a sufficiently large look-ahead to achieve high accuracy <ref type="bibr" target="#b1">(Beuck et al., 2011</ref>).</p><p>Beuck et al. (2011) introduced incremental and predictive parsing using Weighted Constraint De- pendency Grammar. While their approach does not decrease in incremental mode, it is much slower than most other parsers. Another disadvantage is its hand-written grammar which prevents the parser from being adapted to additional languages by sim- ply training it on an annotated corpus and which makes it difficult to derive empirically valid con- clusions from the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Challenges for predictive parsing</head><p>Extending a dependency parser to incremental pars- ing with VNs introduces a significant shift in the problem to be solved: While originally the problem was where to attach each word to (1), in the incre- mental case the additional problem arises, which VNs to include into the analysis (2). Problem (2), however, depends on the syntactic structure of the sentence prefix. Therefore, it is not possible to de- termine the VNs before parsing commences, but the decision has to be made while parsing is going on. We can resolve this issue by transforming prob- lem (2) into problem (1) by providing the parser with an additional node, named unused. It is always attached to the special node 0 (the root node of ev- ery analysis) and it can only dominate VNs. unused and every VN it dominates are not considered part of the analysis. Using this idea, the problem of whether a VN should be included into the analysis is now reduced to the problem of where to attach that VN:</p><formula xml:id="formula_0">John drives a [VirtNoun] [VirtVerb] [unused] S B J D E T OBJ</formula><p>To enable the parser to include VNs into PDAs, a set of VNs has to be provided. While this set could include any number of VNs, we only in- clude a set that covers most cases of prediction since rare virtual nodes have a very low a-priori probability of being included and additional VNs make the parsing problem more complex. This set is language-dependent and has to be determined in advance. It can be obtained by generating PDAs from a treebank and counting the occurrences of VNs in them. Eventually, a set of VNs is used that is a super-set of a large enough percentage (&gt; 90%) of the observed sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gold annotations for sentence prefixes</head><p>Annotating sentence prefixes by hand is pro- hibitively costly because the number of increments is a multitude of the number of sentences in the corpus.  propose an ap- proach to automatically generate predictive depen- dency analyses from the annotation of full sen- tences. Their method tries to generate upper bounds for predictability which are relatively tight. There- fore, not everything that is deemed predictable by the algorithm is predictable in reality, but every- thing that is predictable should be deemed as pre- dictable: Let W be all tokens of the sentence and P the set of tokens that lie in the prefix for which an incremental analysis should be generated. A word w ∈ W \ P is assumed to be predictable (w ∈ Pr) if one of the following three criteria is met: bottom-up prediction w lies on the path from some w ∈ P to 0. E. g., given the sentence prefix "The", an upcoming noun and a verb is predicted:</p><p>The <ref type="bibr">[VirtNoun]</ref> [VirtVerb]</p><p>top down prediction π(w), the head of w, is in P ∪ Pr, and w fills a syntactic role -encoded by its dependency label -that is structurally determined. That means w can be predicted independently of the lexical identity of π(w). An example for this is the subject label: If π(w) is in Pr and w is its subject, w is assumed to be predictable.</p><p>lexical top-down prediction π(w) ∈ P and w fills a syntactic role that is determined by an already observed lexical item, e.g. the object role: If π(w) is a known verb and w is its object, w ∈ Pr because it is required by a valency of the verb.</p><p>While this procedure is language-independent, some language-specific transformations must be applied nonetheless. For English, parts of gapping coordinations can be predicted whereas others can not. For German, the transformations described in ) have been used with- out further changes. Both sets of structurally and lexically determined roles are language dependent. The label sets for German have been adopted from , while the sets for En- glish have been obtained by manually analyzing the PTB ( <ref type="bibr" target="#b8">Marcus et al., 1994)</ref> for predictability.</p><p>For words marked as predictable their existence and word class, but not their lexicalization and position can be predicted. Therefore, we replace the lexical item with " <ref type="bibr">[virtual]</ref>" and generalize the part-of-speech tag to a more coarse grained one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Predictive parsing with TurboParser</head><p>We adapt TurboParser ( <ref type="bibr" target="#b9">Martins et al., 2013</ref>) for incremental parsing because it does not impose structural constraints such as single-headedness in its core algorithm. For each parsing problem, it creates an integer linear program -in the form of a factor graph -with the variables representing the possible edges of the analyses.</p><p>Since well-formedness is enforced by factors, additional constraints on the shape of analyses can be imposed without changing the core algorithm of the parser. We define three additional restrictions with respect to VNs: 1) A VN that is attached to unused may not have any dependents. 2) A VN may not be attached to 0 if it has no dependents. 3) Only VNs may be attached to the unused node.</p><p>For a given sentence prefix, let A be the set of possible edges, V the set of all vertices, N ⊂ V the VNs and u ∈ V the unused node. Moreover, let B ⊂ A be the set of edges building a well-formed analysis and z a I(a ∈ B), where I(.) is the indica- tor function. The three additional conditions can be expressed as linear constraints which ensure that every output is a valid PDA:</p><formula xml:id="formula_1">z n, j + z u,n ≤ 1, n ∈ N, j ∈ V (1) z 0,n ≤ ∑ j∈V z n, j , n ∈ N (2) z u,i = 0, i ∈ V \ N<label>(3)</label></formula><p>The current implementation is pseudo-incremen- tal. It reinitializes the ILP for every increment with- out passing intermediate results from one incremen- tal processing step to the next, although this might be an option for further optimization.</p><p>High quality incremental parsing results can not be expected from models which have only been trained on whole-sentence annotations. If a parser is trained on gold-standard PDAs (generated as de- scribed in section 3), it would include every VN into every analysis because that data does not in- clude any non-attached VNs. We therefore add non- attached VNs to the generated PDAs until they contain at least the set of VNs that is later used during parsing. For instance, each German training increment contains at least one virtual verb and two virtual nouns and each English one at least one virtual verb and one virtual noun. This way, the per- centage of VNs of a specific type being attached in the training data resembles the a priori probability that a VN of that type should be included by the parser while parsing.</p><p>TurboParser is trained on these extended PDAs and no adaptation of the training algorithm is needed. The training data is heavily skewed be- cause words at the beginning of the sentences are more prevalent than the ones at the end. As a com- parison with a version trained on non-incremental data shows, this has no noticeable effect on the parsing quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>The usual methods to determine the quality of a dependency parser -labeled and unlabeled attach- ment scores (AS) -are not sufficient for the evalu- ation of incremental parsers. If the AS is computed for whole sentences, all incremental output is dis- carded and not considered at all. If every intermedi- ate PDA is used, words at the start of a sentence are counted more often than the ones at the end. No in- formation becomes available on how the accuracy of attachments evolves while parsing proceeds, and the prediction quality (i.e. the VNs) is completely ignored. Therefore, we adopt the enhanced mode of evaluation proposed by : In addition to the accuracy for whole sentences, the accuracies of the n newest words of each analy- sis are computed. This yields a curve that shows how good a word can be assumed to be attached depending on its distance to the most recent word.</p><p>Let V, G be the gold standard analysis of an increment and V , P the corresponding parser out- put. V and V are the vertices and G and P the respective edges of the analyses. Let V p and V v be the in-prefix and virtual subset of V , respectively. To evaluate the prediction capabilities of a parser, for each increment an optimal partial, surjective mapping 1 V → V from the output produced by the parser to the (automatically generated) gold stan- dard is computed, where each non-virtual element of V has to be mapped to the corresponding ele- ment in V . Let M be the set of all such mappings. Then the best mapping is defined as follows:</p><formula xml:id="formula_2">φ = arg max m∈M ∑ w∈V I(π(m(w)) = m(π(w))) 1</formula><p>The mapping is partial because for some VNs in V there might be no corresponding VN in the gold standard.</p><p>We define a word w as correctly attached (ignor- ing the label) if π(φ (w)) = φ (π(w)). In an incre- mental analysis, an attachment of a word w can be classified into four cases:</p><formula xml:id="formula_3">correct π(φ (w)) = φ (π(w)), π(w) ∈ V p corr. pred. π(φ (w)) = φ (π(w)), π(w) ∈ V v wrong pred. π(φ (w)) = φ (π(w)), π(w) ∈ V v wrong π(φ (w)) = φ (π(w)), π(w) ∈ V p</formula><p>We can count the number of VNs that have been correctly attached: Let T be the set of all analyses produced by the parser and φ t the best mapping as defined above for each t ∈ T . Furthermore, let vn(t) be the set of VNs in t. The total number of correct predictions of VNs is then defined as:</p><formula xml:id="formula_4">corr = ∑ t∈T ∑ v∈vn(t) I(π(φ t (v)) = φ t (π(v)))</formula><p>Precision and recall for the prediction with VNs can be computed by dividing corr by the number of predicted VNs and the number of VNs in the gold standard, respectively.</p><p>Evaluation has been carried out on the PTB con- verted to dependency structure using the LTH con- verter ( <ref type="bibr" target="#b6">Johansson and Nugues, 2007</ref>) and on the Hamburg Dependency Treebank ( <ref type="bibr" target="#b4">Foth et al., 2014</ref>). From both corpora predictive PDAs padded with unused virtual nodes have been created for training. For English, the sentences of part 1-9 of the PTB were used, for German the first 50,000 sentences of the HDT have been selected. Testing was done using one virtual noun and one virtual verb for En- glish and two virtual nouns and one virtual verb for German because these sets cover about 90% of the prefixes in both training sets. <ref type="figure" target="#fig_0">Figure 1</ref> shows the evaluation results for pars- ing German and English using TurboParser. For both languages the attachment accuracy rises with the amount of context available. The difference be- tween the attachment accuracy of the most recent word (relative time point 0, no word to the right of it) and the second newest word (time point 1) is strongest, especially for English. The word five elements left of the newest word (time point 5) gets attached with an accuracy that is nearly as high as the accuracy for the whole sentence (final).</p><p>The types of errors made for German and En- glish are similar. For both German and English the unlabeled precision reaches more than 70% (see <ref type="table">Table 1</ref>). Even the correct dependency label of up- coming words can be predicted with a fairly high precision. TurboParser parses an increment in about 0.015 seconds, which is much faster than WCDG    . The prediction recall is higher for English than for German which could be due to the differences in gold-standard annotation. Training TurboParser on the non-incremental data sets results in a labeled whole-sentence accu- racy of 93.02% for German.The whole-sentence accuracy for parsing with VNs is 93.33%. This shows that the additional mechanism of VNs has no negative effects on the overall parsing quality.</p><p>To compare TurboParser and WCDG running both in the predictive incremental mode, we use jwcdg, the current implementation of this approach. jwcdg differs from most other parsers in that it does not act on pre-tagged data but runs an exter- nal tagger itself in a multi-tag mode. To compare both systems, TurboParser needs to be run in a tagger-parser pipeline. We have chosen TurboTag- ger without look-ahead for this purpose. Running TurboParser in this pipeline leads to only slightly worse results compared to the use of gold-standard tags (see <ref type="figure" target="#fig_1">Figure 2</ref>). TurboParser's attachment ac- curacy is about ten percentage points better than jwcdg's across the board. In addition, its VN pre- diction is considerably better.</p><p>To measure the stability, let P i be a prefix of the sentence P n and a i and a n be the corresponding analyses produced by the parser. An attachment of a word w ∈ P i is stable if either w's head is the same in a i and a n or w's head is not part of P i in both a i and a n . The second part covers the case where the parser predicts the head of w to lie in the future and it really does, according to the final parse. <ref type="table" target="#tab_1">Table 2</ref> shows the attachment stability of the newest word at time point 0 compared to the word five positions to the left of time point 0. TurboParser's stability turns out to be much higher than jwcdg's: For German  report a stability of only 80% at the most recent word. Interestingly, labeling the newest attachment for English seems to be much harder than for German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Using a parser based on ILP, we were able to an- alyze sentences incrementally and produce con- nected dependency analyses at every point in time. The intermediate structures produced by the parser are highly informative, including predictions for properties and structural embeddings of upcom- ing words. In contrast to previous approaches, we achieve state-of-the-art accuracy for whole sen- tences by abandoning strong monotonicity and aim at high stability instead, allowing the parser to im- prove intermediate results in light of new evidence.</p><p>The parser is trained on treebank data for whole sentences from which prefix annotations are de- rived in a fully automatic manner. To guide this process, a specification of structurally and lexically determined dependency relations and some addi- tional heuristics are needed. For parsing, only a set of possible VNs has to be provided. These are the only language specific components required. There- fore, the approach can be ported to other languages with quite modest effort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Results for TurboParser for German and English with gold standard PoS (labeled)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Results for TurboParser and jwcdg for German with tagger (labeled).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>English 
German 
German&amp;tagger 
German (jwcdg) 

labeled unlabeled labeled unlabeled labeled unlabeled labeled unlabeled 
precision 75.47% 
78.55% 
67.42% 
75.90% 
65.21% 
73.39% 
32.95% 
42.23% 
recall 57.92% 
60,29% 
46.77% 
52.65% 
45.79% 
51.54% 
35.90% 
46.00% 

Table 1: Precision and recall for the prediction of virtual nodes 

time point 0 
time point 5 

unlabeled labeled unlabeled labeled 
En 
89.28% 
84.92% 
97.32% 
97.11% 
De 
90.91% 
88.96% 
96.11% 
95.65% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Stability measures 

where about eight seconds per word are needed to 
achieve a good accuracy (</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structural prediction in incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><surname>Beuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Menzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<editor>Alexander Gelbukh</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">7816</biblScope>
			<biblScope unit="page" from="245" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incremental parsing and the evaluation of partial dependency analyses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><surname>Beuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Köhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Menzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Dependency Linguistics. Depling</title>
		<meeting>the 1st International Conference on Dependency Linguistics. Depling</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Predictive incremental parsing and its evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><surname>Beuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Köhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Menzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Dependency Theory</title>
		<editor>Kim Gerdes, Eva Hajičová, and Leo Wanner</editor>
		<imprint>
			<publisher>IOS press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">258</biblScope>
			<biblScope unit="page" from="186" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A Broad-Coverage Model of Prediction in Human Sentence Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vera Demberg-Winterfors</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Hamburg Dependency Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><surname>Foth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Beuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Köhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Menzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Resources and Evaluation Conference 2014. LREC, European Language Resources Association (ELRA)</title>
		<meeting>the Language Resources and Evaluation Conference 2014. LREC, European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lexicalized semi-incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference RANLP-2009</title>
		<meeting>the International Conference RANLP-2009<address><addrLine>Borovets, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="128" to="134" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extended constituent-to-dependency conversion for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NODALIDA 2007</title>
		<meeting>NODALIDA 2007<address><addrLine>Tartu, Estonia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-05-25" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incremental and predictive dependency parsing under real-time conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Köhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Menzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference Recent Advances in Natural Language Processing RANLP 2013</title>
		<meeting>the International Conference Recent Advances in Natural Language Processing RANLP 2013</meeting>
		<imprint>
			<date type="published" when="2013-09" />
			<biblScope unit="page" from="373" to="381" />
		</imprint>
	</monogr>
	<note>Hissar, Bulgaria. INCOMA Ltd. Shoumen, BULGARIA</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Penn Treebank: Annotating predicate argument structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Britta</forename><surname>Schasberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Language Technology, HLT &apos;94</title>
		<meeting>the Workshop on Human Language Technology, HLT &apos;94<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="114" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order nonprojective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maltparser: A language-independent system for data-driven dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atanas</forename><surname>Chanev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsen</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetoslav</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="135" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A general, abstract model of incremental dialogue processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davin</forename><surname>Schlangen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Skantze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue and Discourse</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="111" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Processing coordinated structures: Incrementality and connectedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Sturt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Lombardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="291" to="305" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
