<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple and Effective Multi-Paragraph Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
							<email>mattg@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Simple and Effective Multi-Paragraph Reading Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="845" to="855"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>845</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Teaching machines to answer arbitrary user- generated questions is a long-term goal of natural language processing. For a wide range of ques- tions, existing information retrieval methods are capable of locating documents that are likely to contain the answer. However, automatically ex- tracting the answer from those texts remains an open challenge. The recent success of neural mod- els at answering questions given a related para- graph ( <ref type="bibr" target="#b29">Wang et al., 2017c;</ref><ref type="bibr" target="#b24">Tan et al., 2017</ref>) sug- gests they have the potential to be a key part of * Work completed while interning at the Allen Institute for Artificial Intelligence a solution to this problem. Most neural models are unable to scale beyond short paragraphs, so typically this requires adapting a paragraph-level model to process document-level input.</p><p>There are two basic approaches to this task. Pipelined approaches select a single paragraph from the input documents, which is then passed to the paragraph model to extract an answer ( <ref type="bibr" target="#b15">Joshi et al., 2017;</ref><ref type="bibr" target="#b27">Wang et al., 2017a</ref>). Confidence based methods apply the model to multiple para- graphs and return the answer with the highest con- fidence ( <ref type="bibr" target="#b3">Chen et al., 2017a</ref>). Confidence meth- ods have the advantage of being robust to errors in the (usually less sophisticated) paragraph selec- tion step, however they require a model that can produce accurate confidence scores for each para- graph. As we shall show, naively trained models often struggle to meet this requirement.</p><p>In this paper we start by proposing an improved pipelined method which achieves state-of-the-art results. Then we introduce a method for training models to produce accurate per-paragraph confi- dence scores, and we show how combining this method with multiple paragraph selection further increases performance.</p><p>Our pipelined method focuses on addressing the challenges that come with training on document- level data. We use a linear classifier to select which paragraphs to train and test on. Since an- notating entire documents is expensive, data of this sort is typically distantly supervised, mean- ing only the answer text, not the answer spans, are known. To handle the noise this creates, we use a summed objective function that marginal- izes the model's output over all locations the an- swer text occurs. We apply this approach with a model design that integrates some recent ideas in reading comprehension models, including self- attention ( <ref type="bibr" target="#b5">Cheng et al., 2016</ref>) and bi-directional at- tention ( <ref type="bibr" target="#b22">Seo et al., 2016</ref>).</p><p>Our confidence method extends this approach to better handle the multi-paragraph setting. Pre- vious approaches trained the model on questions paired with paragraphs that are known a priori to contain the answer. This has several downsides: the model is not trained to produce low confidence scores for paragraphs that do not contain an an- swer, and the training objective does not require confidence scores to be comparable between para- graphs. We resolve these problems by sampling paragraphs from the context documents, includ- ing paragraphs that do not contain an answer, to train on. We then use a shared-normalization ob- jective where paragraphs are processed indepen- dently, but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document. This requires the model to produce globally correct output even though each paragraph is processed independently.</p><p>We evaluate our work on TriviaQA ( <ref type="bibr" target="#b15">Joshi et al., 2017</ref>) in the wiki, web, and unfiltered setting. Our model achieves a nearly 10 point lead over published prior work. We additionally perform an ablation study on our pipelined method, and we show the effectiveness of our multi-paragraph methods on a modified version of SQuAD ( <ref type="bibr" target="#b21">Rajpurkar et al., 2016)</ref> where only the correct docu- ment, not the correct paragraph, is known. Finally, we combine our model with a web search backend to build a demonstration end-to-end QA system 1 , and show it performs well on questions from the TREC question answering task ( <ref type="bibr" target="#b25">Voorhees et al., 1999</ref>). We release our code 2 to facilitate future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pipelined Method</head><p>In this section we propose a pipelined QA system, where a single paragraph is selected and passed to a paragraph-level question answering model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Paragraph Selection</head><p>If there is a single source document, we select the paragraph with the smallest TF-IDF cosine dis- tance with the question. Document frequencies are computed using the individual paragraphs within the document. If there are multiple input docu- ments, we found it beneficial to use a linear clas- sifier that uses the same TF-IDF score, whether the paragraph was the first in its document, how many tokens preceded it, and the number of ques- tion words it includes as features. The classifier is trained on the distantly supervised objective of se- lecting paragraphs that contain at least one answer span. On TriviaQA web, relative to truncating the document as done by prior work, this improves the chance of the selected text containing the correct answer from 83.1% to 85.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Handling Noisy Labels</head><p>Question: Which British general was killed at Khartoum in 1885? Answer: Gordon Context: In February 1885 Gordon returned to the Sudan to evacuate Egyptian forces. Khartoum came under siege the next month and rebels broke into the city, killing Gor- don and the other defenders. The British public reacted to his death by acclaiming 'Gordon of Khartoum', a saint. However, historians have suggested that Gordon... <ref type="figure">Figure 1</ref>: Noisy supervision can cause many spans of text that contain the answer, but are not situated in a context that relates to the question (red), to distract the model from learning from more rele- vant spans (green).</p><p>In a distantly supervised setup we label all text spans that match the answer text as being correct. This can lead to training the model to select un- wanted answer spans. <ref type="figure">Figure 1</ref> contains an exam- ple. To handle this difficulty, we use a summed objective function similar to the one from <ref type="bibr" target="#b16">Kadlec et al. (2016)</ref>, that optimizes the negative log- likelihood of selecting any correct answer span. The models we consider here work by indepen- dently predicting the start and end token of the an- swer span, so we take this approach for both pre- dictions. For example, the objective for predicting the answer start token becomes − log a∈A p a where A is the set of tokens that start an answer and p i is the answer-start probability predicted by the model for token i. This objective has the ad- vantage of being agnostic to how the model dis- tributes probability mass across the possible an- swer spans, allowing the model to focus on only the most relevant spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model</head><p>We use a model with the following layers (shown in <ref type="figure" target="#fig_0">Figure 2</ref>):</p><p>Embedding: We embed words using pre- trained word vectors. We concatenate these with character-derived word embeddings, which are produced by embedding characters using a learned embedding matrix and then applying a convolu- tional neural network and max-pooling.</p><p>Pre-Process: A shared bi-directional GRU ( <ref type="bibr" target="#b6">Cho et al., 2014</ref>) is used to process the question and passage embeddings.</p><p>Attention: The attention mechanism from the Bi-Directional Attention Flow (BiDAF) model ( <ref type="bibr" target="#b22">Seo et al., 2016</ref>) is used to build a query- aware context representation. Let h i and q j be the vector for context word i and question word j, and n q and n c be the lengths of the question and context respectively. We compute attention between context word i and question word j as:</p><formula xml:id="formula_0">a ij = w 1 · h i + w 2 · q j + w 3 · (h i q j )</formula><p>where w 1 , w 2 , and w 3 are learned vectors and is element-wise multiplication. We then compute an attended vector c i for each context token as:</p><formula xml:id="formula_1">p ij = e a ij nq j=1 e a ij c i = nq j=1 q j p ij</formula><p>We also compute a query-to-context vector q c :</p><formula xml:id="formula_2">m i = max 1≤j≤nq a ij p i = e m i nc i=1 e m i q c = nc i=1 h i p i</formula><p>The final vector for each token is built by con- catenating h i , c i , h i c i , and q c c i . In our model we subsequently pass the result through a linear layer with ReLU activations.</p><p>Self-Attention: Next we use a layer of residual self-attention. The input is passed through another bi-directional GRU. Then we apply the same at- tention mechanism, only now between the passage and itself. In this case we do not use query-to- context attention and we set a ij = −inf if i = j.</p><p>As before, we pass the concatenated output through a linear layer with ReLU activations. The result is then summed with the original input.</p><p>Prediction: In the last layer of our model a bi- directional GRU is applied, followed by a linear layer to compute answer start scores for each to- ken. The hidden states are concatenated with the input and fed into a second bi-directional GRU and linear layer to predict answer end scores. The soft- max function is applied to the start and end scores to produce answer start and end probabilities.</p><p>Dropout: We apply variational dropout ( <ref type="bibr" target="#b9">Gal and Ghahramani, 2016)</ref> to the input to all the GRUs and the input to the attention mechanisms at a rate of 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Confidence Method</head><p>We adapt this model to the multi-paragraph setting by using the un-normalized and un-exponentiated (i.e., before the softmax operator is applied) score given to each span as a measure of the model's confidence. For the boundary-based models we use here, a span's score is the sum of the start and end score given to its start and end token. At test time we run the model on each paragraph and se- lect the answer span with the highest confidence. This is the approach taken by <ref type="bibr" target="#b3">Chen et al. (2017a)</ref>.</p><p>Our experiments in Section 5 show that these confidence scores can be very poor if the model is only trained on answer-containing paragraphs, as done by prior work. <ref type="table">Table 1</ref> contains some quali- tative examples of the errors that occur.</p><p>We hypothesize that there are two key sources of error. First, for models trained with the soft- max objective, the pre-softmax scores for all spans can be arbitrarily increased or decreased by a con- stant value without changing the resulting softmax probability distribution. As a result, nothing pre- vents models from producing scores that are arbi- trarily all larger or all smaller for one paragraph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Low Confidence Correct Extraction High Confidence Incorrect Extraction</head><p>When is the Members Debate held?</p><p>Immediately after Decision Time a "Mem- bers Debate" is held, which lasts for 45 min- utes...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>...majority of the Scottish electorate voted for it in a referendum to be held on 1 March</head><p>1979 that represented at least...</p><p>How many tree species are in the rainforest?</p><p>...one 2001 study finding a quarter square kilometer (62 acres) of Ecuadorian rainforest supports more than 1,100 tree species</p><p>The affected region was approximately 1,160,000 square miles (3,000,000 km2) of rainforest, compared to 734,000 square miles Who was Warsz?</p><p>....In actuality, Warsz was a 12th/13th century nobleman who owned a village located at the modern....</p><p>One of the most famous people born in War- saw was Maria Sklodowska -Curie, who achieved international...</p><p>How much did the ini- tial LM weight in kg?</p><p>The initial LM model weighed approximately 33,300 pounds (15,000 kg), and...</p><p>The module was 11.42 feet (3.48 m) tall, and weighed approximately 12,250 pounds (5,560 kg) <ref type="table">Table 1</ref>: Examples from SQuAD where a model was less confident in a correct extraction from one paragraph (left) than in an incorrect extraction from another (right). Even if the passage has no correct answer and does not contain any question words, the model assigns high confidence to phrases that match the category the question is asking about. Because the confidence scores are not well-calibrated, this confidence is often higher than the confidence assigned to correct answer spans in different paragraphs, even when those correct spans have better contextual evidence.</p><p>than another. Second, if the model only sees para- graphs that contain answers, it might become too confident in heuristics or patterns that are only ef- fective when it is known a priori that an answer exists. For example, the model might become too reliant on selecting answers that match semantic type the question is asking about, causing it be eas- ily distracted by other entities of that type when they appear in irrelevant text. This kind of error has also been observed when distractor sentences are added to the context (Jia and Liang, 2017) We experiment with four approaches to training models to produce comparable confidence scores, shown in the following subsections. In all cases we will sample paragraphs that do not contain an answer as additional training points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Shared-Normalization</head><p>In this approach a modified objective function is used where span start and end scores are normal- ized across all paragraphs sampled from the same context. This means that paragraphs from the same context use a shared normalization factor in the final softmax operations. We train on this ob- jective by including multiple paragraphs from the same context in each mini-batch. The key idea is that this will force the model to produce scores that are comparable between paragraphs, even though it does not have access to information about what other paragraphs are being considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Merge</head><p>As an alternative to the previous method, we ex- periment with concatenating all paragraphs sam- pled from the same context together during train- ing. A paragraph separator token with a learned embedding is added before each paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">No-Answer Option</head><p>We also experiment with allowing the model to se- lect a special "no-answer" option for each para- graph. First we re-write our objective as:</p><formula xml:id="formula_3">− log e sa n i=1 e s i − log e g b n j=1 e g j = − log e sa+g b n i=1 n j=1 e s i +g j</formula><p>where s j and g j are the scores for the start and end bounds produced by the model for token j, and a and b are the correct start and end tokens. We have the model compute another score, z, to represent the weight given to a "no-answer" possibility. Our revised objective function becomes:</p><formula xml:id="formula_4">− log (1 − δ)e z + δe sa+g b e z + n i=1 n j=1 e s i +g j</formula><p>where δ is 1 if an answer exists and 0 otherwise. If there are multiple answer spans we use the same objective, except the numerator includes the sum- mation over all answer start and end tokens. We compute z by adding an extra layer at the end of our model. We build input vectors by tak- ing the summed hidden states of the RNNs used to predict the start/end token scores weighed by the start/end probabilities, and using a learned atten- tion vector on the output of the self-attention layer.</p><p>These vectors are fed into a two layer network with an 80 dimensional hidden layer and ReLU activa- tions that produces z as its only output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sigmoid</head><p>As a final baseline, we consider training models with the sigmoid loss objective function. That is, we compute a start/end probability for each token by applying the sigmoid function to the start/end scores of each token. A cross entropy loss is used on each individual probability. The intuition is that, since the scores are being evaluated indepen- dently of one another, they are more likely to be comparable between different paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our approach on four datasets: Triv- iaQA unfiltered ( <ref type="bibr" target="#b15">Joshi et al., 2017)</ref>, a dataset of questions from trivia databases paired with docu- ments found by completing a web search of the questions; TriviaQA wiki, the same dataset but only including Wikipedia articles; TriviaQA web, a dataset derived from TriviaQA unfiltered by treating each question-document pair where the document contains the question answer as an in- dividual training point; and SQuAD ( <ref type="bibr" target="#b21">Rajpurkar et al., 2016)</ref>, a collection of Wikipedia articles and crowdsourced questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Preprocessing</head><p>We note that for TriviaQA web we do not sub- sample as was done by <ref type="bibr" target="#b15">Joshi et al. (2017)</ref>, instead training on the all 530k training examples. We also observe that TriviaQA documents often con- tain many small paragraphs, so we restructure the documents by merging consecutive paragraphs to- gether up to a target size. We use a maximum para- graph size of 400 unless stated otherwise. Para- graph separator tokens with learned embeddings are added between merged paragraphs to preserve formatting information. We are also careful to mark all spans of text that would be considered an exact match by the official evaluation script, which includes some minor text pre-processing, as an- swer spans, not just spans that are an exact string match with the answer text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sampling</head><p>Our confidence-based approaches are trained by sampling paragraphs from the context during training. For SQuAD and TriviaQA web we take  <ref type="table">Table 2</ref>: Results on TriviaQA web using our pipelined method. the top four paragraphs as judged by our paragraph ranking function (see Section 2.1). We sample two different paragraphs from those four each epoch to train on. Since we observe that the higher- ranked paragraphs are more likely to contain the context needed to answer the question, we sample the highest ranked paragraph that contains an an- swer twice as often as the others. For the merge and shared-norm approaches, we additionally re- quire that at least one of the paragraphs contains an answer span, and both of those paragraphs are included in the same mini-batch. For TriviaQA wiki we repeat the process but use the top 8 para- graphs, and for TriviaQA unfiltered we use the top 16, because much more context is given in these settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation</head><p>We train the model with the Adadelta opti- mizer (Zeiler, 2012) with a batch size 60 for Triv- iaQA and 45 for SQuAD. At test time we select the most probable answer span of length less than or equal to 8 for TriviaQA and 17 for SQuAD. The GloVe 300 dimensional word vectors released by <ref type="bibr" target="#b20">Pennington et al. (2014)</ref> are used for word em- beddings. On SQuAD, we use a dimensionality of size 100 for the GRUs and of size 200 for the linear layers employed after each attention mecha- nism. We found for TriviaQA, likely because there is more data, using a larger dimensionality of 140 for each GRU and 280 for the linear layers is bene- ficial. During training, we maintain an exponential moving average of the weights with a decay rate of 0.999. We use the weight averages at test time. We do not update the word vectors during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">TriviaQA Web and TriviaQA Wiki</head><p>First, we do an ablation study on TriviaQA web to show the effects of our proposed methods for our pipeline model. We start with a baseline fol- lowing the one used by <ref type="bibr" target="#b15">Joshi et al. (2017)</ref>   <ref type="figure">Figure 3</ref>: Results on TriviaQA web when apply- ing our models to multiple paragraphs from each document. Most of our training methods improve the model's ability to utilize more text. system uses BiDAF ( <ref type="bibr" target="#b22">Seo et al., 2016)</ref> as the para- graph model, and selects a random answer span from each paragraph each epoch to train on. The first 400 tokens of each document are used during training, and the first 800 during testing. When using the TF-IDF paragraph selection approach, we instead break the documents into paragraphs of size 400 when training and 800 when testing, and select the top-ranked paragraph to feed into the model. As shown in <ref type="table">Table 2</ref>, our baseline out- performs the results reported by <ref type="bibr" target="#b15">Joshi et al. (2017)</ref> significantly, likely because we are not subsam- pling the data. We find both TF-IDF ranking and the sum objective to be effective. Using our re- fined model increases the gain by another 4 points.</p><p>Next we show the results of our confidence- based approaches. For this comparison we split documents into paragraphs of at most 400 to- kens, and rank them using TF-IDF cosine distance. Then we measure the performance of our proposed approaches as the model is used to independently process an increasing number of these paragraphs, and the highest confidence answer is selected as the final output. The results are shown in <ref type="figure">Figure 3</ref>.</p><p>On this dataset even the model trained without any of the proposed training methods ("none") im- proves as more paragraphs are used, showing it does a passable job at focusing on the correct para- graph. The no-answer option training approach lead to a significant improvement, and the shared- norm and merge approaches are even better.</p><p>We use the shared-norm approach for evalua- tion on the TriviaQA test sets. We found that in- creasing the paragraph size to 800 at test time, and to 600 during training, was slightly beneficial, al- lowing our model to reach 66.04 EM and 70.98 F1 on the dev set. As shown in <ref type="table" target="#tab_2">Table 3</ref>, our model is firmly ahead of prior work on both the TriviaQA web and TriviaQA wiki test sets. Since our sub- mission, a few additional entries have been added to the public leader for this dataset 5 , although to the best of our knowledge these results have not yet been published.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TriviaQA Unfiltered</head><p>Next we apply our confidence methods to Trivi- aQA unfiltered. This dataset is of particular inter- est because the system is not told which document contains the answer, so it provides a plausible sim- ulation of answering a question using a document  <ref type="figure">Figure 5</ref>: Results for our confidence methods on document-level SQuAD. The shared-norm model is the only model that does not lose performance when exposed to large numbers of paragraphs.</p><p>retrieval system. We show the same graph as be- fore for this dataset in <ref type="figure" target="#fig_1">Figure 4</ref>. Our methods have an even larger impact on this dataset, probably be- cause there are many more relevant and irrelevant paragraphs for each question, making paragraph selection more important.</p><p>Note the naively trained model starts to lose performance as more paragraphs are used, show- ing that errors are being caused by the model be- ing overly confident in incorrect extractions. We achieve a score of 61.55 EM and 67.61 F1 on the dev set. This advances the only prior result re- ported for this dataset, 50.6 EM and 57.3 F1 from <ref type="bibr" target="#b28">Wang et al. (2017b)</ref>, by 10 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SQuAD</head><p>We additionally evaluate our model on SQuAD. SQuAD questions were not built to be answered independently of their context paragraph, which makes it unclear how effective of an evaluation tool they can be for document-level question an- swering. To assess this we manually label 500 ran- dom questions from the training set.</p><p>We categorize questions as:</p><p>1. Context-independent, meaning it can be un- derstood independently of the paragraph. 2. Document-dependent, meaning it can be un- derstood given the article's title. For exam- ple, "What individual is the school named af- ter?" for the document "Harvard University". 3. Paragraph-dependent, meaning it can only be understood given its paragraph. For example, "What was the first step in the reforms?".</p><p>We find 67.4% of the questions to be context- independent, 22.6% to be document-dependent, and the remaining 10% to be paragraph- dependent. There are many document-dependent questions because questions are frequently about the subject of the document. Since a reasonably high fraction of the questions can be understood given the document they are from, and to isolate our analysis from the retrieval mechanism used, we choose to evaluate on the document-level. We build documents by concatenating all the para- graphs in SQuAD from the same article together into a single document.</p><p>Given the correct paragraph (i.e., in the standard SQuAD setting) our model reaches 72.14 EM and 81.05 F1 and can complete 26 epochs of training in less than five hours. Most of our variations to handle the multi-paragraph setting caused a minor (up to half a point) drop in performance, while the sigmoid version fell behind by a point and a half.</p><p>We graph the document-level performance in <ref type="figure">Figure 5</ref>. For SQuAD, we find it crucial to em- ploy one of the suggested confidence training tech- niques. The base model starts to drop in perfor- mance once more than two paragraphs are used. However, the shared-norm approach is able to reach a peak performance of 72.37 F1 and 64.08 EM given 15 paragraphs. Given our estimate that 10% of the questions are ambiguous if the para- graph is unknown, our approach appears to have adapted to the document-level task very well.</p><p>Finally, we compare the shared-norm model with the document-level result reported by <ref type="bibr" target="#b3">Chen et al. (2017a)</ref>. We re-evaluate our model using the documents used by <ref type="bibr" target="#b3">Chen et al. (2017a)</ref>, which consist of the same Wikipedia articles SQuAD was built from, but downloaded at different dates. The advantage of this dataset is that it does not allow the model to know a priori which paragraphs were filtered out during the construction of SQuAD. The disadvantage is that some of the articles have been edited since the questions were written, so some questions may no longer be answerable. Our model achieves 59.14 EM and 67.34 F1 on this dataset, which significantly outperforms the 49.7 EM reported by <ref type="bibr" target="#b3">Chen et al. (2017a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Curated TREC</head><p>We perform one final experiment that tests our model as part of an end-to-end question answering system. For document retrieval, we re-implement the pipeline from <ref type="bibr" target="#b15">Joshi et al. (2017)</ref>. Given a question, we retrieve up to 10 web documents us-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy S-Norm (ours) 53.31 YodaQA with Bing <ref type="bibr" target="#b0">(Baudiš, 2015)</ref>, <ref type="bibr">37.18 YodaQA (Baudiš, 2015)</ref>, 34.26 DrQA + DS ( <ref type="bibr" target="#b3">Chen et al., 2017a)</ref> 25.7 <ref type="table" target="#tab_4">Table 4</ref>: Results on the Curated TREC corpus, Yo- daQA results extracted from its github page <ref type="bibr">7</ref> ing a Bing web search of the question, and all Wikipedia articles about entities the entity linker TAGME <ref type="bibr" target="#b8">(Ferragina and Scaiella, 2010)</ref> identifies in the question. We then use our linear paragraph ranker to select the 16 most relevant paragraphs from all these documents, which are passed to our model to locate the final answer span. We choose to use the shared-norm model trained on the TriviaQA unfiltered dataset since it is trained using multiple web documents as input. We use the same heuristics as <ref type="bibr" target="#b15">Joshi et al. (2017)</ref> to filter out trivia or QA websites to ensure questions can- not be trivially answered using webpages that di- rectly address the question. A demo of the system is publicly available 8 . We find accuracy on the TriviaQA unfiltered questions remains almost unchanged (within half a percent exact match score) when using our doc- ument retrieval method instead of the given doc- uments, showing our pipeline does a good job of producing evidence documents that are similar to the ones in the training data.</p><p>We test the system on questions from the TREC QA tasks ( <ref type="bibr" target="#b25">Voorhees et al., 1999</ref>), in particular a curated set of questions from Baudiš (2015), the same dataset used in <ref type="bibr" target="#b3">Chen et al. (2017a)</ref>. We apply our system to the 694 test questions without re- training on the train questions.</p><p>We compare against DrQA (Chen et al., 2017a) and YodaQA <ref type="bibr" target="#b0">(Baudiš, 2015)</ref>. It is important to note that these systems use different document corpora (Wikipedia for DrQA, and Wikipedia, several knowledge bases, and optionally Bing web search for YodaQA) and different training data (SQuAD and the TREC training questions for DrQA, and TREC only for YodaQA), so we can- not make assertions about the relative performance of individual components. Nevertheless, it is in- structive to show how the methods we experiment with in this work can advance an end-to-end QA system.</p><p>The results are listed in  racy mark. This is a strong proof-of-concept that neural paragraph reading combined with existing document retrieval methods can advance the state- of-the-art on general question answering. It also shows that, despite the noise, the data from Trivi- aQA is sufficient to train models that can be effec- tive on out-of-domain QA tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussion</head><p>We found that models that have only been trained on answer-containing paragraphs can perform very poorly in the multi-paragraph setting. The results were particularly bad for SQuAD; we think this is partly because the paragraphs are shorter, so the model had less exposure to irrelevant text. The shared-norm approach consistently outper- formed the other methods, especially on SQuAD and TriviaQA unfiltered, where many paragraphs were needed to reach peak performance. <ref type="figure" target="#fig_1">Figures  3, 4</ref>, and 5 show this technique has a minimal ef- fect on the performance when only one paragraph is used, suggesting the model's per-paragraph per- formance is preserved. Meanwhile, it can be seen the accuracy of the shared-norm model never drops as more paragraphs are added, showing it successfully resolves the problem of being dis- tracted by irrelevant text.</p><p>The no-answer and merge approaches were moderately effective, we suspect because they at least expose the model to more irrelevant text. However, these methods do not address the fun- damental issue of requiring confidence scores to be comparable between independent applications of the model to different paragraphs, which is why we think they lagged behind. The sigmoid objec- tive function reduces the paragraph-level perfor- mance considerably, especially on the TriviaQA datasets. We suspect this is because it is vulner- able to label noise, as discussed in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Error Analysis</head><p>We perform an error analysis by labeling 200 ran- dom TriviaQA web dev-set errors made by the shared-norm model. We found 40.5% of the er-rors were caused because the document did not contain sufficient evidence to answer the question, and 17% were caused by the correct answer not being contained in the answer key. The distribu- tion of the remaining errors is shown in <ref type="table" target="#tab_5">Table 5</ref>.</p><p>We found quite a few cases where a sentence contained the answer, but the model was unable to extract it due to complex syntactic structure or paraphrasing. Two kinds of multi-sentence read- ing errors were also common: cases that required connecting multiple statements made in a sin- gle paragraph, and long-range coreference cases where a sentence's subject was named in a previ- ous paragraph. Finally, some questions required background knowledge, or required the model to extract answers that were only stated indirectly (e.g., examining a list to extract the nth element). Overall, these results suggest good avenues for im- provement are to continue advancing the sentence and paragraph level reading comprehension abili- ties of the model, and adding a mechanism to han- dle document-level coreferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Reading Comprehension Datasets. The state of the art in reading comprehension has been rapidly advanced by neural models, in no small part due to the introduction of many large datasets. The first large scale datasets for training neural reading comprehension models used a Cloze-style task, where systems must predict a held out word from a piece of text ( <ref type="bibr" target="#b10">Hermann et al., 2015;</ref><ref type="bibr" target="#b12">Hill et al., 2015)</ref>. Additional datasets including SQuAD <ref type="bibr" target="#b21">(Rajpurkar et al., 2016)</ref>, <ref type="bibr">WikiReading (Hewlett et al., 2016)</ref>, MS Marco ( <ref type="bibr" target="#b18">Nguyen et al., 2016)</ref> and Triv- iaQA ( <ref type="bibr" target="#b15">Joshi et al., 2017</ref>) provided more realis- tic questions. Another dataset of trivia questions, Quasar-T ( <ref type="bibr" target="#b7">Dhingra et al., 2017)</ref>, was introduced recently that uses <ref type="bibr">ClueWeb09 (Callan et al., 2009)</ref> as its source for documents. In this work we choose to focus on SQuAD because it is well stud- ied, and TriviaQA because it is more challenging and features documents and multi-document con- texts (Quasar T is similar, but was released after we started work on this project).</p><p>Neural Reading Comprehension. Neural reading comprehension systems typically use some form of attention ( <ref type="bibr" target="#b26">Wang and Jiang, 2016)</ref>, al- though alternative architectures exist <ref type="bibr" target="#b3">(Chen et al., 2017a;</ref><ref type="bibr" target="#b31">Weissenborn et al., 2017b</ref>). Our model follows this approach, but includes some re- cent advances such as variational dropout <ref type="bibr" target="#b9">(Gal and Ghahramani, 2016)</ref> and bi-directional atten- tion ( <ref type="bibr" target="#b22">Seo et al., 2016)</ref>. Self-attention has been used in several prior works <ref type="bibr" target="#b5">(Cheng et al., 2016;</ref><ref type="bibr" target="#b29">Wang et al., 2017c;</ref><ref type="bibr" target="#b19">Pan et al., 2017)</ref>. Our approach to allowing a reading comprehension model to produce a per-paragraph no-answer score is related to the approach used in the BiDAF- T ( <ref type="bibr" target="#b17">Min et al., 2017</ref>) model to produce per-sentence classification scores, although we use an attention- based method instead of max-pooling.</p><p>Open QA. Open question answering has been the subject of much research, especially spurred by the TREC question answering track ( <ref type="bibr" target="#b25">Voorhees et al., 1999</ref>). Knowledge bases can be used, such as in ( <ref type="bibr" target="#b1">Berant et al., 2013)</ref>, although the re- sulting systems are limited by the quality of the knowledge base. Systems that try to answer ques- tions using natural language resources such as YodaQA <ref type="bibr" target="#b0">(Baudiš, 2015)</ref> typically use pipelined methods to retrieve related text, build answer can- didates, and pick a final output.</p><p>Neural Open QA. Open question answering with neural models was considered by <ref type="bibr" target="#b3">Chen et al. (2017a)</ref>, where researchers trained a model on SQuAD and combined it with a retrieval engine for Wikipedia articles. Our work differs because we focus on explicitly addressing the problem of applying the model to multiple paragraphs. A pipelined approach to QA was recently pro- posed by <ref type="bibr" target="#b27">Wang et al. (2017a)</ref>, where a ranker model is used to select a paragraph for the read- ing comprehension model to process. More recent work has considered evidence aggregation tech- niques ( <ref type="bibr" target="#b28">Wang et al., 2017b;</ref><ref type="bibr" target="#b23">Swayamdipta et al., 2017)</ref>. Our work shows paragraph-level mod- els that produce well-calibrated confidence scores can effectively exploit large amounts of text with- out aggregation, although integrating aggregation techniques could further improve our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have shown that, when using a paragraph-level QA model across multiple paragraphs, our train- ing method of sampling non-answer-containing paragraphs while using a shared-norm objective function can be very beneficial. Combining this with our suggestions for paragraph selection, us- ing the summed training objective, and our model design allows us to advance the state of the art on TriviaQA. As shown by our demo, this work can be directly applied to building deep-learning- powered open question answering systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: High level outline of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results for our confidence methods on TriviaQA unfiltered. The shared-norm approach is the strongest, while the baseline model starts to lose performance as more paragraphs are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>. This Model Web Web Verified Wiki Wiki Verified EM F1 EM F1 EM F1 EM F1 Baseline (Joshi et al., 2017) 40.74 47.06 49.54 55.80 40.32 45.91 44.86 50.71 Smarnet (Chen et al., 2017b) 40.87 47.09 51.11 55.98 42.41 48.84 50.51 55.90 Mnemonic Reader (Hu et al., 2017) 46.65 52.89 56.96 61.48 46.94 52.85 54.45 59.46 (Weissenborn et al., 2017a) 50.56 56.73 63.20 67.97 48.64 55.13 53.42 59.92 Neural Cascade (Swayamdipta et al., 2017) 53.75 58.57 63.20 66.88 51.59 55.95 58.90 62.53 S-Norm (ours) 66.37 71.32 79.97 83.70 63.99 68.93 67.98 72.88</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Published TriviaQA results. Our approach advances the state of the art by about 10 points on 
these datasets 4 

1 
3 
5 
7 
9 
11 
13 
15 
Number of Paragraphs 

0.62 

0.64 

0.66 

0.68 

0.70 

F1 Score 

TriviaQA Web F1 vs. Number of Paragraphs 

none 
sigmoid 
merge 
no-answer 
shared-norm 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Our method 
outperforms prior work, breaking the 50% accu-

8 https://documentqa.allenai.org/ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Error analysis on TriviaQA web. 

</table></figure>

			<note place="foot" n="1"> https://documentqa.allenai.org 2 https://github.com/allenai/document-qa</note>

			<note place="foot" n="4"> Comparison made of 5/01/2018. 5 https://competitions.codalab.org/competitions/17208</note>

			<note place="foot" n="7"> https://github.com/brmson/yodaqa/wiki/Benchmarks</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">YodaQA: A Modular Question Answering System Pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Baudiš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POSTER 2015-19th International Student Conference on Electrical Engineering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic Parsing on Freebase from Question-Answer Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changkuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Clueweb09 Data Set</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Reading Wikipedia to Answer Open-Domain Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheqian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02772</idno>
		<title level="m">Smarnet: Teaching Machines to Read and Comprehend Like Human</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Long Short-Term Memory-Networks for Machine Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fethi Bougares, Holger Schwenk, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
	</analytic>
	<monogr>
		<title level="m">Learning Phrase Representations Using RNN EncoderDecoder for Statistical Machine Translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Quasar: Datasets for Question Answering by Search and Reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03904</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TAGME: On-the-fly Annotation of Short Text Fragments (by Wikipedia Entities)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM international conference on Information and knowledge management</title>
		<meeting>the 19th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fandrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03542</idno>
		<title level="m">Wikireading: A Novel Large-scale Language Understanding Task over Wikipedia</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<title level="m">The Goldilocks Principle: Reading Children&apos;s Books with Explicit Memory Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mnemonic Reader: Machine Comprehension with Iterative Aligning and Multi-hop Answer Pointing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adversarial Examples for Evaluating Reading Comprehension Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07328</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03551</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01547</idno>
		<title level="m">Text Understanding with the Attention Sum Reader Network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Question Answering through Transfer Learning from Large Fine-grained Supervision Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Sewon Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02171</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bin Cao, Deng Cai, and Xiaofei He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Boyuan Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09098</idno>
	</analytic>
	<monogr>
		<title level="m">MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bidirectional Attention Flow for Machine Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min Joon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>abs/1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multi-Mention Learning for Reading Comprehension with Neural Cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04815</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The TREC-8 Question Answering Track Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voorhees</surname></persName>
		</author>
		<editor>Trec</editor>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine Comprehension Using Match-LSTM and Answer Pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00023</idno>
		<title level="m">R: Reinforced Reader-Ranker for Open-Domain Question Answering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Evidence Aggregation for Answer ReRanking in Open-Domain Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gated Self-Matching Networks for Reading Comprehension and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic Integration of Background Knowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02596</idno>
	</analytic>
	<monogr>
		<title level="m">Neural NLU Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">FastQA: A Simple and Efficient Neural Architecture for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Wiese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Seiffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.04816</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">ADADELTA: an Adaptive Learning Rate Method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
