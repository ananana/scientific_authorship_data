<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-Rank Tensors for Scoring Dependency Structures</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Low-Rank Tensors for Scoring Dependency Structures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1381" to="1391"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typically requires rich, high-dimensional feature representations. A small subset of such features is often selected manually. This is problematic when features lack clear linguistic meaning as in embeddings or when the information is blended across features. In this paper, we use tensors to map high-dimensional feature vectors into low dimensional representations. We explicitly maintain the parameters as a low-rank tensor to obtain low dimensional representations of words in their syntactic roles, and to leverage mod-ularity in the tensor for easy training with online algorithms. Our parser consistently outperforms the Turbo and MST parsers across 14 different languages. We also obtain the best published UAS results on 5 languages. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Finding an expressive representation of input sen- tences is crucial for accurate parsing. Syntac- tic relations manifest themselves in a broad range of surface indicators, ranging from morphological to lexical, including positional and part-of-speech (POS) tagging features. Traditionally, parsing re- search has focused on modeling the direct connec- tion between the features and the predicted syntac- tic relations such as head-modifier (arc) relations in dependency parsing. Even in the case of first- order parsers, this results in a high-dimensional vector representation of each arc. Discrete fea- tures, and their cross products, can be further com- plemented with auxiliary information about words participating in an arc, such as continuous vector representations of words. The exploding dimen- sionality of rich feature vectors must then be bal- anced with the difficulty of effectively learning the associated parameters from limited training data.</p><p>A predominant way to counter the high dimen- sionality of features is to manually design or select a meaningful set of feature templates, which are used to generate different types of features <ref type="bibr">(McDonald et al., 2005a;</ref><ref type="bibr">Koo and Collins, 2010;</ref><ref type="bibr">Martins et al., 2013</ref>). Direct manual selection may be problematic for two reasons. First, features may lack clear linguistic interpretation as in distribu- tional features or continuous vector embeddings of words. Second, designing a small subset of tem- plates (and features) is challenging when the rel- evant linguistic information is distributed across the features. For instance, morphological proper- ties are closely tied to part-of-speech tags, which in turn relate to positional features. These features are not redundant. Therefore, we may suffer a per- formance loss if we select only a small subset of the features. On the other hand, by including all the rich features, we face over-fitting problems.</p><p>We depart from this view and leverage high- dimensional feature vectors by mapping them into low dimensional representations. We begin by representing high-dimensional feature vectors as multi-way cross-products of smaller feature vec- tors that represent words and their syntactic rela- tions (arcs). The associated parameters are viewed as a tensor (multi-way array) of low rank, and opti- mized for parsing performance. By explicitly rep- resenting the tensor in a low-rank form, we have direct control over the effective dimensionality of the set of parameters. We obtain role-dependent low-dimensional representations for words (head, modifier) that are specifically tailored for parsing accuracy, and use standard online algorithms for optimizing the low-rank tensor components.</p><p>The overall approach has clear linguistic and computational advantages:</p><p>• Our low dimensional embeddings are tailored to the syntactic context of words (head, modi- fier). This low dimensional syntactic abstrac- tion can be thought of as a proxy to manually constructed POS tags.</p><p>• By automatically selecting a small number of dimensions useful for parsing, we can lever- age a wide array of (correlated) features. Un- like parsers such as MST, we can easily bene- fit from auxiliary information (e.g., word vec- tors) appended as features.</p><p>We implement the low-rank factorization model in the context of first-and third-order depen- dency parsing. The model was evaluated on 14 languages, using dependency data from <ref type="bibr">CoNLL 2008 and</ref><ref type="bibr">CoNLL 2006</ref>. We compare our results against the MST ( <ref type="bibr">McDonald et al., 2005a</ref>) and <ref type="bibr">Turbo (Martins et al., 2013</ref>) parsers. The low-rank parser achieves average performance of 89.08% across 14 languages, compared to 88.73% for the Turbo parser, and 87.19% for MST. The power of the low-rank model becomes evident in the ab- sence of any part-of-speech tags. For instance, on the English dataset, the low-rank model trained without POS tags achieves 90.49% on first-order parsing, while the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags. Finally, we demonstrate that the model can successfully leverage word vec- tor representations, in contrast to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Selecting Features for Dependency Parsing A great deal of parsing research has been dedicated to feature engineering ( <ref type="bibr">Lazaridou et al., 2013;</ref><ref type="bibr">Marton et al., 2010;</ref><ref type="bibr">Marton et al., 2011</ref>). While in most state-of-the-art parsers, features are se- lected manually <ref type="bibr">(McDonald et al., 2005a;</ref><ref type="bibr" target="#b11">McDonald et al., 2005b;</ref><ref type="bibr">Koo and Collins, 2010;</ref><ref type="bibr">Martins et al., 2013;</ref><ref type="bibr">Zhang and McDonald, 2012a;</ref><ref type="bibr" target="#b17">Rush and Petrov, 2012a)</ref>, automatic feature selec- tion methods are gaining popularity <ref type="bibr">(Martins et al., 2011b;</ref><ref type="bibr" target="#b0">Ballesteros and Nivre, 2012;</ref><ref type="bibr" target="#b14">Nilsson and Nugues, 2010;</ref><ref type="bibr" target="#b1">Ballesteros, 2013)</ref>. Following stan- dard machine learning practices, these algorithms iteratively select a subset of features by optimizing parsing performance on a development set. These feature selection methods are particularly promis- ing in parsing scenarios where the optimal feature set is likely to be a small subset of the original set of candidate features. Our technique, in contrast, is suitable for cases where the relevant information is distributed across a larger set of related features.</p><p>Embedding for Dependency Parsing A lot of recent work has been done on mapping words into vector spaces <ref type="bibr" target="#b7">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b24">Turian et al., 2010;</ref><ref type="bibr" target="#b10">Dhillon et al., 2011;</ref><ref type="bibr" target="#b13">Mikolov et al., 2013)</ref>. Traditionally, these vector representations have been derived primarily from co-occurrences of words within sentences, ignoring syntactic roles of the co-occurring words. Nevertheless, any such word-level representation can be used to offset in- herent sparsity problems associated with full lexi- calization <ref type="bibr" target="#b4">(Cirik and S ¸ ensoy, 2013)</ref>. In this sense they perform a role similar to POS tags.</p><p>Word-level vector space embeddings have so far had limited impact on parsing performance. From a computational perspective, adding non- sparse vectors directly as features, including their combinations, can significantly increase the num- ber of active features for scoring syntactic struc- tures (e.g., dependency arc). Because of this is- sue, Cirik and S ¸ ensoy (2013) used word vectors only as unigram features (without combinations) as part of a shift reduce parser ( <ref type="bibr" target="#b16">Nivre et al., 2007)</ref>. The improvement on the overall parsing perfor- mance was marginal. Another application of word vectors is compositional vector grammar <ref type="bibr" target="#b19">(Socher et al., 2013)</ref>. While this method learns to map word combinations into vectors, it builds on ex- isting word-level vector representations. In con- trast, we represent words as vectors in a manner that is directly optimized for parsing. This frame- work enables us to learn new syntactically guided embeddings while also leveraging separately esti- mated word vectors as starting features, leading to improved parsing performance.</p><p>Dimensionality Reduction Many machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters. Such problems include, for example, multi-task learning and collaborative filtering. Rather than assuming that each parameter can be set independently of others, it is helpful to assume that the parameters vary in a low dimensional subspace that has to be estimated together with the parameters. In terms of the parameter matrix, this corresponds to a low-rank assumption. Low-rank constraints are commonly used for improving generalization ( <ref type="bibr">Lee and Seung, 1999;</ref><ref type="bibr" target="#b20">Srebro et al., 2003;</ref><ref type="bibr" target="#b21">Srebro et al., 2004;</ref><ref type="bibr">Evgeniou and Pontil, 2007)</ref> A strict low-rank assumption can be restrictive. Indeed, recent approaches to matrix problems de- compose the parameter matrix as a sum of low- rank and sparse matrices ( <ref type="bibr" target="#b23">Tao and Yuan, 2011;</ref><ref type="bibr">Zhou and Tao, 2011</ref>). The sparse matrix is used to highlight a small number of parameters that should vary independently even if most of them lie on a low-dimensional subspace ( <ref type="bibr">Waters et al., 2011;</ref><ref type="bibr" target="#b3">Chandrasekaran et al., 2011</ref>). We follow this de- composition while extending the parameter matrix into a tensor.</p><p>Tensors are multi-way generalizations of ma- trices and possess an analogous notion of rank. Tensors are increasingly used as tools in spec- tral estimation ( <ref type="bibr">Hsu and Kakade, 2013)</ref>, includ- ing in parsing ( <ref type="bibr" target="#b5">Cohen et al., 2012)</ref> and other NLP problems (de <ref type="bibr" target="#b9">Cruys et al., 2013)</ref>, where the goal is to avoid local optima in maximum likelihood estimation. In contrast, we expand features for parsing into a multi-way tensor, and operate with an explicit low-rank representation of the associ- ated parameter tensor. The explicit representa- tion sidesteps inherent complexity problems asso- ciated with the tensor rank <ref type="bibr">(Hillar and Lim, 2009)</ref>. Our parameters are divided into a sparse set corre- sponding to manually chosen MST or Turbo parser features and a larger set governed by a low-rank tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head><p>We will commence here by casting first-order de- pendency parsing as a tensor estimation problem. We will start by introducing the notation used in the paper, followed by a more formal description of our dependency parsing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Notations</head><p>Let A ∈ R n×n×d be a 3-dimensional tensor (a 3- way array). We denote each element of the tensor as</p><formula xml:id="formula_0">A i,j,k where i ∈ [n], j ∈ [n], k ∈ [d] and [n]</formula><p>is a shorthand for the set of integers {1, 2, · · · , n}. Similarly, we use M i,j and u i to represent the ele- ments of matrix M and vector u, respectively.</p><p>We define the inner product of two tensors (or matrices) as A, B = vec(A) T vec(B), where vec(·) concatenates the tensor (or matrix) ele- ments into a column vector. The squared norm of a tensor/matrix is denoted by A 2 = A, A.</p><p>The Kronecker product of three vectors is de- noted by u ⊗ v ⊗ w and forms a rank-1 tensor such that</p><formula xml:id="formula_1">(u ⊗ v ⊗ w) i,j,k = u i v j w k .</formula><p>Note that the vectors u, v, and w may be column or row vectors. Their orientation is defined based on usage. For example, u ⊗ v is a rank-1 matrix uv T when u and v are column vectors (u T v if they are row vectors). We say that tensor A is in Kruskal form if</p><formula xml:id="formula_2">A = r i=1 U (i, :) ⊗ V (i, :) ⊗ W (i, :)<label>(1)</label></formula><p>where U, V ∈ R r×n , W ∈ R r×d and U (i, :) is the i th row of matrix U . We will directly learn a low- rank tensor A (because r is small) in this form as one of our model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dependency Parsing</head><p>Let x be a sentence and Y(x) the set of possible dependency trees over the words in x. We assume that the score S(x, y) of each candidate depen- dency tree y ∈ Y(x) decomposes into a sum of "local" scores for arcs. Specifically:</p><formula xml:id="formula_3">S(x, y) = h→m ∈ y s(h → m) ∀y ∈ Y(x)</formula><p>where h → m is the head-modifier dependency arc in the tree y. Each y is understood as a col- lection of arcs h → m where h and m index words in x. <ref type="bibr">2</ref> For example, x(h) is the word cor- responding to h. We suppress the dependence on x whenever it is clear from context. For exam- ple, s(h → m) can depend on x in complicated ways as discussed below. The predicted parse is obtained asˆyasˆ asˆy = arg max y∈Y(x) S(x, y).</p><p>A key problem is how we parameterize the arc scores s(h → m). Following the MST parser ( <ref type="bibr">McDonald et al., 2005a</ref>) we can define rich features characterizing each head-modifier arc, compiled into a sparse binary vector φ h→m ∈ R L that depends on the sentence x as well as the chosen arc h → m (again, we suppress the depen- dence on x). Based on this feature representation, we define the score of each arc as s θ (h → m) = Unigram features: form form-p form-n lemma lemma-p lemma-n pos pos-p pos-n morph bias Bigram features: pos-p, pos pos, pos-n pos, lemma morph, lemma Trigram features: pos-p, pos, pos-n <ref type="table">Table 1</ref>: Word feature templates used by our model. pos, form, lemma and morph stand for the fine POS tag, word form, word lemma and the morphology feature (provided in CoNLL format file) of the current word. There is a bias term that is always active for any word. The suffixes -p and -n refer to the left and right of the current word re- spectively. For example, pos-p means the POS tag to the left of the current word in the sentence.</p><p>θ, φ h→m where θ ∈ R L represent adjustable pa- rameters to be learned, and L is the number of pa- rameters (and possible features in φ h→m ).</p><p>We can alternatively specify arc features in terms of rank-1 tensors by taking the Kronecker product of simpler feature vectors associated with the head (vector φ h ∈ R n ), and modifier (vector φ m ∈ R n ), as well as the arc itself (vector φ h,m ∈ R d ). Here φ h,m is much lower dimensional than the MST arc feature vector φ h→m discussed ear- lier. For example, φ h,m may be composed of only indicators for binned arc lengths <ref type="bibr">3</ref> . φ h and φ m , on the other hand, are built from features shown in <ref type="table">Table 1</ref>. By taking the cross-product of all these component feature vectors, we obtain the full fea- ture representation for arc h → m as a rank-1 ten- sor</p><formula xml:id="formula_4">φ h ⊗ φ m ⊗ φ h,m ∈ R n×n×d</formula><p>Note that elements of this rank-1 tensor include feature combinations that are not part of the fea- ture crossings in φ h→m . In this sense, the rank-1 tensor represents a substantial feature expansion. The arc score s tensor (h → m) associated with the tensor representation is defined analogously as</p><formula xml:id="formula_5">s tensor (h → m) = A, φ h ⊗ φ m ⊗ φ h,m</formula><p>where the adjustable parameters A also form a ten- sor. Given the typical dimensions of the compo- nent feature vectors, φ h , φ m , φ h,m , it is not even possible to store all the parameters in A. Indeed, in the full English training set of CoNLL-2008, the tensor involves around 8 × 10 11 entries while the MST feature vector has approximately 1.5 × 10 7 features. To counter this feature explosion, we re- strict the parameters A to have low rank.</p><p>Low-Rank Dependency Scoring We can repre- sent a rank-r tensor A explicitly in terms of pa- rameter matrices U , V , and W as shown in Eq. 1. As a result, the arc score for the tensor reduces to evaluating U φ h , V φ m , and W φ h,m which are all r dimensional vectors and can be computed effi- ciently based on any sparse vectors φ h , φ m , and φ h,m . The resulting arc score</p><formula xml:id="formula_6">s tensor (h → m) is then r i=1 [U φ h ] i [V φ m ] i [W φ h,m ] i<label>(2)</label></formula><p>By learning parameters U , V , and W that function well in dependency parsing, we also learn context- dependent embeddings for words and arcs. Specif- ically, U φ h (for a given sentence, suppressed) is an r dimensional vector representation of the word corresponding to h as a head word. Similarly, V φ m provides an analogous representation for a modifier m. Finally, W φ h,m is a vector embed- ding of the supplemental arc-dependent informa- tion. The resulting embedding is therefore tied to the syntactic roles of the words (and arcs), and learned in order to perform well in parsing.</p><p>We expect a dependency parsing model to ben- efit from several aspects of the low-rank tensor scoring. For example, we can easily incorpo- rate additional useful features in the feature vec- tors φ h , φ m and φ h,m , since the low-rank assump- tion (for small enough r) effectively counters the otherwise uncontrolled feature expansion. More- over, by controlling the amount of information we can extract from each of the component fea- ture vectors (via rank r), the statistical estimation problem does not scale dramatically with the di- mensions of φ h , φ m and φ h,m . In particular, the low-rank constraint can help generalize to unseen arcs. Consider a feature δ(x(h) = a) · δ(x(m) = b) · δ(dis(x, h, m) = c) which is non-zero only for an arc a → b with distance c in sentence x. If the arc has not been seen in the available train- ing data, it does not contribute to the traditional arc score s θ (·). In contrast, with the low-rank con- straint, the arc score in Eq. 2 would typically be non-zero.</p><p>Combined Scoring Our parsing model aims to combine the strengths of both traditional features from the MST/Turbo parser as well as the new low-rank tensor features. In this way, our model is able to capture a wide range of information in- cluding the auxiliary features without having un- controlled feature explosion, while still having the full accessibility to the manually engineered fea- tures that are proven useful. Specifically, we de- fine the arc score s γ (h → m) as the combination</p><formula xml:id="formula_7">(1 − γ)s tensor (h → m) + γs θ (h → m) = (1 − γ) r i=1 [U φ h ] i [V φ m ] i [W φ h,m ] i + γ θ, φ h→m (3)</formula><p>where θ ∈ R L , U ∈ R r×n , V ∈ R r×n , and W ∈ R r×d are the model parameters to be learned. The rank r and γ ∈ [0, 1] (balancing the two scores) represent hyper-parameters in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning</head><p>The training set</p><formula xml:id="formula_8">D = {(ˆ x i , ˆ y i )} N i=1</formula><p>consists of N pairs, where each pair consists of a sentence x i and the corresponding gold (target) parse y i . The goal is to learn values for the parameters θ, U , V and W that optimize the combined scoring func- tion S γ (x, y) = h→m∈y s γ (h → m), defined in Eq. 3, for parsing performance. We adopt a maximum soft-margin framework for this learning problem. Specifically, we find parameters θ, U , V , W , and {ξ i } that minimize</p><formula xml:id="formula_9">C i ξ i + θ 2 + U 2 + V 2 + W 2 s.t. S γ (ˆ x i , ˆ y i ) ≥ S γ (ˆ x i , y i ) + ˆ y i − y i 1 − ξ i ∀y i ∈ Y(ˆ x i ), ∀i.<label>(4)</label></formula><p>wherê y i −y i 1 is the number of mismatched arcs between the two trees, and ξ i is a non-negative slack variable. The constraints serve to separate the gold tree from other alternatives in Y(ˆ x i ) with a margin that increases with distance.</p><p>The objective as stated is not jointly convex with respect to U , V and W due to our explicit representation of the low-rank tensor. However, if we fix any two sets of parameters, for example, if we fix V and W , then the combined score S γ (x, y) will be a linear function of both θ and U . As a re- sult, the objective will be jointly convex with re- spect to θ and U and could be optimized using standard tools. However, to accelerate learning, we adopt an online learning setup. Specifically, we use the passive-aggressive learning algorithm <ref type="bibr" target="#b8">(Crammer et al., 2006</ref>) tailored to our setting, up- dating pairs of parameter sets, (θ, U ), (θ, V ) and (θ, W ) in an alternating manner. This method is described below.</p><p>Online Learning In an online learning setup, we update parameters successively based on each sentence. In order to apply the passive-aggressive algorithm, we fix two of U , V and W (say, for ex- ample, V and W ) in an alternating manner, and apply a closed-form update to the remaining pa- rameters (here U and θ). This is possible since the objective function with respect to (θ, U ) has a similar form as in the original passive-aggressive algorithm. To illustrate this, consider a training sentence x i . The update involves finding first the best competing tree,</p><formula xml:id="formula_10">˜ y i = arg max y i ∈Y(ˆ x i ) S γ (ˆ x i , y i ) + ˆ y i − y i 1 (5)</formula><p>which is the tree that violates the constraint in Eq. 4 most (i.e. maximizes the loss ξ i ). We then obtain parameter increments ∆θ and ∆U by solv- ing min ∆θ, ∆U, ξ≥0</p><formula xml:id="formula_11">1 2 ∆θ 2 + 1 2 ∆U 2 + Cξ s.t. S γ (ˆ x i , ˆ y i ) ≥ S γ (ˆ x i , ˜ y i ) + ˆ y i − ˜ y i 1 − ξ</formula><p>In this way, the optimization problem attempts to keep the parameter change as small as possible, while forcing it to achieve mostly zero loss on this single instance. This problem has a closed form solution</p><formula xml:id="formula_12">∆θ = min C, loss γ 2 dθ 2 + (1 − γ) 2 du 2 γdθ ∆U = min C, loss γ 2 dθ 2 + (1 − γ) 2 du 2 (1 − γ)du where loss = S γ (ˆ x i , ˜ y i ) + ˆ y i − ˜ y i 1 − S γ (ˆ x i , ˆ y i ) dθ = h→m ∈ ˆ y i φ h→m − h→m ∈ ˜ y i φ h→m du = h→m ∈ ˆ y i [(V φ m ) (W φ h,m )] ⊗ φ h − h→m ∈ ˜ y i [(V φ m ) (W φ h,m )] ⊗ φ h</formula><p>where (u v) i = u i v i is the Hadamard (element- wise) product. The magnitude of change of θ and U is controlled by the parameter C. By varying C, we can determine an appropriate step size for the online updates. The updates also illustrate how γ balances the effect of the MST component of the score relative to the low-rank tensor score. When γ = 0, the arc scores are entirely based on the low- rank tensor and ∆θ = 0. Note that φ h , φ m , φ h,m , and φ h→m are typically very sparse for each word or arc. Therefore du and dθ are also sparse and can be computed efficiently.</p><p>Initialization The alternating online algorithm relies on how we initialize U , V , and W since each update is carried out in the context of the other two. A random initialization of these parameters is unlikely to work well, both due to the dimensions involved, and the nature of the alternating updates.</p><p>We consider here instead a reasonable determinis- tic "guess" as the initialization method. We begin by training our model without any low-rank parameters, and obtain parameters θ. The majority of features in this MST component can be expressed as elements of the feature ten- sor, i.e., as [φ h ⊗ φ m ⊗ φ h,m ] i,j,k . We can there- fore create a tensor representation of θ such that B i,j,k equals the corresponding parameter value in θ. We use a low-rank version of B as the ini- tialization. Specifically, we unfold the tensor B into a matrix B (h) of dimensions n and nd, where n = dim(φ h ) = dim(φ m ) and d = dim(φ h,m ). For instance, a rank-1 tensor can be unfolded as u ⊗ v ⊗ w = u ⊗ vec(v ⊗ w). We compute the top-r SVD of the resulting unfolded matrix such that B (h) = P T SQ. U is initialized as P . Each right singular vector S i Q(i, :) is also a matrix in R n×d . The leading left and right singular vectors of this matrix are assigned to V (i, :) and W (i, :) respectively. In our implementation, we run one epoch of our model without low-rank parameters and initialize the tensor A.</p><p>Parameter Averaging The passive-aggressive algorithm regularizes the increments (e.g. ∆θ and ∆U ) during each update but does not include any overall regularization. In other words, keeping up- dating the model may lead to large parameter val- ues and over-fitting. To counter this effect, we use parameter averaging as used in the MST and Turbo parsers. The final parameters are those averaged across all the iterations (cf. <ref type="figure" target="#fig_0">(Collins, 2002)</ref>). For simplicity, in our algorithm we average U , V , W and θ separately, which works well empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Datasets We test our dependency model on 14 languages, including the English dataset from CoNLL 2008 shared tasks and all 13 datasets from CoNLL 2006 shared tasks ( <ref type="bibr" target="#b2">Buchholz and Marsi, 2006;</ref><ref type="bibr" target="#b22">Surdeanu et al., 2008</ref>). These datasets in- clude manually annotated dependency trees, POS tags and morphological information. Following standard practices, we encode this information as features.</p><p>Methods We compare our model to MST and Turbo parsers on non-projective dependency pars- ing. For our parser, we train both a first-order parsing model (as described in Section 3 and 4) as well as a third-order model. The third order parser simply adds high-order features, those typ- ically used in MST and Turbo parsers, into our s θ (x, y) = θ, φ(x, y) scoring component. The decoding algorithm for the third-order parsing is based on ( <ref type="bibr">Zhang et al., 2014</ref>). For the Turbo parser, we directly compare with the recent pub- lished results in <ref type="bibr">(Martins et al., 2013)</ref>. For the MST parser, we train and test using the most re- cent version of the code. <ref type="bibr">4</ref> In addition, we im- plemented two additional baselines, NT-1st (first order) and NT-3rd (third order), corresponding to our model without the tensor component.</p><p>Features For the arc feature vector φ h→m , we use the same set of feature templates as MST v0.5.1. For head/modifier vector φ h and φ m , we show the complete set of feature templates used by our model in <ref type="table">Table 1</ref>   and S ¸ ensoy, 2013), learned from raw data <ref type="bibr">(Globerson et al., 2007;</ref><ref type="bibr">Maron et al., 2010)</ref>. Three languages in our dataset -English, German and Swedish -have corresponding word vectors in this collection. <ref type="bibr">5</ref> The dimensionality of this representa- tion varies by language: English has 50 dimen- sional word vectors, while German and Swedish have 25 dimensional word vectors. Each entry of the word vector is added as a feature value into feature vectors φ h and φ m . For each word in the sentence, we add its own word vector as well as the vectors of its left and right words. We should note that since our model parameter A is represented and learned in the low-rank form, we only have to store and maintain the low-rank projections U φ h , V φ m and W φ h,m rather than ex- plicitly calculate the feature tensor φ h ⊗φ m ⊗φ h,m . Therefore updating parameters and decoding a sentence is still efficient, i.e., linear in the num- ber of values of the feature vector. In contrast, assume we take the cross-product of the auxiliary word vector values, POS tags and lexical items of a word and its context, and add the crossed val- ues into a normal model (in φ h→m ). The number of features for each arc would be at least quadratic, growing into thousands, and would be a significant impediment to parsing efficiency.</p><p>Evaluation Following standard practices, we train our full model and the baselines for 10 5 https://github.com/wolet/sprml13-word-embeddings epochs. As the evaluation measure, we use un- labeled attachment scores (UAS) excluding punc- tuation. In all the reported experiments, the hyper- parameters are set as follows: r = 50 (rank of the tensor), C = 1 for first-order model and C = 0.01 for third-order model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Overall Performance <ref type="table">Table 2</ref> shows the per- formance of our model and the baselines on 14 CoNLL datasets. Our model outperforms Turbo parser, MST parser, as well as its own variants without the tensor component. The improvements of our low-rank model are consistent across lan- guages: results for the first order parser are better on 11 out of 14 languages. By comparing NT-1st and NT-3rd (models without low-rank) with our full model (with low-rank), we obtain 0.7% abso- lute improvement on first-order parsing, and 0.3% improvement on third-order parsing. Our model also achieves the best UAS on 5 languages.</p><p>We next focus on the first-order model and gauge the impact of the tensor component. First, we test our model by varying the hyper-parameter γ which balances the tensor score and the tradi- tional MST/Turbo score components. <ref type="figure">Figure 1</ref> shows the average UAS on CoNLL test datasets after each training epoch. We can see that the im- provement of adding the low-rank tensor is con- sistent across various choices of hyper parame-   <ref type="table">Table 3</ref>: Results of adding unsupervised word vec- tors to the tensor. Adding this information yields consistent improvement for all languages.</p><note type="other">88.0% # Epochs γ=0.0 γ=0.2 γ=0.3 γ=0.4 NT−1st Figure 1: Average UAS on CoNLL testsets af- ter different epochs. Our full model consistently performs better than NT-1st (its variation without tensor component) under different choices of the hyper-parameter γ. no word vector with word vector English 91.</note><p>ter γ. When training with the tensor component alone (γ = 0), the model converges more slowly. Learning of the tensor is harder because the scor- ing function is not linear (nor convex) with respect to parameters U , V and W . However, the tensor scoring component achieves better generalization on the test data, resulting in better UAS than NT- 1st after 8 training epochs.</p><p>To assess the ability of our model to incorpo- rate a range of features, we add unsupervised word vectors to our model. As described in previous section, we do so by appending the values of dif- ferent coordinates in the word vector into φ h and φ m . As <ref type="table">Table 3</ref> shows, adding this information in- creases the parsing performance for all the three languages. For instance, we obtain more than 0.5% absolute improvement on Swedish.</p><p>Syntactic Abstraction without POS Since our model learns a compressed representation of fea- ture vectors, we are interested to measure its per- formance when part-of-speech tags are not pro- vided (See <ref type="table">Table 4</ref>). The rationale is that given all other features, the model would induce representa- tions that play a similar role to POS tags. Note that Our model NT-1st -POS +wv. -POS +POS English 88.89 90.49 86.70 90.58 German 82.63 85.80 78.71 88.50 Swedish 81.84 85.90 79.65 88.75 <ref type="table">Table 4</ref>: The first three columns show parsing re- sults when models are trained without POS tags. The last column gives the upper-bound, i.e. the performance of a parser trained with 12 Core POS tags. The low-rank model outperforms NT-1st by a large margin. Adding word vector features fur- ther improves performance. the performance of traditional parsers drops when tags are not provided. For example, the perfor- mance gap is 10% on German. Our experiments show that low-rank parser operates effectively in the absence of tags. In fact, it nearly reaches the performance of the original parser that used the tags on English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples of Derived Projections</head><p>We manu- ally analyze low-dimensional projections to assess whether they capture syntactic abstraction. For this purpose, we train a model with only a ten- sor component (such that it has to learn an accu- rate tensor) on the English dataset and obtain low dimensional embeddings U φ w and V φ w for each word. The two r-dimension vectors are concate- nated as an "averaged" vector. We use this vector to calculate the cosine similarity between words. <ref type="table" target="#tab_5">Table 5</ref> shows examples of five closest neighbors of queried words. While these lists include some noise, we can clearly see that the neighbors ex- hibit similar syntactic behavior. For example, "on" is close to other prepositions. More interestingly, we can consider the impact of syntactic context on the derived projections. The bottom part of <ref type="table" target="#tab_5">Table 5</ref> shows that the neighbors change substan- tially depending on the syntactic role of the word. For example, the closest words to the word "in- crease" are verbs in the context phrase "will in- crease again", while the closest words become nouns given a different phrase "an increase of". <ref type="table" target="#tab_6">Table 6</ref> illustrates the impact of estimating low-rank tensor parameters on the run- ning time of the algorithm. For comparison, we also show the NT-1st times across three typical languages. The Arabic dataset has the longest av- erage sentence length, while the Chinese dataset  <ref type="table">says  on  when  actively  earnings  adds  with  where  openly  franchisees predicts  into  what  significantly shares  noted  at  why  outright  revenue  wrote  during which  substantially members  contends</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Accurate scoring of syntactic structures such as head-modifier arcs in dependency parsing typi- cally requires rich, high-dimensional feature rep- resentations. We introduce a low-rank factoriza- tion method that enables to map high dimensional feature vectors into low dimensional representa- tions. Our method maintains the parameters as a low-rank tensor to obtain low dimensional repre- sentations of words in their syntactic roles, and to leverage modularity in the tensor for easy train- ing with online algorithms. We implement the approach on first-order to third-order dependency parsing. Our parser outperforms the Turbo and MST parsers across 14 languages. Future work involves extending the tensor com- ponent to capture higher-order structures. In par- ticular, we would consider second-order structures such as grandparent-head-modifier by increasing the dimensionality of the tensor. This tensor will accordingly be a four or five-way array. The online update algorithm remains applicable since each di- mension is optimized in an alternating fashion.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 :</head><label>2</label><figDesc>First-order parsing (left) and high-order parsing (right) results on CoNLL-2006 datasets and the English dataset of CoNLL-2008. For our model, the experiments are ran with rank r = 50 and hyper- parameter γ = 0.3. To remove the tensor in our model, we ran experiments with γ = 1, corresponding to columns NT-1st and NT-3rd. The last column shows results of most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010), Martins et al. (2011a), Martins et al. (2013), Koo et al. (2010), Rush and Petrov (2012b), Zhang and McDonald (2012b) and Zhang et al. (2013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>André</head><label></label><figDesc>FT Martins, Noah A Smith, Pedro MQ Aguiar, and Mário AT Figueiredo. 2011b. Structured spar- sity in structured prediction. In Proceedings of the Conference on Empirical Methods in Natural Lan- guage Processing. Association for Computational Linguistics. André FT Martins, Miguel B Almeida, and Noah A Smith. 2013. Turning on the turbo: Fast third-order non-projective turbo parsers. In Proceedings of the 51th Annual Meeting of the Association for Compu- tational Linguistics. Association for Computational Linguistics. Yuval Marton, Nizar Habash, and Owen Rambow. 2010. Improving arabic dependency parsing with lexical and inflectional morphological features. In Proceedings of the NAACL HLT 2010 First Work- shop on Statistical Parsing of Morphologically-Rich Languages, SPMRL '10. Association for Computa- tional Linguistics. Yuval Marton, Nizar Habash, and Owen Rambow. 2011. Improving arabic dependency parsing with form-based and functional morphological features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computa- tional Linguistics. Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of de- pendency parsers. In Proceedings of the 43rd An- nual Meeting of the Association for Computational Linguistics (ACL'05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. Finally, we use a similar set of feature templates as Turbo v2.1 for 3rd order parsing. To add auxiliary word vector representations, we use the publicly available word vectors (Cirik</figDesc><table>First-order only 

High-order 
Ours 
NT-1st 
MST 
Turbo 
Ours-3rd NT-3rd MST-2nd Turbo-3rd Best Published 
Arabic 
79.60 
78.71 
78.3 
77.23 
79.95 
79.53 
78.75 
79.64 
81.12 (Ma11) 
Bulgarian 
92.30 
91.14 
90.98 
91.76 
93.50 
92.79 
91.56 
93.1 
94.02 (Zh13) 
Chinese 
91.43 
90.85 
90.40 
88.49 
92.68 
92.39 
91.77 
89.98 
91.89 (Ma10) 
Czech 
87.90 
86.62 
86.18 
87.66 
90.50 
89.43 
87.3 
90.32 
90.32 (Ma13) 
Danish 
90.64 
89.80 
89.84 
89.42 
91.39 
90.82 
90.5 
91.48 
92.00 (Zh13) 
Dutch 
84.81 
83.77 
82.89 
83.61 
86.41 
86.08 
84.11 
86.19 
86.19 (Ma13) 
English 
91.84 
91.40 
90.59 
91.21 
93.02 
92.82 
91.54 
93.22 
93.22 (Ma13) 
German 
90.24 
89.70 
89.54 
90.52 
91.97 
92.26 
90.14 
92.41 
92.41 (Ma13) 
Japanese 
93.74 
93.36 
93.38 
92.78 
93.71 
93.23 
92.92 
93.52 
93.72 (Ma11) 
Portuguese 
90.94 
90.67 
89.92 
91.14 
91.92 
91.63 
91.08 
92.69 
93.03 (Ko10) 
Slovene 
84.25 
83.15 
82.09 
82.81 
86.24 
86.07 
83.25 
86.01 
86.95 (Ma11) 
Spanish 
85.27 
84.95 
83.79 
83.61 
88.00 
87.47 
84.33 
85.59 
87.96 (Zh13) 
Swedish 
89.86 
89.66 
88.27 
89.36 
91.00 
90.83 
89.05 
91.14 
91.62 (Zh13) 
Turkish 
75.84 
74.89 
74.81 
75.98 
76.84 
75.83 
74.39 
76.9 
77.55 (Ko10) 
Average 
87.76 
87.05 
86.5 
86.83 
89.08 
88.66 
87.19 
88.73 
89.43 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Five closest neighbors of the queried 
words (shown in bold). The upper part shows our 
learned embeddings group words with similar syn-
tactic behavior. The two bottom parts of the table 
demonstrate that how the projections change de-
pending on the syntactic context of the word. 

#Tok. Len. 
Train. Time (hour) 
NT-1st 
Ours 
Arabic 
42K 
32 
0.13 
0.22 
Chinese 337K 
6 
0.37 
0.65 
English 958K 
24 
1.88 
2.83 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison of training times across three 
typical datasets. The second column is the number 
of tokens in each data set. The third column shows 
the average sentence length. Both first-order mod-
els are implemented in Java and run as a single 
process. 

has the shortest sentence length in CoNLL 2006. 
Based on these results, estimating a rank-50 tensor 
together with MST parameters only increases the 
running time by a factor of 1.7. 

</table></figure>

			<note place="foot" n="1"> Our code is available at https://github.com/ taolei87/RBGParser.</note>

			<note place="foot" n="2"> Note that in the case of high-order parsing, the sum S(x, y) may also include local scores for other syntactic structures, such as grandhead-head-modifier score s(g → h → m). See (Martins et al., 2013) for a complete list of these structures.</note>

			<note place="foot" n="3"> In our current version, φ h,m only contains the binned arc length. Other possible features include, for example, the label of the arc h → m, the POS tags between the head and the modifier, boolean flags which indicate the occurence of in-between punctutations or conjunctions, etc.</note>

			<note place="foot" n="4"> http://sourceforge.net/projects/mstparser/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head><p>The authors acknowledge the support of the MURI program (W911NF-10-1-0533) and the DARPA BOLT program. This research is developed in col-laboration with the Arabic Language Technoligies (ALT) group at Qatar Computing Research Insti-tute (QCRI) within the LYAS project. We thank Volkan Cirik for sharing the unsupervised word vector data. Thanks to Amir Globerson, Andreea Gane, the members of the MIT NLP group and the ACL reviewers for their suggestions and com-ments. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MaltOptimizer: An optimization tool for MaltParser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL. The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Effective morphological feature selection with MaltOptimizer at the SPMRL 2013 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CoNLL-X shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X &apos;06</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning, CoNLL-X &apos;06</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rank-sparsity incoherence for matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkat</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The AI-KU system at the SPMRL 2013 shared task : Unsupervised features for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S ¸</forename><surname>Hüsnü</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ensoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</title>
		<meeting>the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>EMNLP &apos;02. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shai Shalev-Shwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Online passive-aggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A tensor-based factorization model of semantic compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL. The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiview learning of word embeddings via CCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multilingual dependency analysis with a two-stage discriminative parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic discovery of feature sets for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Coling 2010 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Labeled pseudoprojective dependency parsing with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsgüls¸en</forename><surname>Eryiit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetoslav</forename><surname>Marinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MaltParser: A language-independent system for data-driven dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atanas</forename><surname>Chanev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsen</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetoslav</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vine pruning for efficient multi-pass dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL &apos;12)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vine pruning for efficient multi-pass dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weighted low-rank approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum-margin matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL &apos;08</title>
		<meeting>the Twelfth Conference on Computational Natural Language Learning, CoNLL &apos;08</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recovering lowrank and sparse components of matrices from incomplete and noisy observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10. Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
