<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Constituent-Centric Neural Architecture for Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
						</author>
						<title level="a" type="main">A Constituent-Centric Neural Architecture for Reading Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1405" to="1414"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1129</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Reading comprehension (RC), aiming to understand natural texts and answer questions therein, is a challenging task. In this paper, we study the RC problem on the Stanford Question Answering Dataset (SQuAD). Observing from the training set that most correct answers are centered around constituents in the parse tree, we design a constituent-centric neural architecture where the generation of candidate answers and their representation learning are both based on constituents and guided by the parse tree. Under this architecture , the search space of candidate answers can be greatly reduced without sacrificing the coverage of correct answers and the syntactic, hierarchical and compositional structure among constituents can be well captured, which contributes to better representation learning of the candidate answers. On SQuAD, our method achieves the state of the art performance and the ab-lation study corroborates the effectiveness of individual modules.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reading comprehension (RC) aims to answer questions by understanding texts, which is a chal- lenge task in natural language processing. Var- ious RC tasks and datasets have been devel- oped, including Machine Comprehension Test ( <ref type="bibr" target="#b18">Richardson et al., 2013</ref>) for multiple-choice ques- tion answering (QA) ( <ref type="bibr" target="#b19">Sachan et al., 2015;</ref><ref type="bibr">Wang and McAllester, 2015</ref>), Algebra ( <ref type="bibr" target="#b10">Hosseini et al., 2014</ref>) and Science  for passing standardized tests , CNN/Daily Mail ( <ref type="bibr" target="#b7">Hermann et al., 2015)</ref> and Chil- dren's Book Test <ref type="bibr" target="#b8">(Hill et al., 2015</ref>) for cloze-style</p><p>The most authoritative account at the time came from the medical faculty in Paris in a report to the king of France that blamed the heavens. This report became the first and most widely circulated of a series of plague tracts that sought to give advice to sufferers. That the plague was caused by bad air became the most widely accepted theory. Today, this is known as the Miasma theory.</p><p>1. Who was the medical report written for? the king of France 2. What is the newer, more widely accepted theory behind the spread of the plague? bad air 3. What is the bad air theory officially known as? Miasma theory <ref type="figure">Figure 1</ref>: An example of the SQuAD QA task QA ( <ref type="bibr" target="#b21">Shen et al., 2016)</ref>, Wik- iQA ( <ref type="bibr" target="#b31">Yang et al., 2015)</ref>, Stanford Question An- swering Dataset (SQuAD) ( <ref type="bibr" target="#b17">Rajpurkar et al., 2016)</ref> and Microsoft Machine Reading Comprehension ( <ref type="bibr" target="#b15">Nguyen et al., 2016</ref>) for open domain QA. In this paper, we are specifically interested in solving the SQuAD QA task <ref type="figure">(Figure 1</ref> shows an example), in light of its following features: (1) large scale: 107,785 questions, 23,215 paragraphs; (2) non- synthetic: questions are generated by crowdwork- ers; (3) large search space of candidate answers.</p><p>We study two major problems: (1) how to generate candidate answers? Unlike in multiple- choice QA and cloze-style QA where a small amount of answer choices are given, an answer in SQuAD could be any span in the text, resulting in a large search space with size O(n 2 ) ( <ref type="bibr" target="#b17">Rajpurkar et al., 2016)</ref>, where n is the number of words in the sentence. This would incur a lot of noise, ambigu-ity and uncertainty, making it highly difficult to pick up the correct answer. (2) how to effectively represent the candidate answers? First, long-range semantics spanning multiple sentences need to be captured. As noted in ( <ref type="bibr" target="#b17">Rajpurkar et al., 2016)</ref>, the answering of many questions requires multiple- sentence reasoning. For instance, in <ref type="figure">Figure 1</ref>, the last two sentences in the passages are needed to answer the third question. Second, local syntac- tic structure needs to be incorporated into repre- sentation learning. The study by <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref> shows that syntax plays an important role in SQuAD QA: there are a wide range of syntactic di- vergence between a question and the sentence con- taining the answer; the answering of 64.1% ques- tions needs to deal with syntactic variation; exper- iments show that syntactic features are the major contributing factors to good performance.</p><p>To tackle the first problem, motivated by the observation in ( <ref type="bibr" target="#b17">Rajpurkar et al., 2016</ref>) that the correct answers picked up by human are not ar- bitrary spans, but rather centered around con- stituents in the parse tree, we generate candidate answers based upon constituents, which signifi- cantly reduces the search space. Different from <ref type="bibr" target="#b17">(Rajpurkar et al., 2016</ref>) who only consider ex- act constituents, we adopt a constituent expansion mechanism which greatly improves the coverage of correct answers.</p><p>For the representation learning of candidate an- swers which are sequences of constituents, we first encode individual constituents using a chain- of-trees LSTM (CT-LSTM) and tree-guided at- tention mechanism, then feed these encodings into a chain LSTM (Hochreiter and Schmidhu- ber, 1997) to generate representations for the con- stituent sequences. The CT-LSTM seamlessly integrates intra-sentence tree <ref type="bibr">LSTMs (Tai et al., 2015</ref>) which capture the local syntactic properties of constituents and an inter-sentence chain LSTM which glues together the sequence of tree LSTMs such that the semantics of each sentence can be propagated to others. The tree-guided attention leverages the hierarchical relations among con- stituents to learn question-aware representations.</p><p>Putting these pieces together, we design a constituent-centric neural network (CCNN), which contains four layers: a chain-of-trees LSTM encoding layer, a tree-guided attention layer and a candidate-answer generation layer, a prediction layer. Evaluation on SQuAD demonstrates the ef-   2 Constituent-Centric Neural Network for Reading Comprehension</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overall Architecture</head><p>As observed in <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref>, almost all correct answers are centered around the con- stituents. To formally confirm this, we compare the correct answers in the training set with con- stituents generated by the Stanford parser (Man- ning et al., 2014): for each correct answer, we find its "closest" constituent -the longest constituent that is a substring of the answer, and count how many words they differ from (let N denote this number). <ref type="figure">Figure 2</ref> shows the percentage of an- swers whose N equals to 0, · · · , 8 and N &gt; 8.</p><p>As can be seen, ∼70% answers are exactly con- stituents (N = 0) and ∼97% answers differ from the closest constituents by less equal to 4 words. This observation motivates us to approach the reading comprehension problem in a constituent- centric manner, where the generation of candidate answers and their representation learning are both based upon constituents. Specifically, we design a Constituent-Centric Neural Network (CCNN) to perform end-to-end reading comprehension, where the inputs are the passage and question, and the output is a span in the passage that is mostly suitable to answer this question. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the CCNN con- tains four layers. In the encoding layer, the chain- of-trees LSTM and tree LSTM encode the con- stituents in the passage and question respectively. The encodings are fed to the tree-guided atten- tion layer to learn question-aware representations, which are passed to the candidate-answer gener- ation layer to produce and encode the candidate answers based on constituent expansion. Finally, the prediction layer picks up the best answer from the candidates using a feed-forward network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoding</head><p>Given the passages and questions, we first use the Stanford parser to parse them into constituent parse trees, then the encoding layer of CCNN learns representations for constituents in questions and passages, using tree LSTM ( <ref type="bibr" target="#b23">Tai et al., 2015)</ref> and chain-of-trees LSTM respectively. These LSTM encoders are able to capture the syntactic properties of constituents and long-range seman- tics across multiple sentences, which are crucial for SQuAD QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Tree LSTM for Question Encoding</head><p>Each question is a single sentence, having one constituent parse tree. Internal nodes in the tree represent constituents having more than one word and leaf nodes represent single-word constituent. Inspired by <ref type="bibr" target="#b23">(Tai et al., 2015;</ref><ref type="bibr" target="#b24">Teng and Zhang, 2016)</ref>, we build a bi-directional tree LSTM which consists of a bottom-up LSTM and a top-down LSTM, to encode these constituents (as shown in <ref type="figure" target="#fig_3">Figure 4</ref>). Each node (constituent) has two hid- den states: h ↑ produced by the LSTM in bottom- up direction and h ↓ produced by the LSTM in top-down direction. Let T denote the maximum number of children an internal node could have. For each particular node, let L (0 ≤ L ≤ T ) be the number of children it has, h …... …... ↓ be the top-down hidden state and memory cell of the parent.</p><p>In the bottom-up LSTM, each node has an input</p><formula xml:id="formula_0">gate i ↑ , L forget gates {f (l) ↑ } L l=1</formula><p>corresponding to different children, an output gate o ↑ and a memory cell c ↑ . For an internal node, the inputs are the hidden states and memory cells of its children and the transition equations are defined as:</p><formula xml:id="formula_1">i ↑ = σ( L l=1 W (i,l) ↑ h (l) ↑ + b (i) ↑ ) ∀l, f (l) ↑ = σ(W (f,l) ↑ h (l) ↑ + b (f,l) ↑ ) o ↑ = σ( L l=1 W (o,l) ↑ h (l) ↑ + b (o) ↑ ) u ↑ = tanh( L l=1 W (u,l) ↑ h (l) ↑ + b (u) ↑ ) c ↑ = i ↑ u ↑ + L l=1 f (l) ↑ c (l) ↑ h ↑ = o ↑ tanh(c ↑ )<label>(1)</label></formula><p>where the weight parameters W and bias parame- ters b with superscript l such as W</p><formula xml:id="formula_2">(i,l) ↑</formula><p>are specific to the l-th child. For a leaf node which represents a single word, it has no forget gate and the input is the wording embedding ( <ref type="bibr" target="#b16">Pennington et al., 2014)</ref> of this word.</p><p>In the top-down direction, the gates, memory cell and hidden state are defined in a similar fash- ion as the bottom-up direction (Eq. <ref type="formula" target="#formula_1">(1)</ref>). For an in- ternal node except the root, the inputs are the hid- den state h ↑ captures the semantics of all constituents, which is then repli- cated as h r ↓ and propagated downwards to each in- dividual constituent.</p><p>Concatenating the hidden states of two direc- tions, we obtain the LSTM encoding for each node h = [h ↑ ; h ↓ ] which will be the input of the atten- tion layer. The bottom-up hidden state h ↑ com- poses the semantics of sub-constituents contained in this constituent and the top-down hidden state h ↓ captures the contextual semantics manifested in the entire sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Chain-of-Trees LSTM for Passage Encoding</head><p>To encode the passage which contains multiple sentences, we design a chain-of-trees LSTM <ref type="figure" target="#fig_3">(Fig- ure 4)</ref>. A bi-directional tree LSTM is built for each sentence to capture the local syntactic struc- ture and these tree LSTMs are glued together via a bi-directional chain LSTM ( <ref type="bibr" target="#b6">Graves et al., 2013)</ref> to capture long-range semantics spanning multi- ple sentences. The hidden states generated by the bottom-up tree LSTM serves as the input of the chain LSTM. Likewise, the chain LSTM states are fed to the top-down tree LSTM. This enables the encoding of every constituent to be propagated to all other constituents in the passage.</p><p>In the chain LSTM, each sentence t is treated as a unit. The input of this unit is generated by the tree LSTM of sentence t, which is the bottom-up hidden state h ↑t at the root. Sentence t is associ- ated with a forward hidden state − → h t and a back- ward state ← − h t . In the forward direction, the tran- sition equations among the input gate − → i t , forget gate − → f t , output gate − → o t and memory cell − → c t are:</p><formula xml:id="formula_3">− → i t = σ( − → W (i) h ↑t + − → U (i) − → h t−1 + − → b (i) ) − → f t = σ( − → W (f ) h ↑t + − → U (f ) − → h t−1 + − → b (f ) ) − → o t = σ( − → W (o) h ↑t + − → U (o) − → h t−1 + − → b (o) ) − → u t = tanh( − → W (u) h ↑t + − → U (u) − → h t−1 + − → b (u) ) − → c t = − → i t − → u t + − → f t − → c t−1 − → h t = − → o t tanh( − → c t )<label>(2)</label></formula><p>The backward LSTM is defined in a similar way. Subsequently, − → h t and ← − h t , which encapsulate the semantics of all sentences, are inputted to the root of the top-down tree LSTM and propagated to all the constituents in sentence t.</p><p>To sum up, the CT-LSTM encodes a passage in the following way: (1) the bottom-up tree LSTMs compute hidden states h ↑ for each sentence and feed h ↑ of the root node into the chain LSTM; (2) the chain LSTM computes forward and backward states and feed them into the root of the top-down tree LSTMs; (3) the top-down tree LSTMs com- pute hidden states h ↓ . At each constituent C, the bottom-up state h ↑ captures the semantics of sub- constituents in C and the top-down state h ↓ cap- tures the semantics of the entire passage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tree-Guided Attention Mechanism</head><p>We propose a tree-guided attention (TGA) mech- anism to learn a question-aware representation for each constituent in the passage, which consists of three ingredients: (1) constituent-level attention score computation; (2) tree-guided local normal- ization; (3) tree-guided attentional summarization. Given a constituent h (p) in the passage, for each constituent h (q) in the question, an unnormalized attention weight score a is computed as a = h (p) · h (q) which measures the similarity between the two constituents. Then we perform a tree-guided local normalization of these scores. At each in- ternal node in the parse tree, where the unnormal- ized attention scores of its L children are {a l } L l=1 , a local normalization is performed using a softmax operation a l = exp(a l )/ L m=1 exp(a m ) which maps these scores into a probabilistic simplex. This normalization scheme stands in contrast with the global normalization adopted in word-based attention ( <ref type="bibr" target="#b27">Wang and Jiang, 2016;</ref>, where a single softmax is globally applied to the attention scores of all the words in the ques- tion.</p><p>Given these locally normalized attention scores, we merge the LSTM encodings of constituents in the question into an attentional representation in a recursive and bottom-up way. At each internal node, let h be its LSTM encoding, a and {a l } L l=1 be the normalized attention scores of this node and its L children, and {b l } L l=1 be the attentional rep- resentations (which we will define later) generated at the children, then the attentional representation b of this node is defined as:</p><formula xml:id="formula_4">b = a(h + L l=1 a l b l )<label>(3)</label></formula><p>which takes the weighted representation L l=1 a l b l contributed from its children, adds in its own encoding h, then performs a re-weighting using the attention score a. The attentional rep- resentation b (r) at the root node acts as the final summarization of constituents in the question. We concatenate it to the LSTM encoding h (p) of the passage constituent and obtain a concatenated representation z = [h (p) ; b (r) ] which will be the input of the candidate answer generation layer. Expansion of C1 ("the medical faculty") 1. C1 2. from the medical faculty è C2 3. came from the medical faculty è came C2 4. the medical faculty in è C1 in 5. the medical faculty in Paris è C1 C3 6. from the medical faculty in è C2 in 7. from the medical faculty in Paris è C2 C3 8. came from the medical faculty in è came C2 in 9. came from the medical faculty in Paris è C4 came C2 in Unlike the word-based flat-structure attention mechanism ( <ref type="bibr" target="#b27">Wang and Jiang, 2016;</ref> where the attention scores are computed be- tween words and normalized using a single global softmax, and the attentional summary is computed in a flat manner, the tree-guided attention calcu- lates attention scores between constituents, nor- malizes them locally at each node in the parse tree and computes the attentional summary in a hierar- chical way. Tailored to the parse tree, TGA is able to capture the syntactic, hierarchical and composi- tional structures among constituents and arguably generate better attentional representations, as we will validate in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Candidate Answer Generation</head><p>As shown in <ref type="figure">Figure 2</ref>, while most correct answers in the training set are exactly constituents, some of them are not the case. To cover the non-constituent answers, we propose to expand each constituent by appending words adjacent to it. Let C denote a constituent and S = " · · · w i−1 w i Cw j w j+1 · · · " be the sentence containing C. We expand C by ap- pending words preceding C (such as w i−1 and w i ) and words succeeding C (such as w j and w j+1 ) to C. We define an (l, r)-expansion of a constituent C as follows: append l words preceding C in the sentence to C; append r words succeeding C to C. Let M be the maximum expansion number that l ≤ M and r ≤ M . <ref type="figure" target="#fig_6">Figure 5</ref> shows an ex- ample. On the left is the constituent parse tree of the sentence "it came from the medical fac- ulty in Paris". On the upper right are the expan- sions of the constituent C1 -"the medical fac- ulty". To expand this constituent, we trace it back to the sentence and look up the M (M =2 in this case) words preceding C1 (which are "came" and "from") and succeeding C1 (which are "in" and "Paris"). Then combinations of C1 and the preced- ing/succeeding words are taken to generate con- stituent expansions. On both the left and right side of C1, we have three choices of expansion: ex- panding 0,1,2 words. Taking combination of these cases, we obtain 9 expansions, including C1 itself ((0, 0)-expansion).</p><p>The next step is to perform reduction of con- stituent expansions. Two things need to be re- duced. First, while expanding the current con- stituent, new constituents may come into being. For instance, in the expansion "came from C1 in Paris", "in" and "Paris" form a constituent C3; "from" and C1 form a constituent C2; "came", C2 and C3 form a constituent C4. Eventually, this expansion is reduced to C4. Second, the expan- sions generated from different constituents may have overlap and the duplicated expansions need to be removed. For example, the (2, 1)-expansion of C1 -"came from the medical faculty in" -can be reduced to "came C2 in", which is the (1, 1)- expansion of C2. After reduction, each expansion is a sequence of constituents.</p><p>Next we encode these candidate answers and the encodings will be utilized in the prediction layer. In light of the fact that each expansion is a constituent sequence, we build a bi-directional chain LSTM ( <ref type="figure" target="#fig_6">Figure 5</ref>, bottom right) to synthe- size the representations of individual constituents therein. Let E = C 1 · · · C n be an expansion con- sisting of n constituents. In the chain LSTM, the input of unit i is the combined representation of C i . We concatenate the forward hidden state at C n and backward state at C 1 as the final represen- tation of E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Answer Prediction and Parameter Learning</head><p>Given the representation of candidate answers, we use a feed-forward network f : R d → R to predict the correct answer. The input of the network is the feature vector of a candidate answer and the output is a confidence score. The one with the largest score is chosen as the the correct answer. For parameter learning, we normalize the con- fidence scores into a probabilistic simplex using softmax and define a cross entropy loss thereupon. Let J k be the number of candidate answers pro- duced from the k-th passage-question pair and</p><formula xml:id="formula_5">{z (k) j } J k</formula><p>j=1 be their representations. Let t k be the index of the correct answer. Then the cross en- tropy loss of K pairs is defined as</p><formula xml:id="formula_6">K k=1 (−f (z t k ) + log J k j=1 exp(f (z (k) j ))) (4)</formula><p>Model parameters are learned by minimizing this loss using stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>The experiments are conducted on the Stan- ford Question Answering Dataset (SQuAD) v1.1, which contains 107,785 questions and 23,215 pas- sages coming from 536 Wikipedia articles. The data was randomly partitioned into a training set (80%), a development set (10%) and an unreleased test set (10%). <ref type="bibr" target="#b17">Rajpurkar et al. (2016)</ref> build a leaderboard to evaluate and publish results on the test set. Due to software copyright issues, we did not participate this online evaluation. Instead, we use the development set (which is untouched dur- ing model training) as test set. In training, if the correct answer is not in the candidate-answer set, we use the shortest candidate containing the cor- rect answer as the target.</p><p>The Stanford parser is utilized to obtain the con- stituent parse trees for questions and passages. In the parse tree, any internal node which has one child is merged together with its child. For instance, in "(NP (NNS sufferers))", the parent "NP" has only one child "(NNS sufferers)", we merge them into "(NP sufferers)". We use 300- dimensional word embeddings from GloVe <ref type="bibr" target="#b16">(Pennington et al., 2014</ref>) to initialize the model. Words not found in GloVe are initialized as zero vectors.</p><p>We use a feed-forward network with 2 hidden layers (both having the same amount of units) for answer prediction. The activation function is set to rectified linear. Hyperparameters in CCNN are tuned via 5-fold cross validation (CV) on the training set, summarized in <ref type="table" target="#tab_1">Table 1</ref>. We use the ADAM ( <ref type="bibr" target="#b12">Kingma and Ba, 2014)</ref> optimizer to train the model with an initial learning rate 0.001 and a mini-batch size 100. An ensemble model is also trained, consisting of 10 training runs using the same hyperparameters. The performance is eval- uated by two metrics <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref>: (1) exact match (EM) which measures the percentage of predictions that match any one of the ground truth answers exactly; (2) F1 score which mea- sures the average overlap between the prediction and ground truth answer. In the development set each question has about three ground truth an- swers. F1 scores with the best matching answers are used to compute the average F1 score. <ref type="table">Table 2</ref> shows the performance of our model and previous approaches on the development set. CCNN (single model) achieves an EM score of 69.3% and an F1 score of 78.5%, signifi- cantly outperforming all previous approaches (sin- gle model). Through ensembling, the perfor- mance of CCNN is further improved and out- performs the baseline ensemble methods. The key difference between our method and previous approaches is that CCNN is constituent-centric where the generation and encoding of candidate answers are both based on constituents while the baseline approaches are mostly word-based where the candidate answer is an arbitrary span of words and the encoding is performed over in- dividual words rather than at the constituent level. The constituent-centric model-design enjoys two major benefits. First, restricting the candidate answers from arbitrary spans to neighborhoods around the constituents greatly reduces the search space, which mitigates the ambiguity and uncer- tainty in picking up the correct answer. Sec- ond, the tree LSTMs and tree-guided attention mechanism encapsulate the syntactic, hierarchical and compositional structure among constituents, which leads to better representation learning of the candidate answers. We conjecture these are the primary reasons that CCNN outperforms the base- lines and provide a validation in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>To further understand the individual modules in CCNN, we perform an ablation study. The results are shown in <ref type="table">Table 2</ref>.</p><p>Tree LSTM To evaluate the effectiveness of tree LSTM in learning syntax-aware representations, we replace it with a syntax-agnostic chain LSTM. We build a bi-directional chain LSTM (denoted by A) over the entire passage to encode the individ- ual words. Given a constituent C = w i · · · w j , we build another bi-directional chain LSTM (de- noted by B) over C where the inputs are the en- codings of words w i , · · · , w j generated by LSTM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tuning Range</head><p>Best Choice Maximum expansion number M in constituent expansion 0, 1, 2, 3, 4, 5 2 Size of hidden state in all LSTMs 50, 100, 150, 200, 250, 300 100 Size of hidden state in prediction network 100, 200, 300, 400, 500 400  <ref type="table">Table 2</ref>: Results on the development set A. In LSTM B, the forward hidden state of w j and backward state of w i are concatenated to represent C. Note that the attention mechanism remains in- tact, which is still guided by the parse tree. This replacement cause 5.8% and 4.6% drop of the EM and F1 scores respectively, which demonstrates the necessity of incorporating syntactic structure (via tree LSTM) into representation learning.</p><p>Chain-of-Trees LSTM (CT-LSTM) We evalu- ate the effectiveness of CT-LSTM by comparing it with a bag of tree LSTMs: instead of using a chain LSTM to glue the tree LSTMs, we treat them as independent. Keeping the other modules intact and replacing CT-LSTM with a bag of inde- pendent tree LSTMs, the EM and F1 score drop 4.5% and 3.3% respectively. The advantage of CT-LSTM is that it enables the semantics of one sentence to be propagated to others, which makes multiple-sentence reasoning possible.  j in the question (which has R constituents). Then a global softmax op- eration is applied to these scores, {ã ij } R j=1 = sof tmax({a ij } R j=1 ), to project them into a prob- abilistic simplex. Finally, a flat summariza-</p><formula xml:id="formula_7">tion R j=1ãj=1˜j=1ã ij h (q)</formula><p>j is computed and appended to h (p) i . Replacing TGA with flat-structure attention causes the EM and F1 to drop 3.7% and 2.6% re- spectively, which demonstrates the advantage of the tree-guided mechanism.</p><p>Constituent Expansion We study how the max- imum expansion number M affects performance. If M is too small, many correct answers are not contained in the candidate set, which results in low recall. If M is too large, excessive candidates are generated, making it harder to pick up the correct one. <ref type="figure" target="#fig_7">Figure 6</ref>(a) shows how EM and F1 vary as M increases, from which we can see a value of M in the middle ground achieves the best tradeoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Analysis</head><p>In this section, we study how CCNN behaves across different answer length (number of words in the answer) and question types, which are shown in <ref type="figure" target="#fig_7">Figure 6</ref>(b) and (c). In <ref type="figure" target="#fig_7">Figure 6</ref>(b), we compare with the MPCM method ( ). As answer length increases, the perfor- mance of both methods decreases. This is be- cause for longer answers, it is more difficult to pinpoint the precise boundaries. The decreasing of F1 is slower than EM, because F1 is more elas- tic to small mismatches. Our method achieves larger improvement over MPCM at longer an- swers. We conjecture the reason is: longer answers have more complicated syntactic struc- ture, which can be better captured by the tree LSTMs and tree-guided attention mechanism in our method. MPCM is built upon individual words and is syntax-agnostic.</p><p>In <ref type="figure" target="#fig_7">Figure 6</ref>(c), we compare with <ref type="bibr">DCN (Xiong et al., 2016</ref>) on 8 question types. Our method achieves significant improvement over DCN on four types: "what", "where", "why" and "other". The answers of questions in these types are typi- cally longer and have more complicated syntactic structure than the other four types where the an- swers are mostly entities <ref type="bibr">(person, numeric, time, etc.</ref>). The syntax-aware nature of our method makes it outperform DCN whose model design does not explicitly consider syntactic structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Works</head><p>Several neural network based approaches have been proposed to solve the SQuAD QA problem, which we briefly review from three aspects: can- didate answer generation, representation learning and attention mechanism.</p><p>Two ways were investigated for candidate an- swer generation: (1) chunking: candidates are preselected based on lexical and syntactic analy- sis, such as constituent parsing <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref> and part-of-speech pattern ( <ref type="bibr" target="#b33">Yu et al., 2016)</ref>; (2) directly predicting the start and end position of the answer span, using feed-forward neural network ( ), LSTM ( <ref type="bibr" target="#b20">Seo et al., 2016)</ref>, pointer network ( <ref type="bibr" target="#b25">Vinyals et al., 2015;</ref><ref type="bibr" target="#b27">Wang and Jiang, 2016)</ref>, dynamic pointer decoder ( <ref type="bibr" target="#b29">Xiong et al., 2016)</ref>.</p><p>The representation learning in previous ap- proaches is conducted over individual words us- ing the following encoders: LSTM in ( <ref type="bibr" target="#b29">Xiong et al., 2016)</ref>; bi-directional gated re- current unit ( <ref type="bibr" target="#b2">Chung et al., 2014</ref>) in ( <ref type="bibr" target="#b33">Yu et al., 2016)</ref>; match-LSTM in ( <ref type="bibr" target="#b27">Wang and Jiang, 2016)</ref>; bi-directional LSTM in ( <ref type="bibr" target="#b20">Seo et al., 2016)</ref>.</p><p>In previous approaches, the attention ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b30">Xu et al., 2015</ref>) mechanism is mostly word-based and flat-structured ( <ref type="bibr" target="#b11">Kadlec et al., 2016;</ref><ref type="bibr" target="#b22">Sordoni et al., 2016;</ref><ref type="bibr" target="#b27">Wang and Jiang, 2016;</ref><ref type="bibr" target="#b33">Yu et al., 2016</ref>): the at- tention scores are computed between individual words, are normalized globally and are used to summarize word-level encodings in a flat manner. <ref type="bibr" target="#b5">Cui et al. (2016)</ref>; <ref type="bibr" target="#b29">Xiong et al. (2016)</ref> explored a coattention mechanism to learn question-to- passage and passage-to-question summaries. <ref type="bibr" target="#b20">Seo et al. (2016)</ref> proposed to directly use the attention weights as augmented features instead of applying them for early summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>To solve the SQuAD question answering prob- lem, we design a constituent centric neural net- work (CCNN), where the generation and repre- sentation learning of candidate answers are both based on constituents. We use a constituent ex- pansion mechanism to produce candidate answers, which can greatly reduce the search space with- out losing the recall of hitting the correct an- swer. To represent these candidate answers, we propose a chain-of-trees LSTM to encode con- stituents and a tree-guided attention mechanism to learn question-aware representations. Evaluations on the SQuAD dataset demonstrate the effective- ness of the constituent-centric neural architecture.</p><p>For future work, we will investigate the wider applicability of chain-of-trees LSTM as a general text encoder that can simultaneously capture lo- cal syntactic structure and long-range semantic de- pendency. It can be applied to named entity recog- nition, sentiment analysis, dialogue generation, to name a few. We will also apply the tree-guided at- tention mechanism to NLP tasks that need syntax- aware attention, such as machine translation, sen- tence summarization, textual entailment, etc. An- other direction to explore is joint learning of syn- tactic parser and chain-of-trees LSTM. Currently, the two are separated, which may lead to subopti- mal performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Constituent-centric neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>↑</head><label></label><figDesc>and c (l) ↑ be the bottom-up hidden state and memory cell of the l- th (1 ≤ l ≤ L) child (if any) respectively and h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Chain-of-trees LSTM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>↓</head><label></label><figDesc>and memory cell c (p) ↓ of its parents. For a leaf node, in addition to h (p) ↓ and c (p) ↓ , the inputs also contain the word embedding. For the root node, the top-down hidden state h (r) ↓ is set to its bottom-up hidden state h (r) ↑ . h (r)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Constituent expansion. (Left) Parse tree of a sentence in the passage. (Top Right) Expansions of constituent C1 and their reductions (denoted by arrow). (Bottom Right) Learning the representation of an expansion using bidirectional chain-LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance for different (a) M (expansion number), (b) answer length, (c) question type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Hyperparameter Tuning</head><label>1</label><figDesc></figDesc><table>Exact Match (EM,%) F1 (%) 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02858</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">My computer is an honor student-but how intelligent is it? standardized tests as a measure of ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining retrieval, statistics, and inference to answer elementary science questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khashabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2580" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Attention-overattention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02301</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to solve arithmetic word problems with verb categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="523" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01547</idno>
		<title level="m">Text understanding with the attention sum reader network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
		<title level="m">The stanford corenlp natural language processing toolkit</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning answerentailing structures for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinmaya</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="239" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reasonet: Learning to stop reading in machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05284</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02245</idno>
		<title level="m">Iterative alternating neural attention for machine reading</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Bidirectional tree-structured lstm with head lexicalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Machine comprehension with syntax, frames, and semantics</title>
		<editor>Hai Wang and Mohit Bansal Kevin Gimpel David McAllester</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07905</idno>
		<title level="m">Machine comprehension using match-lstm and answer pointer</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multi-perspective context matching for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04211</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dynamic coattention networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01604</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wikiqa: A challenge dataset for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Citeseer</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Words or characters? fine-grained gating for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01724</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">End-to-end answer chunk extraction and ranking for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazi</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09996</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
