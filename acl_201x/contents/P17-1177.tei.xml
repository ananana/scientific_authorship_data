<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1936" to="1945"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1177</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this paper , we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidi-rectional tree encoder which learns both sequential and tree structured representations ; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential atten-tional model as well as a stronger baseline with a bottom-up tree encoder and word coverage. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Most neural machine translation (NMT) models are based on the sequential encoder-decoder framework, which makes no use of syntactic information. In this pa- per, we improve this model by explicitly incorporating source-side syntactic trees. More specifically, we propose (1) a bidi- rectional tree encoder which learns both sequential and tree structured representa- tions; (2) a tree-coverage model that lets the attention depend on the source-side syntax. Experiments on Chinese-English translation demonstrate that our proposed models outperform the sequential atten- tional model as well as a stronger baseline with a bottom-up tree encoder and word coverage. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, neural machine translation (NMT) mod- els ( <ref type="bibr" target="#b19">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> have obtained state-of-the-art performance on many language pairs. Their success depends on the representation they use to bridge the source and target language sentences. However, this rep- resentation, a sequence of fixed-dimensional vec- tors, differs considerably from most theories about mental representations of sentences, and from tra- ditional natural language processing pipelines, in which semantics is built up compositionally using a recursive syntactic structure. Perhaps as evidence of this, current NMT mod- els still suffer from syntactic errors such as at- tachment ( <ref type="bibr" target="#b6">Shi et al., 2016)</ref>. We argue that instead of letting the NMT model rely solely on the im- plicit structure it learns during training ( <ref type="bibr">Cho</ref>  (b) binarized source side tree <ref type="figure">Figure 1</ref>: An example sentence pair (a), with its binarized source side tree (b). We use x i to rep- resent the i-th word in the source sentence. We will use this sentence pairs as the running exam- ple throughout this paper. 2014a), we can improve its performance by aug- menting it with explicit structural information and using this information throughout the model. This has two benefits.</p><p>First, the explicit syntactic information will help the encoder generate better source side represen- tations. <ref type="bibr" target="#b12">Li et al. (2015)</ref> show that for tasks in which long-distance semantic dependencies mat- ter, representations learned from recursive mod- els using syntactic structures may be more pow- erful than those from sequential recurrent models. In the NMT case, given syntactic information, it will be easier for the encoder to incorporate long distance dependencies into better representations, which is especially important for the translation of long sentences.</p><p>Second, it becomes possible for the decoder to use syntactic information to guide its reordering decisions better (especially for language pairs with significant reordering, like Chinese-English). Al- though the attention model ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) and the coverage model ( <ref type="bibr" target="#b22">Tu et al., 2016;</ref><ref type="bibr" target="#b14">Mi et al., 2016)</ref> provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohe- sion between the two languages <ref type="bibr" target="#b7">(Fox, 2002;</ref><ref type="bibr" target="#b8">Kim et al., 2017)</ref>. With explicit syntactic structure, the decoder can generate the translation more in line with the source syntactic structure. For example, when translating the phrase zhu manila dashiguan in <ref type="figure">Figure 1</ref>, the tree structure indicates that zhu 'in' and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incor- rect translation like "in embassy of manila" 2 .</p><p>In this paper, we propose a novel encoder- decoder model that makes use of a precomputed source-side syntactic tree in both the encoder and decoder. In the encoder ( §3.3), we improve the tree encoder of <ref type="bibr" target="#b5">Eriguchi et al. (2016)</ref> by introducing a bidirectional tree encoder. For each source tree node (including the source words), we generate a representation containing information both from below (as with the original bottom-up encoder) and from above (using a top-down encoder). Thus, the annotation of each node summarizes the sur- rounding sequential context, as well as the entire syntactic context.</p><p>In the decoder ( §3.4), we incorporate source syntactic tree structure into the attention model via an extension of the coverage model of <ref type="bibr" target="#b22">Tu et al. (2016)</ref>. With this tree-coverage model, we can bet- ter guide the generation phase of translation, for example, to learn a preference for phrasal cohe- sion <ref type="bibr" target="#b7">(Fox, 2002)</ref>. Moreover, with a tree encoder, the decoder may try to translate both a parent and a child node, even though they overlap; the tree- coverage model enables the decoder to learn to avoid this problem.</p><p>To demonstrate the effectiveness of the pro- posed model, we carry out experiments on Chinese-English translation. Our experiments show that: (1) our bidirectional tree encoder based NMT system achieves significant improvements over the standard attention-based NMT system, and (2) incorporating source tree structure into the attention model yields a further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>According to the source sentence, "embassy" belongs to "australia", not "manila".</p><formula xml:id="formula_0">x 1 x 2 x 3 x 4 x 5 x 6 − → h 1 − → h 2 − → h 3 − → h 4 − → h 5 − → h 6 ← − h 1 ← − h 2 ← − h 3 ← − h 4 ← − h 5 ← − h 6</formula><p>Figure 2: Illustration of the bidirectional sequen- tial encoder. The dashed rectangle represents the annotation of word x i .</p><p>In all, we demonstrate an improvement of +3.54 BLEU over a standard attentional NMT system, and +1.90 BLEU over a stronger NMT system with a Tree-LSTM encoder ( <ref type="bibr" target="#b5">Eriguchi et al., 2016</ref>) and a coverage model ( <ref type="bibr" target="#b22">Tu et al., 2016)</ref>. To the best of our knowledge, this is the first work that uses source-side syntax in both the encoder and decoder of an NMT system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>Most NMT systems follow the encoder-decoder framework with attention, first proposed by <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>. Given a source sentence x = x 1 · · · x i · · · x I and a target sentence y = y 1 · · · y j · · · y J , NMT aims to directly model the translation probability:</p><formula xml:id="formula_1">P(y | x; θ) = J 1 P(y j | y &lt;j , x; θ),<label>(1)</label></formula><p>where θ is a set of parameters and y &lt; j is the sequence of previously generated target words. Here, we briefly describe the underlying frame- work of the encoder-decoder NMT system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Encoder Model</head><p>Following <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, we use a bidi- rectional gated recurrent unit (GRU) ( <ref type="bibr" target="#b3">Cho et al., 2014b</ref>) to encode the source sentence, so that the annotation of each word contains a summary of both the preceding and following words. The bidi- rectional GRU consists of a forward and a back- ward GRU, as shown in <ref type="figure">Figure 2</ref>. The forward GRU reads the source sentence from left to right and calculates a sequence of forward hidden states (</p><formula xml:id="formula_2">− → h 1 , . . . , − → h I ).</formula><p>The backward GRU scans the source sentence from right to left, resulting in a sequence of backward hidden states (</p><formula xml:id="formula_3">← − h 1 , . . . , ← − h I ). Thus − → h i = GRU( − − → h i−1 , s i ) ← − h i = GRU( ← − − h i−1 , s i )<label>(2)</label></formula><p>where s i is the i-th source word's word embedding, and GRU is a gated recurrent unit; see the paper by <ref type="bibr" target="#b3">Cho et al. (2014b)</ref> for a definition. The annotation of each source word x i is ob- tained by concatenating the forward and backward hidden states:</p><formula xml:id="formula_4">← → h i =        − → h i ← − h i        .</formula><p>The whole sequence of these annotations is used by the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoder Model</head><p>The decoder is a forward GRU predicting the translation y word by word. The probability of generating the j-th word y j is:</p><formula xml:id="formula_5">P(y j | y &lt;j , x; θ) = softmax(t j−1 , d j , c j ) (3)</formula><p>where t j−1 is the word embedding of the ( j − 1)- th target word, d j is the decoder's hidden state of time j, and c j is the context vector at time j. The state d j is computed as</p><formula xml:id="formula_6">d j = GRU(d j−1 , t j−1 , c j ),<label>(4)</label></formula><p>where GRU(·) is extended to more than two argu- ments by first concatenating all arguments except the first. The attention mechanism computes the context vector c i as a weighted sum of the source annota- tions,</p><formula xml:id="formula_7">c j = I i=1 α j,i ← → h i (5)</formula><p>where the attention weight α j,i is</p><formula xml:id="formula_8">α j,i = exp (e j,i ) I i =1 exp (e j,i ) (6) and e j,i = v T a tanh (W a d j−1 + U a ← → h i ) (7)</formula><p>where v a , W a and U a are the weight matrices of the attention model, and e j,i is an attention model that scores how well d j−1 and ← → h i match. With this strategy, the decoder can attend to the source annotations that are most relevant at a given time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tree Structure Enhanced Neural Machine Translation</head><p>Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems <ref type="bibr" target="#b25">(Yamada and Knight, 2001;</ref><ref type="bibr" target="#b9">Koehn et al., 2003;</ref><ref type="bibr" target="#b13">Liu et al., 2006;</ref><ref type="bibr" target="#b1">Chiang, 2007)</ref>, most pro- posed NMT models (a notable exception being that of <ref type="bibr" target="#b5">Eriguchi et al. (2016)</ref>) process a sentence only as a sequence of words, and do not explic- itly exploit the inherent structure of natural lan- guage sentences. In this section, we present mod- els which directly incorporate source syntactic trees into the encoder-decoder framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Like Eriguchi et al. <ref type="formula" target="#formula_1">(2016)</ref>, we currently focus on source side syntactic trees, which can be computed prior to translation. Whereas <ref type="bibr" target="#b5">Eriguchi et al. (2016)</ref> use HPSG trees, we use phrase-structure trees as in the Penn Chinese Treebank ( <ref type="bibr" target="#b23">Xue et al., 2005</ref>). Currently, we are only using the structure infor- mation from the tree without the syntactic labels.</p><p>Thus our approach should be applicable to any syntactic grammar that provides such a tree struc- ture ( <ref type="figure">Figure 1(b)</ref>). More formally, the encoder is given a source sentence x = x 1 · · · x I as well as a source tree whose leaves are labeled x 1 , . . . , x I . We assume that this tree is strictly binary branching. For con- venience, each node is assigned an index. The leaf nodes get indices 1, . . . , I, which is the same as their word indices. For any node with index k, let p(k) denote the index of the node's parent (if it ex- ists), and L(k) and R(k) denote the indices of the node's left and right children (if they exist).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tree-GRU Encoder</head><p>We first describe tree encoders <ref type="bibr" target="#b20">(Tai et al., 2015;</ref><ref type="bibr" target="#b5">Eriguchi et al., 2016)</ref>, and then discuss our im- provements.</p><p>Following <ref type="bibr" target="#b5">Eriguchi et al. (2016)</ref>, we build a tree encoder on top of the sequential encoder (as shown in <ref type="figure" target="#fig_0">Figure 3(a)</ref>). If node k is a leaf node, its hidden state is the annotation produced by the sequential encoder:</p><formula xml:id="formula_9">h ↑ k = ← → h k .</formula><p>Thus, the encoder is able to capture both sequen- tial context and syntactic context. If node k is an interior node, its hidden state is the combination of its previously calculated left child hidden state h L(k) and right child hidden state h R(k) :</p><formula xml:id="formula_10">h ↑ k = f (h ↑ L(k) , h ↑ R(k) )<label>(8)</label></formula><p>where f (·) is a nonlinear function, originally a Tree-LSTM ( <ref type="bibr" target="#b20">Tai et al., 2015;</ref><ref type="bibr" target="#b5">Eriguchi et al., 2016</ref>). The first improvement we make to the above tree encoder is that, to be consistent with the se- quential encoder model, we use Tree-GRU units instead of Tree-LSTM units. Similar to Tree- LSTMs, the Tree-GRU has gating mechanisms to control the information flow inside the unit for every node without separate memory cells. Then, Eq. 8 is calculated by a Tree-GRU as follows:</p><formula xml:id="formula_11">r L = σ(U (rL) L h ↑ L(k) + U (rL) R h ↑ R(k) + b (rL) ) r R = σ(U (rR) L h ↑ L(k) + U (rR) R h ↑ R(k) + b (rR) ) z L = σ(U (zL) L h ↑ L(k) + U (zL) R h ↑ R(k) + b (zL) ) z R = σ(U (zR) L h ↑ L(k) + U (zR) R h ↑ R(k) + b (zR) ) z = σ(U (z) L h ↑ L(k) + U (z) R h ↑ R(k) + b (z) ) ˜ h ↑ k = tanh U L (r L h ↑ L(k) ) + U R (r R h ↑ R(k) ) h ↑ k = z L h ↑ L(k) + z R h ↑ R(k) + z ˜ h ↑ k</formula><p>where r L , r R are the reset gates and z L , z R are the update gates for the left and right children, and z is the update gate for the internal hidden state˜hstate˜ state˜h ↑ k . The U (·) and b (·) are the weight matrices and bias vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bidirectional Tree Encoder</head><p>Although the bottom-up tree encoder can take ad- vantage of syntactic structure, the learned repre- sentation of a node is based on its subtree only; it contains no information from higher up in the tree. In particular, the representation of leaf nodes is still the sequential one. Thus no syntactic infor- mation is fed into words. By analogy with the bidi- rectional sequential encoder, we propose a natural extension of the bottom-up tree encoder: the bidi- rectional tree encoder <ref type="figure" target="#fig_0">(Figure 3(b)</ref>).</p><p>Unlike the bottom-up tree encoder or the right- to-left sequential encoder, the top-down encoder by itself would have no lexical information as in- put. To address this issue, we feed the hidden states of the bottom-up encoder to the top-down encoder. In this way, the information of the whole syntactic tree is handed to the root node and prop- agated to its offspring by the top-down encoder. In the top-down encoder, each hidden state has only one predecessor. In fact, the top-down path from root of a tree to any node can be viewed as a sequential recurrent neural network. We can calcu- late the hidden states of each node top-down using a standard sequential GRU.</p><formula xml:id="formula_12">x 1 x 2 x 3 x 4 x 5 x 6 − → h 1 − → h 2 − → h 3 − → h 4 − → h 5 − → h 6 − → h 1 ← − h 2 ← − h 3 ← − h 4 ← − h 5 ← − h 6 h ↑ 7 h ↑ 8 h ↑ 9 h ↑ 10 h ↑ 11 (a) Tree-GRU Encoder x 1 x 2 x 3 x 4 x 5 x 6 h ↑ 1 h ↑ 2 h ↑ 3 h ↑ 4 h ↑ 5 h ↑ 6 h ↓ 1 h ↓ 2 h ↓ 3 h ↓ 4 h ↓ 5 h ↓ 6 h ↑ 7 h ↓ 7 h ↑ 8 h ↓ 8 h ↑ 9 h ↓ 9 h ↑ 10 h ↓ 10 h ↑ 11 h ↓ 11 (b) Bidirectional Tree Encoder</formula><p>First, the hidden state of the root node ρ is sim- ply computed as follows:</p><formula xml:id="formula_13">h ↓ ρ = tanh (Wh ↑ ρ + b)<label>(9)</label></formula><p>where W and b are a weight matrix and bias vector. Then, other nodes are calculated by a GRU. For hidden state h ↓ k :</p><formula xml:id="formula_14">h ↓ k = GRU(h ↓ p(k) , h ↑ k )<label>(10)</label></formula><p>where p(k) is the parent index of k. We replace the weight matrices W r , U r , W z , U z , W and U in the standard GRU with P r D , Q r D , P z D , Q z D , P D , and Q D , respectively. The subscript D is either L or R depending on whether node k is a left or right child, respectively.</p><p>Finally, the annotation of each node is obtained by concatenating its bottom-up hidden state and top-down hidden state:</p><formula xml:id="formula_15">h k =         h ↑ k h ↓ k         .</formula><p>This allows the tree structure information flow from the root to the leaves (words). Thus, all the annotations are based on the full context of word sequence and syntactic tree structure. <ref type="bibr" target="#b11">Kokkinos and Potamianos (2017)</ref> propose a similar bidirectional Tree-GRU for sentiment analysis, which differs from ours in several re- spects: in the bottom-up encoder, we use separate reset/update gates for left and right children, anal- ogous to Tree-LSTMs ( <ref type="bibr" target="#b20">Tai et al., 2015)</ref>; in the top- down encoder, we use separate weights for left and right children.</p><p>Teng and Zhang (2016) also propose a bidirec- tional Tree-LSTM encoder for classification tasks. They use a more complex head-lexicalization scheme to feed the top-down encoder. We will compare their model with ours in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Tree-Coverage Model</head><p>We also extend the decoder to incorporate infor- mation about the source syntax into the attention model. We have observed two issues in transla- tions produced using the tree encoder. First, a syn- tactic phrase in the source sentence is often incor- rectly translated into discontinuous words in the output. Second, since the non-leaf node annota- tions contain more information than the leaf node annotations, the attention model prefers to attend to the non-leaf nodes, which may aggravate the over-translation problem (translating the same part of the sentence more than once).</p><p>As shown in <ref type="figure" target="#fig_1">Figure 4</ref>(a), almost all the non-leaf nodes are attended too many times during decod- ing. As a result, the Chinese phrase zhu manila is translated twice because the model attends to the node spanning zhu manila even though both words have already been translated; there is no mecha- nism to prevent this.  <ref type="formula">)</ref>, we propose to use prior knowl- edge to control the attention mechanism. In our case, the prior knowledge is the source syntactic information.</p><p>In particular, we build our model on top of the word coverage model proposed by <ref type="bibr" target="#b22">Tu et al. (2016)</ref>, which alleviate the problems of over-translation and under-translation (failing to translate part of a sentence). The word coverage model makes the attention at a given time step j dependent on the attention at previous time steps via coverage vec- tors:</p><formula xml:id="formula_16">C j,i = GRU(C j−1,i , α j,i , d j−1 , h i ).<label>(11)</label></formula><p>The coverage vectors are, in turn, used to update the attention at the next time step, by a small mod- ification to the calculation of e j,i in Eq. <ref type="formula">(7)</ref>:</p><formula xml:id="formula_17">e j,i = v T a tanh (W a d j−1 + U a h i + V a C j−1,i ). (12)</formula><p>The word coverage model could be interpreted as a control mechanism for the attention model. Like the standard attention model, this coverage model sees the source-sentence annotations as a bag of vectors; it knows nothing about word order, still less about syntactic structure.</p><p>For our model, we extend the word coverage model to coverage on the tree structure by adding a coverage vector for each node in the tree. We further incorporate source tree structure informa- tion into the calculation of the coverage vector by requiring each node's coverage vector to depend on its children's coverage vectors and attentions at the previous time step:</p><formula xml:id="formula_18">C j,i = GRU(C j−1,i , α j,i , d j−1 , h i , C j−1,L(i) , α j,L(i) , C j−1,R(i) , α j,R(i) ).<label>(13)</label></formula><p>Although both child and parent nodes of a sub- tree are helpful for translation, they may supply re- dundant information. With our mechanism, when the child node is used to produce a translation, the coverage vector of its parent node will re- flect this fact, so that the decoder may avoid using the redundant information in the parent node. <ref type="figure" target="#fig_1">Fig- ure 4(b)</ref> shows a heatmap of the attention of our tree structure enhanced attention model. The atten- tion of non-leaf nodes becomes more concentrated and the over-translation of zhu manila is corrected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We conduct experiments on the NIST Chinese- English translation task. The parallel training data consists of 1.6M sentence pairs extracted from LDC corpora, 3 with 46.6M Chinese words and 52.5M English words, respectively. We use NIST MT02 as development data, and NIST MT03-06 as test data. These data are mostly in the same genre (newswire), avoiding the extra consideration of domain adaptation. <ref type="table">Table 1</ref> shows the statis- tics of the data sets. The Chinese side of the cor- pora is word segmented using ICTCLAS. <ref type="bibr">4</ref>   <ref type="bibr" target="#b17">and Klein, 2007)</ref> and binarize the resulting trees following <ref type="bibr" target="#b28">Zhang and Clark (2009)</ref>. The English side of the corpora is lowercased and tokenized.</p><p>We filter out any translation pairs whose source sentences fail to be parsed. For efficient training, we also filter out the sentence pairs whose source or target lengths are longer than 50. We use a shortlist of the 30,000 most frequent words in each language to train our models, covering approxi- mately 98.2% and 99.5% of the Chinese and En- glish tokens, respectively. All out-of-vocabulary words are mapped to a special symbol UNK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model and Training Details</head><p>We compare our proposed models with several state-of-the-art NMT systems and techniques:</p><p>• NMT: the standard attentional NMT model ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>).</p><p>• Tree-LSTM: the attentional NMT model extended with the Tree-LSTM encoder ( <ref type="bibr" target="#b5">Eriguchi et al., 2016</ref>).</p><p>• Coverage: the attentional NMT model ex- tended with word coverage ( <ref type="bibr" target="#b22">Tu et al., 2016</ref>).</p><p>We used the dl4mt implementation of the atten- tional model, <ref type="bibr">6</ref> reimplementing the tree encoder and word coverage models. The word embed- ding dimension is 512. The hidden layer sizes of both forward and backward sequential encoder are 1024 (except where indicated). Since our Tree- GRU encoders are built on top of the bidirectional sequential encoder, the size of the hidden layer (in each direction) is 2048. For the coverage model, we set the size of coverage vectors to 50.  <ref type="table">Table 2</ref>: BLEU scores of different systems. "Sequential", "Tree-LSTM", "Tree-GRU" and "Bidirec- tional" denote the encoder part for the standard sequential encoder, Tree-LSTM encoder, Tree-GRU encoder and the bidirectional tree encoder, respectively. "no", "word" and "tree" in column "Coverage" represents the decoder part for using no coverage (standard attention), word coverage ( <ref type="bibr" target="#b22">Tu et al., 2016)</ref> and our proposed tree-coverage model, respectively.  <ref type="table">Table 3</ref>: BLEU scores of different systems based on LSTM. "Seq-LSTM" denotes both the encoder and decoder parts for the sequential model are based on LSTM; "SeqTree-LSTM" means using Tree-LSTM encoder on top of "Seq-LSTM".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># System</head><p>We use Adadelta (Zeiler, 2012) for optimization using a mini-batch size of 32. All other settings are the same as in <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>.</p><p>We use case insensitive 4-gram BLEU <ref type="bibr" target="#b15">(Papineni et al., 2002</ref>) for evaluation, as calculated by multi-bleu.perl in the Moses toolkit. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tree Encoders</head><p>This set of experiments evaluates the effectiveness of our proposed tree encoders. <ref type="table">Table 2</ref>, row 2 con- firms the finding of <ref type="bibr" target="#b5">Eriguchi et al. (2016)</ref> that a Tree-LSTM encoder helps, and row 3 shows that our Tree-GRU encoder gets a better result (+0.87 BLEU, v.s. row 2). To verify our assumption that model consistency is important for performance, we also conduct experiments to compare Tree- LSTM and Tree-GRU on top of LSTM-based encoder-decoder settings. Tree-Lstm with LSTM based sequential model can obtain 1.02 BLEU im- provement <ref type="table">(Table 3,</ref>  has more parameters(+1.6M) and takes 1.3 times longer for training.</p><p>Since the annotation size of our bidirectional tree encoder is twice of the Tree-LSTM encoder, we halved the size of the hidden layers in the se- quential encoder to 512 in each direction, to make fair comparison. These results are shown in Ta- ble 4. Row 4 shows that, even with the same an- notation size, our bidirectional tree encoder works better than the original Tree-LSTM encoder (row 2). In fact, our halved-sized unidirectional Tree- GRU encoder (row 3 ) also works better than the Tree-LSTM encoder (row 2) with half of its anno- tation size.</p><p>We also compared our bidirectional tree en- coder with the head-lexicalization based bidirec- tional tree encoder proposed by <ref type="bibr" target="#b21">Teng and Zhang (2016)</ref>, which forms the input vector for each non- leaf node by a bottom-up head propagation mech- anism <ref type="table">(Table 4</ref>, row 14 ). Our bidirectional tree encoder gives a better result, suggesting that head word information may not be as helpful for ma- chine translation as it is for syntactic parsing.</p><p>When we set the hidden size back to 1024, we found that training the bidirectional tree encoder  <ref type="table">Table 4</ref>: Experiments with 512 hidden units in each direction of the sequential encoder. The bidirectional tree encoder using head-lexicalization (Bidirectional-head), proposed by <ref type="bibr" target="#b21">(Teng and Zhang, 2016)</ref>, does not work as well as our simpler bidirectional tree encoder (Bidirectional).</p><p>was more difficult. Therefore, we adopted a two- phase training strategy: first, we train the param- eters of the bottom-up encoder based NMT sys- tem; then, with the initialization of bottom-up en- coder and random initialization of the top-down part and decoder, we train the bidirectional tree encoder based NMT system. <ref type="table">Table 2</ref>, row 4 shows the results of this two-phase training: the bidirec- tional model (row 4) is 0.79 BLEU better than our unidirectional Tree-GRU (row 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tree-Coverage Model</head><p>Rows 5-8 in <ref type="table">Table 2</ref> show that the word cover- age model of <ref type="bibr" target="#b22">Tu et al. (2016)</ref> consistently helps when used with our proposed tree encoders, with the bidirectional tree encoder remaining the best. However, the improvements of the tree encoder models are smaller than that of the baseline sys- tem. This may be caused by the fact that the word coverage model neglects the relationship among the trees, e.g. the relationship between children and parent nodes. Our tree-coverage model consis- tently improves performance further (rows 9-11). Our best model combines our bidirectional tree encoder with our tree-coverage model (row 11), yielding a net improvement of +3.54 BLEU over the standard attentional model (row 1), and +1.90 BLEU over the stronger baseline that implements both the bottom-up tree encoder and coverage model from previous work (row 6).</p><p>As noted before, the original coverage model does not take word order into account. For com- parison, we also implement an extension of the coverage model that lets each coverage vector also depend on those of its left and right neighbors at the previous time step. This model does not help; in fact, it reduces BLEU by about 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis By Sentence Length</head><p>Following <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, we bin the de- velopment and test sentences by length and show BLEU scores for each bin in <ref type="figure">Figure 5</ref>. The pro- posed bidirectional tree encoder outperforms the <ref type="figure">Figure 5</ref>: Performance of translations with respect to the lengths of the source sentences. "+" indi- cates the improvement over the baseline sequential model. sequential NMT system and the Tree-GRU en- coder across all lengths. The improvements be- come larger for sentences longer than 20 words, and the biggest improvement is for sentences longer than 50 words. This provides some evi- dence for the importance of syntactic information for long sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Recently, many studies have focused on using ex- plicit syntactic tree structure to help learn sen- tence representations for various sentence classifi- cation tasks. For example, <ref type="bibr" target="#b21">Teng and Zhang (2016)</ref> and <ref type="bibr" target="#b11">Kokkinos and Potamianos (2017)</ref> extend the bottom-up model to a bidirectional model for clas- sification tasks, using Tree-LSTMs with head lex- icalization and Tree-GRUs, respectively. We draw on some of these ideas and apply them to machine translation. We use the representation learnt from tree structures to enhance the original sequential model, and make use of these syntactic informa- tion during the generation phase.</p><p>In NMT systems, the attention model ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) becomes a crucial part of the decoder model. <ref type="bibr" target="#b4">Cohn et al. (2016) and</ref><ref type="bibr" target="#b6">Feng et al. (2016)</ref> extend the attentional model to include structural biases from word based alignment mod- els. <ref type="bibr" target="#b8">Kim et al. (2017)</ref> incorporate richer structural distributions within deep networks to extend the attention model. Our contribution to the decoder model is to directly exploit structural information in the attention model combined with a coverage mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have investigated the potential of using explicit source-side syntactic trees in NMT by proposing a novel syntax-aware encoder-decoder model. Our experiments have demonstrated that a top-down encoder is a useful enhancement for the original bottom-up tree encoder ( <ref type="bibr" target="#b5">Eriguchi et al., 2016)</ref>; and incorporating syntactic structure information into the decoder can better control the translation. Our analysis suggests that the benefit of source-side syntax is especially strong for long sentences.</p><p>Our current work only uses the structure part of the syntactic tree, without the labels. For future work, it will be interesting to make use of node labels from the tree, or to use syntactic information on the target side, as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the proposed encoder models for the running example. The non-leaf nodes are assigned with index 7-11. The annotations h ↑ i of leaf nodes in (b) are identical to the annotations (dashed rectangles) of leaf nodes in (a). The dotted rectangles in (b) indicate the annotation produced by the bidirectional tree encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The attention heapmap plotting the attention weights during different translation steps, for translating the sentence in Figure 1(a). The nodes [7]-[11] correspond to non-leaf nodes indexed in Figure 3. Incorporating Tree-Coverage Model produces more concentrated alignments and alleviates the over-translation problem.</figDesc><graphic url="image-3.png" coords="5,309.52,245.97,216.00,160.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder Huadong Chen † , Shujian Huang † * , David Chiang ‡ , Jiajun Chen † †</head><label></label><figDesc></figDesc><table>State Key Laboratory for Novel Software Technology, Nanjing University 
{chenhd,huangsj,chenjj}@nlp.nju.edu.cn 

 ‡ 

Department of Computer Science and Engineering, University of Notre Dame 
dchiang@nd.edu 

</table></figure>

			<note place="foot" n="5"> https://github.com/slavpetrov/ berkeleyparser 6 https://github.com/nyu-dl/dl4mt-tutorial</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their valuable comments. This work is supported by the National Science Foundation of <ref type="bibr">China (No. 61672277, 61300158, 61472183)</ref>. Part of Huadong Chen's contribution was made when visiting University of Notre Dame. His visit was supported by the joint PhD program of China Scholarship Council.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.0473" />
	</analytic>
	<monogr>
		<title level="m">ICLR 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<idno type="doi">10.1162/coli.2007.33.2.201</idno>
		<ptr target="https://doi.org/10.1162/coli.2007.33.2.201" />
	</analytic>
	<monogr>
		<title level="j">Compututational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="228" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-4012" />
	</analytic>
	<monogr>
		<title level="m">Proc. Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1102" />
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1078" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving attention modeling with implicit distortion and fertility for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/C16-1290" />
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3082" to="3092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Phrasal cohesion and statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidi</forename><forename type="middle">J</forename><surname>Fox</surname></persName>
		</author>
		<idno type="doi">10.3115/1118693.1118732</idno>
		<ptr target="https://doi.org/10.3115/1118693.1118732" />
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="304" to="3111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Structured attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1702.00887" />
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<idno type="doi">10.3115/1073445.1073462</idno>
		<ptr target="https://doi.org/10.3115/1073445.1073462" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structural attention neural networks for improved sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippos</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/E17-2093" />
	</analytic>
	<monogr>
		<title level="m">Proc. EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">When are tree structures necessary for deep learning of representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Treeto-string alignment template for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
		<idno type="doi">10.3115/1220175.1220252</idno>
		<ptr target="https://doi.org/10.3115/1220175.1220252" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Coverage embedding models for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1096" />
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="955" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Does string-based neural MT learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1150" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06788</idno>
		<ptr target="http://arxiv.org/abs/1611.06788" />
		<title level="m">Bidirectional tree-structured LSTM with head lexicalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1008" />
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The penn chinese treebank: Phrase structure annotation of a large corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="238" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="doi">10.1017/S135132490400364X</idno>
		<ptr target="https://doi.org/10.1017/S135132490400364X" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A syntax-based statistical translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<idno type="doi">10.3115/1073012.1073079</idno>
		<ptr target="https://doi.org/10.3115/1073012.1073079" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR abs/1212.5701</idno>
		<ptr target="http://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transition-based parsing of the Chinese Treebank using a global discriminative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W09-3825" />
	</analytic>
	<monogr>
		<title level="m">Proc. IWPT</title>
		<meeting>IWPT</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="162" to="171" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
