<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Relation Extraction with Multi-lingual Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
						</author>
						<title level="a" type="main">Neural Relation Extraction with Multi-lingual Attention</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="34" to="43"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1004</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Relation extraction has been widely used for finding unknown relational facts from the plain text. Most existing methods focus on exploiting mono-lingual data for relation extraction, ignoring massive information from the texts in various languages. To address this issue, we introduce a multilingual neural relation extraction framework, which employs mono-lingual attention to utilize the information within mono-lingual texts and further proposes cross-lingual attention to consider the information consistency and comple-mentarity among cross-lingual texts. Experimental results on real-world datasets show that our model can take advantage of multilingual texts and consistently achieve significant improvements on relation extraction as compared with base-lines. The source code of this paper can be obtained from https://github. com/thunlp/MNRE</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>People build many large-scale knowledge bases (KBs) to store structured knowledge about the real world, such as Wikidata 1 and DBpedia 2 . KBs are playing an important role in many AI and NLP applications such as information retrieval and question answering. The facts in KBs are typically organized in the form of triplets, e.g., <ref type="bibr">(New York, CityOf, United States)</ref>. Since ex- isting KBs are far from complete and new facts are growing infinitely, meanwhile manual anno- tation of these knowledge is time-consuming and human-intensive, many works have been devoted to automated extraction of novel facts from vari- ous Web resources, where relation extraction (RE) from plain texts is one the most important knowl- edge sources.</p><p>Among various methods for relation extraction, distant supervision is the most promising approach ( <ref type="bibr" target="#b10">Mintz et al., 2009;</ref><ref type="bibr" target="#b11">Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b14">Surdeanu et al., 2012)</ref>, which can au- tomatically generate training instances via aligning KBs and texts to address the issue of lacking super- vised data. As the development of deep learning, <ref type="bibr" target="#b17">Zeng et al. (2015)</ref> introduce neural networks to ex- tract relations with automatically learned features from training instances. To address the wrong labelling issue of distant supervision data, <ref type="bibr" target="#b9">Lin et al. (2016)</ref> further employ sentence-level atten- tion mechanism in neural relation extraction, and achieves the state-of-the-art performance.</p><p>However, most RE systems concentrate on ex- tracting relational facts from mono-lingual data. In fact, people describe knowledge about the world using various languages. And people speaking different languages also share similar knowledge about the world due to the similarities of human experiences and human cognitive systems. For in- stance, though New York and United States are ex- pressed as 纽约 and 美国 respectively in Chinese, both Americans and Chinese share the fact that "New York is a city of USA."</p><p>It is straightforward to build mono-lingual RE systems separately for each single language. But if so, it won't be able to take full advantage of di- verse information hidden in the data of various lan- guages. Multi-lingual data will benefit relation ex- traction for the following two reasons: 1. Consis- tency. According to the distant supervision data in our experiments <ref type="bibr">3</ref> , we find that over half of Chinese <ref type="bibr">3</ref> The data is generated by aligning Wikidata with Chinese Relation City in English 1. New York is a city in the northeastern United States. Chinese 1. 纽约美国纽约 美国.</p><p>(New York is in the United States New York and on the Atlantic coast of the southeast At- lantic, is the largest city and largest port in the United States.) 2. 纽约美国. (New York is the most populous city in the United States) <ref type="table">Table 1</ref>: An example of Chinese sentences and En- glish sentence about the same relational fact (New York, CityOf, United States). Important parts are highlighted with bold face. and English sentences are longer than 20 words, in which only several words are related to the re- lational facts. <ref type="table">Take Table 1</ref> for example. The first Chinese sentence has over 20 words, in which only "纽约" (New York) and "美国 " (is the biggest city in the United States) ac- tually directly reflect the relational fact CityOf. It is thus non-trivial to locate and learn these rela- tional patterns from complicated sentences for re- lation extraction. Fortunately, a relational fact is usually expressed with certain patterns in various languages, and the correspondence of these pat- terns among languages is substantially consistent. The pattern consistency among languages provides us augmented clues to enhance relational pattern learning for relation extraction.</p><p>2. Complementarity. From our experiment data, we also find that 42.2% relational facts in English data and 41.6% ones in Chinese data are unique. Moreover, for nearly half of relations, the number of sentences expressing relational facts of these relations varies a lot in different languages. It is thus straightforward that the texts in differ- ent languages can be complementary to each other, especially from those resource-rich languages to resource-poor languages, and improve the overall performance of relation extraction.</p><p>To take full consideration of these issues, we propose Multi-lingual Attention-based Neural Relation Extraction (MNRE). We first employ a convolutional neural network (CNN) to embed the relational patterns in sentences into real-valued vectors. Afterwards, to consider the complemen- tarity of all informative sentences in various lan- Baidu Baike and English Wikipedia articles, which will be introduced in details in the section of experiments. guages and capture the consistency of relational patterns, we apply mono-lingual attention to select the informative sentences within each language and propose cross-lingual attention to take advan- tages of pattern consistency and complementarity among languages. Finally, we classify relations according to the global vector aggregated from all sentence vectors weighted by mono-lingual atten- tion and cross-lingual attention.</p><p>In experiments, we build training instances via distant supervision by aligning Wikidata with Chi- nese Baidu Baike and English Wikipedia articles and evaluate the performance of relation extraction in both English and Chinese. The experimental results show that our framework achieves signif- icant improvement for relation extraction as com- pared to all baseline methods including both mono- lingual and multi-lingual ones. It indicates that our framework can take full advantages of sentences in different languages and better capture sophisti- cated patterns expressing relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent years KBs have been widely used on var- ious AI and NLP applications. As an impor- tant approach to enrich KBs, relation extraction from plain text has attracted many research in- terests. Relation extraction typically classifies each entity pair into various relation types ac- cording to supporting sentences that the both enti- ties appear, which needs human-labelled relation- specific training instances. Many works have been invested to relation extraction including kernel- based model ( <ref type="bibr" target="#b16">Zelenko et al., 2003)</ref>, embedding- based model <ref type="bibr" target="#b6">(Gormley et al., 2015)</ref>, CNN-based models ( <ref type="bibr" target="#b18">Zeng et al., 2014;</ref><ref type="bibr" target="#b4">dos Santos et al., 2015)</ref>, and RNN-based model <ref type="bibr" target="#b12">(Socher et al., 2012)</ref>.</p><p>Nevertheless, these RE systems are insuffi- cient due to the lack of training data. Different from these works, our framework aims to jointly model the texts in multiple languages to enhance relation ex- traction with distant supervision. To the best of our knowledge, this is the first effort to multi-lingual neural relation extraction.</p><p>The scope of multi-lingual analysis has been widely considered in many tasks besides relation extraction, such as sentiment analysis <ref type="bibr" target="#b0">(Boiy and Moens, 2009)</ref>, cross-lingual document summa- rization (Boudin et al., 2011), information retrieval in Web search ( <ref type="bibr" target="#b3">Dong et al., 2014</ref>) and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe our proposed MNRE framework in detail. The key motivation of MNRE is that, for each relational fact, the relation pat- terns in sentences of different languages should be substantially consistent, and MNRE can utilize the pattern consistency and complementarity among languages to achieve better results for relation ex- traction.</p><p>Formally, given two entities, their correspond- ing sentences in m different languages are de- fined as T = {S 1 , S 2 , . . . , S m }, where</p><formula xml:id="formula_0">S j = {x 1 j , x 2 j , . . . , x</formula><p>n j j } corresponds to the sentence set in the jth language with n j sentences. Our model measures a score f (T, r) for each relation r, which is expected to be high when r is the valid one, oth- erwise low. The MNRE framework contains two main components:</p><p>1. Sentence Encoder. Given a sentence x and two target entities, we employ CNN to encode re- lation patterns in x into a distributed representation x. The sentence encoder can also be implemented with GRU ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>) or LSTM (Hochre- iter and <ref type="bibr" target="#b7">Schmidhuber, 1997)</ref>. In experiments, we find CNN can achieve a better trade-off between computational efficiency and performance effec- tiveness. Thus, in this paper, we focus on CNN as the sentence encoder.</p><p>2. Multi-lingual Attention. With all sentences in various languages encoded into distributed vec- tor representations, we apply mono-lingual and cross-lingual attentions to capture those infor- mative sentences with accurate relation patterns. MNRE further aggregates these sentence vectors with weighted attentions into global representa- tions for relation prediction.</p><p>We introduce the two components in detail as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Encoder</head><p>The sentence encoder aims to transform a sentence x into its distributed representation x via CNN.</p><p>First, it embeds the words in the input sentence into dense real-valued vectors. Next, it employs convolutional, max-pooling and non-linear trans- formation layers to construct the distributed repre- sentation of the sentence, i.e., x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Input Representation</head><p>Following ( <ref type="bibr" target="#b18">Zeng et al., 2014</ref>), we transform each input word into the concatenation of two kinds of representations: (1) a word embedding which cap- tures syntactic and semantic meanings of the word, and (2) a position embedding which specifies the position information of this word with respect to two target entities. In this way, we can repre- sent the input sentence as a vector sequence w = {w 1 , w 2 , . . .} with w i ∈ R d , where d = d a +d b ×2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Convolution, Max-pooling and</head><p>Non-linear Layers After encoding the input sentence, we use a con- volutional layer to extract the local features, max- pooling, and non-linear layers to merge all local features into a global representation.</p><p>First, the convolutional layer extracts local fea- tures by sliding a window of length l over the sen- tence and perform a convolution within each slid- ing window. Formally, the output of convolutional layer for the ith sliding window is computed as:</p><formula xml:id="formula_1">p i = Ww i−l+1:i + b,<label>(1)</label></formula><p>where w i−l+1:i indicates the concatenation of l word embeddings within the i-th window, W ∈ R d c ×(l×d) is the convolution matrix and b ∈ R d c is the bias vector. ( d c is the dimension of output embeddings of the convolution layer) After that, we combines all local features via a max-pooling operation and apply a hyperbolic tan- gent function to obtain a fixed-sized sentence vec- tor for the input sentence. Formally, the ith ele- ment of the output vector x ∈ R d c is calculated as:</p><formula xml:id="formula_2">[x] j = tanh ( max i (p ij ) ) .<label>(2)</label></formula><p>The final vector x is expected to efficiently en- code relation patterns about target entities from the input sentence.</p><p>Here, instead of max pooling operation, we can use piecewise max pooling operation adopted by PCNN ( <ref type="bibr" target="#b17">Zeng et al., 2015)</ref> which is a variation of CNN to better capture the relation patterns in the input sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-lingual Attention</head><p>To exploit the information of the sentences from all languages, our model adopts two kinds of at- tention mechanisms for multi-lingual relation ex- traction, including: (1) the mono-lingual atten- tion which selects the informative sentences within one language and (2) the cross-lingual attention which measures the pattern consistency among languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Mono-lingual Attention</head><p>To address the wrong-labelling issue in distant su- pervision, we follow the idea of sentence-level at- tention ( <ref type="bibr" target="#b9">Lin et al., 2016</ref>) and set mono-lingual at- tention for MNRE. It is intuitive that each hu- man language has its own characteristics. Hence we adopt different mono-lingual attentions to de- emphasize those noisy sentences within each lan- guage.</p><p>More specifically, for the j-th language and the sentence set S j , we aim to aggregate all sentence vectors into a real-valued vector S j for relation pre- diction. The mono-lingual vector S j is computed as a weighted sum of those sentence vectors x i j :</p><formula xml:id="formula_3">S j = ∑ i α i j x i j ,<label>(3)</label></formula><p>where α i j is the attention score of each sentence vector x i j , defined as:</p><formula xml:id="formula_4">α i j = exp(e i j ) ∑ k exp(e k j ) ,<label>(4)</label></formula><p>where e i j is referred as a query-based function which scores how well the input sentence x i j re- flects its labelled relation r. There are many ways to obtain e i j , and here we simply compute e i as the inner product:</p><formula xml:id="formula_5">e i j = x i j · r j .<label>(5)</label></formula><p>Here r j is the query vector of the relation r with respect to the j-th language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cross-lingual Attention</head><p>Besides mono-lingual attention, we propose cross- lingual attention for neural relation extraction to better take advantages of multi-lingual data.</p><p>The key idea of cross-lingual attention is to em- phasize those sentences which have strong con- sistency among different languages. On the basis of mono-lingual attention, cross-lingual attention is capable of further removing unlikely sentences and resulting in more concentrated and informa- tive sentences, with the factor of consistent cor- respondence of relation patterns among different languages.</p><p>Cross-lingual attention works similar to mono- lingual attention. Suppose j indicates a language and k is a another language (k ̸ = j). Formally, the cross-lingual representation S jk is defined as a weighted sum of those sentence vectors x i j in the jth language:</p><formula xml:id="formula_6">S jk = ∑ i α i jk x i j ,<label>(6)</label></formula><p>where α i jk is the cross-lingual attention score of each sentence vector x i j with respect to the kth lan- guage. The cross-lingual attention α i jk is defined as:</p><formula xml:id="formula_7">α i jk = exp(e i jk ) ∑ k exp(e k jk ) ,<label>(7)</label></formula><p>where e i jk is referred as a query-based function which scores the consistency between the input sentence x i j in the jth language and the relation patterns in the kth language for expressing the se- mantic meanings of the labelled relation r. Similar to the mono-lingual attention, we compute e i jk as follows:</p><formula xml:id="formula_8">e i jk = x i j · r k ,<label>(8)</label></formula><p>where r k is the query vector of the relation r with respect to the kth language. Note that, for convenience, we denote those mono-lingual attention vectors S j as S jj in the re- mainder of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prediction</head><p>For each entity pair and its corresponding sentence set T in m languages, we can obtain m × m vec- tors {S jk |j, k ∈ {1, . . . , m}} from the neural net- works with multi-lingual attention. Those vectors with j = k are mono-lingual attention vectors, and those with j ̸ = k are cross-lingual attention vec- tors.</p><p>We take all vectors {S jk } together and define the overall score function f (T, r) as follows:</p><formula xml:id="formula_9">f (T, r) = ∑ j,k∈{1,...,m} log p(r|S jk , θ),<label>(9)</label></formula><p>where p(r|S jk , θ) is the probability of predicting the relation r conditional on S jk , computed using a softmax layer as follows:</p><formula xml:id="formula_10">p(r|S jk , θ) = softmax(MS jk + d),<label>(10)</label></formula><p>where d ∈ R nr is a bias vector, n r is the number of relation types and M ∈ R nr×R c is a global relation matrix initialized randomly.</p><p>To better consider the characteristics of each hu- man language, we further introduce R k as the spe- cific relation matrix of the kth language. Here we simply define R k as composed by r k in <ref type="figure">Eq. (8)</ref>. Hence, Eq. (10) can be extended to:</p><formula xml:id="formula_11">p(r|S jk , θ) = softmax[(R k + M)S jk + d], (11)</formula><p>where M encodes global patterns for predicting relations and R k encodes those language-specific characteristics.</p><p>Note that, in the training phase, the vectors {S jk } are constructed using Eq. (3) and (6) using the labelled relation. In the testing phase, since the relation is not known in advance, we will construct different vectors {S jk } for each possible relation r to compute f (T, r) for relation prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>Here we introduce the learning and optimization details of our MNRE framework. We define the objective function as follows:</p><formula xml:id="formula_12">J(θ) = s ∑ i=1 f (T i , r i ),<label>(12)</label></formula><p>where s indicates the number of all entity pairs with each corresponding to a sentence set in dif- ferent languages, and θ indicates all parameters of our framework. To solve the optimization problem, we adopt mini-batch stochastic gradient descent (SGD) to minimize the objective function. For learning, we iterate by randomly selecting a mini-batch from the training set until converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first introduce the datasets and evaluation met- rics used in the experiments. Next, we use a vali- dation set to determine the best model parameters and choose the best model via early stopping. Af- terwards, we show the effectiveness of our frame- work of considering pattern complementarity and consistency for multi-lingual relation extraction by quantitative and qualitative analysis. Finally, we compare the effect of two kinds of relation matri- ces in Eq. (11) used for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We generate a new multi-lingual relation extrac- tion dataset to evaluate our MNRE framework. Without loss of generality, the experiments fo- cus on relation extraction from two languages in- cluding English and Chinese. In this dataset, the Chinese instances are generated by aligning Chinese Baidu Baike with Wikidata, and the En- glish instances are generated by aligning English Wikipedia articles with Wikidata. The relational facts of Wikidata in this dataset are divided into three parts for training, validation and testing re- spectively. There are 176 relations including a spe- cial relation NA indicating there is no relation be- tween entities. And we set both validation and test- ing sets for Chinese and English parts contain the same facts. We list the statistics about the dataset in  We follow previous works <ref type="bibr" target="#b10">(Mintz et al., 2009</ref>) and investigate the performance of RE systems us- ing the held-out evaluation, by comparing the re- lational facts discovered by RE systems from the testing set with those facts in KB. The evaluation method assumes that if a RE system accurately finds more relational facts in KBs from the test- ing set, it will achieve better performance for rela- tion extraction. The held-out evaluation provides an approximate measure of RE performance with- out time-consuming human evaluation. In experi- ments, we report the precision/recall curves as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>We tune the parameters of our MNRE framework by grid searching using validation set. For train- ing, we set the iteration number over all the train- ing data as 15. The best models were selected by early stopping using the evaluation results on the validation set. In <ref type="table">Table 3</ref> we show the best setting of all parameters used in our experiments.   <ref type="table">Table 3</ref>: Parameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of Consistency</head><p>To demonstrate the effectiveness of considering pattern consistency among languages, we empir- ically compare different methods through held-out evaluation. We select CNN proposed in <ref type="table">(</ref> From <ref type="figure" target="#fig_4">Fig. 2</ref>, we have the following observa- tions:</p><p>(1) Both [P]CNN+joint and [P]CNN+share achieve better performances as compared to <ref type="bibr">[P]</ref>CNN-En and [P]CNN-Zh. It indicates that uti- lizing Chinese and English sentences jointly is beneficial to extracting novel relational facts. The reason is that those relational facts that are discov- ered from multiple languages are more reliable to be true.</p><p>(2) CNN+share only has similar performance as compared to CNN+joint, even through a bit worse when recall ranges from 0.1 to 0.2. Besides, PCNN+share performs worse than PCNN+joint nearly over the entire range of recall. It demon- strates that a simple combination of multiple lan- guages by sharing relation embedding matrices cannot further capture more implicit correlations among various languages.</p><p>(3) Our MNRE model achieves the highest pre- cision over the entire range of recall as com- pared to other methods including <ref type="bibr">[</ref>  <ref type="table">Table 4</ref>: An example of our multi-lingual attention. Low, medium and high indicate the attention weights. parameters for these baseline models, we can ob- serve that both [P]CNN+joint and [P]CNN+share cannot achieve competitive results compared to MNRE even when increasing the size of the output layer. This indicates that no more useful informa- tion can be captured by simply increasing model size. On the contrary, our proposed MNRE model can successfully improve multi-lingual relation ex- traction by considering pattern consistency among languages.</p><p>We further give an example of cross-lingual at- tention in <ref type="table">Table 4</ref>. It shows four sentences hav- ing the highest and lowest Chinese-to-English and English-to-Chinese attention weights respectively with respect to the relation PlaceOfBirth in MNRE. We highlight the entity pairs in bold face. For comparison, we also show their attention weights from CNN+Zh and CNN+En. From the table we find that, although all of the four sentences actually express the fact that Barzun was born in France, the first and third sentences contain much more noisy information that may confuse RE sys- tems. By considering pattern consistency between sentences in two languages with cross-lingual at- tention, MNRE can identify the second and fourth sentences that unambiguously express the relation PlaceOfBirth with higher attention as com- pared to CNN+Zh and CNN+En.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of Complementarity</head><p>To demonstrate the effectiveness of consider- ing pattern complementarity among languages, we empirically compare the following methods through held-out evaluation: MNRE for English      nese and English relation extractors can take full advantages of texts in both languages via our pro- pose multi-lingual attention scheme. <ref type="table" target="#tab_5">Table 5</ref> shows the detailed results (in preci- sion@1) of some specific relations of which the training instances are un-balanced on English and Chinese sides. From the table, we can see that:</p><note type="other">-En #Sent-Zh CNN-En CNN-Zh MNRE-En MNRE-</note><p>(1) For the relation Contains of which the number of English training instances is only 1/7 of Chinese ones, CNN-En gets much worse per- formance as compared to CNN-Zh due to the lack of training data. Nevertheless, by jointly training through multi-lingual attention, MNRE(CNN)- En is comparable to and slightly better than MNRE(CNN)-Zh.</p><p>(2) For the relation HeadquartersLoca- tion of which the number of Chinese training in- stances is only 1/9 of English ones, CNN-Zh even cannot predict any correct results. The reason is perhaps that, CNN-Zh of the relation is not suf- ficiently trained because there are only 210 Chi- nese training instances for this relation. Simi- larly, by jointly training through multi-lingual at- tention, MNRE(CNN)-En and MNRE(CNN)-Zh both achieve promising results.</p><p>(3) For the relations Father and Country- OfCitizenship of which the sentence number in English and Chinese are not so un-balanced, our MNRE can still improve the performance of rela- tion extraction on both English and Chinese sides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison of Relation Matrix</head><p>For relation prediction, we use two kinds of re- lation matrices including: M that considers the global consistency of relations, and R that consid- ers the specific characteristics of relations for each language. To measure the effect of the two relation matrices, we compare the performance of MNRE using the both matrices with those only using M (MNRE-M) and only using R (MNRE-R). <ref type="figure" target="#fig_9">Fig. 4</ref> shows the precision-recall curves for each method. From the figure, we observe that:t (1) The performance of MNRE-M is much worse than both MNRE-R and MNRE. It indicates that we cannot just use global relation matrix for relation prediction. The reason is that each lan- guage has its own specific characteristics to ex- press relation patterns, which cannot be well in- tegrated into a single relation matrix.</p><p>(2) MNRE(CNN)-R has similar performance as compared to MNRE(CNN) when the recall is low. However, it has a sharp decline when the recall reaches 0.25. It suggests there also exists global consistency of relation patterns among languages which cannot be neglected. Hence, we should combine both M and R together for multi-lingual relation extraction, as proposed in our MNRE </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce a neural relation extrac- tion framework with multi-lingual attention to take pattern consistency and complementarity among multiple languages into consideration. We evalu- ate our framework on multi-lingual relation extrac- tion task, and the results show that our framework can effectively model relation patterns among lan- guages and achieve state-of-the-art results.</p><p>We will explore the following directions as fu- ture work: (1) In this paper, we only consider sentence-level multi-lingual attention for relation extraction. In fact, we find that the word alignment information may be also helpful for capturing rela- tion patterns. Hence, the word-level multi-lingual attention, which may discover implicit alignments between words in multiple languages, will fur- ther improve multi-lingual relation extraction. We will explore the effectiveness of word-level multi- lingual attention for relation extraction as our fu- ture work. (2) MNRE can be flexibly implemented in the scenario of multiple languages, and this pa- per focuses on two languages of English and Chi- nese. In future, we will extend MNRE to more lan- guages and explore its significance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall architecture of our multi-lingual attention which contains two languages including English and Chinese. The solid lines indicates mono-lingual attention and the dashed lines indicates cross-lingual attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>d a and d b are the dimensions of word embeddings and position embeddings respectively)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Hyper</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Zeng et al., 2014) as our sentence encoder and imple- ment it by ourselves which achieves comparable results as the authors reported on their experimen- tal dataset NYT10 4 . And we compare the perfor- mance of our framework with the [P]CNN model trained with only English data ([P]CNN-En), only Chinese data ([P]CNN-Zh), a joint model ([P]CNN+joint) which predicts using [P]CNN-En and [P]CNN-Zh jointly, and another joint model with shared embeddings ([P]CNN+share) which trains [P]CNN-En and [P]CNN-Zh with common relation embedding matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Top: Aggregated precision/recall curves of CNN-En, CNN-Zh, CNN+joint, CNN+share, and MNRE(CNN). Bottom: Aggregated precision/recall curves of PCNN-En, PCNN-Zh, PCNN+joint, PCNN+share, and MNRE(PCNN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(MNRE-En) and MNRE for Chinese (MNRE-Zh) which only use the mono-lingual vectors to predict relations, and [P]CNN-En and [P]CNN-Zh mod- els. Fig. 3 shows the aggregated precision/recall curves of the four models for both CNN and PCNN. From the figure, we find that: (1) MNRE-En and MNRE-Zh outperform [P]CNN-En and [P]CNN-Zh almost in entire range of recall. It indicates that by jointly training with multi-lingual attention, both Chinese and English relation extractors are beneficial from those sentences from the other language. (2) Although [P]CNN-En underperforms as compared to [P]CNN-Zh, MNRE-En is compara- ble to MNRE-Zh by jointly training through multi- lingual attention. It demonstrates that both Chi-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Top: Aggregate precision/recall curves of CNN-En, CNN-Zh, MNRE(CNN)-En and MNRE(CNN)-Zh. Bottom: Aggregate precision/recall curves of PCNN-En, PCNN-Zh, MNRE(PCNN)-En and MNRE(PCNN)-Zh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Top: Aggregated precion/recall curves of MNRE(CNN)-M, MNRE(CNN)-R and MNRE. Bottom: Aggregated precion/recall curves of MNRE(PCNN)-M, MNRE(PCNN)-R and MNRE(PCNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Dataset 
#Rel 
#Sent 
#Fact 
Train 
1,022,239 47,638 
English Valid 
176 
80,191 
2,192 
Test 
162,018 
4,326 
Train 
940,595 42,536 
Chinese Valid 
176 
82,699 
2,192 
Test 
167,224 
4,326 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Statistics of the dataset.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Detailed results (precision@1) of some specific relations. #Sent-En and #Sent-Zh indicate</head><label>5</label><figDesc></figDesc><table>the 
</table></figure>

			<note place="foot" n="4"> http://iesl.cs.umass.edu/riedel/ecml/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A machine learning approach to sentiment analysis in multilingual web texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Boiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information retrieval</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="526" to="558" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A graph-based approach to cross-language multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Torres-Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polibits</title>
		<imprint>
			<biblScope unit="issue">43</biblScope>
			<biblScope unit="page" from="113" to="118" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Query lattice for translation retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Izuha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2031" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Multilingual open relation extraction using cross-lingual projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.06450</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved relation extraction with feature-rich compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJCNLP</title>
		<meeting>ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06396</idno>
		<title level="m">Multilingual relation extraction using compositional universal schema</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
