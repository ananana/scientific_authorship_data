<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Word Representations by Jointly Modeling Syntagmatic and Paradigmatic Relations</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">CAS Key Lab of Network Data Science and Technology Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Word Representations by Jointly Modeling Syntagmatic and Paradigmatic Relations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="136" to="145"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Vector space representation of words has been widely used to capture fine-grained linguistic regularities, and proven to be successful in various natural language processing tasks in recent years. However, existing models for learning word representations focus on either syntagmatic or paradigmatic relations alone. In this paper , we argue that it is beneficial to jointly modeling both relations so that we can not only encode different types of linguistic properties in a unified way, but also boost the representation learning due to the mutual enhancement between these two types of relations. We propose two novel dis-tributional models for word representation using both syntagmatic and paradigmatic relations via a joint training objective. The proposed models are trained on a public Wikipedia corpus, and the learned representations are evaluated on word analogy and word similarity tasks. The results demonstrate that the proposed models can perform significantly better than all the state-of-the-art baseline methods on both tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Vector space models of language represent each word with a real-valued vector that captures both semantic and syntactic information of the word. The representations can be used as basic features in a variety of applications, such as information re- trieval ( <ref type="bibr" target="#b14">Manning et al., 2008)</ref>, named entity recog- nition <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>), question answer- ing ( <ref type="bibr" target="#b28">Tellex et al., 2003)</ref>, disambiguation <ref type="bibr" target="#b25">(Sch√ºtze, 1998)</ref>, and parsing <ref type="bibr" target="#b27">(Socher et al., 2011)</ref>.</p><p>A common paradigm for acquiring such repre- sentations is based on the distributional hypothe- sis <ref type="bibr" target="#b6">(Harris, 1954;</ref><ref type="bibr" target="#b5">Firth, 1957)</ref>, which states that words occurring in similar contexts tend to have similar meanings. Based on this hypothesis, vari- ous models on learning word representations have been proposed during the last two decades. According to the leveraged distributional infor- mation, existing models can be grouped into two categories <ref type="bibr" target="#b24">(Sahlgren, 2008)</ref>. The first category mainly concerns the syntagmatic relations among the words, which relate the words that co-occur in the same text region. For example, "wolf" is close to "fierce" since they often co-occur in a sen- tence, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. This type of models learn the distributional representations of words based on the text region that the words occur in, as exemplified by Latent Semantic Analysis (LSA) model <ref type="bibr" target="#b3">(Deerwester et al., 1990</ref>) and Non-negative Matrix Factorization (NMF) model <ref type="bibr" target="#b10">(Lee and Seung, 1999</ref>). The second category mainly cap- tures paradigmatic relations, which relate words that occur with similar contexts but may not co- occur in the text. For example, "wolf" is close to "tiger" since they often have similar context words. This type of models learn the word rep- resentations based on the surrounding words, as exemplified by the Hyperspace Analogue to Lan- guage (HAL) model <ref type="bibr" target="#b12">(Lund et al., 1995)</ref>, Con- tinuous Bag-of-Words (CBOW) model and Skip- Gram (SG) model <ref type="bibr" target="#b15">(Mikolov et al., 2013a)</ref>.</p><p>In this work, we argue that it is important to take both syntagmatic and paradigmatic relations into account to build a good distributional model. Firstly, in distributional meaning acquisition, it is expected that a good representation should be able to encode a bunch of linguistic properties. For example, it can put semantically related words close (e.g., "microsoft" and "office"), and also be able to capture syntactic regularities like "big is to bigger as deep is to deeper". Obviously, these linguistic properties are related to both syntag- matic and paradigmatic relations, and cannot be well modeled by either alone. Secondly, syntag- matic and paradigmatic relations are complimen- tary rather than conflicted in representation learn- ing. That is relating the words that co-occur within the same text region (e.g., "wolf" and "fierce" as well as "tiger" and "fierce") can better relate words that occur with similar contexts (e.g., "wolf" and "tiger"), and vice versa. Based on the above analysis, we propose two new distributional models for word representa- tion using both syntagmatic and paradigmatic re- lations. Specifically, we learn the distributional representations of words based on the text region (i.e., the document) that the words occur in as well as the surrounding words (i.e., word sequences within some window size). By combining these two types of relations either in a parallel or a hier- archical way, we obtain two different joint training objectives for word representation learning. We evaluate our new models in two tasks, i.e., word analogy and word similarity. The experimental results demonstrate that the proposed models can perform significantly better than all of the state-of- the-art baseline methods in both of the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The distributional hypothesis has provided the foundation for a class of statistical methods for word representation learning. According to the leveraged distributional information, existing models can be grouped into two categories, i.e., syntagmatic models and paradigmatic models.</p><p>Syntagmatic models concern combinatorial re- lations between words (i.e., syntagmatic rela- tions), which relate words that co-occur within the same text region (e.g., sentence, paragraph or doc- ument).</p><p>For example, sentences have been used as the text region to acquire co-occurrence information by <ref type="bibr" target="#b23">(Rubenstein and Goodenough, 1965;</ref><ref type="bibr" target="#b17">Miller and Charles, 1991)</ref>. However, as pointed our by <ref type="bibr" target="#b21">Picard (1999)</ref>, the smaller the context regions are that we use to collect syntagmatic information, the worse the sparse-data problem will be for the resulting representation. Therefore, syntagmatic models tend to favor the use of larger text regions as context. Specifically, a document is often taken as a natural context of a word following the liter- ature of information retrieval. In these methods, a words-by-documents co-occurrence matrix is built to collect the distributional information, where the entry indicates the (normalized) frequency of a word in a document. A low-rank decomposition is then conducted to learn the distributional word representations. For example, LSA <ref type="bibr" target="#b3">(Deerwester et al., 1990</ref>) employs singular value decomposition by assuming the decomposed matrices to be or- thogonal. In ( <ref type="bibr" target="#b10">Lee and Seung, 1999</ref>), non-negative matrix factorization is conducted over the words- by-documents matrix to learn the word represen- tations.</p><p>Paradigmatic models concern substitutional relations between words (i.e., paradigmatic rela- tions), which relate words that occur in the same context but may not at the same time. Unlike syntagmatic model, paradigmatic models typically collect distributional information in a words-by- words co-occurrence matrix, where entries indi- cate how many times words occur together within a context window of some size.</p><p>For example, the Hyperspace Analogue to Lan- guage (HAL) model <ref type="bibr" target="#b12">(Lund et al., 1995</ref>) con- structed a high-dimensional vector for words based on the word co-occurrence matrix from a large corpus of text. However, a major problem with HAL is that the similarity measure will be dominated by the most frequent words due to its weight scheme. Various methods have been pro- posed to address the drawback of HAL. For exam- ple, the Correlated Occurrence Analogue to Lexi- cal Semantic (COALS) ( <ref type="bibr" target="#b22">Rohde et al., 2006</ref>) trans- formed the co-occurrence matrix by an entropy or correlation based normalization. <ref type="bibr" target="#b1">Bullinaria and Levy (2007)</ref>, and Levy and Goldberg (2014b) sug- gested that positive pointwise mutual information (PPMI) is a good transformation. More recently, <ref type="bibr" target="#b9">Lebret and Collobert (2014)</ref> obtained the word representations through a Hellinger PCA (HPCA) of the words-by-words co-occurrence matrix. <ref type="bibr" target="#b20">Pennington et al. (2014)</ref> explicitly factorizes the words-by-words co-occurrence matrix to obtain the Global Vectors (GloVe) for word representa- tion.</p><p>Alternatively, neural probabilistic language models (NPLMs) ( <ref type="bibr" target="#b0">Bengio et al., 2003</ref>) learn word representations by predicting the next word given previously seen words. Unfortunately, the training of NPLMs is quite time consuming, since com- puting probabilities in such model requires nor- malizing over the entire vocabulary. Recently, <ref type="bibr" target="#b18">Mnih and Teh (2012)</ref> applied Noise Contrastive Estimation (NCE) to approximately maximize the probability of the softmax in NPLM. <ref type="bibr" target="#b15">Mikolov et al. (2013a)</ref> further proposed continuous bag- of-words (CBOW) and skip-gram (SG) models, which use a simple single-layer architecture based on inner product between two word vectors. Both models can be learned efficiently via a simple vari- ant of Noise Contrastive Estimation, i.e., Negative sampling (NS) ( <ref type="bibr" target="#b16">Mikolov et al., 2013b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Models</head><p>In this paper, we argue that it is important to jointly model both syntagmatic and paradigmatic rela- tions to learn good word representations. In this way, we not only encode different types of linguis- tic properties in a unified way, but also boost the representation learning due to the mutual enhance- ment between these two types of relations.</p><p>We propose two joint models that learn the dis- tributional representations of words based on both the text region that the words occur in (i.e., syntag- matic relations) and the surrounding words (i.e., paradigmatic relations). To model syntagmatic re- lations, we follow the previous work <ref type="bibr" target="#b3">(Deerwester et al., 1990;</ref><ref type="bibr" target="#b10">Lee and Seung, 1999</ref>) to take docu- ment as a nature text region of a word. To model paradigmatic relations, we are inspired by the re- cent work from <ref type="bibr" target="#b15">Mikolov et al. (Mikolov et al., 2013a;</ref><ref type="bibr" target="#b16">Mikolov et al., 2013b)</ref>, where simple mod- els over word sequences are introduced for effi- cient and effective word representation learning.</p><p>In the following, we introduce the notations used in this paper, followed by detailed model de- scriptions, ending with some discussions of the proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>Before presenting our models, we first list the no- tations used in this paper.  </p><formula xml:id="formula_0">c n i+1 c n i+2 c n i‚àí2 d n w n i . . .</formula><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Projection</head><p>Figure 2: The framework for PDC model. Four words ("the", "cat", "on" and "the") are used to predict the center word ("sat"). Besides, the doc- ument in which the word sequence occurs is also used to predict the center word ("sat").</p><formula xml:id="formula_1">w n i ‚ààW (i.e. i-th word in document d n ) are the words surrounding it in an L-sized window (c n i‚àíL , . . . , c n i‚àí1 , c n i+1 , . . . , c n i+L ) ‚àà H, where c n j ‚àà W, j‚àà{i‚àíL, . . . , i‚àí1, i+1, . . . , i+L}. Each doc- ument d ‚àà D, each word w ‚àà W and each con- text c ‚àà W is associated with a vector ‚Éó d ‚àà R K , ‚Éó w ‚àà R K and ‚Éó c ‚àà R K , respectively,</formula><p>where K is the embedding dimensionality. The entries in the vectors are treated as parameters to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parallel Document Context Model</head><p>The first proposed model architecture is shown in <ref type="figure">Figure 2</ref>. In this model, a target word is predicted by its surrounding context, as well as the docu- ment it occurs in. The former prediction task cap- tures the paradigmatic relations, since words with similar context will tend to have similar represen- tations. While the latter prediction task models the syntagmatic relations, since words co-occur in the same document will tend to have similar represen- tations. More detailed analysis on this will be pre- sented in Section 3.4. The model can be viewed as an extension of CBOW model <ref type="bibr" target="#b15">(Mikolov et al., 2013a)</ref>, by adding an extra document branch. Since both the context and document are parallel in predicting the target word, we call this model the Parallel Document Context (PDC) model.</p><p>More formally, the objective function of PDC model is the log likelihood of all words</p><formula xml:id="formula_2">‚Ñì = N ‚àë n=1 ‚àë w n i ‚ààdn ( log p(w n i |h n i )+ log p(w n i |d n ) )</formula><p>where h n i denotes the projection of w n i 's contexts, defined as</p><formula xml:id="formula_3">h n i = f (c n i‚àíL , . . . , c n i‚àí1 , c n i+1 , . . . , c n i+L )</formula><p>where f (¬∑) can be sum, average, concatenate or max pooling of context vectors <ref type="bibr">1</ref> . In this paper, we use average, as that of word2vec tool. We use softmax function to define the probabil- ities p(w n i |h n i ) and p(w n i |d n ) as follows:</p><formula xml:id="formula_4">p(w n i |h n i ) = exp( ‚Éó w n i ¬∑ ‚Éó h n i ) ‚àë w‚ààW exp( ‚Éó w ¬∑ ‚Éó h n i )<label>(1)</label></formula><formula xml:id="formula_5">p(w n i |d n ) = exp( ‚Éó w n i ¬∑ ‚Éó d n ) ‚àë w‚ààW exp( ‚Éó w ¬∑ ‚Éó d n )<label>(2)</label></formula><p>where ‚Éó h n i denotes projected vector of w n i 's con- texts.</p><p>To learn the model, we adopt the negative sam- pling technique ( <ref type="bibr" target="#b16">Mikolov et al., 2013b</ref>) for effi- cient learning since the original objective is in- tractable for direct optimization. The negative sampling actually defines an alternate training ob- jective function as follows</p><formula xml:id="formula_6">‚Ñì= N ‚àë n=1 ‚àë w n i ‚ààdn ( log œÉ( ‚Éó w n i ¬∑ ‚Éó h n i )+ log œÉ( ‚Éó w n i ¬∑ ‚Éó d n ) + k ¬∑ E w ‚Ä≤ ‚àºPnw log œÉ( ‚Éó w ‚Ä≤ ¬∑ ‚Éó h n i ) + k ¬∑ E w ‚Ä≤ ‚àºPnw log œÉ( ‚Éó w ‚Ä≤ ¬∑ ‚Éó d n ) )<label>(3)</label></formula><p>where œÉ(x) = 1/(1 + exp(‚àíx)), k is the num- ber of "negative" samples, w ‚Ä≤ denotes the sampled word, and P nw denotes the distribution of negative word samples. We use stochastic gradient descent (SGD) for optimization, and the gradient is calcu- lated via back-propagation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Document Context Model</head><p>Since the above PDC model can be viewed as an extension of CBOW model, it is natural to in- troduce the same document-word prediction layer into the SG model. This becomes our second <ref type="bibr">1</ref> Note that the context window size L can be a function of the target word w n i . In this paper, we use the same strategy as word2vec tools which uniformly samples from the set {1, 2, ¬∑ ¬∑ ¬∑ , L}. </p><note type="other">d n c n i‚àí1 c n i+1 c n i+2 c n i‚àí2 ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ Projection Projection w n i Figure 3:</note><p>The framework for HDC model. The document is used to predict the target word ("sat"). Then, the word ("sat") is used to predict the sur- rounding words ("the", "cat", "on" and "the").</p><p>model architecture as shown in <ref type="figure">Figure 3</ref>. Specif- ically, the document is used to predict a target word, and the target word is further used to pre- dict its surrounding context words. Since the pre- diction is conducted in a hierarchical manner, we name this model the Hierarchical Document Con- text (HDC) model. Similar as the PDC model, the syntagmatic relation in HDC is modeled by the document-word prediction layer and the word- context prediction layer models the paradigmatic relation.</p><p>Formally, the objective function of HDC model is the log likelihood of all words:</p><formula xml:id="formula_7">‚Ñì= N ‚àë n=1 ‚àë w n i ‚ààdn ( i+L ‚àë j=i‚àíL jÃ∏ =i log p(c n j |w n i )+ log p(w n i |d n ) )</formula><p>where p(w n i |d n ) is defined the same as in Equa- tion (2), and p(c n j |w n i ) is also defined by a softmax function as follows:</p><formula xml:id="formula_8">p(c n j |w n i ) = exp( ‚Éó c n j ¬∑ ‚Éó w n i ) ‚àë c‚ààW exp(‚Éó c ¬∑ ‚Éó w n i )</formula><p>Similarly, we adopt the negative sampling tech- nique for learning, which defines the following training objective function</p><formula xml:id="formula_9">‚Ñì = N ‚àë n=1 ‚àë w n i ‚ààdn ( i+L ‚àë j=i‚àíL jÃ∏ =i ( log œÉ( ‚Éó c n j ¬∑ ‚Éó w n i ) + k ¬∑ E c ‚Ä≤ ‚àºPnc log œÉ( ‚Éó c ‚Ä≤ ¬∑ ‚Éó w n i ) ) + log œÉ( ‚Éó w n i ¬∑ ‚Éó d n ) + k¬∑E w ‚Ä≤ ‚àºPnw log œÉ( ‚Éó w ‚Ä≤ ¬∑ ‚Éó d n ) )</formula><p>where k is the number of the negative samples, c ‚Ä≤ and w ‚Ä≤ denotes the sampled context and word re- spectively, and P nc and P nw denotes the distribu- tion of negative context and word samples respec- tively 2 . We also employ SGD for optimization, and calculate the gradient via back-propagation al- gorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussions</head><p>In this section we first show how PDC and HDC models capture the syntagmatic and paradigmatic relations from the viewpoint of matrix factoriza- tion. We then talk about the relationship of our models with previous work. As pointed out in <ref type="bibr" target="#b24">(Sahlgren, 2008)</ref>, to capture syntagmatic relations, the implementational basis is to collect text data in a words-by-documents co- occurrence matrix in which the entry indicates the (normalized) frequency of occurrence of a word in a document (or, some other type of text region, e.g., a sentence). While the implementational ba- sis for paradigmatic relations is to collect text data in a words-by-words co-occurrence matrix that is populated by counting how many times words oc- cur together within the context window. We now take the proposed PDC model as an example to show how it achieves these goals, and similar re- sults can be shown for HDC model. The objective function of PDC with negative sampling in Equation (3) can be decomposed into the following two parts:</p><formula xml:id="formula_10">‚Ñì 1 = ‚àë w‚ààW ‚àë h‚ààH ( #(w, h)¬∑ log œÉ( ‚Éó w ¬∑ ‚Éó h) +k¬∑#(h)¬∑p nw (w)log œÉ(‚àí ‚Éó w¬∑ ‚Éó h) )<label>(4)</label></formula><formula xml:id="formula_11">‚Ñì 2 = ‚àë d‚ààD ‚àë w‚ààW ( #(w, d)¬∑ log œÉ( ‚Éó w ¬∑ ‚Éó d) +k¬∑|d|¬∑p nw (w)log œÉ(‚àí ‚Éó w¬∑ ‚Éó d) )<label>(5)</label></formula><p>where #(¬∑, ¬∑) denotes the number of times the pair</p><formula xml:id="formula_12">(¬∑, ¬∑) appears in D, #(h)= ‚àë w‚ààW #(w, h), |d|</formula><p>2 Pnc is not necessary to be the same as Pnw.</p><p>denotes the length of document d, the objective function ‚Ñì 1 corresponds to the context-word pre- diction task and ‚Ñì 2 corresponds to the document- word prediction task. Following the idea introduced by <ref type="bibr" target="#b11">(Levy and Goldberg, 2014a)</ref>, it is easy to show that the so- lution of the objective function ‚Ñì 1 follows that</p><formula xml:id="formula_13">‚Éó w ¬∑ ‚Éó h = log( #(w, h) #(h) ¬∑ p nw (w)</formula><p>) ‚àí log k and the solution of the objective function ‚Ñì 2 fol- lows that</p><formula xml:id="formula_14">‚Éó w ¬∑ ‚Éó d = log( #(w, d) |d| ¬∑ p nw (w)</formula><p>) ‚àí log k</p><p>It reveals that the PDC model with negative sam- pling is actually factorizing both a words-by- contexts co-occurrence matrix and a words-by- documents co-occurrence matrix simultaneously.</p><p>In this way, we can see that the implementational basis of the PDC model is consistent with that of syntagmatic and paradigmatic models. In other words, PDC can indeed capture both syntagmatic and paradigmatic relations by processing the right distributional information. Please notice that the PDC model is not equivalent to direct combina- tion of existing matrix factorization methods, due to the fact that the matrix entries defined in PDC model are more complicated than the simple co- occurrence frequency ( <ref type="bibr" target="#b10">Lee and Seung, 1999</ref>). When considering existing models, one may connect our models to the Distributed Memory model of Paragraph Vectors (PV-DM) and the Dis- tributed Bag of Words version of Paragraph Vec- tors (PV-DBOW) ( <ref type="bibr" target="#b8">Le and Mikolov, 2014</ref>). How- ever, both of them are quite different from our models. In PV-DM, the paragraph vector and con- text vectors are averaged or concatenated to pre- dict the next word. Therefore, the objective func- tion of PV-DM can no longer decomposed as the PDC model as shown in Equation (4) and (5). In other words, although PV-DM leverages both paragraph and context information, it is unclear how these information is collected and used in this model. As for PV-DBOW, it simply lever- ages paragraph vector to predict words in the para- graph. It is easy to show that it only uses the words-by-documents co-occurrence matrix, and thus only captures syntagmatic relations.</p><p>Another close work is the Global Context- Aware Neural Language Model (GCANLM for short) <ref type="bibr" target="#b7">(Huang et al., 2012</ref>). The model defines two scoring components that contribute to the fi- nal score of a (word sequence, document) pair. The architecture of GCANLM seems similar to our PDC model, but exhibits lots of differences as follows: (1) GCANLM employs neural net- works as components while PDC resorts to simple model structure without non-linear hidden layers; (2) GCANLM uses weighted average of all word vectors to represent the document, which turns out to model words-by-words co-occurrence (i.e., paradigmatic relations) again rather than words- by-documents co-occurrence (i.e., syntagmatic re- lations); (3) GCANLM is a language model which predicts the next word given the preceding words, while PDC model leverages both preceding and succeeding contexts for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first describe our experimen- tal settings including the corpus, hyper-parameter selections, and baseline methods. Then we com- pare our models with baseline methods on two tasks, i.e., word analogy and word similarity. Af- ter that, we conduct some case studies to show that our model can better capture both syntagmatic and paradigmatic relations and how it improves the performances on semantic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We select Wikipedia, the largest online knowl- edge base, to train our models. We adopt the publicly available April 2010 dump 3 <ref type="bibr" target="#b26">(Shaoul and Westbury, 2010)</ref>, which is also used by <ref type="bibr" target="#b7">(Huang et al., 2012;</ref><ref type="bibr" target="#b13">Luong et al., 2013;</ref><ref type="bibr" target="#b19">Neelakantan et al., 2014</ref>). The corpus in total has 3, 035, 070 articles and about 1 billion tokens. In preprocessing, we lowercase the corpus, remove pure digit words and non-English characters <ref type="bibr">4</ref> .</p><p>Following the practice in ( <ref type="bibr" target="#b20">Pennington et al., 2014</ref>), we set context window size as 10 and use 10 negative samples. The noise distributions for context and words are set as the same as used in ( <ref type="bibr" target="#b15">Mikolov et al., 2013a</ref>), p nw (w) ‚àù #(w) 0.75 . We also adopt the same linear learning rate strat- egy described in ( <ref type="bibr" target="#b15">Mikolov et al., 2013a)</ref>, where the initial learning rate of PDC model is 0.05, and  <ref type="table" target="#tab_2">Wikipedia 2014+ Gigaword5  6B  GCANLM, CBOW, SG  Wikipedia 2010  1B  PV-DBOW, PV-DM</ref> HDC is 0.025. No additional regularization is used in our models <ref type="bibr">5</ref> .</p><p>We compare our models with various state-of- the-art models including C&amp;W <ref type="bibr" target="#b2">(Collobert et al., 2011</ref>), GCANLM <ref type="figure" target="#fig_0">(Huang et al., 2012)</ref>, CBOW, SG (Mikolov et al., 2013a), GloVe ( <ref type="bibr" target="#b20">Pennington et al., 2014</ref>), PV-DM, PV-DBOW ( <ref type="bibr" target="#b8">Le and Mikolov, 2014</ref>) and HPCA ( <ref type="bibr" target="#b9">Lebret and Collobert, 2014</ref>). For C&amp;W, GCANLM 6 , GloVe and HPCA, we use the word embeddings they provided. For CBOW and SG model, we reimplement these two mod- els since the original word2vec tool uses SGD but cannot shuffle the data. Besides, we also im- plement PV-DM and PV-DBOW models due to ( <ref type="bibr" target="#b8">Le and Mikolov, 2014)</ref> has not released source codes. We train these four models on the same dataset with the same hyper-parameter settings as our models for fair comparison. The statistics of the corpora used in baseline models are shown in <ref type="table" target="#tab_2">Table 1</ref>. Moreover, since different papers re- port different dimensionality, to be fair, we con- duct evaluations on three dimensions (i.e., 50, 100, 300) to cover the publicly available results 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Analogy</head><p>The word analogy task is introduced by <ref type="bibr" target="#b15">Mikolov et al. (2013a)</ref> to quantitatively evaluate the linguistic regularities between pairs of word representations. The task consists of questions like "a is to b as c is to ", where is missing and must be guessed from the entire vocabulary. To answer such ques- tions, we need to find a word vector ‚Éó x, which is the closest to ‚Éó b ‚àí ‚Éó a + ‚Éó c according to the cosine similarity:</p><formula xml:id="formula_15">arg max x‚ààW,xÃ∏ =a xÃ∏ =b, xÃ∏ =c ( ‚Éó b + ‚Éó c ‚àí ‚Éó a) ¬∑ ‚Éó x</formula><p>The question is judged as correctly answered only if x is exactly the answer word in the evaluation <ref type="table">Table 2</ref>: Results on the word analogy task. Un- derlined scores are the best within groups of the same dimensionality, while bold scores are the best overall. set. The evaluation metric for this task is the per- centage of questions answered correctly. The dataset contains 5 types of semantic analo- gies and 9 types of syntactic analogies 8 . The se- mantic analogy contains 8, 869 questions, typi- cally about people and place like "Beijing is to China as Paris is to France", while the syntac- tic analogy contains 10, 675 questions, mostly on forms of adjectives or verb tense, such as "good is to better as bad to worse".</p><p>Result <ref type="table">Table 2</ref> shows the results on word analogy task. As we can see that CBOW, SG and GloVe are much stronger baselines as com- pare with C&amp;W, GCANLM and HPCA. Even so, our PDC model still performs significantly bet- ter than these state-of-the-art methods (p-value &lt; 0.01), especially with smaller vector dimen- sionality. More interestingly, by only training on 1 billion words, our models can outperform the GloVe model which is trained on 6 billion words. The results demonstrate that by model- ing both syntagmatic and paradigmatic relations, we can learn better word representations capturing linguistic regularities.</p><p>Besides, CBOW, SG and PV-DBOW can be viewed as sub-models of our proposed models, since they use either context (i.e., paradigmatic re- lations) or document (i.e., syntagmatic relations) alone to predict the target word. By comparing with these sub-models, we can see that the PDC and HDC models can perform significantly better on both syntactic and semantic subtasks. It shows that by jointly modeling the two relations, one can boost the representation learning and better cap- ture both semantic and syntactic regularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word Similarity</head><p>Besides the word analogy task, we also evalu- ate our models on three different word similar- ity tasks, including <ref type="bibr">WordSim-353 (Finkelstein et al., 2002</ref>), Stanford's Contextual Word Similari- ties (SCWS) ( <ref type="bibr" target="#b7">Huang et al., 2012</ref>) and rare word (RW) ( <ref type="bibr" target="#b13">Luong et al., 2013</ref>). These datasets contain word paris together with human assigned similar- ity scores. We compute the Spearman rank corre- lation between similarity scores based on learned word representations and the human judgements.</p><p>In all experiments, we removed the word pairs that cannot be found in the vocabulary.</p><p>Results <ref type="figure" target="#fig_2">Figure 4</ref> shows results on three differ- ent word similarity datasets. First of all, our pro- posed PDC model always achieves the best per- formances on the three tasks. Besides, if we com- pare the PDC and HDC models with their cor- responding sub-models (i.e., CBOW and SG) re- spectively, we can see performance gain by adding syntagmatic information via document. This gain becomes even larger for rare words with low di- mensionality as shown on RW dataset. More- over, on the SCWS dataset, our PDC model us- ing the single-prototype representations under di- mensionality 50 can achieve a comparable result (65.63) to the state-of-the-art GCANLM (65.7 as the best performance reported in ( <ref type="bibr" target="#b7">Huang et al., 2012)</ref>) which uses multi-prototype vectors 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>Here we conduct some case studies to (1) gain some intuition on how these two relations affect   <ref type="formula" target="#formula_5">(2)</ref> analyze why the joint model can perform better.</p><p>To show how syntagmatic and paradigmatic relations affect the learned representations, we present the 5 most similar words (by cosine simi- larity with 50-dimensional vectors) to a given tar- get word under the PDC and HDC models, as well as three sub-models, i.e., CBOW, SG, and PV- DBOW. The results are shown in table 3, where words in italic are those often co-occurred with the target word (i.e., syntagmatic relations), while words in bold are whose substitutable to the target word (i.e., paradigmatic relation).</p><p>Clearly, top words from CBOW and SG mod- els are more under paradigmatic relations, while those from PV-DBOW model are more under syn- tagmatic relations, which is quite consistent with the model design. By modeling both relations, the top words from PDC and HDC models become more diverse, i.e., more syntagmatic relations than CBOW and SG models, and more paradigmatic re- lations than PV-DBOW model. The results reveal that the word representations learned by PDC and HDC models are more balanced with respect to the two relations as compared with sub-models. The next question is why learning a joint model can work better on previous tasks? We first take one example from the word analogy task, which is the question "big is to bigger as deep is to " with the correct answer as "deeper". Our PDC model produce the right answer but the CBOW model fails with the answer "shallower". We thus embedding the learned word vectors from the two models into a 3-D space to illustrate and analyze the reason.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, we can see that by jointly modeling two relations, PDC model not only re- quires that "deep" to be close to "deeper" (in co- sine similarity), but also requires that "deep" and "deeper" to be close to "crevasses". The additional requirements further drag these three words closer as compared with those from the CBOW model, and this make our model outperform the CBOW model on this question. As for the word similarity tasks, we find that the word pairs are either syntag- matic (e.g., "bank" and "money") or paradigmatic (e.g., "left" and "abandon"). It is, therefore, not surprising to see that a more balanced representa- tion can achieve much better performance than a biased representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Existing work on word representations models ei- ther syntagmatic or paradigmatic relations. In this paper, we propose two novel distributional models for word representation, using both syntagmatic and paradigmatic relations via a joint training ob- jective. The experimental results on both word analogy and word similarity tasks show that the proposed joint models can learn much better word representations than the state-of-the-art methods.</p><p>Several directions remain to be explored. In this paper, the syntagmatic and paradigmatic rela- tions are equivalently important in both PDC and HDC models. An interesting question would then be whether and how we can add different weights for syntagmatic and paradigmatic relations. Be- sides, we may also try to learn the multi-prototype word representations for polysemous words based on our proposed models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example for syntagmatic and paradigmatic relations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Spearman rank correlation on three datasets. Results are grouped by dimensionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The 3-D embedding of learned word vectors of "deep", "deeper" and "crevasses" under CBOW and PDC models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : Corpora used in baseline models.</head><label>1</label><figDesc></figDesc><table>model 
corpus 
size 
C&amp;W 
Wikipedia 2007 + Reuters RCV1 
0.85B 
HPCA 
Wikipedia 2012 
1.6B 
GloVe 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Target words and their 5 most similar words under different representations. Words in italic often co-occur with the target words, while words in bold are substitutable to the target words.</figDesc><table>feynman 

CBOW 
einstein, schwinger, bohm, bethe 
relativity 

SG 
schwinger, quantum, bethe, einstein 
semiclassical 

PDC 
geometrodynamics, bethe, semiclassical 
schwinger, perturbative 

HDC 
schwinger, electrodynamics, bethe 
semiclassical, quantum 

PV-DBOW 
physicists, spacetime, geometrodynamics 
tachyons, einstein 
moon 
CBOW 
earth, moons, pluto, sun, nebula 
SG 
earth, sun, mars, planet, aquarius 
PDC 
sun, moons, lunar, heavens, earth 
HDC 
earth, sun, mars, planet, heavens 
PV-DBOW lunar, moons, celestial, sun, ecliptic 

the representation learning, and </table></figure>

			<note place="foot" n="3"> http://www.psych.ualberta.ca/‚àºwestburylab/downloads/ westburylab.wikicorp.download.html 4 We ignore the words less than 20 occurrences during training.</note>

			<note place="foot" n="5"> Codes avaiable at http://www.bigdatalab.ac.cn/benchma rk/bm/bd?code=PDC, http://www.bigdatalab.ac.cn/benchma rk/bm/bd?code=HDC. 6 Here, we use GCANLM&apos;s single-prototype embedding. 7 C&amp;W and GCANLM only released the vectors with 50 dimensions, and HPCA released vectors with 50 and 100 dimensions.</note>

			<note place="foot" n="8"> http://code.google.com/p/word2vec/source/browse/trunk /questions-words.txt</note>

			<note place="foot" n="9"> Note, in Figure 4, the performance of GCANLM is computed based on their released single-prototype vectors.</note>

			<note place="foot">Omer Levy and Yoav Goldberg, 2014b. Proceedings of the Eighteenth Conference on Computational Natural Language Learning, chapter Linguistic Regularities in Sparse and Explicit Word Representations, pages 171-180. Association for Computational Linguistics.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R√©jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extracting semantic representations from word cooccurrence statistics: A computational study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">P</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="510" to="526" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L√©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A synopsis of linguistic theory 193055</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Firth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in Linguistic Analysis</title>
		<imprint>
			<date type="published" when="1957" />
			<biblScope unit="page" from="1952" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zellig</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<editor>Tony Jebara and Eric P. Xing</editor>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word embeddings through hellinger pca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R√©mi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc., Montreal</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic and associative priming in a highdimensional semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curt</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><forename type="middle">Ann</forename><surname>Atchley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 17th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="660" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch√ºtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop of ICLR</title>
		<meeting>Workshop of ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
		<title level="m">Contextual correlates of semantic similarity. Language &amp; Cognitive Processes</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1751" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient non-parametric estimation of multiple embeddings per word in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeevan</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1059" to="1069" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finding content-bearing terms using term similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL &apos;99</title>
		<meeting>the Ninth Conference on European Chapter of the Association for Computational Linguistics, EACL &apos;99<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="241" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An improved model of semantic similarity based on lexical co-occurence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">C</forename><surname>Gonnerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The distributional hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Italian Journal of Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="54" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic word sense discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch√ºtze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="123" />
			<date type="published" when="1998-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The westbury lab wikipedia corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shaoul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Westbury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Edmonton, AB</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Alberta</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<editor>Lise Getoor and Tobias Scheffer</editor>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Quantitative evaluation of passage retrieval algorithms for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Marton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="41" to="47" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
