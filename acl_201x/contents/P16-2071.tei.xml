<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Linguistic Features for Sentence Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aubrie</forename><forename type="middle">M</forename><surname>Woods</surname></persName>
							<email>amwoods@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Avenue Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Linguistic Features for Sentence Completion</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="438" to="442"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a novel approach to automated sentence completion based on pointwise mutual information (PMI). Feature sets are created by fusing the various types of input provided to other classes of language models, ultimately allowing multiple sources of both local and distant information to be considered. Furthermore , it is shown that additional precision gains may be achieved by incorporating feature sets of higher-order n-grams. Experimental results demonstrate that the PMI model outperforms all prior models and establishes a new state-of-the-art result on the Microsoft Research Sentence Completion Challenge.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Skilled reading is a complex cognitive process that requires constant interpretation and evaluation of written content. To develop a coherent picture, one must reason from the material encountered to construct a mental representation of meaning. As new information becomes available, this repre- sentation is continually refined to produce a glob- ally consistent understanding. Sentence comple- tion questions, such as those previously featured on the Scholastic Aptitude Test (SAT), were de- signed to assess this type of verbal reasoning abil- ity. Specifically, given a sentence containing 1-2 blanks, the test taker was asked to select the cor- rect answer choice(s) from the provided list of op- tions <ref type="bibr">(College Board, 2014)</ref>. A sample sentence completion question is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>To date, relatively few publications have fo- cused on automatic methods for solving sentence completion questions. This scarcity is likely at- tributable to the difficult nature of the task, which Certain clear patterns in the metamorphosis of a butterfly indicate that the process is ---.  occasionally involves logical reasoning in addition to both general and semantic knowledge ( <ref type="bibr" target="#b13">Zweig et al., 2012b</ref>). Fundamentally, text completion is a challenging semantic modeling problem, and so- lutions require models that can evaluate the global coherence of sentences <ref type="bibr" target="#b3">(Gubbins and Vlachos, 2013</ref>). Thus, in many ways, text completion epito- mizes the goals of natural language understanding, as superficial encodings of meaning will be insuf- ficient to determine which responses are accurate.</p><p>In this paper, a model based on pointwise mu- tual information (PMI) is proposed to measure the degree of association between answer options and other sentence tokens. The PMI model considers multiple sources of information present in a sen- tence prior to selecting the most likely alternative.</p><p>The remainder of this report is organized as fol- lows. Section 2 describes the high-level character- istics of existing models designed to perform auto- mated sentence completion. This prior work pro- vides direct motivation for the PMI model, intro- duced in Section 3. In Section 4, the model's per- formance on the Microsoft Research (MSR) Sen- tence Completion Challenge and a data set com- prised of SAT questions are juxtaposed. Finally, Section 5 offers concluding remarks on this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>438</head><p>Previous research expounds on various architec- tures and techniques applied to sentence comple- tion. Below, models are roughly categorized on the basis of complexity and type of input analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">N-gram Models</head><p>Advantages of n-gram models include their abil- ity to estimate the likelihood of particular token sequences and automatically encode word order- ing. While relatively simple and efficient to train on large, unlabeled text corpora, n-gram models are nonetheless limited by their dependence on lo- cal context. In fact, such models are likely to over- value sentences that are locally coherent, yet im- probable due to distant semantic dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dependency Models</head><p>Dependency models circumvent the sequentiality limitation of n-gram models by representing each word as a node in a multi-child dependency tree. Unlabeled dependency language models assume that each word is (1) conditionally independent of the words outside its ancestor sequence, and <ref type="formula" target="#formula_1">(2)</ref> generated independently from the grammatical re- lations. To account for valuable information ig- nored by this model, e.g., two sentences that dif- fer only in a reordering between a verb and its ar- guments, the labeled dependency language model instead treats each word as conditionally indepen- dent of the words and labels outside its ancestor path <ref type="bibr" target="#b3">(Gubbins and Vlachos, 2013)</ref>.</p><p>In addition to offering performance superior to n-gram models, advantages of this representation include relative ease of training and estimation, as well as the ability to leverage standard smoothing methods. However, the models' reliance on out- put from automatic dependency extraction meth- ods and vulnerability to data sparsity detract from their real-world practicality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Continuous Space Models</head><p>Neural networks mitigate issues with data sparsity by learning distributed representations of words, which have been shown to excel at preserving lin- ear regularities among tokens. Despite drawbacks that include functional opacity, propensity toward overfitting, and elevated computational demands, neural language models are capable of outper- forming n-gram and dependency models <ref type="bibr" target="#b3">(Gubbins and Vlachos, 2013;</ref><ref type="bibr" target="#b4">Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Mnih and Kavukcuoglu, 2013)</ref>.</p><p>Log-linear model architectures have been pro- posed to address the computational cost associated with neural networks ( <ref type="bibr" target="#b4">Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Mnih and Kavukcuoglu, 2013)</ref>. The continuous bag-of- words model attempts to predict the current word using n future and n historical words as context. In contrast, the continuous skip-gram model uses the current word as input to predict surrounding words. Utilizing an ensemble architecture com- prised of the skip-gram model and recurrent neu- ral networks, <ref type="bibr" target="#b4">Mikolov et al. (2013)</ref> achieved prior state-of-the-art performance on the MSR Sentence Completion Challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PMI Model</head><p>This section describes an approach to sentence completion based on pointwise mutual informa- tion. The PMI model was designed to account for both local and distant sources of information when evaluating overall sentence coherence.</p><p>Pointwise mutual information is an information-theoretic measure used to dis- cover collocations <ref type="bibr" target="#b1">(Church and Hanks, 1990;</ref><ref type="bibr" target="#b11">Turney and Pantel, 2010)</ref>. Informally, PMI represents the association between two words, i and j, by comparing the probability of observing them in the same context with the probabilities of observing each independently.</p><p>The first step toward applying PMI to the sen- tence completion task involved constructing a word-context frequency matrix from the train- ing corpus. The context was specified to in- clude all words appearing in a single sentence, which is consistent with the hypothesis that it is necessary to examine word co-occurrences at the sentence level to achieve appropriate granu- larity. During development/test set processing, all words were converted to lowercase and stop words were removed based on their part-of-speech tags ( <ref type="bibr" target="#b10">Toutanova et al., 2003)</ref>. To determine whether a particular part-of-speech tag type did, in fact, sig- nal the presence of uninformative words, tokens assigned a hypothetically irrelevant tag were re- moved if their omission positively affected perfor- mance on the development portion of the MSR data set. This non-traditional approach, selected to increase specificity and eliminate dependence on a non-universal stop word list, led to the re- moval of determiners, coordinating conjunctions, pronouns, and proper nouns. 1 Next, feature sets were defined to capture the various sources of in- formation available in a sentence. While feature set number and type is configurable, composition varies, as sets are dynamically generated for each sentence at run time. Enumerated below are the three feature sets utilized by the PMI model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Reduced Context. This feature set con-</head><p>sists of words that remain following the pre- processing steps described above.</p><p>2. Dependencies. Sentence words that share a semantic dependency with the candidate word(s) are included in this set ( <ref type="bibr" target="#b0">Chen and Manning, 2014</ref>). Absent from the set of dependencies are words removed during the pre-processing phase. <ref type="figure" target="#fig_2">Figure 2</ref> depicts an ex- ample dependency parse tree along with fea- tures provided to the PMI model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>Keywords. Providing the model with a col- lection of salient tokens effectively increases the tokens' associated weights. An analo- gous approach to the one described for stop word identification was applied to discover that common nouns consistently hold greater significance than other words assigned hypo- thetically informative part-of-speech tags.</p><p>Let X represent a word-context matrix with n rows and m columns. Row x i: corresponds to word i and column x :j refers to context j. The term x(i,j) indicates how many times word i occurs in context j. Applying PMI to X results in the n x m matrix Y, where term y(i,j) is defined by (1). To avoid overly penalizing words that are unrelated to the context, the positive variant of PMI is considered, in which negative scores are replaced with zero (4).</p><formula xml:id="formula_0">P (i, j) = x(i, j) n i=1 m j=1 x(i, j)<label>(1)</label></formula><formula xml:id="formula_1">P (i * ) = m j=1 x(i, j) n i=1 m j=1 x(i, j)<label>(2)</label></formula><formula xml:id="formula_2">P ( * j) = n i=1 x(i, j) n i=1 m j=1 x(i, j)<label>(3)</label></formula><formula xml:id="formula_3">pmi(i, j) = max 0, log P (i, j) P (i * )P ( * j)<label>(4)</label></formula><p>In addition, the discounting factor described by <ref type="bibr" target="#b7">Pantel and Lin (2002)</ref> is applied to reduce bias to- ward infrequent words <ref type="bibr">(7)</ref>.</p><formula xml:id="formula_4">mincontext = min( n k=1 x(k, j), m k=1 x(i, k)) (5) δ(i, j) = x(i, j) x(i, j) + 1 · mincontext mincontext + 1 (6) dpmi(i, j) = pmi(i, j) · δ(i, j)<label>(7)</label></formula><formula xml:id="formula_5">similarity(i, S) = j∈S dpmi(i, j) · γ (8)</formula><p>The PMI model evaluates each possible re- sponse to a sentence completion question by sub- stituting each candidate answer, i, in place of the blank and scoring the option according to <ref type="bibr">(8)</ref>. This equation measures the semantic similarity be- tween each candidate answer and all other words in the sentence, S. Prior to being summed, individ- ual PMI values associated with a particular word i and context word j are multiplied by γ, which re- flects the number of feature sets containing j. Ulti- mately, the candidate option with the highest sim- ilarity score is selected as the most likely answer.</p><p>Using the procedure described above, addi- tional feature sets of bigrams and trigrams were created and subsequently incorporated into the semantic similarity assessment. This extended model accounts for both word-and phrase- level information by considering windowed co- occurrence statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>Since its introduction, the Microsoft Research Sentence Completion Challenge (Zweig and Burges, 2012a) has become a commonly used benchmark for evaluating semantic models. The data is comprised of material from nineteenth- century novels featured on Project Gutenberg. Each of the 1,040 test sentences contains a single blank that must be filled with one of five candidate words. Associated candidates consist of the cor- rect word and decoys with similar distributional statistics.</p><p>To further validate the proposed method, 285 sentence completion problems were collected from SAT practice examinations given from <ref type="bibr">2000</ref><ref type="bibr">(College Board, 2014</ref>. While the MSR data set includes a list of specified training texts, there is no comparable material for SAT ques- tions. Therefore, the requisite word-context ma- trices were constructed by computing token co- occurrence frequencies from the New York Times portion of the English Gigaword corpus <ref type="bibr" target="#b8">(Parker et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The overall accuracy achieved on the MSR and SAT data sets reveals that the PMI model is able to outperform prior models applied to sentence completion. <ref type="table" target="#tab_0">Table 1 provides a comparison of the  accuracy values attained by various architectures,  while Table 2</ref> summarizes the PMI model's per- formance given feature sets of context words, de- pendencies, and keywords. Recall that the n-gram variant reflects how features are partitioned.</p><p>It appears that while introducing phrase-level information obtained from higher-order n-grams leads to gains in precision on the MSR data set, the same cannot be stated for the set of SAT ques- Language Model MSR Random chance 20.00 N-gram <ref type="bibr" target="#b13">[Zweig (2012b)]</ref> 39.00 Skip-gram <ref type="bibr" target="#b4">[Mikolov (2013)]</ref> 48.00 LSA <ref type="bibr" target="#b13">[Zweig (2012b)]</ref> 49.00 Labeled Dependency <ref type="bibr" target="#b3">[Gubbins (2013)</ref>] 50.00 Dependency RNN <ref type="bibr" target="#b5">[Mirowski (2015)]</ref> 53.50 RNNs <ref type="bibr" target="#b4">[Mikolov (2013)]</ref> 55.40 Log-bilinear <ref type="bibr" target="#b6">[Mnih (2013)]</ref> 55.50 Skip-gram + RNNs <ref type="bibr" target="#b4">[Mikolov (2013)]</ref> 58.90 PMI 61.44  <ref type="table">Table 2</ref>: PMI model performance improvements (% accurate) from incorporating feature sets of higher-order n-grams.</p><p>tions. The most probable explanation for this is twofold. First, informative context words are much less likely to occur within 2-3 tokens of the target word. Second, missing words, which are selected to test knowledge of vocabulary, are rarely found in the training corpus. Bigrams and trigrams containing these infrequent terms are ex- tremely uncommon. Regardless of sentence struc- ture, the sparsity associated with higher-order n- grams guarantees diminishing returns for larger values of n. When deciding whether or not to in- corporate this information, it is also important to consider the significant trade-off with respect to information storage requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper described a novel approach to answer- ing sentence completion questions based on point- wise mutual information. To capture unique in- formation stemming from multiple sources, sev- eral features sets were defined to encode both lo- cal and distant sentence tokens. It was shown that while precision gains can be achieved by augment- ing these feature sets with higher-order n-grams, a significant cost is incurred as a result of the in- creased data storage requirements. Finally, the su- periority of the PMI model is demonstrated by its performance on the Microsoft Research Sentence Completion Challenge, during which a new state- of-the-art result was established.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example sentence completion question (The Princeton Review, 2007).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The dependency parse tree for Question 17 in the MSR data set. Words that share a grammatical relationship with the missing word rising are underscored. Following stop word removal, the feature set for this question is [darkness, was, hidden].</figDesc><graphic url="image-1.png" coords="3,82.36,68.63,425.00,93.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Best performance of various models on 
the MSR Sentence Completion Challenge. Values 
reflect overall accuracy (%). 

Features 
MSR SAT 
Unigrams 
58.46 58.95 
Unigrams + Bigrams 
60.87 58.95 
Unigrams + Bigrams + Trigrams 61.44 58.95 

</table></figure>

			<note place="foot" n="1"> Perhaps counterintuitively, most proper nouns are uninformative for sentence completion, since they refer to specific named entities (e.g. people, locations, organizations, etc.).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="29" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sat reading practice questions: Sentence completion</title>
		<ptr target="https://sat.collegeboard.org/practice/sat-practice-questions-reading/sentence-completion" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>The College Board</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dependency language models for sentence completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gubbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1405" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop Proceedings of the International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dependency recurrent neural language models for sentence completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="511" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discovering word senses from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="613" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>English gigaword fourth edition ldc2009t13</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">11 Practice Tests for the SAT and PSAT</title>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Random House, Inc</publisher>
			<pubPlace>New York City, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A challenge set for advancing language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</title>
		<meeting>the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computational approaches to sentence completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
