<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrent Neural Network based Rule Sequence Model for Statistical Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrent Neural Network based Rule Sequence Model for Statistical Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="132" to="138"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The inability to model long-distance dependency has been handicapping SMT for years. Specifically, the context independence assumption makes it hard to capture the dependency between translation rules. In this paper, we introduce a novel recurrent neural network based rule sequence model to incorporate arbitrary long contextual information during estimating probabilities of rule sequences. Moreover , our model frees the translation model from keeping huge and redundant grammars, resulting in more efficient training and decoding. Experimental results show that our method achieves a 0.9 point BLEU gain over the baseline, and a significant reduction in rule table size for both phrase-based and hierarchical phrase-based systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling long-distance dependency has always been a bottleneck for statistical machine translation (SMT). While lots of efforts have been made in solv- ing long-distance reordering ( <ref type="bibr" target="#b9">Xiong et al., 2006</ref>; <ref type="bibr" target="#b12">Zens and Ney, 2006;</ref><ref type="bibr">Kumar and Byrne, 2005</ref>), long- span n-gram matching <ref type="bibr">(Charniak et al., 2003;</ref><ref type="bibr" target="#b3">Shen et al., 2008;</ref><ref type="bibr" target="#b11">Yu et al., 2014</ref>), much less attention has been concentrated on capturing translation rule de- pendency, which is not explicitly modeled in most translation systems ( <ref type="bibr" target="#b8">Wu et al., 2014)</ref>.</p><p>SMT systems typically model the translation pro- cess as a sequence of translation steps, each of which uses a translation rule. These rules are usually ap- plied independently of each other, which violates the conventional wisdom that translation should be done in context ( <ref type="bibr">Giménez and M` arquez, 2007)</ref>. However, it is not an easy task to capture the rule dependency, which entails much longer context and more severe data sparsity. There are two major solutions: the first one is breaking the rules into bilingual word- pairs and use a n-gram translation model to incorpo- rate lexical dependencies that span rule boundaries ( <ref type="bibr">Marino et al., 2006;</ref><ref type="bibr">Durrani et al., 2013</ref>). These n- gram models (also known as tuple sequence model) could help phrase-based translation models to over- come the phrasal independence assumption, but they rely on word alignment to extract bilingual tuples, which brings in additional alignment error ( <ref type="bibr" target="#b8">Wu et al., 2014</ref>). The other direction lies in utilizing the rule Markov model <ref type="bibr" target="#b7">(Vaswani et al., 2011;</ref><ref type="bibr" target="#b2">Quirk and Menezes, 2006</ref>), which directly explores depen- dencies in rule derivation history and achieves both good performance and slimmer translation model in syntax-based SMT systems. However, the sparsity of translation rules entails aggressive pruning of the training data and constrains the model from scaling to high order grams, significantly limiting the ability of the model. In this paper we follow the second line and pro- pose a novel recurrent neural network based rule sequence model (RNN-RSM), which utilizes the representational power of recurrent neural network (RNN) to capture arbitrary distance of contextual in- formation in estimating the probability of rule se- quences, rather than constrained to n-gram local context limited by Markov assumption. Compared with previous studies, our contributions are as fol- lows:</p><p>First, we lift the Markov assumption in rule se- quence model and use RNN to capture arbitrary- length of contextual information, which is proven to be more accurate in estimating sequential probabili- ties ( <ref type="bibr">Mikolov et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>history.</head><p>Lastly, we apply our model to both phrase- based and hierarchical phrase-based (HPB) systems and achieve an average improvement of 0.9 BLEU points with much slimmer translation models in hy- pergraph reranking task <ref type="bibr">(Huang, 2008</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Rule Sequence Model</head><p>We will first brief our rule sequence model with an example from phrase-based system ( <ref type="bibr">Koehn et al., 2007)</ref>. Consider the following translation from Chi- nese to English:</p><formula xml:id="formula_0">B` ushí Bush yˇuyˇu with Sh¯ alóng Sharon jˇuxíngjˇuxíng hold le -ed hù ıtán meeting '</formula><p>Bush held a meeting with Sharon'</p><p>So one possible rule derivation of the above ex- ample could be: Each row is a derivation step, where s n denotes a hypothesis with a coverage vector capturing the source language words translated so far, and a • in the coverage vector indicates the source word at this position is "covered". Each hypothesis s n−1 can be extended into a longer hypothesis s n by a rule r n translating an uncovered segment. Note that in phrase-based translation we need to set a distortion limit to prohibit long distance reordering, so the end- ing position of last phrase is maintained (e.g., <ref type="bibr">1</ref> and 6 in the coverage vector).</p><formula xml:id="formula_1">( 0 ) : (s 0 , "") (• 1 ) : (s 1 , "Bush") r1 (• ••• 6 ) : (s 2 , "Bush held talks") r2 (••• 3 •••) : (s 3 , "</formula><p>In our example, translation rules r 1 , r 2 , r 3 form a derivation T which leads to a complete translation. So for rule sequence model, the probability of r n depends on its derivation history H(r n ):</p><formula xml:id="formula_2">P (r n ) = P (r n |H(r n ))<label>(1)</label></formula><p>and the probability of a rule derivation T is</p><formula xml:id="formula_3">P (T ) = r i ∈T P (r i |H(r i ))<label>(2)</label></formula><p>Hidden layer, h n U W delayed copy h n-1 r n-1 S n-1 t n-1</p><p>Output layer, y n Input layer, x n Distribution on source phrases</p><p>Distribution on classes of source phrases So the rule sequence model does not make any con- text independence assumption and generate a rule by looking at a context of previous rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training</head><p>The rule sequence model can then be trained on the path set of rule derivations. To obtain golden deriva- tions of translation rules for each sentence pair, We follow <ref type="bibr" target="#b10">Yu et al. (2013)</ref> to utilize force decoding to get golden rule derivations. Specifically, we define a new forced decoding LM which only accepts two consecutive words (denote as p, q) in the reference translation (y i ):</p><formula xml:id="formula_4">P forced (q | p) = 1 if ∃j, s.t. p = y j and q = y j+1 0 otherwise</formula><p>For each hypothesis, we keep the bourndary words as its signiture (only right side for phrase-based model and both sides for HPB). If a boundary word does not occur in the reference, its language model score will be set to −∞; if a boundary word occurs more than once in the reference, the hypothesis is split into multiple hypotheses, one for each index of occurance.</p><p>According to the definition, we can see that the rule sequence [r 1 , r 2 , r 3 ] in the example could pro- duce the exact reference translation, which is ideal for the training of rule sequence model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recurrent Neural Network based Rule Sequence Model</head><p>In order to capture long-span context, we introduce recurrent neural network based rule sequence model to estimate the probability P (r n |H(r n )). Our RNN- RSM can potentially capture arbitrary long context rather than n-1 previous rules limited by Markov assumption. Following <ref type="bibr">Mikolov et al. (2010)</ref>, we adopt the standard RNN architecture: the input layer encodes previous translation rule using one-hot cod- ing, the output layer produces a probability distribu- tion over all translation rules, and the hidden layer maintains a representation of rule derivation history. However, the standard implementation has severe data sparsity problem due to the large size of rule table couple with the limited training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Factorized RNN-RSM</head><p>To solve the sparsity problem, we extend the RNN-RSM model with factorizing rules in the input layer, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. It con- sists of an input layer x, a hidden layer h (state layer), and an output layer y. The connection weights among layers are denoted by matrixes U and W respectively. Unlike the RNN-RSM, which predicts probability P (r n |r n−1 , H(r n−1 )), the factorized RNN-RSM predicts probability P (r n |r n−1 , H(r n−1 ), ¯ s n−1 , ¯ t n−1 ) to generate fol- lowing rule r n , where ¯ s n−1 / ¯ t n−1 are the source/tar- get side of r n−1 , However, ¯ s n−1 and ¯ t n−1 are still too sparse considering the huge vocabulary size and the diversity in forming phrases, so here we use re- cursive auto-encoder <ref type="bibr" target="#b4">(Socher et al., 2011;</ref><ref type="bibr">Li et al., 2013</ref>) to learn phrase embeddings on both source and target side in an unsupervised mannner, mini- mizing the reconstruction error.</p><p>For those rules that are not contained in the train- ing data, the factorized RNN-RSM backs off to the source/target side embedding E s i−1 /E t i−1 . In the special case that E s i−1 and E t i−1 are dropped, the factorized RNN-RSM goes back to RNN-RSM. Fi- nally, the input layer x n is formed by concatenating the input vectors and hidden layer h n−1 at the pre- ceding time step, as shown in the following equa- tion.</p><formula xml:id="formula_5">x n = [v u n−1 , v ¯ s n−1 , v ¯ t n−1 , h n−1 ]<label>(3)</label></formula><p>The neurons in the hidden and output layers are computed as follows:</p><formula xml:id="formula_6">h n = f (U × x n ), y n = g(w × h n )<label>(4)</label></formula><formula xml:id="formula_7">f (z) = 1 1 + e −z , g(z) = e zm k e z k (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Factorized RNN-RSM on source and target phrases</head><p>The above factorized RNN-RSM is conditioned on the previous context during computing the probabil- ity of rule r n . Since r n may still suffer from sparsity, we further factorize r n into its source side phrase ¯ s n and target side phrase ¯ t n . So the probability formula could be rewrite as: P (r n |H(r n )) = P (s n , t n |H(r n )) = P (s n |H(r n )) × P (t n |s n , H(r n )) <ref type="formula">(6)</ref> The first sub-model P (s n , |H(r n )) computes the probability distribution over source phrases. Then the second sub-model P (t n |s n , H(r n )) computes the probability distribution over t n that are trans- lated from s n . The two sub-models are computed with the similar recurrent network shown in <ref type="figure" target="#fig_1">Figure  1</ref> except adding the source side information s n of the current rule r n into the input layer. This method share the same spirit with the RNN-based translation model ( <ref type="bibr" target="#b6">Sundermeyer et al., 2014;</ref><ref type="bibr">Cho et al., 2014</ref>), except that we focus on capturing rule dependencies which has a much small search space. Noted that this new factorize model provides richer information for prediction, and actually is faster to train since the vocabulary of source/target phrases are much small than that of the translation rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>The training corpus consists of 1M sentence pairs with 25M/21M words of Chinese/English respec- tively. Our development and test set are NIST 2006 and 2008 (newswire portion) respectively.</p><p>We obtained alignments by running GIZA++ ( <ref type="bibr">Och and Ney, 2004</ref>) and used the SRILM toolkit <ref type="bibr" target="#b5">(Stolcke, 2002</ref>) to train a 4-gram language model with KN-smoothing on the English side of the train- ing data. Case-insensitive BLEU ( <ref type="bibr" target="#b1">Papineni et al., 2002</ref>) and MERT <ref type="bibr" target="#b0">(Och, 2003)</ref> were used for evalua- tion and tuning.</p><p>We test our method on both phrase-based and hierarchical phrase-based translation models. For phrase-based system, we use Moses with standard features ( <ref type="bibr">Koehn et al., 2007)</ref>. While for hierarchical phrase-based model, we use a in-house implemen- tation of Hiero ( <ref type="bibr">Chiang, 2005</ref> to 5 for the extraction of both phrase-based rule and SCFG rule, as well as beam size to 100 and distor- tion limit to 7 in decoding.</p><p>Since the rule sequence model belongs to the fam- ily of non-local feature <ref type="bibr">(Huang, 2008)</ref>, traditional testing methods like nbest reranking are not suit- able for our experiments. So we adopt hypergraph reranking <ref type="bibr">(Huang and Chiang, 2007;</ref><ref type="bibr">Huang, 2008)</ref>, which proves to be effective for integrating nonlo- cal features into dynamic programming. The de- coding process is divided into two passes. In the first pass, only standard features (i.e., standard fea- tures for phrase-based or HPB model) are used to produce a hypergraph. In the second pass, we use the hypergraph reranking algorithm <ref type="bibr">(Huang, 2008)</ref> to find promising translations using additional rule sequence feature.</p><p>For RNN training, we set the hidden layer size to 512 and classes in the output layer to 256. To obtain phrase-embedding, we use open source tool str2vec 1 ( <ref type="bibr">Li et al., 2013</ref>) to train two autoencoders on the source and target side of rule-table respec- tively.  Also, we conduct an interesting experiment to see if our fRNN-RSM could somehow replace the role of composed rules (rules that can be formed out of smaller rules in the grammar) and guides more fine- grained rule-set to produce better translation results. We re-implement <ref type="bibr">He et al. (2009)</ref>'s work to filter out monotone composed rules for both Hiero and Moses. We are able to filter out a large number of monotone composed rules, about 50% rules for Hi-ero and 31% for Moses. The results are shown in <ref type="table" target="#tab_2">Table 2. Interestingly the performance of slimmer  translation model with fRNN-RSM exceeds baseline  with full rule-table,</ref> and catches up with the orig- inal fRNN-RSM. The reason is two-folded: first, deleting monotone composed rules doesn't effect the overall coverage of the rule-set, making limited harm to the system. Second, with less rules, the data sparse problem of RNN training is further alleviated, resulting in a better fRNN-RSM for probability pre- diction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Besides the work of Vaswani et al. (2011) discussed in Section 1, there are several other works using a rule bigram or trigram model in machine translation, <ref type="bibr">Ding and Palmer (2005)</ref> use n-gram rule Markov model in the dependency treelet model, <ref type="bibr">Liu and Gildea (2008)</ref> applies the same method in a tree-to- string model. Our work is different from theirs in that we lift the Markov assumption and use recur- rent neural network to capture much longer contex- tual information to help probability prediction.</p><p>Our work is also in the same spirit with tuple se- quence models <ref type="bibr">(Marino et al., 2006;</ref><ref type="bibr">Durrani et al., 2013;</ref><ref type="bibr">Hui Zhang, 2013;</ref><ref type="bibr" target="#b8">Wu et al., 2014</ref>), which break the translation sequence into bilingual tuples and use a Markov model to capture the dependency of tuples. Comparing to them, we take a more di- rect approach to use translation rule dependency to guide translation process, rather than rely on tuples which will be significant affected by word alignment errors.</p><p>Outside of machine translation, the idea of weak- ening independence assumption by modeling the derivation history is also found in parsing <ref type="bibr">(Johnson, 1998)</ref>, where rule probabilities are conditioned on parent and grand-parent nonterminals. Inspired by it, we successfully find a solution for the translation field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have presented a novel recurrent neural network based rule sequence model to esti- mate the probability of translation rule sequences. One of the major advantages of our model is its po- tential to capture long-span dependency compared with n-gram Markov models. In addition, our factor- ized model with phrase embedding could further al- leviate the data sparse problem in RNN training. Fi- nally we conduct experiments on both phrase-based and hierarchical phrase-based models and get an av- erage improvement of 0.9 BLEU points over the baseline. In the future we will investigate stronger network structure such as LSTM to further improve the prediction power of our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Bush held talks with Sharon") r3 r 1 : B` ushí → Bush r 2 : jˇuxíngjˇuxíng lehù ıtán → held talks r 3 : yˇuyˇu Sh¯ alóng → with Sharon</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Factorized recurrent neural network with source and target side phrase embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). We set phrase-limit</figDesc><table>System 

Moses 
Hiero 
dev-set test-set dev-set test-set 
Baseline 
28.4 
27.7 
30.4 
30.0 
+RMM 
28.7 
28.3 
30.7 
30.2 
+fRNN-RSM (1) 
28.9 
28.6 
30.9 
30.6 
+fRNN-RSM st (2) 
29.3 
28.5 
31.2 
30.7 
+(1)+(2) 
29.6 
28.7 
31.4 
30.8 

Table 1: Main results. RMM is the re-implementation of Vaswani et al. (2011), fRNN-RSM denotes for factorized 
RNN-RSM describe in Section 3.1, fRNN-RSM st denotes for RNN-RSM factorized by source/target side in Section 
3.2. Results in bold mean that the improvements over "Baseline" are statistically significant (p &lt; 0.05) (Koehn, 2004). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 presents</head><label>1</label><figDesc>the main results of our paper. To show the merits of our RNN-RSM, we also re- implement Vaswani et al. (2011)'s work, denote as rule Markov model (RMM). It utilize tri-gram rule derivation history for prediction, whereas our RNN- RSM could capture arbitrary length of contextual in- formation. We can see that RMM provides a mod- est improvement over the baseline, 0.6/0.2 points over Moses/Hiero, thanks to the positive guidance</figDesc><table>1 https://github.com/pengli09/str2vec 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>BLEU score comparison on different rule-set, 
"w/o monotone" denotes we filter out monotone com-
posed rules in both rule table and our RNN-RSM, full 
denotes we use the total rule-set. 

of short-span rule dependency. On the other hand, 
our factorized RNN-RSM with phrase embeddings 
(fRNN-RSM) provides a more significant BLEU 
score improvement (0.9 for Moses, 0.6 for Hiero), 
which exemplifies that the long-span rule depen-
dency captured by RNN could provides additional 
boost in translation quality. At the same time, fac-
torized RNN-RSM on source and target phrases 
(fRNN-RSM st ) alleviate the data sparse problem 
in RNN training, resulting in slightly better per-
formance. Finally, when we combine both factor-
ized model, we get the best performance at 28.7 for 
Moses and 30.8 for Hiero, both significantly better 
than baseline systems. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Joseph</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Philadephia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do we need phrases?: challenging the conventional wisdom in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on human language technology conference of the north american chapter of the association of computational linguistics</title>
		<meeting>the main conference on human language technology conference of the north american chapter of the association of computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new string-to-dependency machine translation algorithm with a target dependency language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph M</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semisupervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP</title>
		<meeting>ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="901" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Translation modeling with bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rule markov models for fast tree-tostring translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2011</title>
		<meeting>ACL 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Portland, OR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural network-based tuple sequence model for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1908" to="1917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum entropy based phrase reordering model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Max-violation perceptron and forced decoding for scalable mt training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1112" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A structured language model for incremental tree-tostring translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative reordering models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Statistical Machine Translation</title>
		<meeting>the Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="55" to="63" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
