<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
						</author>
						<title level="a" type="main">Learning with Noise: Enhance Distantly Supervised Relation Extraction with Dynamic Transition Matrix</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="430" to="439"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1040</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distant supervision significantly reduces human efforts in building training data for many classification tasks. While promising , this technique often introduces noise to the generated training data, which can severely affect the model performance. In this paper, we take a deep look at the application of distant supervision in relation extraction. We show that the dynamic transition matrix can effectively characterize the noise in the training data built by distant supervision. The transition matrix can be effectively trained using a novel curriculum learning based method without any direct supervision about the noise. We thoroughly evaluate our approach under a wide range of extraction scenarios. Experimental results show that our approach consistently improves the extraction results and outperforms the state-of-the-art in various evaluation scenarios.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distant supervision (DS) is rapidly emerging as a viable means for supporting various classification tasks -from relation extraction ( <ref type="bibr" target="#b8">Mintz et al., 2009)</ref> and sentiment classification ( <ref type="bibr" target="#b3">Go et al., 2009</ref>) to cross-lingual semantic analysis <ref type="bibr" target="#b2">(Fang and Cohn, 2016)</ref>. By using knowledge learned from seed ex- amples to label data, DS automatically prepares large scale training data for these tasks.</p><p>While promising, DS does not guarantee per- fect results and often introduces noise to the gener- ated data. In the context of relation extraction, DS works by considering sentences containing both the subject and object of a &lt;subj, rel, obj&gt; triple as its supports. However, the generated data are not always perfect. For instance, DS could match the knowledge base (KB) triple, &lt;Donald Trump, born-in, New York&gt; in false positive contexts like Donald Trump worked in New York City. Prior works ( <ref type="bibr" target="#b18">Takamatsu et al., 2012;</ref><ref type="bibr" target="#b15">Ritter et al., 2013)</ref> show that DS often mistakenly labels real posi- tive instances as negative (false negative) or versa vice (false positive), and there could be confu- sions among positive labels as well. These noises can severely affect training and lead to poorly- performing models.</p><p>Tackling the noisy data problem of DS is non- trivial, since there usually lacks of explicit super- vision to capture the noise. Previous works have tried to remove sentences containing unreliable syntactic patterns ( <ref type="bibr" target="#b18">Takamatsu et al., 2012)</ref>, design new models to capture certain types of noise or aggregate multiple predictions under the at-least- one assumption that at least one of the aligned sentences supports the triple in KB ( <ref type="bibr" target="#b13">Riedel et al., 2010;</ref><ref type="bibr" target="#b17">Surdeanu et al., 2012;</ref><ref type="bibr" target="#b15">Ritter et al., 2013;</ref><ref type="bibr" target="#b7">Min et al., 2013)</ref>. These approaches represent a substantial leap forward towards making DS more practical. however, are either tightly couple to cer- tain types of noise, or have to rely on manual rules to filter noise, thus unable to scale. Recent break- through in neural networks provides a new way to reduce the influence of incorrectly labeled data by aggregating multiple training instances atten- tively for relation classification, without explicitly characterizing the inherent noise ( <ref type="bibr" target="#b5">Lin et al., 2016;</ref><ref type="bibr" target="#b22">Zeng et al., 2015)</ref>. Although promising, modeling noise within neural network architectures is still in its early stage and much remains to be done.</p><p>In this paper, we aim to enhance DS noise mod- eling by providing the capability to explicitly char- acterize the noise in the DS-style training data within neural networks architectures. We show that while noise is inevitable, it is possible to char- acterize the noise pattern in a unified framework along with its original classification objective. Our key insight is that the DS-style training data typi- cally contain useful clues about the noise pattern. For example, we can infer that since some peo- ple work in their birthplaces, DS could wrongly la- bel a training sentence describing a working place as a born-in relation. Our novel approach to noisy modeling is to use a dynamically-generated transition matrix for each training instance to (1) characterize the possibility that the DS labeled re- lation is confused and (2) indicate its noise pat- tern. To tackle the challenge of no direct guidance over the noise pattern, we employ a curriculum learning based training method to gradually model the noise pattern over time, and utilize trace regu- larization to control the behavior of the transition matrix during training. Our approach is flexible - while it does not make any assumptions about the data quality, the algorithm can make effective use of the data-quality prior knowledge to guide the learning procedure when such clues are available.</p><p>We apply our method to the relation extraction task and evaluate under various scenarios on two benchmark datasets. Experimental results show that our approach consistently improves both ex- traction settings, outperforming the state-of-the- art models in different settings.</p><p>Our work offers an effective way for tackling the noisy data problem of DS, making DS more practical at scale. Our main contributions are to (1) design a dynamic transition matrix structure to characterize the noise introduced by DS, and (2) design a curriculum learning based framework to adaptively guide the training procedure to learn with noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head><p>The task of distantly supervised relation extraction is to extract knowledge triples, &lt;subj, rel, obj&gt;, from free text with the training data constructed by aligning existing KB triples with a large cor- pus. Specifically, given a triple in KB, DS works by first retrieving all the sentences containing both subj and obj of the triple, and then constructing the training data by considering these sentences as support to the existence of the triple. This task can be conducted in both the sentence and the bag levels. The former takes a sentence s containing  Figure 1: Overview of our approach both subj and obj as input, and outputs the rela- tion expressed by the sentence between subj and obj. The latter setting alleviates the noisy data problem by using the at-least-one assumption that at least one of the retrieved sentences containing both subj and obj supports the &lt;subj, rel, obj&gt; triple. It takes a bag of sentences S as input where each sentence s ∈ S contains both subj and obj, and outputs the relation between subj and obj ex- pressed by this bag.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our approach</head><p>In order to deal with the noisy training data ob- tained through DS, our approach follows four steps as depicted in <ref type="figure">Figure 1</ref>. First, each input sentence is fed to a sentence encoder to generate an embed- ding vector. Our model then takes the sentence embeddings as input and produce a predicted re- lation distribution, p, for the input sentence (or the input sentence bag). At the same time, our model dynamically produces a transition matrix, T, which is used to characterize the noise pattern of sentence (or the bag). Finally, the predicted distribution is multiplied by the transition matrix to produce the observed relation distribution, o, which is used to match the noisy relation labels assigned by DS while the predicted relation dis- tribution p serves as output of our model during testing. One of the key challenges of our approach is on determining the element values of the transi- tion matrix, which will be described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence-level Modeling</head><p>Sentence Embedding and Prediction In this work, we use a piecewise convolutional neural net- work ( <ref type="bibr" target="#b22">Zeng et al., 2015)</ref> for sentence encoding, but other sentence embedding models can also be used. We feed the sentence embedding to a full connection layer, and use softmax to generate the predicted relation distribution, p.</p><p>Noise Modeling First, each sentence embedding x, generated b sentence encoder, is passed to a full connection layer as a non-linearity to obtain the sentence embedding x n used specifically for noise modeling. We then use softmax to calculate the transition matrix T, for each sentence:</p><formula xml:id="formula_0">T ij = exp(w T ij x n + b) |C| j=1 exp(w T ij x n + b)<label>(1)</label></formula><p>where T ij is the conditional probability for the in- put sentence to be labeled as relation j by DS, given i as the true relation, b is a scalar bias, |C| is the number of relations, w ij is the weight vector characterizing the confusion between i and j.</p><p>Here, we dynamically produce a transition ma- trix, T, specifically for each sentence, but with the parameters (w ij ) shared across the dataset. By do- ing so, we are able to adaptively characterize the noise pattern for each sentence, with a few pa- rameters only. In contrast, one could also pro- duce a global transition matrix for all sentences, with much less computation, where one need not to compute T on the fly (see Section 6.1).</p><p>Observed Distribution When we characterize the noise in a sentence with a transition matrix T, if its true relation is i, we can assume that i might be erroneously labeled as relation j by DS with probability T ij . We can therefore capture the ob- served relation distribution, o, by multiplying T and the predicted relation distribution, p:</p><formula xml:id="formula_1">o = T T · p (2)</formula><p>where o is then normalized to ensure i o i = 1. Rather than using the predicted distribution p to directly match the relation labeled by DS ( <ref type="bibr" target="#b22">Zeng et al., 2015;</ref><ref type="bibr" target="#b5">Lin et al., 2016</ref>), here we utilize o to match the noisy labels during training and still use p as output during testing, which actually captures the procedure of how the noisy label is produced and thus protects p from the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bag Level Modeling</head><p>Bag Embedding and Prediction One of the key challenges for bag level model is how to aggre- gate the embeddings of individual sentences into the bag level. In this work, we experiment two methods, namely average and attention aggrega- tion ( <ref type="bibr" target="#b5">Lin et al., 2016</ref>). The former calculates the bag embedding, s, by averaging the embeddings of each sentence, and then feed it to a softmax classi- fier for relation classification.</p><p>The attention aggregation calculates an atten- tion value, a ij , for each sentence i in the bag with respect to each relation j, and aggregates to the bag level as s j , by the following equations 1 :</p><formula xml:id="formula_2">s j = n i a ij x i ; a ij = exp(x T i r j ) n i exp(x T i r j )<label>(3)</label></formula><p>where x i is the embedding of sentence i, n the number of sentences in the bag, and r j is the ran- domly initialized embedding for relation j. In sim- ilar spirit to ( <ref type="bibr" target="#b5">Lin et al., 2016)</ref>, the resulting bag embedding s j is fed to a softmax classifier to pre- dict the probability of relation j for the given bag.</p><p>Noise Modeling Since the transition matrix ad- dresses the transition probability with respect to each true relation, the attention mechanism ap- pears to be a natural fit for calculating the tran- sition matrix in bag level. Similar to attention ag- gregation above, we calculate the bag embedding with respect to each relation using Equation 3, but with a separate set of relation embeddings r j . We then calculate the transition matrix, T, by:</p><formula xml:id="formula_3">T ij = exp(s T i r j + b i ) |C| j=1 exp(s T i r j + b i ) (4)</formula><p>where s i is the bag embedding regarding relation i, and r j is the embedding for relation j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Curriculum Learning based Training</head><p>One of the key challenges of this work is on how to train and produce the transition matrix to model the noise in the training data without any direct guidance and human involvement. A straightforward solution is to directly align the ob- served distribution, o, with respect to the noisy labels by minimizing the sum of the two terms:</p><formula xml:id="formula_4">CrossEntropy(o) + Regularization.</formula><p>However, doing so does not guarantee that the prediction dis- tribution, p, will match the true relation distribu- tion. The problem is at the beginning of the train- ing, we have no prior knowledge about the noise pattern, thus, both T and p are less reliable, mak- ing the training procedure be likely to trap into some poor local optimum. Therefore, we require a technique to guide our model to gradually adapt to the noisy training data, e.g., learning something simple first, and then trying to deal with noises.</p><p>Fortunately, this is exactly what curriculum learning can do. The idea of curriculum learn- ing ( <ref type="bibr" target="#b0">Bengio et al., 2009</ref>) is simple: starting with the easiest aspect of a task, and leveling up the dif- ficulty gradually, which fits well to our problem. We thus employ a curriculum learning framework to guide our model to gradually learn how to char- acterize the noise. Another advantage is to avoid falling into poor local optimum.</p><p>With curriculum learning, our approach pro- vides the flexibility to combine prior knowledge of noise, e.g., splitting a dataset into reliable and less reliable subsets, to improve the effectiveness of the transition matrix and better model the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Trace Regularization</head><p>Before proceeding to training details, we first dis- cuss how we characterize the noise level of the data by controlling the trace of its transition ma- trix. Intuitively, if the noise is small, the transition matrix T will tend to become an identity matrix, i.e., given a set of annotated training sentences, the observed relations and their true relations are al- most identical. Since each row of T sums to 1, the similarity between the transition matrix and the identity matrix can be represented by its trace, trace(T). The larger the trace(T) is, the larger the diagonal elements are, and the more similar the transition matrix T is to the identity matrix, indicating a lower level of noise. Therefore, we can characterize the noise pattern by controlling the expected value of trace(T) in the form of reg- ularization. For example, we will expect a larger trace(T) for reliable data, but a smaller trace(T) for less reliable data. Another advantage of em- ploying trace regularization is that it could help re- duce the model complexity and avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training</head><p>To tackle the challenge of no direct guidance over the noise patterns, we implement a curriculum learning based training method to first train the model without considerations for noise. In other words, we first focus on the loss from the predic- tion distribution p , and then take the noise model- ing into account gradually along the training pro- cess, i.e., gradually increasing the importance of the loss from the observed distribution o while de- creasing the importance of p. In this way, the pre- diction branch is roughly trained before the model managing to characterize the noise, thus avoids be- ing stuck into poor local optimum. We thus design to minimize the following loss function:</p><formula xml:id="formula_5">L = N i=1 −((1 − α)log(o iy i ) + αlog(p iy i )) − βtrace(T i )<label>(5)</label></formula><p>where 0&lt;α≤1 and β&gt;0 are two weighting param- eters, y i is the relation assigned by DS for the i-th instance, N the total number of training instances, o iy i is the probability that the observed relation for the i-th instance is y i , and p iy i is the probability to predict relation y i for the i-th instance.</p><p>Initially, we set α=1, and train our model com- pletely by minimizing the loss from the prediction distribution p. That is, we do not expect to model the noise, but focus on the prediction branch at this time. As the training progresses, the predic- tion branch gradually learns the basic prediction ability. We then decrease α and β by 0&lt;ρ&lt;1 (α * =ρα and β * =ρβ) every τ epochs, i.e., learning more about the noise from the observed distribu- tion o and allowing a relatively smaller trace(T) to accommodate more noise. The motivation be- hind is to put more and more effort on learning the noise pattern as the training proceeds, with the essence of curriculum learning. This gradu- ally learning paradigm significantly distinguishes from prior work on noise modeling for DS seen to date. Moreover, as such a method does not rely on any extra assumptions, it can serve as our default training method for T.</p><p>With Prior Knowledge of Data Quality On the other hand, if we happen to have prior knowledge about which part of the training data is more re- liable and which is less reliable, we can utilize this knowledge as guidance to design the curricu- lum. Specifically, we can build a curriculum by first training the prediction branch on the reliable data for several epochs, and then adding the less reliable data to train the full model. In this way, the prediction branch is roughly trained before ex- posed to more noisy data, thus is less likely to fall into poor local optimum.</p><p>Furthermore, we can take better control of the training procedure with trace regularization, e.g., encouraging larger trace(T) for reliable sub- set and smaller trace(T) for less relaibale ones. Specifically, we propose to minimize:</p><formula xml:id="formula_6">L = M m=1 Nm i=1 −log(o mi,y mi ) − β m trace(T mi )<label>(6)</label></formula><p>where β m is the regularization weight for the m-th data subset, M is the total number of subsets, N m the number of instances in m-th subset, and T mi , y mi and o mi,y mi are the transition matrix, the re- lation labeled by DS and the observed probability of this relation for the i-th training instance in the m-th subset, respectively. Note that different from Equation 5, this loss function does not need to ini- tiate training by minimizing the loss regarding the prediction distribution p, since one can easily start by learning from the most reliable split first.</p><p>We also use trace regularization for the most re- liable subset, since there are still some noise anno- tations inevitably appearing in this split. Specifi- cally, we expect its trace(T) to be large (using a positive β) so that the elements of T will be cen- tralized to the diagonal and T will be more similar to the identity matrix. As for the less reliable sub- set, we expect the trace(T) to be small (using a negative β) so that the elements of the transition matrix will be diffusive and T will be less similar to the identity matrix. In other words, the transi- tion matrix is encouraged to characterize the noise.</p><p>Note that this loss function only works for sen- tence level models. For bag level models, since reliable and less reliable sentences are all aggre- gated into a sentence bag, we can not determine which bag is reliable and which is not. However, bag level models can still build a curriculum by changing the content of a bag, e.g., keeping re- liable sentences in the bag first, then gradually adding less reliable ones, and training with Equa- tion 5, which could benefit from the prior knowl- edge of data quality as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Methodology</head><p>Our experiments aim to answer two main ques- tions: (1) is it possible to model the noise in the training data generated through DS, even when there is no prior knowledge to guide us? and <ref type="formula">(2)</ref> whether the prior knowledge of data quality can help our approach better handle the noise.</p><p>We apply our approach to both sentence level and bag level extraction models, and evaluate in the situations where we do not have prior knowl- edge of the data quality as well as where such prior knowledge is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluate our approach on two datasets. TIMERE We build TIMERE by using DS to align time-related Wikidata <ref type="bibr" target="#b19">(Vrandeči´Vrandeči´c and Krötzsch, 2014</ref>) KB triples to Wikipedia text. It contains 278,141 sentences with 12 types of re- lations between an entity mention and a time ex- pression. We choose to use time-related relations because time expressions speak for themselves in terms of reliability. That is, given a KB triple &lt;e, rel, t&gt; and its aligned sentences, the finer- grained the time expression t appears in the sen- tence, the more likely the sentence supports the existence of this triple. For example, a sentence containing both Alphabet and October-2-2015 is very likely to express the inception-time of Alphabet, while a sentence containing both Al- phabet and 2015 could instead talk about many events, e.g., releasing financial report of 2015, hir- ing a new CEO, etc. Using this heuristics, we can split the dataset into 3 subsets according to different granularities of the time expressions in- volved, indicating different levels of reliability. Our criteria for determining the reliability are as follows. Instances with full date expressions, i.e., Year-Month-Day, can be seen as the most re- liable data, while those with partial date expres- sions, e.g., Month-Year and Year-Only, are considered as less reliable. Negative data are con- structed heuristically that any entity-time pairs in a sentence without corresponding triples in Wiki- data are treated as negative data. During training, we can access 184,579 negative and 77,777 pos- itive sentences, including 22,214 reliable, 2,094 and 53,469 less reliable ones. The validation set and test set are randomly sampled from the reli- able (full-date) data for relatively fair evaluations and contains 2,776, 2,771 positive sentences and 5,143, 5,095 negative sentences, respectively.</p><p>ENTITYRE is a widely-used entity relation ex- traction dataset, built by aligning triples in Free- base to the New York Times (NYT) corpus ( <ref type="bibr" target="#b13">Riedel et al., 2010</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>Hyper-parameters We use 200 convolution kernels with widow size 3. During training, we use stochastic gradient descend (SGD) with batch size 20. The learning rates for sentence-level and bag-level models are 0.1 and 0.01, respectively.</p><p>Sentence level experiments are performed on TIMERE, using 100-d word embeddings pre- trained using GloVe ( <ref type="bibr" target="#b11">Pennington et al., 2014</ref>) on Wikipedia and Gigaword <ref type="bibr" target="#b10">(Parker et al., 2011</ref>), and 20-d vectors for distance embeddings. Each of the three subsets of TIMERE is added after the previ- ous phase has run for 15 epochs. The trace regu- larization weights are β 1 = 0.01, β 2 = −0.01 and β 3 = −0.1, respectively, from the reliable to the most unreliable, with the ratio of β 3 and β 2 fixed to 10 or 5 when tuning.</p><p>Bag level experiments are performed on both TIMERE and ENTITYRE. For TIMERE, we use the same parameters as above. For ENTITYRE, we use 50-d word embeddings pre-trained on the NYT corpus using word2vec <ref type="bibr" target="#b6">(Mikolov et al., 2013)</ref>, and 5-d vectors for distance embedding. For both datasets, α and β in Eq. 5 are initialized to 1 and 0.1, respectively. We tried various decay rates, {0.95, 0.9, 0.8}, and steps, {3, 5, 8}. We found that using a decay rate of 0.9 with step of 5 gives best performance in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metric</head><p>The performance is reported using the precision-recall (PR) curve, which is a standard evaluation metric in relation extraction. Specifically, the extraction results are first ranked decreasingly by their confidence scores, then the precision and recall are calculated by setting the threshold to be the score of each extraction result one by one.</p><p>Naming Conventions We evaluate our ap- proach under a wide range of settings for sentence level (sent ) and bag level (bag ) models: (1) mix: trained on all three subsets of TIMERE mixed together; (2) reliable: trained using the reliable subset of TIMERE only; (3) PR: trained with prior knowledge of annotation qual- ity, i.e., starting from the reliable data and then adding the unreliable data; (4) TM: trained with dynamic transition matrix; (5) GTM: trained with a global transition matrix. In bag level, we also in- vestigate the performance of average aggregation ( avg) and attention aggregation ( att).   <ref type="figure" target="#fig_3">Figure  2</ref>. We can see that mixing all subsets together (sent mix) gives the worst performance, signif- icantly worse than using the reliable subset only (sent reliable). This suggests the noisy na- ture of the training data obtained through DS and properly dealing with the noise is the key for DS for a wider range of applications. When getting help from our dynamic transition matrix, the model (sent mix TM) significantly improves sent mix, delivering the same level of perfor- mance as sent reliable in most cases. This suggests that our transition matrix can help to mit- igate the bad influence of noisy training instances. Now let us consider the PR scenario where one can build a curriculum by first training on the reli- able subset, then gradually moving to both reliable and less reliable data. We can see that, this simple curriculum learning based model (sent PR) fur- ther outperforms sent reliable significantly, indicating that the curriculum learning framework not only reduces the effect of noise, but also helps the model learn from noisy data. When apply- ing the transition matrix approach into this cur- riculum learning framework using one reliable subset and one unreliable subset generated by mixing our two less reliable subsets, our model (sent PR seg2 TM) further improves sent PR by utilizing the dynamic transition matrix to model the noise. It is not surprising that when we use all three subsets separately, our model (sent PR TM) significantly outperforms all other models by a large margin. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">. 0 0 s e n t _ m i x _ T M s e n t _ P R _ s e g 2 _ T M s e n t _ P R _ T M P r e c i s i o n R e c a l l s e n t _ m i x s e n t _ r e l i a b l e s e n t _ P R</head><note type="other">0 . 2 0 . 4 0 . 6 0 . 8 0 . 9 0 0 . 9 2 0 . 9 4 0 . 9 6 0 . 9 8 1 . 0 0 P r e c i s i o n</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R e c a l l b a g _ a t t _ m i x b a g _ a t t _ r e l i a b l e b a g _ a t t _ P R b a g _ a t t _ m i x _ T M b a g _ a t t _ P R _ T M</head><note type="other">(a) Attention Aggregation 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 0 . 9 0 0 . 9 2 0 . 9 4 0 . 9 6 0 . 9 8 1 . 0 0 P r e c i s i o n</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R e c a l l b a g _ a v g _ m i x b a g _ a v g _ r e l i a b l e b a g _ a v g _ P R b a g _ a v g _ m i x _ T M b a g _ a v g _ P R _ T M</head><p>(b) Average Aggregation  <ref type="figure">(bag att mix)</ref>. In contrast to the sentence level, bag att mix outperforms bag att reliable by a large margin, because bag att mix has taken the at-least-one assump- tion into consideration through the attention ag- gregation mechanism (Eq. 3), which can be seen as a denoising step within the bag. This may also be the reason that when we introduce either our dynamic transition matrix (bag att mix TM) or the curriculum of using prior knowledge of data quality (bag att PR) into the bag level models, the improvement regarding bag att mix is not as significant as in the sentence level.</p><p>However, when we apply our dynamic transi- tion matrix into the curriculum built upon prior knowledge of data quality (bag att PR TM), the performance gets further improved. This hap- pens especially in the high precision part com- pared to bag att PR. We also note that the bag level's at-least-one assumption does not always hold, and there are still false negative and false positive problems. Therefore, using our transi- tion matrix approach with or without prior knowl- edge of data quality, i.e., bag att mix TM and bag att PR TM, both improve the performance, and bag att PR TM performs slightly better.</p><p>The results of bag level models with average ag- gregation are shown in <ref type="figure" target="#fig_5">Figure 3(b)</ref>, where the rel- ative ranking of various settings is similar to those with attention aggregation. A notable difference 0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 0 . 9 0 0 . 9 2 0 . 9 4 0 . 9 6 0 . 9 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">. 0 0 s e n t _ P R s e n t _ P R _ G T M s e n t _ P R _ T M b a g _ a t t _ P R b a g _ a t t _ P R _ G T M b a g _ a t t _ P R _ T M</head><p>P r e c i s i o n R e c a l l <ref type="figure">Figure 4</ref>: Global TM v.s. Dynamic TM is that both bag avg PR and bag avg mix TM improve bag avg mix by a larger margin com- pared to that in the attention aggregation setting. The reason may be that the average aggregation mechanism is not as good as the attention aggre- gation in denoising within the bag, which leaves more space for our transition matrix approach or curriculum learning with prior knowledge to im- prove. Also note that bag avg reliable per- forms best in the very-low-recall region but worst in general. This is because that it ranks higher the sentences expressing either birth-date or death-date, the simplest but the most com- mon relations in the dataset, but fails to learn other relations with limited or noisy training instances, given its relatively simple aggregation strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global v.s. Dynamic Transition Matrix</head><p>We also compare our dynamic transition matrix method with the global transition matrix method, which maintains only one transition matrix for all training instances. Specifically, instead of dynam-ically generating a transition matrix for each da- tum, we first initialize an identity matrix T ∈ R |C|×|C| , where |C| is the number of relations (in- cluding no-relation). Then the global transi- tion matrix T is built by applying softmax to each row of T so that j T ij = 1:</p><formula xml:id="formula_7">T ij = e T ij |C| j=1 e T ij (7)</formula><p>where T ij and T ij are the elements in the i th row and j th column of T and T . The element values of matrix T are also updated via backpropagation during training. As shown in <ref type="figure">Figure 4</ref>, using one global transition matrix ( GTM) is also beneficial and improves both the sentence level (sent PR) and bag level (bag att PR) models. However, since the global transition matrix only captures the global noise pattern, it fails to characterize individ- uals with subtle differences, resulting in a perfor- mance drop compared to the dynamic one ( TM).</p><p>Case Study We find our transition matrix method tends to obtain more significant im- provement on noisier relations.</p><p>For exam- ple, time of spacecraft landing is noisier than time of spacecraft launch since compared to the launching of a spacecraft, there are fewer sen- tences containing the landing time of a space- craft that talks directly about the landing. Instead, many of these sentences tend to talk about the activities of the crew. Our sent PR TM model improves the F1 of time of spacecraft landing and time of spacecraft launch over sent PR by 9.09% and 2.78%, respectively. The transition matrix makes more significant improvement on time of spacecraft landing since there are more noisy sentences for our method to handle, which results in more significant improvement on the quality of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance on ENTITYRE</head><p>We evaluate our bag level models on ENTI- TYRE. As shown in <ref type="figure">Figure 5</ref>, it is not surpris- ing that the basic model with attention aggrega- tion (att) significantly outperforms the average one (avg), where att in our bag embedding is similar in spirit to ( <ref type="bibr" target="#b5">Lin et al., 2016)</ref>, which has re- ported the-state-of-the-art performance on ENTI- TYRE. When injected with our transition matrix approach, both att TM and avg TM clearly out- perform their basic versions. 0 . 7 0 . 8 0 . 9 1 . 0 P r e c i s i o n R e c a l l a v g a t t a v g _ T M a t t _ T M Similar to the situations in TIMERE, since att has taken the at-least-one assumption into account through its attention-based bag embedding mech- anism, thus the improvement made by att TM is not as large as by avg TM.</p><p>We also include the comparison with three feature-based methods: <ref type="bibr" target="#b8">Mintz (Mintz et al., 2009</ref>) is a multiclass logistic regression model; <ref type="bibr">MultiR (Hoffmann et al., 2011</ref>) is a probabilistic graphical model that can handle overlapping rela- tions; MIML ( <ref type="bibr" target="#b17">Surdeanu et al., 2012</ref>) is also a prob- abilistic graphical model but operates in the multi- instance multi-label paradigm. As shown in Ta- ble 1, although traditional feature-based methods have reasonable results in the low recall region, their performances drop quickly as the recall goes up, and MultiR and MIML did not even reach the 30% recall. This indicates that, while human- designed featurs can effectively capture certain re- lation patterns, their coverage is relatively low. On the other hand, neural network models have more stable performance across different recalls, and att TM performs generally better than other models, indicating again the effectiveness of our transition matrix method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>In addition to relation extraction, distant supervi- sion (DS) is shown to be effective in generating training data for various NLP tasks, e.g., tweet sentiment classification ( <ref type="bibr" target="#b3">Go et al., 2009)</ref>, tweet named entity classifying ( <ref type="bibr" target="#b14">Ritter et al., 2011</ref>), etc. However, these early applications of DS do not well address the issue of data noise.</p><p>In relation extraction (RE), recent works have been proposed to reduce the influence of wrongly labeled data. The work presented by ( <ref type="bibr" target="#b18">Takamatsu et al., 2012</ref>) removes potential noisy sentences by identifying bad syntactic patterns at the pre- processing stage. ( <ref type="bibr" target="#b21">Xu et al., 2013</ref>) use pseudo- relevance feedback to find possible false nega- tive data. ( <ref type="bibr" target="#b13">Riedel et al., 2010</ref>) make the at-least- one assumption and propose to alleviate the noise problem by considering RE as a multi-instance classification problem. Following this assumption, people further improves the original paradigm us- ing probabilistic graphic models ( <ref type="bibr" target="#b4">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b17">Surdeanu et al., 2012)</ref>, and neural network methods ( <ref type="bibr" target="#b22">Zeng et al., 2015)</ref>. Recently, ( <ref type="bibr" target="#b5">Lin et al., 2016)</ref> propose to use attention mechanism to re- duce the noise within a sentence bag. Instead of characterizing the noise, these approaches only aim to alleviate the effect of noise.</p><p>The at-least-one assumption is often too strong in practice, and there are still chances that the sen- tence bag may be false positive or false negative. Thus it is important to model the noise pattern to guide the learning procedure. ( <ref type="bibr" target="#b15">Ritter et al., 2013)</ref> and <ref type="bibr" target="#b7">(Min et al., 2013</ref>) try to employ a set of la- tent variables to represent the true relation. Our approach differs from them in two aspects. We tar- get noise modeling in neutral networks while they target probabilistic graphic models. We further ad- vance their models by providing the capability to model the fine-grained transition from the true re- lation to the observed, and the flexibility to com- bine indirect guidance.</p><p>Outside of NLP, various methods have been proposed in computer vision to model the data noise using neural networks. ( <ref type="bibr" target="#b16">Sukhbaatar et al., 2015</ref>) utilize a global transition matrix with weight decay to transform the true label distribution to the observed. ( <ref type="bibr" target="#b12">Reed et al., 2014</ref>) use a hidden layer to represent the true label distribution but try to force it to predict both the noisy label and the in- put. <ref type="bibr" target="#b1">(Chen and Gupta, 2015;</ref><ref type="bibr" target="#b20">Xiao et al., 2015</ref>) first estimate the transition matrix on a clean dataset and apply to the noisy data. Our model shares similar spirit with ( <ref type="bibr" target="#b9">Misra et al., 2016</ref>) in that we all dynamically generate a transition matrix for each training instance, but, instead of using vanilla SGD, we train our model with a novel curriculum learning training framework with trace regulariza- tion to control the behavior of transition matrix. In NLP, the only work in neural-network-based noise modeling is to use one single global transi- tion matrix to model the noise introduced by cross- lingual projection of training data <ref type="bibr" target="#b2">(Fang and Cohn, 2016)</ref>. Our work advances them through gener- ating a transition matrix dynamically for each in- stance, to avoid using one single component to characterize both reliable and unreliable data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this paper, we investigate the noise problem in- herent in the DS-style training data. We argue that the data speak for themselves by providing use- ful clues to reveal their noise patterns. We thus propose a novel transition matrix based method to dynamically characterize the noise underlying such training data in a unified framework along the original prediction objective. One of our key inno- vations is to exploit a curriculum learning based training method to gradually learn to model the underlying noise pattern without direct guidance, and to provide the flexibility to exploit any prior knowledge of the data quality to further improve the effectiveness of the transition matrix. We eval- uate our approach in two learning settings of the distantly supervised relation extraction. The ex- perimental results show that the proposed method can better characterize the underlying noise and consistently outperform start-of-the-art extraction models under various scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Encoder sentences embeddings Prediction Noise Modeling predicted distr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sentence Level Results on TIMERE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Bag Level Results on TIMERE Bag Level Models In this setting, we first look at the performance of the bag level models with attention aggregation. The results are shown in Figure 3(a). Consider the comparison between the model trained on the reliable subset only (bag att reliable) and the one trained on the mixed dataset (bag att mix). In contrast to the sentence level, bag att mix outperforms bag att reliable by a large margin, because bag att mix has taken the at-least-one assumption into consideration through the attention aggregation mechanism (Eq. 3), which can be seen as a denoising step within the bag. This may also be the reason that when we introduce either our dynamic transition matrix (bag att mix TM) or the curriculum of using prior knowledge of data quality (bag att PR) into the bag level models, the improvement regarding bag att mix is not as significant as in the sentence level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). It contains 52 relations, 136,947 pos- itive and 385,664 negative sentences for training, and 6,444 positive and 166,004 negative sentences for testing. Unlike TIMERE, this dataset does not contain any prior knowledge about the data qual- ity. Since the sentence level annotations in EN- TITYRE are too noisy to serve as gold standard, we only evaluate bag-level models on ENTITYRE, a standard practice in previous works (Surdeanu et al., 2012; Zeng et al., 2015; Lin et al., 2016).</figDesc><table></table></figure>

			<note place="foot" n="1"> While (Lin et al., 2016) use bilinear function to calculate aij, we simply use dot product since we find these two functions perform similarly in our experiments.</note>

			<note place="foot" n="0">. 0 0. 2 0. 4 0. 6 0. 8 0. 8 0 0. 8 5</note>

			<note place="foot" n="0">. 0 0. 1 0. 2 0. 3 0. 4 0. 2 0. 3 0. 4 0. 5 0. 6</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. ACM</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Webly supervised learning of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning when to trust distant supervision: An application to lowresource pos tagging using cross-lingual projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CONLL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">CS224N Project Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Seeing through the human reporting bias: Visual classifiers from noisy humancentric labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2930" to="2939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">English gigaword fifth edition, linguistic data consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<meeting><address><addrLine>Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling missing data in distant supervision for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="367" to="378" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing wrong labels in distant supervision for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shingo</forename><surname>Takamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="721" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrandeči´vrandeči´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Filling knowledge base gaps for distant supervision of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="665" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
