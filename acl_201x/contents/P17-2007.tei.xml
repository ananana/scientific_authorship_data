<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Architectures for Multilingual Semantic Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">Hendy</forename><surname>Susanto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
						</author>
						<title level="a" type="main">Neural Architectures for Multilingual Semantic Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="38" to="44"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2007</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we address semantic parsing in a multilingual context. We train one multilingual model that is capable of parsing natural language sentences from multiple different languages into their corresponding formal semantic representations. We extend an existing sequence-to-tree model to a multi-task learning framework which shares the decoder for generating semantic representations. We report evaluation results on the multilingual GeoQuery corpus and introduce a new multilingual version of the ATIS corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this work, we address multilingual seman- tic parsing -the task of mapping natural lan- guage sentences coming from multiple different languages into their corresponding formal seman- tic representations. We consider two multilin- gual scenarios: 1) the single-source setting, where the input consists of a single sentence in a single language, and 2) the multi-source setting, where the input consists of parallel sentences in multi- ple languages. Previous work handled the for- mer by means of monolingual models <ref type="bibr">(Wong and Mooney, 2006</ref>; <ref type="bibr">Lu et al., 2008;</ref><ref type="bibr">Jones et al., 2012)</ref>, while the latter has only been explored by <ref type="bibr">Jie and Lu (2014)</ref> who ensembled many monolingual models together. Unfortunately, training a model for each language separately ignores the shared information among the source languages, which may be potentially beneficial for typologically re- lated languages. Practically, it is also inconvenient to train, tune, and configure a new model for each language, which can be a laborious process.</p><p>In this work, we propose a parsing architec- ture that accepts as input sentences in several languages. We extend an existing sequence-to- tree model <ref type="bibr" target="#b4">(Dong and Lapata, 2016</ref>) to a multi- task learning framework, motivated by its success in other fields, e.g., neural machine translation (MT) ( <ref type="bibr" target="#b3">Dong et al., 2015;</ref><ref type="bibr" target="#b5">Firat et al., 2016)</ref>. Our model consists of multiple encoders, one for each language, and one decoder that is shared across source languages for generating semantic repre- sentations. In this way, the proposed model po- tentially benefits from having a generic decoder that works well across languages. Intuitively, the model encourages each source language encoder to find a common structured representation for the decoder. We further modify the attention mech- anism ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> to integrate multi- source information, such that it can learn where to focus during parsing; i.e., which input positions in which languages.</p><p>Our contributions are as follows:</p><p>• We investigate semantic parsing in two mul- tilingual scenarios that are relatively unex- plored in past research,</p><p>• We present novel extensions to the sequence- to-tree architecture that integrates multilin- gual information for semantic parsing, and</p><p>• We release a new ATIS semantic dataset an- notated in two new languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we summarize semantic pars- ing approaches from previous works. <ref type="bibr">Wong and Mooney (2006)</ref> created WASP, a semantic parser based on statistical machine translation. <ref type="bibr">Lu et al. (2008)</ref> proposed generative hybrid tree structures, which were augmented with a discriminative re- ranker. CCG-based semantic parsing systems have been developed, such as ZC07 ( <ref type="bibr">Zettlemoyer and Collins, 2007</ref>) and UBL ( <ref type="bibr">Kwiatkowski et al., 2010)</ref>. Researchers have proposed sequence-to- sequence parsing models <ref type="bibr">(Jia and Liang, 2016;</ref><ref type="bibr" target="#b4">Dong and Lapata, 2016;</ref><ref type="bibr">Kočisk`Kočisk`y et al., 2016)</ref>. Re- cently, <ref type="bibr">Susanto and Lu (2017)</ref> extended the hybrid tree with neural features. Recent progress in multilingual NLP has moved towards building a unified model that can work across different languages, such as in multilingual dependency parsing <ref type="bibr" target="#b0">(Ammar et al., 2016)</ref>, multi- lingual MT <ref type="bibr" target="#b5">(Firat et al., 2016)</ref>, and multilingual word embedding ( <ref type="bibr" target="#b8">Guo et al., 2016)</ref>. Nonetheless, multilingual approaches for semantic parsing are relatively unexplored, which motivates this work. <ref type="bibr">Jones et al. (2012)</ref> evaluated an individually- trained tree transducer on a multilingual semantic dataset. Jie and Lu (2014) ensembled monolingual hybrid tree models on the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we describe our approach to multilingual semantic parsing, which extends the sequence-to-tree model by <ref type="bibr" target="#b4">Dong and Lapata (2016)</ref>. Unlike the mainstream approach that trains one monolingual parser per source lan- guage, our approach integrates N encoders, one for each language, into a single model. This model encodes a sentence from the n-th language X = x 1 , x 2 , ..., x |X| as a vector and then uses a shared decoder to decode the encoded vector into its cor- responding logical form Y = y 1 , y 2 , ..., y |Y | . We consider two types of input: 1) a single sentence in one of N languages in the single-source setting and 2) parallel sentences in N languages in the multi-source setting. We elaborate on each setting in Section 3.1 and 3.2, respectively.</p><p>The encoder is implemented as a unidirectional RNN with long short-term memory (LSTM) units <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997)</ref>, which takes a sequence of natural language tokens as input. Similar to previous multi-task frameworks, e.g., in neural MT ( <ref type="bibr" target="#b5">Firat et al., 2016;</ref><ref type="bibr">Zoph and Knight, 2016)</ref>, we create one encoder per source language, i.e., {Ψ n enc } N n=1 . For the n-th language, it updates the hidden vector at time step t by:</p><formula xml:id="formula_0">h n t = Ψ n enc (h n t−1 , E n x [x t ])<label>(1)</label></formula><p>where Ψ n enc is the LSTM function and E n x ∈ R |V |×d is an embedding matrix containing row vectors of the source tokens in the n-th language. Each encoder may be configured differently, such as by the number of hidden units and the embed- ding dimension for the source symbol.</p><p>In the basic sequence-to-sequence model, the decoder generates each target token in a linear fashion. However, in semantic parsing, such a model ignores the hierarchical structure of logi- cal forms. In order to alleviate this issue, <ref type="bibr" target="#b4">Dong and Lapata (2016)</ref> proposed a decoder that gen- erates logical forms in a top-down manner, where they define a "non-terminal" token &lt;n&gt; to indi- cate subtrees. At each depth in the tree, logical forms are generated sequentially until the end-of- sequence token is output.</p><p>Unlike in the single language setting, here we define a single, shared decoder Ψ dec as opposed to one decoder per source language. We augment the parent non-terminal's information p when com- puting the decoder state z t , as follows:</p><formula xml:id="formula_1">z t = Ψ dec (z t−1 , E y [˜ y t−1 ], p) (2)</formula><p>where Ψ dec is the LSTM function and˜yand˜ and˜y t−1 is the previous target symbol. The attention mechanism ( <ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr">Luong et al., 2015</ref>) computes a time- dependent context vector c t (as defined later in Section 3.1 and 3.2), which is subsequently used for computing the probability distribution over the next symbol, as follows:</p><formula xml:id="formula_2">˜ z t = tanh(Uz t + Vc t ) (3) p(y t |y &lt;t , X) ∝ exp(W˜zW˜z t )<label>(4)</label></formula><p>where U, V, and W are weight matrices. Finally, the model is trained to maximize the following conditional log-likelihood:</p><formula xml:id="formula_3">L(θ) = (X,Y )∈D |Y | t=1 log p(y t |y &lt;t , X) (5)</formula><p>where (X, Y ) refers to a ground-truth sentence- semantics pair in the training data D. We use the same formulation above for the en- coders and the decoder in both multilingual set- tings. Each setting differs in terms of: 1) the de- coder state initialization, 2) the computation of the context vector c t , and 3) the training procedure, which are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single-Source Setting</head><p>In this setting, the input is a source sentence com- ing from the n-th language. <ref type="figure" target="#fig_0">Figure 1</ref> (a) depicts a scenario where the model is parsing Indonesian input, with English and Chinese being non-active.</p><p>The last state of the n-th encoder is used to ini- tialize the first state of the decoder. We may need to first project the encoder vector into a suitable dimension for the decoder, i.e., z 0 = φ n dec (h n |X| ), where φ n dec can be an affine transformation. Simi- larly, we may do so before computing the attention scores, i.e., ˜ h n k = φ n att (h n k ). Then, we compute the context vector c n t as a weighted sum of the hidden vectors in the n-th encoder:</p><formula xml:id="formula_4">α n k,t = exp( ˜ h n k · z t ) |X| k =1 exp( ˜ h n k · z t )<label>(6)</label></formula><formula xml:id="formula_5">c n t = |X| k=1 α n k,t ˜ h n k<label>(7)</label></formula><p>We set c t = c n t for computing Equation 3. We pro- pose two variants of the model under this setting. In the first version, we define separate weight ma- trices for each language, i.e., {U n , V n , W n } N n=1 . In the second version, the three weight matrices are shared across languages, essentially reducing the number of parameters by a factor of N .</p><p>The training data consists of the union of sentence-semantics pairs in N languages, where the source sentences are not necessarily parallel. We implement a scheduling mechanism that cy- cles through all languages during training, one lan- guage at a time. Specifically, model parameters are updated after one batch from one language before moving to the next one. Similar to <ref type="bibr" target="#b5">Firat et al. (2016)</ref>, this mechanism prevents excessive updates from a specific language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Source Setting</head><p>In this setting, the input are semantically equiva- lent sentences in N languages. <ref type="figure" target="#fig_0">Figure 1 (b)</ref> de- picts a scenario where the model is parsing En- glish, Indonesian, and Chinese simultaneously. It includes a combiner module (denoted by the grey box), which we will explain next.</p><p>The decoder state at the first time step is ini- tialized by first combining the N final states from each encoder, i.e.,</p><formula xml:id="formula_6">z 0 = φ init (h 1 |X| , · · · , h N |X| )</formula><p>, where we implement φ init by max-pooling.</p><p>We propose two ways of computing c t that inte- grates source-side information from multiple en- coders. First, we consider word-level combina- tion, where we combine N encoder states at every time step, as follows:</p><formula xml:id="formula_7">α n k,t = exp( ˜ h n k · z t ) N n =1 |X| k =1 exp( ˜ h n k · z t )<label>(8)</label></formula><formula xml:id="formula_8">c t = N n=1 |X| k=1 α n k,t ˜ h n k<label>(9)</label></formula><p>Alternatively, in sentence-level combination, we first compute the context vector for each lan- guage in the same way as Equation 6 and 7. Then, we perform a simple concatenation of N context vectors:</p><formula xml:id="formula_9">c t = c 1 t ; · · · ; c N t .</formula><p>Unlike the single-source setting, the train- ing data consists of N -way parallel sentence- semantics pairs. That is, each training instance consists of N semantically equivalent sentences and their corresponding logical form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Settings</head><p>We conduct our experiments on two multilingual benchmark datasets, which we describe below. Both datasets use a meaning representation based on lambda calculus.</p><p>The GeoQuery (GEO) dataset is a standard benchmark evaluation for semantic parsing. The multilingual version consists of 880 instances of natural language queries related to US geography facts in four languages (English, German, Greek, and Thai) ( <ref type="bibr">Jones et al., 2012</ref>). We use the standard split which consists of 600 training examples and 280 test examples.</p><p>The ATIS dataset contains natural language queries to a flight database. The data is split into 4,434 instances for training, 491 for development, and 448 for evaluation, same as <ref type="bibr">Zettlemoyer and Collins (2007)</ref>. The original version only includes English. In this work, we annotate the corpus in Indonesian and Chinese. The Chinese corpus was annotated (with segmentations) by hiring profes- sional translation service. The Indonesian corpus was annotated by a native Indonesian speaker.</p><p>We use the same pre-processing as <ref type="bibr" target="#b4">Dong and Lapata (2016)</ref>, where entities and numbers are re- placed with their type names and unique IDs. <ref type="bibr">1</ref> English words are stemmed using NLTK <ref type="bibr" target="#b2">(Bird et al., 2009)</ref>. Each query is paired with its cor- responding semantic representation in lambda cal- culus ( <ref type="bibr">Zettlemoyer and Collins, 2005)</ref>.</p><p>In all experiments, following Dong and Lap- ata (2016), we use a one-layer LSTM with 200- dimensional cells and embeddings. We use a mini- batch size of 20 with RMSProp updates (Tieleman and Hinton, 2012) for a fixed number of epochs, with gradient clipping at 5. Parameters are uni- formly initialized at [-0.08,0.08] and regularized using dropout ( <ref type="bibr">Srivastava et al., 2014</ref>). Input se- quences are reversed. See Appendix A for detailed experimental settings.</p><p>For each model configuration, all experiments are repeated 3 times with different random seed values, in order to make sure that our findings are reliable. We found empirically that the ran- dom seed may affect SEQ2TREE performance. This is especially important due to the relatively small dataset. As previously done in multi- task sequence-to-sequence learning ( <ref type="bibr">Luong et al., 2016)</ref>, we report the average performance for the baseline and our model. The evaluation metric is defined in terms of exact match accuracy with the ground-truth logical forms. See Appendix B for the accuracy of individual runs. <ref type="table">Table 1</ref> compares the performance of the mono- lingual sequence-to-tree model <ref type="bibr" target="#b4">(Dong and Lapata, 2016)</ref>, SINGLE, and our multilingual model, MULTI, with separate and shared output param- eters under the single-source setting as described in Section 3.1. On average, both variants of the multilingual model outperform the monolingual model by up to 1.34% average accuracy on GEO. Parameter sharing is shown to be helpful, in partic- ular for GEO. We observe that the average perfor- mance increase on ATIS mainly comes from Chi- nese and Indonesian. We also learn that although including English is often helpful for the other lan- guages, it may affect its individual performance.  multi-source parsing by combining 3 to 4 lan- guages for GEO and 2 to 3 languages for ATIS. For RANKING, we combine the predictions from each language by selecting the one with the high- est probability. Indeed, we observe that system combination at the model level is able to give bet- ter performance on average (up to 4.29% on GEO) than doing so at the output level. Combining at the word level and sentence level shows compara- ble performance on both datasets. It can be seen that the benefit is more apparent when we include English in the system combination. Regarding comparison to previous monolingual works, we want to highlight that there exist two different versions of the GeoQuery dataset anno- tated with completely different semantic represen- tations: semantic tree and lambda calculus. As noted in Section 5 of <ref type="bibr">Lu (2014)</ref>, results obtained from these two versions are not comparable. We use lambda calculus same as <ref type="bibr" target="#b4">Dong and Lapata (2016)</ref>. Under the multilingual setting, the closest work is Jie and Lu <ref type="bibr">(2014)</ref>. Nonetheless, they used the semantic tree version of GeoQuery. They eval- Output SINGLE (en) list the airlines with flights to or from ci0 lambda $0 e ( and ( airline $0 ) ( exists $1 ( and ( flight $1 ) ( or ( from $1 ci0 ) ( to $1 ci0 ) ) ( airline $1 $0 ) ) ) ) SINGLE (id) daftarkan maskapai dengan penerbangan ke atau dari ci0 lambda $0 e ( and ( airline $0 ) ( exists $1 ( and ( flight $1 ) ( from $1 ci0 ) ( airline $1 $0 ) ) ) ) SINGLE (zh) ci0 lambda $0 e ( and ( airline $0 ) ( services $0 ci0 ) )   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MULTI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>In this section, we report a qualitative analysis of our multilingual model. <ref type="table" target="#tab_2">Table 3</ref> shows exam- ple output from the monolingual model, SINGLE, trained on the three languages in ATIS and the multilingual model, MULTI, with sentence-level combination. This example demonstrates a sce- nario when the multilingual model successfully parses the three input sentences into the correct logical form, whereas the individual models are unable to do so. <ref type="figure" target="#fig_3">Figure 2</ref> shows the alignments produced by MULTI (sentence) when parsing ATIS in the multi- source setting. Each cell in the alignment matrix corresponds to α n k,t which is computed by Equa- tion 6. Semantically related words are strongly aligned, such as the alignments between ground (en), darat (id), (zh) and ground transport. This shows that such correspondences can be jointly learned by our multilingual model.</p><p>In <ref type="table" target="#tab_3">Table 4</ref>, we summarize the number of param- eters in the baseline and our multilingual model. The number of parameters in SINGLE and RANK- ING is equal to the sum of the number of parame- ters in their monolingual components. It can be seen that the size of our multilingual model is about 50-60% smaller than that of the baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented a multilingual semantic parser that extends the sequence-to-tree model to a multi- task learning framework. Through experiments, we show that our multilingual model performs bet- ter on average than 1) monolingual models in the single-source setting and 2) ensemble ranking in the multi-source setting. We hope that this work will stimulate further research in multilingual se- mantic parsing. Our code and data is available at http://statnlp.org/research/sp/. <ref type="table" target="#tab_0">MULTI  separate  shared  1  2  3  1  2  3  1  2  3  GEO</ref>    all multilingual models, we initialize the encoders using the encoder weights learned by the mono- lingual models. For the multi-source setting, we also initialize the decoder using the first language in the list of the combined languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SINGLE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experimental Results</head><p>In <ref type="table" target="#tab_5">Table 6</ref> and 7, we report the accuracy of the 3 runs for each model and dataset. In both settings, we observe that the best accuracy on both datasets is often achieved by MULTI. This is the same con- clusion that we reached when averaging the results over all runs.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the model with three language encoders and a shared logical form decoder (in λ-calculus). Two scenarios are considered: (a) single-source and (b) multi-source with a combiner module (in grey color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attention score matrices computed by MULTI when parsing English, Indonesian, and Chinese inputs from ATIS. Darker color represents higher attention score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 shows the average performance on</head><label>2</label><figDesc></figDesc><table>1 See Section 3.6 of (Dong and Lapata, 2016). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Multi-source parsing results in terms of 
average accuracy % over 3 runs. Best results are 
in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Example output from monolingual and multilingual models trained on ATIS. 

Model 
Number of parameters 
GEO 
ATIS 
SINGLE/RANKING 3.7 × 10 6 3.1 × 10 6 
MULTI (single) 
-separate 
2.3 × 10 6 2.1 × 10 6 
-shared 
2.0 × 10 6 1.9 × 10 6 
MULTI (multi) 
-word 
2.0 × 10 6 1.9 × 10 6 
-sentence 
2.1 × 10 6 1.9 × 10 6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Model size 

uated extrinsically on a database query task while 
we use exact match accuracy, so their work is not 
directly comparable to ours. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Single-source parsing results showing the accuracy of the 3 runs. Best results are in bold. 

RANKING 
MULTI 
word 
sentence 
1 
2 
3 
1 
2 
3 
1 
2 
3 
GEO 
en+de+el 
85.00 82.50 82.14 87.14 84.64 84.64 87.50 
85.36 
86.43 
en+de+th 
84.29 81.07 80.71 87.86 85.00 85.71 85.71 
86.43 
84.29 
en+el+th 
84.29 82.14 81.43 87.50 84.29 85.00 84.64 
85.71 
85.36 
de+el+th 
80.00 79.29 79.64 71.07 72.86 72.50 77.86 
74.64 
76.79 
en+de+el+th 83.93 81.79 81.79 85.71 86.07 84.64 87.50 
86.79 
86.07 
ATIS 
en+id 
83.48 82.14 82.81 83.48 83.48 84.82 85.27 
80.58 
85.49 
en+zh 
84.60 80.80 83.04 83.26 82.14 83.48 85.49 
80.13 
83.26 
id+zh 
79.24 78.57 77.68 77.46 78.35 74.55 80.58 
78.13 
74.55 
en+id+zh 
84.15 81.92 83.26 82.14 81.25 83.26 85.49 
81.03 
85.04 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Multi-source parsing results showing the accuracy of the 3 runs. Best results are in bold.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 : Hyperparameter values 44</head><label>5</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Christopher Bryant, Li Dong, and the anonymous reviewers for helpful comments and feedback. This work is supported by MOE Tier 1 grant SUTDT12015008, and is partially supported by project 61472191 under the National Natural Science Foundation of China.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head> <ref type="table">Table 5</ref> <p>lists the number of training epochs and the dropout probability used in the LSTM cell and the hidden layers before the softmax classifiers, which were chosen based on preliminary experiments on a held-out dataset. We use a training schedule where we switch to the next language after train- ing one mini-batch for GEO and 500 for ATIS. For</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Many languages, one parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="431" to="444" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Natural language processing with Python: analyzing text with the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="doi">10.3115/v1/P15-1166</idno>
		<ptr target="https://doi.org/10.3115/v1/P15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="doi">10.18653/v1/P16-1004</idno>
		<ptr target="https://doi.org/10.18653/v1/P16-1004" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="doi">10.18653/v1/N16-1101</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1101" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A representation learning framework for multi-source transfer parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
