<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Text to 3D Scene Generation with Rich Lexical Grounding</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Text to 3D Scene Generation with Rich Lexical Grounding</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="53" to="62"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The ability to map descriptions of scenes to 3D geometric representations has many applications in areas such as art, education , and robotics. However, prior work on the text to 3D scene generation task has used manually specified object categories and language that identifies them. We introduce a dataset of 3D scenes annotated with natural language descriptions and learn from this data how to ground tex-tual descriptions to physical objects. Our method successfully grounds a variety of lexical terms to concrete referents, and we show quantitatively that our method improves 3D scene generation over previous work using purely rule-based methods. We evaluate the fidelity and plau-sibility of 3D scenes generated with our grounding approach through human judgments. To ease evaluation on this task, we also introduce an automated metric that strongly correlates with human judgments.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We examine the task of text to 3D scene gener- ation. The ability to map descriptions of scenes to 3D geometric representations has a wide vari- ety of applications; many creative industries use 3D scenes. Robotics applications need to interpret commands referring to real-world environments, and the ability to visualize scenarios given high- level descriptions is of great practical use in educa- tional tools. Unfortunately, 3D scene design user interfaces are prohibitively complex for novice users. Prior work has shown the task remains chal- lenging and time intensive for non-experts, even with simplified interfaces ( ). * The first two authors are listed in alphabetical order.  <ref type="figure">Figure 1</ref>: We learn how to ground references such as "L-shaped room" to 3D models in a paired cor- pus of 3D scenes and natural language descrip- tions. Sentence fragments in bold were identified as high-weighted references to the shown objects.</p><p>Language offers a convenient way for designers to express their creative goals. Systems that can interpret natural descriptions to build a visual rep- resentation allow non-experts to visually express their thoughts with language, as was demonstrated by WordsEye, a pioneering work in text to 3D scene generation <ref type="bibr" target="#b2">(Coyne and Sproat, 2001</ref>).</p><p>WordsEye and other prior work in this area ( <ref type="bibr" target="#b19">Seversky and Yin, 2006;</ref> used manually chosen mappings between lan- guage and objects in scenes. To our knowledge, we present the first 3D scene generation approach that learns from data how to map textual terms to objects. First, we collect a dataset of 3D scenes along with textual descriptions by people, which we contribute to the community. We then train a classifier on a scene discrimination task and extract high-weight features that ground lexical terms to 3D models. We integrate our learned lexical groundings with a rule-based scene gener- ation approach, and we show through a human- judgment evaluation that the combination outper- forms both approaches in isolation. Finally, we introduce a scene similarity metric that correlates with human judgments.</p><p>There is a desk and there is a notepad on the desk. There is a pen next to the notepad.  <ref type="figure">Figure 2</ref>: Illustration of the text to 3D scene generation pipeline. The input is text describing a scene (left), which we parse into an abstract scene template representation capturing objects and relations (mid- dle). The scene template is then used to generate a concrete 3D scene visualizing the input description (right). The 3D scene is constructed by retrieving and arranging appropriate 3D models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Template Input Text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>In the text to 3D scene generation task, the input is a natural language description, and the output is a 3D representation of a plausible scene that fits the description and can be viewed and rendered from multiple perspectives. More precisely, given an utterance x as input, the output is a scene y: an arrangement of 3D models representing objects at specified positions and orientations in space.</p><p>In this paper, we focus on the subproblem of lexical grounding of textual terms to 3D model ref- erents (i.e., choosing 3D models that represent ob- jects referred to by terms in the input utterance x). We employ an intermediate scene template repre- sentation parsed from the input text to capture the physical objects present in a scene and constraints between them. This representation is then used to generate a 3D scene <ref type="figure">(Figure 2)</ref>.</p><p>A na¨ıvena¨ıve approach to scene generation might use keyword search to retrieve 3D models. How- ever, such an approach is unlikely to generalize well in that it fails to capture important object at- tributes and spatial relations. In order for the gen- erated scene to accurately reflect the input descrip- tion, a deep understanding of language describ- ing environments is necessary. Many challenging subproblems need to be tackled: physical object mention detection, estimation of object attributes such as size, extraction of spatial constraints, and placement of objects at appropriate relative posi- tions and orientations. The subproblem of lexical grounding to 3D models has a larged impact on the quality of generated scenes, as later stages of scene generation rely on having a correctly chosen set of objects to arrange.</p><p>Another challenge is that much common knowl- edge about the physical properties of objects and the structure of environments is rarely mentioned in natural language (e.g., that most tables are sup- ported on the floor and in an upright orienta- tion). Unfortunately, common 3D representations of objects and scenes used in computer graph- ics specify only geometry and appearance, and rarely include such information. Prior work in text to 3D scene generation focused on collecting manual annotations of object properties and rela- tions <ref type="bibr" target="#b17">(Rouhizadeh et al., 2011;</ref><ref type="bibr" target="#b2">Coyne et al., 2012)</ref>, which are used to drive rule-based generation sys- tems. Regrettably, the task of scene generation has not yet benefited from recent related work in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>There is much prior work in image retrieval given textual queries; a recent overview is provided by Siddiquie et al. (2011). The image retrieval task bears some similarity to our task insofar as 3D scene retrieval is an approach that can approx- imate 3D scene generation.</p><p>However, there are fundamental differences be- tween 2D images and 3D scenes. Generation in image space has predominantly focused on com- position of simple 2D clip art elements, as exem- plified recently by <ref type="bibr" target="#b23">Zitnick et al. (2013)</ref>. The task of composing 3D scenes presents a much higher- dimensional search space of scene configurations where finding plausible and desirable configura- tions is difficult. Unlike prior work in clip art gen- eration which uses a small pre-specified set of ob- jects, we ground to a large database of objects that can occur in various indoor environments: 12490 3D models from roughly 270 categories.</p><p>There is a table and there are four chairs. There are four plates and there are four sandwiches. There is a chair and a table. There is a bed and there is a nightstand next to the bed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text to Scene Systems</head><p>Pioneering work on the SHRDLU system <ref type="bibr" target="#b22">(Winograd, 1972</ref>) demonstrated linguistic manipulation of objects in 3D scenes. However, the dis- course domain was restricted to a micro-world with simple geometric shapes to simplify parsing and grounding of natural language input. More re- cently, prototype text to 3D scene generation sys- tems have been built for broader domains, most notably the WordsEye system <ref type="bibr" target="#b2">(Coyne and Sproat, 2001</ref>) and later work by <ref type="bibr" target="#b19">Seversky and Yin (2006)</ref>.  showed it is possible to learn spatial priors for objects and relations directly from 3D scene data.</p><p>These systems use manually defined mappings between language and their representation of the physical world. This prevents generalization to more complex object descriptions, variations in word choice and spelling, and other languages. It also forces users to use unnatural language to ex- press their intent (e.g., the table is two feet to the south of the window).</p><p>We propose reducing reliance on manual lex- icons by learning to map descriptions to objects from a corpus of 3D scenes and associated textual descriptions. While we find that lexical knowledge alone is not sufficient to generate high-quality scenes, a learned approach to lexical grounding can be used in combination with a rule-based sys- tem for handling compositional knowledge, result- ing in better scenes than either component alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Related Tasks</head><p>Prior work has generated sentences that describe 2D images <ref type="bibr" target="#b3">(Farhadi et al., 2010;</ref><ref type="bibr" target="#b12">Kulkarni et al., 2011;</ref><ref type="bibr" target="#b9">Karpathy et al., 2014</ref>) and referring expres- sions for specific objects in images ( <ref type="bibr" target="#b5">FitzGerald et al., 2013;</ref><ref type="bibr" target="#b10">Kazemzadeh et al., 2014</ref>). How- ever, generating scenes is currently out of reach for purely image-based approaches. 3D scene rep- resentations serve as an intermediate level of struc- ture between raw image pixels and simpler micro- cosms (e.g., grid and block worlds). This level of structure is amenable to the generation task but still realistic enough to present a variety of chal- lenges associated with natural scenes.</p><p>A related line of work focuses on grounding referring expressions to referents in 3D worlds with simple colored geometric shapes ( <ref type="bibr" target="#b7">Gorniak and Roy, 2004;</ref><ref type="bibr" target="#b8">Gorniak and Roy, 2005</ref>). More re- cent work grounds text to object attributes such as color and shape in images ( <ref type="bibr" target="#b14">Matuszek et al., 2012;</ref><ref type="bibr" target="#b11">Krishnamurthy and Kollar, 2013)</ref>. <ref type="bibr" target="#b6">Golland et al. (2010)</ref> ground spatial relationship language in 3D scenes (e.g., to the left of, behind) by learning from pairwise object relations provided by crowd- workers. In contrast, we ground general descrip- tions to a wide variety of possible objects. The objects in our scenes represent a broader space of possible referents than the first two lines of work. Unlike the latter work, our descriptions are pro- vided as unrestricted free-form text, rather than filling in specific templates of object references and fixed spatial relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>55</head><p>We introduce a new dataset of 1128 scenes and 4284 free-form natural language descriptions of these scenes. <ref type="bibr">1</ref> To create this training set, we used a simple online scene design interface that allows users to assemble scenes using available 3D models of common household objects (each model is annotated with a category label and has a unique ID). We used a set of 60 seed sentences describing simple configurations of interior scenes as prompts and asked workers on the Amazon Mechanical Turk crowdsourcing platform to cre- ate scenes corresponding to these seed descrip- tions. To obtain more varied descriptions for each scene, we asked other workers to describe each scene. <ref type="figure" target="#fig_0">Figure 3</ref> shows examples of seed descrip- tion sentences, 3D scenes created by people given those descriptions, and new descriptions provided by others viewing the created scenes.</p><p>We manually examined a random subset of the descriptions (approximately 10%) to elimi- nate spam and unacceptably poor descriptions. When we identified an unacceptable description, we also examined all other descriptions by the same worker, as most poor descriptions came from a small number of workers. From our sample, we estimate that less than 3% of descriptions were spam or unacceptably incoherent. To reflect nat- ural use, we retained minor typographical and grammatical errors.</p><p>Despite the small set of seed sentences, the Turker-provided scenes exhibit much variety in the specific objects used and their placements within the scene. Over 600 distinct 3D models appear in at least one scene, and more than 40% of non- room objects are rotated from their default orienta- tion, despite the fact that this requires an extra ma- nipulation in the scene-building interface. The de- scriptions collected for these scenes are similarly diverse and usually differ substantially in length and content from the seed sentences. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model</head><p>To create a model for generating scene templates from text, we train a classifier to learn lexical groundings. We then combine our learned lexi- cal groundings with a rule-based scene generation model. The learned groundings allow us to select better models, while the rule-based model offers simple compositionality for handling coreference and relationships between objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning lexical groundings</head><p>To learn lexical mappings from examples, we train a classifier on a related grounding task and ex- tract the weights of lexical features for use in scene generation. This classifier learns from a "discrim- ination" version of our scene dataset, in which the scene in each scene-description pair is hid- den among four other distractor scenes sampled uniformly at random. The training objective is to maximize the L 2 -regularized log likelihood of this scene discrimination dataset under a one-vs.- all logistic regression model, using each true scene and each distractor scene as one example (with true/distractor as the output label).</p><p>The learned model uses binary-valued fea- tures indicating the co-occurrence of a unigram or bigram and an object category or model ID. For example, features extracted from the scene-description pair shown in <ref type="figure">Figure 2</ref> would include the tuples (desk, modelId:132) and (the notepad, category:notepad).</p><p>To evaluate our learned model's performance at discriminating scenes, independently of its use in scene generation, we split our scene and descrip- tion corpus (augmented with distractor scenes) randomly into train, development, and test por- tions 70%-15%-15% by scene. Using only model ID features, the classifier achieves a discrimina- tion accuracy of 0.715 on the test set; adding fea- tures that use object categories as well as model IDs improves accuracy to 0.833.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Rule-based Model</head><p>We use the rule-based parsing component de- scribed in . This system in- corporates knowledge that is important for scene generation and not addressed by our learned model (e.g., spatial relationships and coreference). In Section 5.3, we describe how we use our learned model to augment this model. This rule-based approach is a three-stage pro- cess using established NLP systems: 1) The input text is split into multiple sentences and parsed us- ing the Stanford CoreNLP pipeline (Manning et <ref type="figure">Figure 4</ref>: Some examples extracted from the top 20 highest-weight features in our learned model: lexical terms from the descriptions in our scene corpus are grounded to 3D models within the scene corpus.</p><note type="other">red cup round yellow table green room black top tan love seat black bed open window</note><p>al., 2014). Head words of noun phrases are iden- tified as candidate object categories, filtered using WordNet <ref type="bibr" target="#b15">(Miller, 1995)</ref> to only include physical objects. 2) References to the same object are col- lapsed using the Stanford coreference system. 3) Properties are attached to each object by extract- ing other adjectives and nouns in the noun phrase. These properties are later used to query the 3D model database.</p><p>We use the same model database as  and also extract spatial relations between objects using the same set of dependency patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Combined Model</head><p>The rule-based parsing model is limited in its abil- ity to choose appropriate 3D models. We integrate our learned lexical groundings with this model to build an improved scene generation system.</p><p>Identifying object categories Using the rule- based model, we extract all noun phrases as po- tential objects. For each noun phrase p, we extract features {ϕ i } and compute the score of a category c being described by the noun phrase as the sum of the feature weights from the learned model in Section 5.1:</p><formula xml:id="formula_0">Score(c | p) = ∑ ϕ i ∈ϕ(p) θ (i,c) ,</formula><p>where θ (i,c) is the weight for associating feature ϕ i with category c. From categories with a score higher than T c = 0.5, we select the best-scoring category as the representative for the noun phrase. If no category's score exceeds T c , we use the head of the noun phrase for the object category.</p><p>3D model selection For each object mention detected in the description, we use the feature weights from the learned model to select a specific object to add to the scene. After using dependency rules to extract spatial relationships and descrip- tive terms associated with the object, we compute the score of a 3D model m given the category c and a set of descriptive terms d using a similar sum of feature weights. As the rule-based system may not accurately identify the correct set of terms d, we augment the score with a sum of feature weights over the entire input description x:</p><formula xml:id="formula_1">m = arg max m∈{c} λ d ∑ ϕ i ∈ϕ(d) θ (i,m) + λ x ∑ ϕ i ∈ϕ(x) θ (i,m)</formula><p>For the results shown here, λ d = 0.75 and λ x = 0.25. We select the best-scoring 3D model with positive score. If no model has positive score, we assume the object mention was spurious and omit the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Learned lexical groundings</head><p>By extracting high-weight features from our learned model, we can visualize specific models to which lexical terms are grounded (see <ref type="figure">Figure 4</ref>). These features correspond to high frequency text- 3D model pairs within the scene corpus. <ref type="table">Table 1</ref> shows some of the top learned lexical ground- ings to model database categories. We are able to recover many simple identity mappings with- out using lexical similarity features, and we cap- ture several lexical variants (e.g., sofa for Couch).</p><p>A few erroneous mappings reflect common co- occurrences; for example, fruit is mapped to Bowl due to fruit typically being observed in bowls in our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description</head><p>In between the doors and the window, there is a black couch with red cushions, two white pillows, and one black pillow. There is a desk and a computer.</p><p>Seed sentence:</p><p>MTurk sentences:</p><p>Figure 5: Qualitative comparison of generated scenes for three input descriptions (one Seed and two MTurk), using the four different methods: random, learned, rule, combo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head><p>We conduct a human judgment experiment to compare the quality of generated scenes using the approaches we presented and baseline methods.</p><p>To evaluate whether lexical grounding improves scene generation, we need a method to arrange the chosen models into 3D scenes. Since 3D scene layout is not a focus of our work, we use an ap- proach based on prior work in 3D scene synthesis and text to scene generation ( <ref type="bibr" target="#b4">Fisher et al., 2012;</ref>), simplified by using sampling rather than a hill climbing strategy.</p><p>Conditions We compare five conditions: {random, learned, rule, combo, human}. The random condition represents a baseline which synthesizes a scene with randomly-selected models, while human represents scenes created by people. The learned condition takes our learned lexical groundings, picks the four 3 most likely objects, and generates a scene based on them. The rule and combo conditions use scenes generated by the rule-based approach and the combined model, respectively.</p><p>Descriptions We consider two sets of input de- scriptions: {Seeds, MTurk}. The Seeds descrip- tions are 50 of the initial seed sentences from which workers were asked to create scenes. These seed sentences were simple (e.g., There is a desk and a chair, There is a plate on a table) and did not have modifiers describing the objects. The MTurk descriptions are much more descriptive and exhibit a wider variety in language (including mis- spellings and ungrammatical constructs). Our hy- pothesis was that the rule-based system would per- form well on the simple Seeds descriptions, but it would be insufficient for handling the complexi- ties of the more varied MTurk descriptions. For these more natural descriptions, we expected our combination model to perform better. Our experi- mental results confirm this hypothesis. <ref type="figure">Figure 5</ref> shows a qualitative comparison of 3D scenes generated from example input descriptions using each of the four methods. In the top row, the rule-based approach selects a CPU chassis for computer, while combo and learned select a more iconic monitor. In the bottom row, the rule-based approach selects two newspapers and places them on the floor, while the combined approach cor- rectly selects a coffee table with two newspapers on it. The learned model is limited to four objects and does not have a notion of object identity, so it often duplicates objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Qualitative Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Human Evaluation</head><p>We performed an experiment in which people rated the degree to which scenes match the tex- tual descriptions from which they were generated. Such ratings are a natural way to evaluate how well our approach can generate scenes from text: in practical use, a person would provide an input description and then judge the suitability of the re- sulting scenes. For the MTurk descriptions, we randomly sampled 100 descriptions from the de- velopment split of our dataset.</p><p>Procedure During the experiment, each partici- pant was shown 30 pairs of scene descriptions and generated 3D scenes drawn randomly from all five conditions. All participants provided 30 responses each for a total of 5040 scene-description ratings. Participants were asked to rate how well the gen- erated scene matched the input description on a 7- point Likert scale, with 1 indicating a poor match and 7 a very good one (see <ref type="figure" target="#fig_1">Figure 6</ref>). In a sep- arate task with the same experimental procedure, we asked other participants to rate the overall plau- sibility of each generated scene without a refer- ence description. This plausibility rating measures whether a method can generate plausible scenes irrespective of the degree to which the input de- scription is matched. We used Amazon Mechan- ical Turk to recruit 168 participants for rating the match of scenes to descriptions and 63 participants for rating scene plausibility.</p><p>Design The experiment followed a within- subjects factorial design. The dependent measure was the Likert rating. Since per-participant and per-scene variance on the rating is not accounted for by a standard ANOVA, we use a mixed effects model which can account for both fixed effects and random effects to determine the statistical signifi- method Seeds MTurk random 2.03 (1.88 -2.18) 1.68 (1.57 -1.79) learned 3.51 (3.23 -3.77) 2.61 (2.40 -2.84) rule 5.44 <ref type="bibr">(5.26 -5.61)</ref> 3.15 (2.91 -3.40) combo 5.23 <ref type="bibr">(4.96 -5.44)</ref> 3.73 <ref type="bibr">(3.48 -3.95)</ref> human 6.06 <ref type="bibr">(5.90 -6.19)</ref> 5.87 (5.74 -6.00) <ref type="table">Table 2</ref>: Average scene-description match ratings across sentence types and methods (95% C.I.). cance of our results. <ref type="bibr">4</ref> We treat the participant and the specific scene as random effects of varying in- tercept, and the method condition as the fixed ef- fect.</p><p>Results There was a significant effect of the method condition on the scene-description match rating: χ 2 (4, N = 5040) = 1378.2, p &lt; 0.001. <ref type="table">Table 2</ref> summarizes the average scene-description match ratings and 95% confidence intervals for all sentence type-condition pairs. All pairwise differences between ratings were significant un- der Wilcoxon rank-sum tests with the Bonferroni- Holm correction (p &lt; 0.05). The scene plausibility ratings, which were obtained independent of de- scriptions, indicated that the only significant dif- ference in plausibility was between scenes cre- ated by people (human) and all the other condi- tions. We see that for the simple seed sentences both the rule-based and combined model approach the quality of human-created scenes. However, all methods have significantly lower ratings for the more complex MTurk sentences. In this more challenging scenario, the combined model is clos- est to the manually created scenes and signifi- cantly outperforms both rule-based and learned models in isolation. <ref type="figure" target="#fig_2">Figure 7</ref> shows some common error cases in our system. The top left scene was generated with the rule-based method, the top right with the learned method, and the bottom two with the combined approach. At the top left, there is an erroneous selection of concrete object category (wood logs) for the four wood chairs reference in the input description, due to an incorrect head identifica- tion. At top right, the learned model identifies the  <ref type="table">There in the middle is a  table, on the table is a cup.</ref> presence of brown desk and lamp but erroneously picks two desks and two lamps (since we always pick the top four objects). The scene on the bot- tom right does not obey the expressed spatial con- straints (in the corner of the room) since our sys- tem does not understand the grounding of room corner and that the top right side is not a good fit due to the door. In the bottom left, incorrect coref- erence resolution results in two tables for There in the middle is a table, on the table is a cup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Error Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Scene Similarity Metric</head><p>We introduce an automated metric for scoring scenes given a scene template representation, the aligned scene template similarity (ASTS). Given a one-to-one alignment A between the nodes of a scene template and the objects in a scene, let the alignment penalty J(A) be the sum of the number of unaligned nodes in the scene template and the number of unaligned objects in the scene. For the aligned nodes, we compute a similarity score S per node pair (n, n ′ ) where S(n, n ′ ) = 1 if the model ID matches, S(n, n ′ ) = 0.5 if only the category matches and 0 otherwise.</p><p>We define the ASTS of a scene with respect to a scene template to be the maximum alignment  score over all such alignments:</p><formula xml:id="formula_2">ASTS(s, z) = max A ∑ (n,n ′ )∈A S(n, n ′ ) J(A) + |A| .</formula><p>With this definition, we compare average ASTS scores for each method against average human rat- ings <ref type="table" target="#tab_6">(Table 3)</ref>. We test the correlation of the ASTS metric against human ratings using Pearson's r and Kendall's rank correlation coefficient r τ . We find that ASTS and human ratings are strongly cor- related (r = 0.70, r τ = 0.49, p &lt; 0.001). This suggests ASTS scores could be used to train and algorithmically evaluate scene generation systems that map descriptions to scene templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future Work</head><p>Many error cases in our generated scenes resulted from not interpreting spatial relations. An obvi- ous improvement would be to expand our learned lexical grounding approach to include spatial rela- tions. This would help with spatial language that is not handled by the rule-based system's depen- dency patterns (e.g., around, between, on the east side). One approach would be to add spatial con- straints to the definition of our scene similarity score and use this improved metric in training a semantic parser to generate scene templates.</p><p>To choose objects, our current system uses information obtained from language-object co- occurrences and sparse manually-annotated cate- gory labels; another promising avenue for achiev- ing better lexical grounding is to propagate cate- gory labels using geometric and image features to learn the categories of unlabeled objects. Novel categories can also be extracted from Turker de- scriptions. These new labels could be used to im- prove the annotations in our 3D model database, enabling a wider range of object types to be used in scene generation.</p><p>Our approach learns object references without using lexical similarity features or a manually- assembled lexicon. Thus, we expect that our method for lexical grounding can facilitate de- velopment of text-to-scene systems in other lan- guages. However, additional data collection and experiments are necessary to confirm this and identify challenges specific to other languages.</p><p>The necessity of handling omitted information suggests that a model incorporating a more so- phisticated theory of pragmatic inference could be beneficial. Another important problem not ad- dressed here is the role of context and discourse in interpreting scene descriptions. For example, several of our collected descriptions include lan- guage imagining embodied presence in the scene (e.g., The wooden table is to your right, if you're entering the room from the doors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Prior work in 3D scene generation relies on purely rule-based methods to map object references to concrete 3D objects. We introduce a dataset of 3D scenes annotated with natural language descrip- tions which we believe will be of great interest to the research community. Using this corpus of scenes and descriptions, we present an approach that learns from data how to ground textual de- scriptions to objects.</p><p>To evaluate how our grounding approach im- pacts generated scene quality, we collect human judgments of generated scenes. In addition, we present a metric for automatically comparing gen- erated scene templates to scenes, and we show that it correlates strongly with human judgments.</p><p>We demonstrate that rich lexical grounding can be learned directly from an unaligned corpus of 3D scenes and natural language descriptions, and that our model can successfully ground lexical terms to concrete referents, improving scene gen- eration over baselines adapted from prior work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scenes created by participants from seed description sentences (top). Additional descriptions provided by other participants from the created scene (bottom). Our dataset contains around 19 scenes per seed sentence, for a total of 1129 scenes. Scenes exhibit variation in the specific objects chosen and their placement. Each scene is described by 3 or 4 other people, for a total of 4358 descriptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Screenshot of the UI for rating scenedescription match.</figDesc><graphic url="image-28.png" coords="7,72.00,62.61,218.41,181.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Common scene generation errors. From top left clockwise: Wood table and four wood chairs in the center of the room; There is a black and brown desk with a table lamp and flowers; There is a white desk, a black chair, and a lamp in the corner of the room; There in the middle is a table, on the table is a cup.</figDesc><graphic url="image-30.png" coords="8,52.34,32.08,372.88,284.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>on(o0,o1) 3D Scene o0 room on(o1,o2) Parsing o0 -category:room, modelId:420 o1 -category:desk, modelId:132 o2 -category:notepad, modelId:343 o3 -category:pen, modelId:144 on(o1,o3) next_to(o3,o2) o1 desk o3 pen o2 notepad Generation</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Average human ratings (out of 7) and 
aligned scene template similarity scores. 

</table></figure>

			<note place="foot" n="1"> Available at http://nlp.stanford.edu/data/ text2scene.shtml. 2 Mean 26.2 words, SD 17.4; versus mean 16.6, SD 7.2 for the seed sentences. If one considers seed sentences to be the &quot;reference,&quot; the macro-averaged BLEU score (Papineni et al., 2002) of the Turker descriptions is 12.0.</note>

			<note place="foot" n="3"> The average number of objects in a scene in our humanbuilt dataset was 3.9.</note>

			<note place="foot" n="4"> We used the lme4 R package and optimized fit with maximum log-likelihood (Baayen et al., 2008). We report significance results using the likelihood-ratio (LR) test.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Katherine Breeden for valuable feed-back. The authors gratefully acknowledge the sup-port of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filter-ing of Text (DEFT) Program under Air Force Re-search Laboratory (AFRL) contract no. FA8750-13-2-0040, the National Science Foundation un-der grant no. IIS 1159679, the Department of the Navy, Office of Naval Research, under grant no. N00014-10-1-0109, and the Stanford Grad-uate Fellowship fund. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Sci-ence Foundation, the Office of Naval Research, DARPA, AFRL, or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixed-effects modeling with crossed random effects for subjects and items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="390" to="412" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning spatial knowledge for text to 3D scene generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Annotation tools and knowledge representation for a text-to-scene system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques<address><addrLine>Bob Coyne, Alexander Klapheke, Masoud Rouhizadeh, Richard Sproat, and Daniel Bauer</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012: Technical Papers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Amin</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Example-based synthesis of 3D object arrangements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">135</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning distributions over logical forms for referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to generating spatial descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Grounded semantic composition for visual scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gorniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="429" to="470" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic grounding of situated speech using plan recognition and reference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gorniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Multimodal Interfaces</title>
		<meeting>the 7th International Conference on Multimodal Interfaces</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jointly learning to parse and perceive: Connecting natural language to the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visruth</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagnik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WordNet: A lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collecting semantic data by Mechanical Turk for the lexical knowledge resource of a text-to-picture generating system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Rouhizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margit</forename><surname>Bowler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coyne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Computational Semantics</title>
		<meeting>the Ninth International Conference on Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On being the right scale: Sizing large collections of 3D models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilbert</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia 2014 Workshop on Indoor Scene Understanding: Where Graphics meets Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time automatic 3D scene generation from natural language voice and text descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Seversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual ACM International Conference on Multimedia</title>
		<meeting>the 14th Annual ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behjat</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogério</forename><surname>Schmidt Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image ranking and retrieval based on multi-attribute queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="191" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning the visual interpretation of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
