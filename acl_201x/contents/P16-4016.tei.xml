<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">META: A Unified Toolkit for Text Retrieval and Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Massung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">College of Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chase</forename><surname>Geigle</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">College of Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">College of Engineering</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">META: A Unified Toolkit for Text Retrieval and Analysis</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics-System Demonstrations</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics-System Demonstrations <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="91" to="96"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>META is developed to unite machine learning, information retrieval, and natural language processing in one easy-to-use toolkit. Its focus on indexing allows it to perform well on large datasets, supporting online classification and other out-of-core algorithms. META&apos;s liberal open source license encourages contributions, and its extensive online documentation, forum, and tutorials make this process straightforward. We run experiments and show META&apos;s performance is competitive with or better than existing software.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">A Unified Framework</head><p>As NLP techniques become more and more ma- ture, we have great opportunities to use them to develop and support many applications, such as search engines, classifiers, and integrative applica- tions that involve multiple components. It's possi- ble to develop each application from scratch, but it's much more efficient to have a general toolkit that supports multiple application types.</p><p>Existing tools tend to specialize on one partic- ular area, and as such there is a wide variety of tools one must sample when performing different data science tasks. For text-mining tasks, this is even more apparent; it is extremely difficult (if not impossible) to find tools that support both tradi- tional information retrieval tasks (like tokeniza- tion, indexing, and search) alongside traditional machine learning tasks (like document classifica- tion, regression, and topic modeling). <ref type="table">Table 1</ref> compares META's many features across various dimensions. Note that only META satisfies all the areas while other toolkits focus on a particular area. In the case where the desired functionality is scattered, data science students, re- searchers, and practitioners must find the appro- priate software packages for their needs and com- pile and configure each appropriate tool. Then, there is the problem of data formatting-it is un- likely that the tools all have standardized upon a single input format, so a certain amount of "data munging" is required. All of this detracts from the actual task at hand, which has a marked impact on productivity.</p><p>The goal of the META project is to address these issues. In particular, we provide a uni- fying framework for existing machine learning and natural language processing algorithms, al- lowing researchers to quickly run controlled ex- periments. We have modularized the feature gen- eration, instance representation, data storage for- mats, and algorithm implementations; this allows users to make seamless transitions along any of these dimensions with minimal effort. Finally, META is dual-licensed under the University of Illinois/NCSA Open Source Licence and the MIT License to reach the broadest audience possible.</p><p>Due to space constraints, in this paper, we only delve into META's natural language processing (NLP), information retrieval (IR), and machine learning (ML) components in section 3. However, we briefly outline all of its components here:</p><p>Feature generation. META has a collection of tokenizers, filters, and analyzers that convert raw text into a feature representation. Basic features are n-gram words, but other analyzers make use of different parts of the toolkit, such as POS tag n- grams and parse tree features. An arbitrary num- ber of feature representations may be combined; for example, a document could be represented as unigram words, bigram POS tags, and parse tree rewrite rules. Users can easily add their own fea- ture types as well, such as sentence length distri- bution in a document.</p><p>Search. The META search engine can store document feature vectors in an inverted index and score them with respect to a query. Rankers include vector space models such as Okapi BM25 <ref type="bibr" target="#b18">(Robertson et al., 1994</ref>) and probabilistic models like Dirichlet prior smoothing ( <ref type="bibr" target="#b27">Zhai and Lafferty, 2004)</ref>. A search demo is online <ref type="bibr">1</ref> .</p><p>Classification. META includes a normalized adaptive stochastic gradient descent (SGD) im- plementation ( <ref type="bibr" target="#b19">Ross et al., 2013</ref>) with pluggable loss functions, allowing creation of an SVM clas- sifier (among others). Both 1 (Tsuruoka et al., 2009) and 2 regularization are supported. Ensem- ble methods for binary classifiers allow multiclass classification. Other classifiers like na¨ıvena¨ıve Bayes and k-nearest neighbors also exist. A confusion matrix class and significance testing framework al- low evaluation and comparison of different meth- ods and feature representations.</p><p>Regression. Regression via SGD predicts real-valued responses from featurized documents. Evaluation metrics such as mean squared error and R 2 score allow model comparison.</p><p>POS tagging. META contains a linear-chain conditional random field for POS tagging and chunking applications, learned using 2 regular- ized SGD <ref type="bibr" target="#b24">(Sutton and McCallum, 2012)</ref>. It also contains an efficient greedy averaged perceptron tagger <ref type="bibr" target="#b3">(Collins, 2002)</ref>.</p><p>Parsing. A fast shift-reduce constituency parser using generalized averaged perceptron ( <ref type="bibr" target="#b28">Zhu et al., 2013</ref>) is META's grammatical parser. Parse tree featurizers implement different types of structural tree representations ( <ref type="bibr" target="#b12">Massung et al., 2013</ref>). An NLP demo online presents tokenization, POS- tagging, and parsing 2 .</p><p>Topic models. n-gram language models (LMs). META takes an ARPA-formatted input <ref type="bibr">3</ref> and creates a language model that can be queried for token sequence probabilities or used in downstream applications like SyntacticDiff ( <ref type="bibr" target="#b11">Massung and Zhai, 2015)</ref>.</p><p>Word embeddings. The GloVe algorithm <ref type="bibr" target="#b17">(Pennington et al., 2014</ref>) is implemented in a streaming framework and also features an interactive seman- tic relationship demo. Word vectors can be used in other applications as part of the META API.</p><p>Graph algorithms. Directed and undirected graph implementations exist and various algo- rithms such as betweenness centrality, PageRank, and myopic search are available. Random graph generation models like Watts-Strogatz and prefer- ential attachment exist. For these algorithms see <ref type="bibr" target="#b4">Easley and Kleinberg (2010)</ref>.</p><p>Multithreading. When possible, META algo- rithms and applications are parallelized using C++ threads to make full use of available resources.</p><p>vide an efficient format for storing processed data. The third layer-the application layer-interfaces solely with indexes. This means that we may use the same index for running an SVM as we do to evaluate a ranking function, without processing the data again.</p><p>Since all applications use these indexes, META supports out-of-core classification with some clas- sifiers. We ran our large classification dataset that doesn't fit in memory-Webspam ( <ref type="bibr" target="#b26">Webb et al., 2006</ref>)-using the sgd classifier. Where LIBLIN- EAR failed to run, META was able to finish the classification in a few minutes.</p><p>Besides using META's rich built-in feature gen- eration, it is possible to directly use LIBSVM- formatted data. This allows preprocessed datasets to be run under META's algorithms. Additionally, META's forward index (used for classifica- tion), is easily convertible to LIBSVM format. The reverse is also true: you may do feature genera- tion with META, and use it to generate input for any other program that supports LIBSVM format.</p><p>META is hosted publicly on GitHub <ref type="bibr">4</ref> , which provides the project with community involvement through its bug/issue tracker and fork/pull request model. Its API is heavily documented 5 , allowing the creation of Web-based applications (listed in section 1). The project website contains several tu- torials that cover the major aspects of the toolkit <ref type="bibr">6</ref> to enable users to get started as fast as possible with little friction. Additionally, a public forum 7 is accessible for all users to view and participate in user support topics, community-written documen- tation, and developer discussions.</p><p>A major design point in META is to allow for most of the functionality to be configured via a configuration file. This enables minimal effort ex- ploratory data analysis without having to write (or recompile) any code. Designing the code in this way also encourages the components of the system to be pluggable: the entire indexing process, for example, consists of several modular layers which can be controlled by the configuration file.</p><p>An example snippet of a config file is given below; this creates a bigram part-of-speech ana- lyzer. Multiple <ref type="bibr">[[analyzers]</ref>] sections may be added, which META automatically combines while processing input. <ref type="bibr">[[analyzers]</ref>] method = "ngram-pos" ngram = 2 filter = [{type = "icu-tokenizer"}, {type = "ptb-normalizer"}] crf-prefix = "crf/model/folder" A simple class hierarchy allows users to add fil- ters, analyzers, ranking functions, and classifiers with full integration to the toolkit (e.g. one may specify user-defined classes in the config file). The process for adding these is detailed in the META online tutorials.</p><p>This low barrier of entry experiment setup ease led to META's use in text mining and analysis MOOCs reaching over 40,000 students <ref type="bibr">8,</ref><ref type="bibr">9</ref> .</p><p>Multi-language support is hard to do correctly. Many toolkits sidestep this issue by only support- ing ASCII text or the OS language; META sup- ports multiple (non-romance) languages by de- fault, using the industry standard ICU library <ref type="bibr">10</ref> . This allows META to tokenize arbitrarily-encoded text in many languages.</p><p>Unit tests ensure that contributors are confident that their modifications do not break the toolkit. Unit tests are automatically run after each commit and pull request, so developers immediately know if there is an issue (of course, unit tests may be run manually before committing). The unit tests are run in a continuous integration setup where META is compiled and run on Linux, Mac OS X 11 , and Windows 12 under a variety of compilers and soft- ware development configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate META's performance in NLP, IR, and ML tasks. All experiments were performed on a workstation with an Intel(R) Core(TM) i7-5820K CPU, 16 GB of RAM, and a 4 TB 5900 RPM disk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Natural Language Processing</head><p>META's part-of-speech taggers for English pro- vide quite reasonable performance. It provides a linear-chain CRF tagger (CRF) as well as an av- eraged perceptron based greedy tagger (AP). We report the token level accuracy on sections 22-24 of the Penn Treebank, with a few prior model re- sults trained on sections 0-18 in <ref type="table" target="#tab_3">Table 3</ref>. "Hu- man annotators" is an estimate based on a 3% er- ror rate reported in the Penn Treebank README    META and CoreNLP both provide implementa- tions of shift-reduce constituency parsers, follow- ing the framework of <ref type="bibr" target="#b28">Zhu et al. (2013)</ref>. These can be trained greedily or via beam search. We com- pared the parser implementations in META and CoreNLP along two dimensions-speed, mea- sured in wall time, and memory consumption, measured as maximum resident set size-for both training and testing a greedy and beam search parser (with a beam size of 4). Training was per- formed on the standard training split of sections 2- 21 of the Penn Treebank, with section 22 used as a development set (only used by CoreNLP). Sec- tion 23 was held out for evaluation. The results are summarized in <ref type="table" target="#tab_2">Table 2.</ref> META consistently uses less RAM than CoreNLP, both at training time and testing time. Its training time is slower than CoreNLP for the greedy parser, but less than half of CoreNLP's training time for the beam parser. META's beam parser has worse labeled F 1 score, likely the result</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indri</head><p>Lucene MeTA Blog06 55m 40s 20m 23s 11m 23s Gov2 8h 13m 43s 1h 59m 42s 1h 12m 10s  of its simpler model averaging strategy <ref type="bibr">13</ref> . Overall, however, META's shift-reduce parser is competi- tive and particularly lightweight. We use the TREC blog06 ( <ref type="bibr" target="#b15">Ounis et al., 2006</ref>) permalink documents and TREC gov2 cor- pus ( <ref type="bibr" target="#b1">Clarke et al., 2004</ref>). To ensure a more uni- form indexing environment, all HTML is cleaned before indexing. In addition, each corpus is con- verted into a single file with one document per line to reduce the effects of many file operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Information Retrieval</head><p>During indexing, terms are lower-cased, stop words are removed from a common list of 431 stop words, Porter2 (META) or Porter (Indri, Lucene) stemming is performed, a maximum word length of 32 characters is set, original documents are not stored in the index, and term position information is not stored <ref type="bibr">16</ref> .</p><p>We compare the following: indexing speed (Ta- ble 5), index size <ref type="table" target="#tab_6">(Table 6</ref>), query speed <ref type="table" target="#tab_7">(Table 7)</ref>, and query accuracy <ref type="table" target="#tab_8">(Table 8</ref>) with BM25 using k 1 = 0.9 and b = 0.4. We use the standard TREC queries associated with each dataset and <ref type="bibr">13</ref> At training time, both CoreNLP and META perform model averaging, but META computes the average over all updates and CoreNLP performs cross- validation over a default of the best 8 models on the development set.</p><p>14 http://lucene.apache.org/ 15 Indri 5.10 does not provide source code packages and thus could not be used. It is also known as LEMUR. <ref type="bibr">16</ref> For Indri, we are unable to disable positions information storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Indri Lucene</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MeTA Blog06</head><p>55.0s 1.60s 3.67s Gov2 24m 6.73s 57.53s 1m 3.98s   <ref type="bibr">18</ref> . META creates the smallest index for gov2 while LUCENE creates the smallest index for blog06; INDRI greatly lags behind both. META follows LUCENE closely in retrieval speed, with INDRI again lagging. As expected, query performance between the three systems is relatively even, and we attribute any small difference in MAP or preci- sion to idiosyncrasies during tokenization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Machine Learning</head><p>META's ML performance is compared with LI- BLINEAR <ref type="bibr" target="#b5">(Fan et al., 2008)</ref>, SCIKIT-LEARN (Pe- dregosa et al., 2011), and SVMMULTICLASS 19 . We focus on linear classification with SVM across these tools (MALLET <ref type="bibr" target="#b13">(McCallum, 2002</ref>) does not provide an SVM, so it is excluded from the com- parisons). Statistics for the four ML datasets can be found in <ref type="table" target="#tab_10">Table 9</ref>.</p><p>The 20news dataset <ref type="bibr" target="#b8">(Lang, 1995)</ref>  <ref type="bibr">20</ref> is split into its standard 60% training and 40% testing sets by post date. The Blog dataset ( <ref type="bibr" target="#b20">Schler et al., 2006</ref>) is split into 80% training and 20% testing randomly. Both of these two textual datasets were prepro- cessed using META using the same settings from the IR experiments.</p><p>The rcv1 dataset ( <ref type="bibr" target="#b9">Lewis et al., 2004</ref>) was pro- cessed into a training and testing set using the prep rcv1 tool provided with Leon Bottou's SGD tool <ref type="bibr">21</ref>    The corpus was processed using the provided convert.py into byte trigrams. The first 80% of the resulting file is used for training and the last 20% for testing.</p><p>In <ref type="table" target="#tab_11">Table 10</ref>, we can see that META performs well both in terms of speed and accuracy. Both LIBLINEAR and SVMMULTICLASS were unable to produce models on the Webspam dataset due to memory limitations and lack of a minibatch frame- work. For SCIKIT-LEARN and META, we broke the training data into 4 equal sized batches and ran one iteration of SGD per batch. The timing result includes the time to load each chunk into memory; for META this is from its forward-index format <ref type="bibr">23</ref> and for SCIKIT-LEARN this is from LIB- SVM-formatted text files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>META is a valuable resource for text mining ap- plications; it is a viable and competitive alternative to existing toolkits that unifies algorithms from NLP, IR, and ML. META is an extensible, con- sistent framework that enables quick development of complex application systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>META can learn topic 1 https://meta-toolkit.org/search-demo.html 2 https://meta-toolkit.org/nlp-demo.html models over any feature representation using collapsed variational Bayes (Asuncion et al., 2009), collapsed Gibbs sampling (Griffiths and Steyvers, 2004), stochastic collapsed variational Bayes (Foulds et al., 2013), or approximate dis- tributed LDA (Newman et al., 2009).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>META's IR performance is compared with two well-known search engine toolkits: LUCENE's latest version 5.5.0 14 and INDRI's version 5.9 (Strohman et al., 2005) 15 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : (NLP) Training/testing performance for the shift-reduce constituency parsers. All models were trained for 40 iterations on the standard training split of the Penn Treebank. Accuracy is reported as labeled F1 from evalb on section 23.</head><label>2</label><figDesc></figDesc><table>Extra Data Accuracy 
Human annotators 
97.0% 
CoreNLP 

97.3% 
LTag-Spinal 
97.3% 
SCCN 

97.5% 
META (CRF) 
97.0% 
META (AP) 
96.9% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>(NLP) Part-of-speech tagging token-level accura-
cies. "Extra data" implies the use of large amounts of extra 
unlabeled data (e.g. for distributional similarity features). 

Docs 
Size |D|avg 
|V | 
Blog06 
3,215,171 
26 GB 
782.3 10,971,746 
Gov2 
25,205,179 147 GB 
515.5 21,203,125 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : (IR) The two TREC datasets used.</head><label>4</label><figDesc></figDesc><table>Uncleaned ver-
sions of blog06 and gov2 were 89 GB and 426 GB respec-
tively. 

and is likely overly optimistic (Manning, 2011). 
CoreNLP's model is the result of Manning (2011), 
LTag-Spinal is from Shen et al. (2007), and SCCN 
is from Søgaard (2011). Both of META's taggers 
are within 0.6% of the existing literature. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : (IR) Indexing speed.</head><label>5</label><figDesc></figDesc><table>Indri 
Lucene 
MeTA 
Blog06 
31.02 GB 
2.06 GB 
2.84 GB 
Gov2 
170.50 GB 11.02 GB 10.24 GB 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 : (IR) Index size.</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 7 : (IR) Query speed.</head><label>7</label><figDesc></figDesc><table>Indri 
Lucene 
MeTA 
MAP P@10 MAP P@10 MAP P@10 
Blog06 29.13 63.20 29.10 63.60 32.34 64.70 
Gov2 
25.96 53.69 30.23 59.26 29.97 57.43 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>(IR) Query performance via Mean Average Precision 
and Precision at 10 documents. 

score each system's search results with the usual 
trec eval program 17 . 
META leads in indexing speed, though we 
note that META's default indexer is multithreaded 
and LUCENE does not provide a parallel one </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head></head><label></label><figDesc>. The resulting training set has 781,265 documents and the testing set has 23,149. The</figDesc><table>Docs 
Size 
k 
Features 
20news 
18,846 
86 MB 20 
112,377 
Blog 
19,320 778 MB 
3 
548,812 
rcv1 
804,414 
1.1 GB 
2 
47,152 
Webspam 350,000 
24 GB 
2 16,609,143 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 9 : (ML) Datasets used for k-class categorization.</head><label>9</label><figDesc></figDesc><table>liblinear 
scikit SVM mult META 

20news 
79.4% 
74.3% 
67.1% 
80.1% 
2.58s 
0.326s 
2.54s 
0.648s 

Blog 
75.8% 
76.2% 
72.2% 
72.2% 
61.3s 
0.801s 
17.5s 
1.11s 

rcv1 
94.7% 
94.0% 
83.6% 
94.8% 
17.6s 
1.66s 
2.01s 
3.44s 

Webspam 

97.4% 

99.4% 
11m 52s 
1m 16s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>(ML) Accuracy and speed classification results. 
Reported time is to both train and test the model. For all 
except Webspam, this excludes IO. 

Webspam corpus (Webb et al., 2006) consists 
of the subset of the Webb Spam Corpus used 
in the Pascal Large Scale Learning Challenge 22 . 
</table></figure>

			<note place="foot" n="2"> Usability Consistency across components is a key feature that allows META to work well with large datasets. This is accomplished via a three-layer architecture. On the first layer, we have tokenizers, analyzers, and all the text processing that accompanies them. Once a document representation is determined, this tool chain is run on a corpus. The indexes are the second layer; they pro3 http://www.speech.sri.com/projects/srilm/ manpages/ngram-format.5.html</note>

			<note place="foot" n="4"> https://github.com/meta-toolkit/meta/ 5 https://meta-toolkit.org/doxygen/namespaces.html 6 https://meta-toolkit.org/ 7 https://forum.meta-toolkit.org/</note>

			<note place="foot" n="8"> https://www.coursera.org/course/textretrieval 9 https://www.coursera.org/course/textanalytics 10 http://site.icu-project.org/ 11 https://travis-ci.org/meta-toolkit/meta 12 https://ci.appveyor.com/project/skystrife/meta</note>

			<note place="foot" n="17"> http://trec.nist.gov/trec_eval/ 18 Additionally, we did not feel that writing a correct and threadsafe indexer as a user is something to be reasonably expected. 19 http://www.cs.cornell.edu/people/tj/svm_light/ svm_multiclass.html 20 http://qwone.com/ ˜ jason/20Newsgroups/ 21 http://leon.bottou.org/projects/sgd</note>

			<note place="foot" n="22"> ftp://largescale.ml.tu-berlin.de/largescale/ 23 It took 12m 24s to generate the index.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This material is based upon work supported by the NSF GRFP under Grant Number DGE-1144245.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On Smoothing and Inference for Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nick Craswell, and Ian Soboroff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Overview of the TREC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Terabyte Track</title>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Networks, Crowds, and Markets: Reasoning About a Highly Connected World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Easley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">LIBLINEAR: A Library for Large Linear Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1871" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Boyles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Finding Scientific Topics. PNAS</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">RCV1: A New Benchmark Collection for Text Categorization Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging from 97% to 100%: Is it time for some linguistics?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CICLing</title>
		<meeting>CICLing</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SyntacticDiff: Operator-based Transformation for Comparative Text Mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Massung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structural Parse Tree Features for Text Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Massung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICSC</title>
		<meeting>IEEE ICSC</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="http://mallet.cs.umass.edu/" />
		<title level="m">MALLET: A Machine Learning for Language Toolkit</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed Algorithms for Topic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2006 Blog Track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning in Python. JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Scikitlearn</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Okapi at TREC-3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Normalized online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Mineiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effects of Age and Gender on Blogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Guided learning for bidirectional sequence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised condensed nearest neighbor for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Indri: A languagemodel based search engine for complex queries (extended version)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IR</title>
		<imprint>
			<biblScope unit="volume">407</biblScope>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An Introduction to Conditional Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends in Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Jun&amp;apos;ichi Tsujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACLIJCNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introducing the webb spam corpus: Using email spam to identify web spam automatically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlton</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEAS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Study of Smoothing Methods for Language Models Applied to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and Accurate Shift-Reduce Constituent Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
