<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
						</author>
						<title level="a" type="main">Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2078" to="2088"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1190</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to un-derperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture , the GATED RECURRENT AVERAGING NETWORK, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models , finding evidence of preferences for particular parts of speech and dependency relations. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling sentential compositionality is a funda- mental aspect of natural language semantics. Re- searchers have proposed a broad range of com- positional functional architectures <ref type="bibr" target="#b29">(Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b33">Socher et al., 2011;</ref><ref type="bibr" target="#b18">Kalchbrenner et al., 2014</ref>) and evaluated them on a large vari- ety of applications. Our goal is to learn a general- purpose sentence embedding function that can be used unmodified for measuring semantic textual similarity (STS) ( <ref type="bibr" target="#b4">Agirre et al., 2012</ref>) and can also serve as a useful initialization for downstream tasks. We wish to learn this embedding function such that sentences with high semantic similar- ity have high cosine similarity in the embedding space. In particular, we focus on the setting of <ref type="bibr" target="#b40">Wieting et al. (2016b)</ref>, in which models are trained on noisy paraphrase pairs and evaluated on both STS and supervised semantic tasks.</p><p>Surprisingly, Wieting et al. found that sim- ple embedding functions-those based on aver- aging word vectors-outperform more powerful architectures based on long short-term memory (LSTM) <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997)</ref>. In this paper, we revisit their experimental setting and present several techniques that together im- prove the performance of the LSTM to be superior to word averaging.</p><p>We first change data sources: rather than train on noisy phrase pairs from the Paraphrase Database (PPDB; <ref type="bibr" target="#b11">Ganitkevitch et al., 2013)</ref>, we use noisy sentence pairs obtained automatically by aligning Simple English to standard English Wikipedia ( <ref type="bibr" target="#b8">Coster and Kauchak, 2011</ref>). Even though this data was intended for use by text sim- plification systems, we find it to be efficient and ef- fective for learning sentence embeddings, outper- forming much larger sets of examples from PPDB.</p><p>We then show how we can modify and regular- ize the LSTM to further improve its performance. The main modification is to simply average the hidden states instead of using the final one. For regularization, we experiment with two kinds of dropout and also with randomly scrambling the words in each input sequence. We find that these techniques help in the transfer learning setting and on two supervised semantic similarity datasets as well. Further gains are obtained on the super- vised tasks by initializing with our models from the transfer setting.</p><p>Inspired by the strong performance of both av- eraging and LSTMs, we introduce a novel recur- rent neural network architecture which we call the GATED RECURRENT AVERAGING NETWORK (GRAN). The GRAN outperforms averaging and the LSTM in both the transfer and supervised learning settings, forming a promising new recur- rent architecture for semantic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Modeling sentential compositionality has received a great deal of attention in recent years. A com- prehensive survey is beyond the scope of this pa- per, but we mention popular functional families: neural bag-of-words models ( <ref type="bibr" target="#b18">Kalchbrenner et al., 2014</ref>), deep averaging networks (DANs) <ref type="bibr" target="#b17">(Iyyer et al., 2015)</ref>, recursive neural networks using syn- tactic parses <ref type="bibr" target="#b33">(Socher et al., 2011</ref><ref type="bibr" target="#b34">(Socher et al., , 2012</ref><ref type="bibr" target="#b35">(Socher et al., , 2013</ref><ref type="bibr" target="#b16">˙ Irsoy and Cardie, 2014</ref>), convolutional neural net- works ( <ref type="bibr" target="#b18">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b20">Kim, 2014;</ref><ref type="bibr" target="#b15">Hu et al., 2014)</ref>, and recurrent neural networks using long short-term memory <ref type="bibr" target="#b37">(Tai et al., 2015;</ref><ref type="bibr" target="#b24">Ling et al., 2015;</ref><ref type="bibr" target="#b25">Liu et al., 2015)</ref>. Simple operations based on vector addition and multiplication typi- cally serve as strong baselines <ref type="bibr">Lapata, 2008, 2010;</ref><ref type="bibr" target="#b7">Blacoe and Lapata, 2012)</ref>.</p><p>Most work cited above uses a supervised learn- ing framework, so the composition function is learned discriminatively for a particular task. In this paper, we are primarily interested in creating general purpose, domain independent embeddings for word sequences. Several others have pursued this goal <ref type="bibr" target="#b33">(Socher et al., 2011;</ref><ref type="bibr" target="#b23">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b32">Pham et al., 2015;</ref><ref type="bibr" target="#b22">Kiros et al., 2015;</ref><ref type="bibr" target="#b13">Hill et al., 2016;</ref><ref type="bibr" target="#b5">Arora et al., 2017;</ref><ref type="bibr" target="#b31">Pagliardini et al., 2017)</ref>, though usually with the intent to extract useful features for supervised sentence tasks rather than to capture semantic similarity.</p><p>An exception is the work of <ref type="bibr" target="#b40">Wieting et al. (2016b)</ref>. We closely follow their experimental setup and directly address some outstanding ques- tions in their experimental results. Here we briefly summarize their main findings and their attempts at explaining them. They made the surprising dis- covery that word averaging outperforms LSTMs by a wide margin in the transfer learning setting. They proposed several hypotheses for why this oc- curs. They first considered that the LSTM was un- able to adapt to the differences in sequence length between phrases in training and sentences in test. This was ruled out by showing that neither model showed any strong correlation between sequence length and performance on the test data.</p><p>They next examined whether the LSTM was overfitting on the training data, but then showed that both models achieve similar values of the training objective and similar performance on in- domain held-out test sets. Lastly, they considered whether their hyperparameters were inadequately tuned, but extensive hyperparameter tuning did not change the story. Therefore, the reason for the per- formance gap, and how to correct it, was left as an open problem. This paper takes steps toward ad- dressing that problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models and Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Models</head><p>Our goal is to embed a word sequence s into a fixed-length vector. We focus on three composi- tional models in this paper, all of which use words as the smallest unit of compositionality. We de- note the tth word in s as s t , and we denote its word embedding by x t .</p><p>Our first two models have been well-studied in prior work, so we describe them briefly. The first, which we call AVG, simply averages the embed- dings x t of all words in s. The only parameters learned in this model are those in the word em- beddings themselves, which are stored in the word embedding matrix W w . This model was found by <ref type="bibr" target="#b40">Wieting et al. (2016b)</ref> to perform very strongly for semantic similarity tasks.</p><p>Our second model uses a long short-term mem- ory (LSTM) recurrent neural network <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997)</ref> to embed s. We use the LSTM variant from <ref type="bibr" target="#b12">Gers et al. (2003)</ref> including its "peephole" connections. We consider two ways to obtain a sentence embedding from the LSTM. The first uses the final hidden vector, which we denote h −1 . The second, denoted LSTMAVG, averages all hidden vectors of the LSTM. In both variants, the learnable parameters include both the LSTM parameters W c and the word embeddings W w .</p><p>Inspired by the success of the two models above, we propose a third model, which we call the GATED RECURRENT AVERAGING NETWORK (GRAN). The GATED RECURRENT AVERAGING NETWORK combines the benefits of AVG and LSTMs. In fact it reduces to AVG if the output of the gate is all ones. We first use an LSTM to generate a hidden vector, h t , for each word s t in s. Then we use h t to compute a gate that will be elementwise-multiplied with x t , resulting in a new, gated hidden vector a t for each step t:</p><formula xml:id="formula_0">a t = x t σ(W x x t + W h h t + b)<label>(1)</label></formula><p>where W x and W h are parameter matrices, b is a parameter vector, and σ is the elementwise logis- tic sigmoid function. After all a t have been gener- ated for a sentence, they are averaged to produce the embedding for that sentence. This model in- cludes as learnable parameters those of the LSTM, the word embeddings, and the additional parame- ters in Eq. (1). For both the LSTM and GRAN models, we use W c to denote the "compositional" parameters, i.e., all parameters other than the word embeddings.</p><p>The motivation for the GRAN is that we are contextualizing the word embeddings prior to av- eraging. The gate can be seen as an attention, at- tending to the prior context of the sentence. <ref type="bibr">2</ref> We also experiment with four other variations of this model, though they generally were more com- plex and showed inferior performance. In the first, GRAN-2, the gate is applied to h t (rather than x t ) to produce a t , and then these a t are averaged as before.</p><p>GRAN-3 and GRAN-4 use two gates: one ap- plied to x t and one applied to a t−1 . We tried two different ways of computing these gates: for each gate i, σ(</p><formula xml:id="formula_1">W x i x t + W h i h t + b i ) (GRAN-3) or σ(W x i x t + W h i h t + W a i a t−1 + b i ) (GRAN-4).</formula><p>The sum of these two terms comprised a t . In this model, the last average hidden state, a −1 , was used as the sentence embedding after dividing it by the length of the sequence. In these models, we are additionally keeping a running average of the em- beddings that is being modified by the context at every time step. In GRAN-4, this running average is also considered when producing the contextual- ized word embedding.</p><p>Lastly, we experimented with a fifth GRAN, GRAN-5, in which we use two gates, calculated by σ(W x i x t + W h i h t + b i ) for each gate i. The first is applied to x t and the second is applied to h t . The output of these gates is then summed. There- fore GRAN-5 can be reduced to either word- averaging or averaging LSTM states, depending on the behavior of the gates. If the first gate is all ones and the second all zeros throughout the sequence, the model is equivalent to word- averaging. Conversely, if the first gate is all ze- ros and the second is all ones throughout the se- quence, the model is equivalent to averaging the LSTM states. Further analysis of these models is included in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>We follow the training procedure of <ref type="bibr" target="#b41">Wieting et al. (2015)</ref> and <ref type="bibr" target="#b40">Wieting et al. (2016b)</ref>, described be- low. The training data consists of a set S of phrase or sentence pairs s 1 , s 2 from either the Para- phrase Database (PPDB; <ref type="bibr" target="#b11">Ganitkevitch et al., 2013)</ref> or the aligned Wikipedia sentences <ref type="bibr" target="#b8">(Coster and Kauchak, 2011</ref>) where s 1 and s 2 are assumed to be paraphrases. We optimize a margin-based loss:</p><formula xml:id="formula_2">min Wc,Ww 1 |S| s 1 ,s 2 ∈S max(0, δ − cos(g(s1), g(s2)) + cos(g(s1), g(t1))) + max(0, δ − cos(g(s1), g(s2)) + cos(g(s2), g(t2))) + λc Wc 2 + λw Ww initial − Ww 2 (2)</formula><p>where g is the model in use (e.g., AVG or LSTM), δ is the margin, λ c and λ w are regularization parameters, W w initial is the initial word embed- ding matrix, and t 1 and t 2 are carefully-selected negative examples taken from a mini-batch dur- ing optimization. The intuition is that we want the two phrases to be more similar to each other (cos(g(s 1 ), g(s 2 ))) than either is to their respec- tive negative examples t 1 and t 2 , by a margin of at least δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Selecting Negative Examples</head><p>To select t 1 and t 2 in Eq. <ref type="formula">(2)</ref>, we simply choose the most similar phrase in some set of phrases (other than those in the given phrase pair). For simplicity we use the mini-batch for this set, but it could be a different set. That is, we choose t 1 for a given s 1 , s 2 as follows:</p><formula xml:id="formula_3">t 1 = argmax t:t,··∈S b \{{s 1 ,s 2 } cos(g(s 1 ), g(t))</formula><p>where S b ⊆ S is the current mini-batch. That is, we want to choose a negative example t i that is similar to s i according to the current model. The downside is that we may occasionally choose a phrase t i that is actually a true paraphrase of s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments are designed to address the em- pirical question posed by <ref type="bibr" target="#b40">Wieting et al. (2016b)</ref>: why do LSTMs underperform AVG for transfer learning? In Sections 4.1.2-4.2, we make progress on this question by presenting methods that bridge the gap between the two models in the transfer set- ting. We then apply these same techniques to im- prove performance in the supervised setting, de- scribed in Section 4.3. In both settings we also evaluate our novel GRAN architecture, finding it to consistently outperform both AVG and the LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transfer Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets and Tasks</head><p>We train on large sets of noisy paraphrase pairs and evaluate on a diverse set of 22 textual sim- ilarity datasets, including all datasets from every SemEval semantic textual similarity (STS) task from 2012 to 2015. We also evaluate on the Sem- Eval 2015 Twitter task ( <ref type="bibr" target="#b42">Xu et al., 2015</ref>) and the SemEval 2014 SICK Semantic Relatedness task ( <ref type="bibr" target="#b28">Marelli et al., 2014</ref>). Given two sentences, the aim of the STS tasks is to predict their similar- ity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 indicates that they are completely equivalent. We report the average Pearson's r over these 22 sentence similarity tasks. Each STS task consists of 4-6 datasets covering a wide variety of domains, including newswire, tweets, glosses, machine translation outputs, web forums, news headlines, image and video captions, among others. Further details are provided in the official task descriptions ( <ref type="bibr" target="#b4">Agirre et al., 2012</ref><ref type="bibr" target="#b3">Agirre et al., , 2013</ref><ref type="bibr" target="#b1">Agirre et al., , 2014</ref><ref type="bibr" target="#b0">Agirre et al., , 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Experiments with Data Sources</head><p>We first investigate how different sources of train- ing data affect the results. We try two data sources. The first is phrase pairs from the Para- phrase Database (PPDB). PPDB comes in differ- ent sizes (S, M, L, XL, XXL, and XXXL), where each larger size subsumes all smaller ones. The pairs in PPDB are sorted by a confidence mea- sure and so the smaller sets contain higher preci- sion paraphrases. PPDB is derived automatically from naturally-occurring bilingual text, and ver- sions of PPDB have been released for many lan- guages without the need for any manual annota- tion ( <ref type="bibr" target="#b10">Ganitkevitch and Callison-Burch, 2014</ref>).</p><p>The second source of data is a set of sen- tence pairs automatically extracted from Simple English Wikipedia and English Wikipedia arti- cles by <ref type="bibr" target="#b8">Coster and Kauchak (2011)</ref> embed- dings ( <ref type="bibr" target="#b41">Wieting et al., 2015</ref>) to initialize the word embedding matrix (W w ) for all models. For all ex- periments, we fix the mini-batch size to 100, and λ c to 0. We tune the margin δ over {0.4, 0.6, 0.8} and λ w over {10 −4 , 10 −5 , 10 −6 , 10 −7 , 10 −8 , 0}. We train AVG for 7 epochs, and the LSTM for 3, since it converges much faster and does not benefit from 7 epochs. For optimization we use Adam ( <ref type="bibr" target="#b21">Kingma and Ba, 2015</ref>) with a learning rate of 0.001. We use the 2016 STS tasks ( <ref type="bibr" target="#b2">Agirre et al., 2016)</ref> for model selection, where we average the Pearson's r over its 5 datasets. We refer to this type of model selection as test. For evaluation, we report the average Pearson's r over the 22 other sentence similarity tasks.</p><p>The results are shown in <ref type="table">Table 1</ref>. We first note that, when training on PPDB, we find the same result as <ref type="bibr" target="#b40">Wieting et al. (2016b)</ref>: AVG outperforms the LSTM by more than 13 points. However, when training both on sentence pairs, the gap shrinks to about 9 points. It appears that part of the inferior performance for the LSTM in prior work was due to training on phrase pairs rather than on sentence pairs. The AVG model also benefits from train- ing on sentences, but not nearly as much as the LSTM. <ref type="bibr">4</ref> Our hypothesis explaining this result is that in PPDB, the phrase pairs are short fragments of text which are not necessarily constituents or phrases in any syntactic sense. Therefore, the sentences in the STS test sets are quite different from the fragments seen during training. We hypothesize that while word-averaging is relatively unaffected by this difference, the recurrent models are much more sensitive to overall characteristics of the word sequences, and the difference between train and test matters much more.</p><p>These results also suggest that the SimpWiki data, even though it was developed for text simpli- fication, may be useful for other researchers work- ing on semantic textual similarity tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Experiments with LSTM Variations</head><p>We next compare LSTM and LSTMAVG. The lat- ter consists of averaging the hidden vectors of the LSTM rather than using the final hidden vector as in prior work ( <ref type="bibr" target="#b40">Wieting et al., 2016b</ref>). We hy- pothesize that the LSTM may put more empha- sis on the words at the end of the sentence than those at the beginning. By averaging the hidden states, the impact of all words in the sequence is better taken into account. Averaging also makes the LSTM more like AVG, which we know to per- form strongly in this setting.</p><p>The results on AVG and the LSTM models are shown in <ref type="table">Table 1</ref>. When training on PPDB, mov- ing from LSTM to LSTMAVG improves perfor- mance by 10 points, closing most of the gap with AVG. We also find that LSTMAVG improves by moving from PPDB to SimpWiki, though in both cases it still lags behind AVG. ing and test sentences, adding both, and adding neither. We treated adding these tags as hyperparameters and tuned over these four settings along with the other hyperparameters in the original experiment. Interestingly, we found that adding these tags, especially EOS, had a large effect on the LSTM when training on SimpWiki, improving performance by 6 points. When training on PPDB, adding EOS tags only im- proved performance by 1.6 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments with Regularization</head><p>The addition of the tags had a smaller effect on LSTMAVG. Adding EOS tags improved performance by 0.3 points on SimpWiki and adding SOS tags on PPDB improved perfor- mance by 0.9 points. as well as several additional regularization meth- ods we describe below.</p><p>We try two forms of dropout. The first is just standard dropout ( <ref type="bibr" target="#b36">Srivastava et al., 2014</ref>) on the word embeddings. The second is "word dropout", which drops out entire word embeddings with some probability <ref type="figure">(Iyyer et al., 2015)</ref>.</p><p>We also experiment with scrambling the inputs. For a given mini-batch, we go through each sen- tence pair and, with some probability, we shuf- fle the words in each sentence in the pair. When scrambling a sentence pair, we always shuffle both sentences in the pair. We do this before selecting negative examples for the mini-batch. The moti- vation for scrambling is to make it more difficult for the LSTM to memorize the sequences in the training data, forcing it to focus more on the iden- tities of the words and less on word order. Hence it will be expected to behave more like the word averaging model. <ref type="bibr">5</ref> We also experiment with combining scrambling and dropout. In this setting, we tune over scram- bling with either word dropout or dropout.</p><p>The settings for these experiments are largely the same as those of the previous section with the exception that we tune λ w over a smaller set of values: {10 −5 , 0}. When using L 2 regulariza- tion, we tune λ c over {10 −3 , 10 −4 , 10 −5 , 10 −6 }. When using dropout, we tune the dropout rate over {0.2, 0.4, 0.6}. When using scrambling, we tune the scrambling rate over {0.25, 0.5, 0.75}. We also include a bidirectional model ("Bi") for both LSTMAVG and the GATED RECURRENT AVERAG- ING NETWORK. We tune over two ways to com- bine the forward and backward hidden states; the first simply adds them together and the second uses a single feedforward layer with a tanh ac- tivation.</p><p>We try two approaches for model selection. The first, test , is the same as was done in Section 4.1.2, where we use the average Pearson's r on the 5 2016 STS datasets. The second tunes based on the average Pearson's r of all 22 datasets in our evaluation. We refer to this as oracle.</p><p>The results are shown in <ref type="table" target="#tab_2">Table 2</ref>   bling input sequences is very effective in improv- ing the result of the LSTM, while neither type of dropout improves AVG. Moreover, averaging the hidden states of the LSTM is the most effective modification to the LSTM in improving perfor- mance. All of these modifications can be com- bined to significantly improve the LSTM, finally allowing it to overtake AVG.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, we compare the various GRAN ar- chitectures. We find that the GRAN provides a small improvement over the best LSTM configu- ration, possibly because of its similarity to AVG. It also outperforms the other GRAN models, despite being the simplest.</p><p>In <ref type="table" target="#tab_5">Table 4</ref>, we show results on all individual STS evaluation datasets after using STS 2016 for model selection (unidirectional models only). The LSTMAVG and GATED RECURRENT AVERAGING NETWORK are more closely correlated in perfor- mance, in terms of Spearman's ρ and Pearson'r r, than either is to AVG. But they do differ sig- nificantly in some datasets, most notably in those comparing machine translation output with its ref-  Upon examination, we found that these datasets, especially 2013 OnWN, contain examples of low similarity with high word overlap. For exam- ple, the pair the act of preserving or protect- ing something., the act of decreasing or reducing something. from 2013 OnWN has a gold similar- ity score of 0.4. It appears that AVG was fooled by the high amount of word overlap in such pairs, while the other two models were better able to rec- ognize the semantic differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Supervised Text Similarity</head><p>We also investigate if these techniques can im- prove LSTM performance on supervised semantic textual similarity tasks. We evaluate on two super- vised datasets. For the first, we start with the 20 SemEval STS datasets from 2012-2015 and then use 40% of each dataset for training, 10% for val- idation, and the remaining 50% for testing. There are 4,481 examples in training, 1,207 in validation, and 6,060 in the test set. The second is the SICK 2014 dataset, using its standard training, valida- tion, and test sets. There are 4,500 sentence pairs in the training set, 500 in the development set, and 4,927 in the test set. The SICK task is an eas- ier learning problem since the training examples are all drawn from the same distribution, and they are mostly shorter and use simpler language. As these are supervised tasks, the sentence pairs in the training set contain manually-annotated semantic similarity scores. We minimize the loss function 6 from <ref type="bibr" target="#b37">Tai et al. (2015)</ref>. Given a score for a sentence pair in the range <ref type="bibr">[1, K]</ref>, where K is an integer, with sentence representations h L and h R , and model parameters θ, they first compute:</p><formula xml:id="formula_4">h × = h L h R , h + = |h L − h R |, h s = σ W (×) h × + W (+) h + + b (h) , ˆ p θ = softmax W (p) h s + b (p) , ˆ y = r T ˆ p θ ,</formula><p>where r T = [1 2 . . . K]. They then define a sparse target distribution p that satisfies y = r T p:</p><formula xml:id="formula_5">p i =      y − y, i = y + 1 y − y + 1, i = y 0 otherwise for 1 ≤ i ≤ K.</formula><p>Then they use the following loss, the regularized KL-divergence between p andˆpandˆ andˆp θ :</p><formula xml:id="formula_6">J(θ) = 1 m m k=1 KL p (k) ˆ p (k) θ ,</formula><p>where m is the number of training pairs. We experiment with the LSTM, LSTMAVG, and AVG models with dropout, word dropout, and scrambling tuning over the same hyperparameter as in Section 4.2. We again regularize the word embeddings back to their initial state, tuning λ w over {10 −5 , 0}. We used the validation set for each respective dataset for model selection.</p><p>The results are shown in <ref type="table" target="#tab_7">Table 5</ref>. The GATED RECURRENT AVERAGING NETWORK has the best performance on both datasets. Dropout helps the word-averaging model in the STS task, unlike in the transfer learning setting. The LSTM bene- fits slightly from dropout, scrambling, and aver- aging on their own individually with the excep- tion of word dropout on both datasets and aver- aging on the SICK dataset. However, when com- bined, these modifications are able to significantly   improve the performance of the LSTM, bringing it much closer in performance to AVG. This ex- periment indicates that these modifications when training LSTMs are beneficial outside the trans- fer learning setting, and can potentially be used to improve performance for the broad range of prob- lems that use LSTMs to model sentences.</p><p>In <ref type="table" target="#tab_8">Table 6</ref> we compare the various GRAN ar- chitectures under the same settings as the previous experiment. We find that the GRAN still has the best overall performance.</p><p>We also experiment with initializing the super- vised models using our pretrained sentence model parameters, for the AVG model (no regularization), LSTMAVG (dropout, scrambling), and GATED RECURRENT AVERAGING NETWORK (dropout, scrambling) models from <ref type="table" target="#tab_2">Table 2 and Table 3</ref>. We both initialize and then regularize back to these initial values, referring to this setting as "univer- sal". 7 # Sentence 1 Sentence 2 LAVG AVG Gold 1 the lamb is looking at the camera.</p><p>a cat looking at the camera.</p><p>3.42 4.13 0.8 2 he also said shockey is "living the dream life of a new york athlete. "jeremy's a good guy," barber said, adding:"jeremy is living the dream life of the new york athlete.   The results are shown in <ref type="table" target="#tab_11">Table 8</ref>. Initializ- ing and regularizing to the pretrained models sig- nificantly improves the performance for all three models, justifying our claim that these models serve a dual purpose: they can be used a black box semantic similarity function, and they possess rich knowledge that can be used to improve the perfor- mance of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Error Analysis</head><p>We analyze the predictions of AVG and the recur- rent networks, represented by LSTMAVG, on the 20 STS datasets. We choose LSTMAVG as it cor- relates slightly less strongly with AVG than the GRAN on the results over all SemEval datasets used for evaluation. We scale the models' cosine similarities to lie within <ref type="bibr">[0,</ref><ref type="bibr">5]</ref>, then compare the predicted similarities of LSTMAVG and AVG to the gold similarities. We analyzed instances in which each model would tend to overestimate or under- estimate the gold similarity relative to the other. These are illustrated in <ref type="table" target="#tab_10">Table 7</ref>.</p><p>We find that AVG tends to overestimate the se- mantic similarity of a sentence pair, relative to LSTMAVG, when the two sentences have a lot of and λc over {10, 1, 10 −1 , 10 −2 , 10 −3 , 10 −4 , 10 −5 , 10 −6 , 0}.</p><p>word or synonym overlap, but have either impor- tant differences in key semantic roles or where one sentence has significantly more content than the other. These phenomena are shown in examples 1 and 2 in <ref type="table" target="#tab_10">Table 7</ref>. Conversely, AVG tends to under- estimate similarity when there are one-word-to- multiword paraphrases between the two sentences as shown in examples 3 and 4.</p><p>LSTMAVG tends to overestimate similarity when the two inputs have similar sequences of syntactic categories, but the meanings of the sen- tences are different (examples 5, 6, and 7). In- stances of LSTMAVG underestimating the similar- ity relative to AVG are relatively rare, and those that we found did not have any systematic patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">GRAN Gate Analysis</head><p>We also investigate what is learned by the gating function of the GATED RECURRENT AVERAGING NETWORK. We are interested to see whether its estimates of importance correlate with those of tra- ditional syntactic and (shallow) semantic analysis.</p><p>We use the oracle trained GATED RECURRENT AVERAGING NETWORK from <ref type="table" target="#tab_3">Table 3</ref> and cal- culate the L 1 norm of the gate after embedding 10,000 sentences from English Wikipedia. 8 We also automatically tag and parse these sentences using the Stanford dependency parser ( <ref type="bibr" target="#b26">Manning et al., 2014</ref>). We then compute the average gate L 1 norms for particular part-of-speech tags, de- pendency arc labels, and their conjunction. <ref type="table" target="#tab_12">Table 9</ref> shows the highest/lowest average norm tags and dependency labels. The network prefers nouns, especially proper nouns, as well as cardinal numbers, which is sensible as these are among the most discriminative features of a sentence.</p><p>Analyzing the dependency relations, we find POS Dep. Label top 10 bot. 10 top 10 bot. <ref type="table" target="#tab_13">10  NNP TO  number possessive  NNPS WDT nn  cop  CD  POS  num  det  NNS DT  acomp auxpass  VBG WP  appos  prep  NN  IN  pobj  cc  JJ  CC  vmod  mark  UH  PRP  dobj  aux  VBN EX  amod  expl  JJS</ref> WRB conj neg  that nouns in the object position tend to have higher weight than nouns in the subject position. This may relate to topic and focus; the object may be more likely to be the "new" information related by the sentence, which would then make it more likely to be matched by the other sentence in the paraphrase pair. We find that the weights of adjectives depend on their position in the sentence, as shown in Ta- ble 10. The highest norms appear when an ad- jective is an xcomp, acomp, or root; this typically means it is residing in an object-like position in its clause. Adjectives that modify a noun (amod) have  medium weight, and those that modify another ad- jective or verb (advmod) have low weight. Lastly, we analyze words tagged as VBG, a highly ambiguous tag that can serve many syn- tactic roles in a sentence. As shown in <ref type="table" target="#tab_15">Table 11</ref>, we find that when they are used to modify a noun (amod) or in the object position of a clause (xcomp, pcomp) they have high weight. Medium weight appears when used in verb phrases <ref type="bibr">(root, vmod)</ref> and low weight when used as prepositions or auxiliary verbs (prep, auxpass).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We showed how to modify and regularize LSTMs to improve their performance for learning para- phrastic sentence embeddings in both transfer and supervised settings. We also introduced a new re- current network, the GATED RECURRENT AVER- AGING NETWORK, that improves upon both AVG and LSTMs for these tasks, and we release our code and trained models. Furthermore, we analyzed the different errors produced by AVG and the recurrent methods and found that the recurrent methods were learning composition that wasn't being captured by AVG. We also investigated the GRAN in order to better understand the compositional phenomena it was learning by analyzing the L 1 norm of its gate over various inputs.</p><p>Future work will explore additional data sources, including from aligning different trans- lations of novels ( <ref type="bibr" target="#b6">Barzilay and McKeown, 2001</ref>), aligning new articles of the same topic ( <ref type="bibr" target="#b9">Dolan et al., 2004</ref>), or even possibly using machine trans- lation systems to translate bilingual text into para- phrastic sentence pairs. Our new techniques, com- bined with the promise of new data sources, of- fer a great deal of potential for improved universal paraphrastic sentence embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>We next experiment with various forms of regu- larization. Previous work (Wieting et al., 2016b,a) only used L 2 regularization. Wieting et al. (2016b) also regularized the word embeddings back to their initial values. Here we use L 2 regularization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>This data 
was extracted for developing text simplification 

AVG LSTM LSTMAVG 
PPDB 
67.7 54.2 
64.2 
SimpWiki 68.4 59.3 
67.5 

Table 1: Test results on SemEval semantic textual 
similarity datasets (Pearson's r × 100) when train-
ing on different sources of data: phrase pairs from 
PPDB or simple-to-standard English Wikipedia 
sentence pairs from Coster and Kauchak (2011). 

systems, where each instance pairs a simple and 
complex sentence representing approximately the 
same information. Though the data was obtained 
for simplification, we use it as a source of train-
ing data for learning paraphrastic sentence embed-
dings. The dataset, which we call SimpWiki, con-
sists of 167,689 sentence pairs. 
To ensure a fair comparison, we select a sample 
of pairs from PPDB XL such that the number of 
tokens is approximately the same as the number 
of tokens in the SimpWiki sentences. 3 
We 
use 

PARAGRAM-SL999 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>. They show that dropping entire word embeddings and scram-</figDesc><table>Model 

Regularization 
Oracle 2016 STS 

AVG 

none 
68.5 
68.4 
dropout 
68.4 
68.3 
word dropout 
68.3 
68.3 

LSTM 

none 
60.6 
59.3 
L2 
60.3 
56.5 
dropout 
58.1 
55.3 
word dropout 
66.2 
65.3 
scrambling 
66.3 
65.1 
dropout, scrambling 68.4 
68.4 

LSTMAVG 
none 
67.7 
67.5 
dropout, scrambling 69.2 
68.6 
BiLSTMAVG dropout, scrambling 69.4 
68.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on SemEval textual similarity 
datasets (Pearson's r × 100) when experimenting 
with different regularization techniques. 

Model 
Oracle STS 2016 
GRAN (no reg.) 68.0 
68.0 
GRAN 
69.5 
68.9 
GRAN-2 
68.8 
68.1 
GRAN-3 
69.0 
67.2 
GRAN-4 
68.6 
68.1 
GRAN-5 
66.1 
64.8 
BiGRAN 
69.7 
68.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on SemEval textual similarity 
datasets (Pearson's r × 100) for the GRAN ar-
chitectures. The first row, marked as (no reg.) is 
the GRAN without any regularization. The other 
rows show the result of the various GRAN models 
using dropout and scrambling. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on SemEval textual similarity 
datasets (Pearson's r × 100). The highest score in 
each row is in boldface. 

erence. Interestingly, both the LSTMAVG and 
GATED RECURRENT AVERAGING NETWORK sig-
nificantly outperform AVG in the datasets focused 
on comparing glosses like OnWN and FNWN. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results from supervised training on 
the STS and SICK datasets (Pearson's r × 100). 
The last column is the average result on the two 
datasets. 

Model 
STS SICK Avg. 
GRAN 
81.6 85.3 83.5 
GRAN-2 77.4 85.1 81.3 
GRAN-3 81.3 85.4 83.4 
GRAN-4 80.1 85.5 82.8 
GRAN-5 70.9 83.0 77.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Results from supervised training on the 
STS and SICK datasets (Pearson's r × 100) for 
the GRAN architectures. The last column is the 
average result on the two datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Illustrative sentence pairs from the STS datasets showing errors made by LSTMAVG and 
AVG. The last three columns show the gold similarity score, the similarity score of LSTMAVG, and the 
similarity score of AVG. Boldface indicates smaller error compared to gold scores. 

Model 
Regularization 
STS SICK 

AVG 

dropout 
80.7 84.5 
dropout, universal 
82.9 85.6 

LSTMAVG 
dropout, scrambling 
76.5 84.0 
dropout, scrambling, universal 81.3 85.2 

GRAN 
dropout, scrambling 
81.6 85.1 
dropout, scrambling, universal 82.7 86.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Impact of initializing and regularizing 
toward universal models (Pearson's r ×100) in su-
pervised training. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>POS tags and dependency labels with 
highest and lowest average GATED RECURRENT 
AVERAGING NETWORK gate L 1 norms. The lists 
are ordered from highest norm to lowest in the top 
10 columns, and lowest to highest in the bottom 
10 columns. 

Dep. Label Weight 
xcomp 
170.6 
acomp 
167.1 
root 
157.4 
amod 
143.1 
advmod 
121.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>Average L 1 norms for adjectives (JJ) 
with selected dependency labels. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table>Average L 1 norms for words with the 
tag VBG with selected dependency labels. 

</table></figure>

			<note place="foot" n="1"> Trained models and code are available at http:// ttic.uchicago.edu/ ˜ wieting.</note>

			<note place="foot" n="2"> We tried a variant of this model without the gate. We obtain at from f (Wxxt + W h ht + b), where f is a nonlinearity, tuned over tanh and ReLU. The performance of the model is significantly worse than the GRAN in all experiments.</note>

			<note place="foot" n="3"> The PPDB data consists of 1,341,188 phrase pairs and contains 3 more tokens than the SimpWiki data. 4 We experimented with adding EOS tags at the end of training and test sentences, SOS tags at the start of train</note>

			<note place="foot" n="5"> We also tried some variations on scrambling that did not yield significant improvements: scrambling after obtaining the negative examples, partially scrambling by performing n swaps where n comes from a Poisson distribution with a tunable λ, and scrambling individual sentences with some probability instead of always scrambling both in the pair.</note>

			<note place="foot" n="6"> This objective function has been shown to perform very strongly on text similarity tasks, significantly better than squared or absolute error.</note>

			<note place="foot" n="7"> In these experiments, we tuned λw over {10, 1, 10 −1 , 10 −2 , 10 −3 , 10 −4 , 10 −5 , 10 −6 , 10 −7 , 10 −8 , 0}</note>

			<note place="foot" n="8"> We selected only sentences of less than or equal to 15 tokens to ensure more accurate parsing.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valu-able comments. This research used resources of the Argonne Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357. We thank the developers of <ref type="bibr">Theano (Theano Development Team, 2016)</ref> and NVIDIA Corporation for donating GPUs used in this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larraitz</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>English, Spanish</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>SemEval-2015 task 2: Semantic textual similarity</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval pages</title>
		<meeting>SemEval pages</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">*SEM 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalezagirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity</title>
		<meeting>the Main Conference and the Shared Task: Semantic Textual Similarity</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the First Joint Conference on Lexical and Computational Semantics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting paraphrases from a parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kathleen R Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 39th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comparison of vector-based representations for semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Blacoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simple english wikipedia: a new text simplification task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Coster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The multilingual paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PPDB: The Paraphrase Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning precise timing with LSTM recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">˙</forename><surname>Ozan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-timescale long shortterm memory neural network for modelling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd</title>
		<meeting>52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Nghia The Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">From paraphrase database to compositional paraphrase model and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the ACL (TACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
		<title level="m">SemEval-2015 task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
