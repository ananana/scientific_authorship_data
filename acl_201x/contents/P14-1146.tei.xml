<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification *</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>June 23-25 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification *</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics <address><addrLine>Baltimore, Maryland, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1555" to="1565"/>
							<date type="published">June 23-25 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a method that learns word embedding for Twitter sentiment classification in this paper. Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text. This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentiment-specific word embedding (SSWE), which encodes sentiment information in the continuous representation of words. Specifically , we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sentences or tweets) in their loss functions. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SS-WE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenat-ing SSWE with existing feature set.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Twitter sentiment classification has attracted in- creasing research interest in recent years <ref type="bibr" target="#b20">(Jiang et al., 2011;</ref><ref type="bibr" target="#b19">Hu et al., 2013</ref>). The objective is to clas- sify the sentiment polarity of a tweet as positive, negative or neutral. The majority of existing ap- proaches follow <ref type="bibr" target="#b33">Pang et al. (2002)</ref> and employ ma- chine learning algorithms to build classifiers from tweets with manually annotated sentiment polar- ity. Under this direction, most studies focus on designing effective features to obtain better clas- sification performance. For example, <ref type="bibr" target="#b29">Mohammad et al. (2013)</ref> build the top-performed system in the Twitter sentiment classification track of <ref type="bibr">SemEval 2013</ref><ref type="bibr" target="#b30">(Nakov et al., 2013</ref>, using diverse sentiment lexicons and a variety of hand-crafted features.</p><p>Feature engineering is important but labor- intensive. It is therefore desirable to discover ex- planatory factors from the data and make the learn- ing algorithms less dependent on extensive fea- ture engineering <ref type="bibr" target="#b3">(Bengio, 2013)</ref>. For the task of sentiment classification, an effective feature learn- ing method is to compose the representation of a sentence (or document) from the representation- s of the words or phrases it contains <ref type="bibr" target="#b39">(Socher et al., 2013b;</ref><ref type="bibr" target="#b46">Yessenalina and Cardie, 2011</ref>). Ac- cordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learn- ing algorithms <ref type="bibr" target="#b8">(Collobert et al., 2011;</ref><ref type="bibr" target="#b26">Mikolov et al., 2013</ref>) are intuitive choices, they are not effec- tive enough if directly used for sentiment classi- fication. The most serious problem is that tradi- tional methods typically model the syntactic con- text of words but ignore the sentiment information of text. As a result, words with opposite polari- ty, such as good and bad, are mapped into close vectors. It is meaningful for some tasks such as pos-tagging ( <ref type="bibr" target="#b48">Zheng et al., 2013)</ref> as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity.</p><p>In this paper, we propose learning sentiment- specific word embedding (SSWE) for sentiment analysis. We encode the sentiment information in-to the continuous representation of words, so that it is able to separate good and bad to opposite ends of the spectrum. To this end, we extend the ex- isting word embedding learning algorithm <ref type="bibr" target="#b8">(Collobert et al., 2011</ref>) and develop three neural net- works to effectively incorporate the supervision from sentiment polarity of text (e.g. sentences or tweets) in their loss functions. We learn the sentiment-specific word embedding from tweet- s, leveraging massive tweets with emoticons as distant-supervised corpora without any manual an- notations. These automatically collected tweet- s contain noises so they cannot be directly used as gold training data to build sentiment classifier- s, but they are effective enough to provide weak- ly supervised signals for training the sentiment- specific word embedding.</p><p>We apply SSWE as features in a supervised learning framework for Twitter sentiment classi- fication, and evaluate it on the benchmark dataset in SemEval 2013. In the task of predicting posi- tive/negative polarity of tweets, our method yields 84.89% in macro-F1 by only using SSWE as fea- ture, which is comparable to the top-performed system based on hand-crafted features (84.70%). After concatenating the SSWE feature with ex- isting feature set, we push the state-of-the-art to 86.58% in macro-F1. The quality of SSWE is al- so directly evaluated by measuring the word sim- ilarity in the embedding space for sentiment lexi- cons. In the accuracy of polarity consistency be- tween each sentiment word and its top N closest words, SSWE outperforms existing word embed- ding learning algorithms.</p><p>The major contributions of the work presented in this paper are as follows.</p><p>â€¢ We develop three neural networks to learn sentiment-specific word embedding (SSWE) from massive distant-supervised tweets with- out any manual annotations;</p><p>â€¢ To our knowledge, this is the first work that exploits word embedding for Twitter senti- ment classification. We report the results that the SSWE feature performs comparably with hand-crafted features in the top-performed system in SemEval 2013;</p><p>â€¢ We release the sentiment-specific word em- bedding learned from 10 million tweets, which can be adopted off-the-shell in other sentiment analysis tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we present a brief review of the related work from two perspectives, Twitter senti- ment classification and learning continuous repre- sentations for sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Twitter Sentiment Classification</head><p>Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest <ref type="bibr" target="#b20">(Jiang et al., 2011;</ref><ref type="bibr" target="#b19">Hu et al., 2013</ref>) in recent years. Gen- erally, the methods employed in Twitter sentiment classification follow traditional sentiment classifi- cation approaches. The lexicon-based approaches <ref type="bibr" target="#b43">(Turney, 2002;</ref><ref type="bibr" target="#b10">Ding et al., 2008;</ref><ref type="bibr" target="#b40">Taboada et al., 2011;</ref><ref type="bibr" target="#b41">Thelwall et al., 2012</ref>) mostly use a dictio- nary of sentiment words with their associated sen- timent polarity, and incorporate negation and in- tensification to compute the sentiment polarity for each sentence (or document).</p><p>The learning based methods for Twitter sen- timent classification follow <ref type="bibr" target="#b33">Pang et al. (2002)</ref>'s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification <ref type="bibr" target="#b31">(Pak and Paroubek, 2010;</ref><ref type="bibr" target="#b9">Davidov et al., 2010;</ref><ref type="bibr" target="#b0">Barbosa and Feng, 2010;</ref><ref type="bibr" target="#b21">Kouloumpis et al., 2011;</ref><ref type="bibr" target="#b47">Zhao et al., 2012</ref>) leverage massive noisy-labeled tweets selected by positive and negative emoticon- s as training set and build sentiment classifiers di- rectly, which is called distant supervision ( <ref type="bibr" target="#b16">Go et al., 2009</ref>). Instead of directly using the distant- supervised data as training set,  adopt the tweets with emoticons to smooth the lan- guage model and <ref type="bibr" target="#b19">Hu et al. (2013)</ref> incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification.</p><p>Many existing learning based methods on Twit- ter sentiment classification focus on feature engi- neering. The reason is that the performance of sen- timent classifier being heavily dependent on the choice of feature representation of tweets. The most representative system is introduced by <ref type="bibr" target="#b29">Mohammad et al. (2013)</ref>, which is the state-of-the- art system (the top-performed system in SemEval 2013 Twitter Sentiment Classification Track) by implementing a number of hand-crafted features. Unlike the previous studies, we focus on learning discriminative features automatically from mas- sive distant-supervised tweets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning Continuous Representations for</head><p>Sentiment Classification <ref type="bibr" target="#b33">Pang et al. (2002)</ref> pioneer this field by using bag- of-word representation, representing each word as a one-hot vector. It has the same length as the size of the vocabulary, and only one dimension is 1, with all others being 0. Under this assumption, many feature learning algorithms are proposed to obtain better classification performance <ref type="bibr" target="#b32">(Pang and Lee, 2008;</ref><ref type="bibr" target="#b24">Liu, 2012;</ref><ref type="bibr" target="#b13">Feldman, 2013)</ref>. However, the one-hot word representation cannot sufficient- ly capture the complex linguistic characteristics of words.</p><p>With the revival of interest in deep learn- ing ( , incorporating the con- tinuous representation of a word as features has been proving effective in a variety of NLP tasks, such as parsing ( <ref type="bibr" target="#b38">Socher et al., 2013a</ref>), language modeling ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b28">Mnih and Hinton, 2009</ref>) and NER ( <ref type="bibr" target="#b42">Turian et al., 2010</ref>). In the field of sentiment analysis, <ref type="bibr" target="#b4">Bespalov et al. (2011;</ref><ref type="bibr" target="#b24">2012)</ref> initialize the word embedding by Laten- t Semantic Analysis and further represent each document as the linear weighted of ngram vec- tors for sentiment classification. <ref type="bibr" target="#b46">Yessenalina and Cardie (2011)</ref> model each word as a matrix and combine words using iterated matrix multiplica- tion. <ref type="bibr" target="#b15">Glorot et al. (2011)</ref> explore Stacked Denois- ing Autoencoders for domain adaptation in sen- timent classification. Socher et al. propose Re- cursive Neural Network (RNN) (2011b), matrix- vector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composi- tionality of phrases of any length based on the representation of each pair of children recursively. <ref type="bibr">Hermann et al. (2013)</ref> present Combinatory Cate- gorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Cat- egorial Grammar with Recursive Autoencoder.</p><p>The representation of words heavily relies on the applications or tasks in which it is used <ref type="bibr" target="#b22">(Labutov and Lipson, 2013)</ref>. This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Un- like <ref type="bibr" target="#b25">Maas et al. (2011)</ref> that follow the proba- bilistic document model ( <ref type="bibr" target="#b6">Blei et al., 2003)</ref> and give an sentiment predictor function to each word, we develop neural networks and map each n- gram to the sentiment polarity of sentence. Un- like <ref type="bibr" target="#b36">Socher et al. (2011c)</ref> that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word em- bedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word em- bedding from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sentiment-Specific Word Embedding for Twitter Sentiment Classification</head><p>In this section, we present the details of learn- ing sentiment-specific word embedding (SSWE) for Twitter sentiment classification. We pro- pose incorporating the sentiment information of sentences to learn continuous representations for words and phrases. We extend the existing word embedding learning algorithm (Collobert et al., 2011) and develop three neural networks to learn SSWE. In the following sections, we introduce the traditional method before presenting the details of SSWE learning algorithms. We then describe the use of SSWE in a supervised learning framework for Twitter sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">C&amp;W Model</head><p>Collobert et al. (2011) introduce C&amp;W model to learn word embedding based on the syntactic con- texts of words. Given an ngram "cat chills on a mat", C&amp;W replaces the center word with a ran- dom word w r and derives a corrupted ngram "cat chills w r a mat". The training objective is that the original ngram is expected to obtain a higher lan- guage model score than the corrupted ngram by a margin of 1. The ranking objective function can be optimized by a hinge loss,</p><formula xml:id="formula_0">loss cw (t, t r ) = max(0, 1 âˆ’ f cw (t) + f cw (t r ))</formula><p>(1) where t is the original ngram, t r is the corrupted ngram, f cw (Â·) is a one-dimensional scalar repre- senting the language model score of the input n- gram. <ref type="figure" target="#fig_0">Figure 1</ref>(a) illustrates the neural architec- ture of C&amp;W, which consists of four layers, name- ly lookup â†’ linear â†’ hT anh â†’ linear (from bottom to top). The original and corrupted ngram- s are treated as inputs of the feed-forward neural network, respectively. The output f cw is the lan- guage model score of the input, which is calculat- ed as given in Equation 2, where L is the lookup table of word embedding, w 1 , w 2 , b 1 , b 2 are the pa- rameters of linear layers. </p><formula xml:id="formula_1">f cw (t) = w 2 (a) + b 2 (2)</formula><formula xml:id="formula_2">a = hT anh(w 1 L t + b 1 ) (3) hT anh(x) = ï£± ï£´ ï£² ï£´ ï£³ âˆ’1 if x &lt; âˆ’1 x if âˆ’ 1 â‰¤ x â‰¤ 1 1 if x &gt; 1<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentiment-Specific Word Embedding</head><p>Following the traditional C&amp;W model (Collobert et al., 2011), we incorporate the sentiment infor- mation into the neural network to learn sentiment- specific word embedding. We develop three neural networks with different strategies to integrate the sentiment information of tweets.</p><p>Basic Model 1 (SSWE h ). As an unsupervised approach, C&amp;W model does not explicitly capture the sentiment information of texts. An intuitive solution to integrate the sentiment information is predicting the sentiment distribution of text based on input ngram. We do not utilize the entire sen- tence as input because the length of different sen- tences might be variant. We therefore slide the window of ngram across a sentence, and then pre- dict the sentiment polarity based on each ngram with a shared neural network. In the neural net- work, the distributed representation of higher lay- er are interpreted as features describing the input. Thus, we utilize the continuous vector of top layer to predict the sentiment distribution of text. Assuming there are K labels, we modify the di- mension of top layer in C&amp;W model as K and add a sof tmax layer upon the top layer. The neural network <ref type="bibr">(SSWE h</ref> ) is given in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. Sof tmax layer is suitable for this scenario be- cause its outputs are interpreted as conditional probabilities. Unlike C&amp;W, SSWE h does not gen- erate any corrupted ngram. Let f g (t), where K denotes the number of sentiment polarity label- s, be the gold K-dimensional multinomial distri- bution of input t and k f g k (t) = 1. For pos- itive/negative classification, the distribution is of the form <ref type="bibr">[1,</ref><ref type="bibr">0]</ref> for positive and <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> for negative. The cross-entropy error of the sof tmax layer is :</p><formula xml:id="formula_3">loss h (t) = âˆ’ k={0,1} f g k (t) Â· log(f h k (t)) (5)</formula><p>where f g (t) is the gold sentiment distribution and f h (t) is the predicted sentiment distribution.</p><p>Basic Model 2 (SSWE r ). SSWE h is trained by predicting the positive ngram as <ref type="bibr">[1,</ref><ref type="bibr">0]</ref> and the neg- ative ngram as <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. However, the constraint of SSWE h is too strict. The distribution of [0.7,0.3] can also be interpreted as a positive label because the positive score is larger than the negative s- core. Similarly, the distribution of [0.2,0.8] indi- cates negative polarity. Based on the above obser- vation, the hard constraints in SSWE h should be relaxed. If the sentiment polarity of a tweet is pos- itive, the predicted positive score is expected to be larger than the predicted negative score, and the exact reverse if the tweet has negative polarity. We model the relaxed constraint with a rank- ing objective function and borrow the bottom four layers from SSWE h , namely lookup â†’ linear â†’ hT anh â†’ linear in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, to build the re- laxed neural network (SSWE r ). Compared with SSWE h , the sof tmax layer is removed because SSWE r does not require probabilistic interpreta- tion. The hinge loss of SSWE r is modeled as de-scribed below.</p><formula xml:id="formula_4">loss r (t) = max(0, 1 âˆ’ Î´ s (t)f r 0 (t) + Î´ s (t)f r 1 (t) )<label>(6)</label></formula><p>where f r 0 is the predicted positive score, f r 1 is the predicted negative score, Î´ s (t) is an indicator function reflecting the sentiment polarity of a sen- tence,</p><formula xml:id="formula_5">Î´ s (t) = 1 if f g (t) = [1, 0] âˆ’1 if f g (t) = [0, 1]<label>(7)</label></formula><p>Similar with SSWE h , SSWE r also does not gen- erate the corrupted ngram.</p><p>Unified Model (SSWE u ). The C&amp;W model learns word embedding by modeling syntactic contexts of words but ignoring sentiment infor- mation. By contrast, SSWE h and SSWE r learn sentiment-specific word embedding by integrating the sentiment polarity of sentences but leaving out the syntactic contexts of words. We develop a uni- fied model (SSWE u ) in this part, which captures the sentiment information of sentences as well as the syntactic contexts of words. SSWE u is illus- trated in <ref type="figure" target="#fig_0">Figure 1(c)</ref>. Given an original (or corrupted) ngram and the sentiment polarity of a sentence as the in- put, SSWE u predicts a two-dimensional vector for each input ngram. The two scalars (f u 0 , f u 1 ) s- tand for language model score and sentiment s- core of the input ngram, respectively. The training objectives of SSWE u are that (1) the original n- gram should obtain a higher language model score f u 0 (t) than the corrupted ngram f u 0 (t r ), and (2) the sentiment score of original ngram f u 1 (t) should be more consistent with the gold polarity annotation of sentence than corrupted ngram f u 1 (t r ). The loss function of SSWE u is the linear combination of t- wo hinge losses,</p><formula xml:id="formula_6">loss u (t, t r ) = Î± Â· loss cw (t, t r )+ (1 âˆ’ Î±) Â· loss us (t, t r )<label>(8)</label></formula><p>where loss cw (t, t r ) is the syntactic loss as given in Equation 1, loss us (t, t r ) is the sentiment loss as described in Equation 9. The hyper-parameter Î± weighs the two parts.</p><formula xml:id="formula_7">loss us (t, t r ) = max(0, 1 âˆ’ Î´ s (t)f u 1 (t) + Î´ s (t)f u 1 (t r ) )<label>(9)</label></formula><p>Model Training. We train sentiment-specific word embedding from massive distant-supervised tweets collected with positive and negative emoti- cons <ref type="bibr">1</ref> . We crawl tweets from April 1st, 2013 to April 30th, 2013 with TwitterAPI. We tokenize each tweet with TwitterNLP ( <ref type="bibr" target="#b14">Gimpel et al., 2011</ref>), remove the @user and URLs of each tweet, and fil- ter the tweets that are too short (&lt; 7 words). Final- ly, we collect 10M tweets, selected by 5M tweets with positive emoticons and 5M tweets with nega- tive emoticons. We train SSWE h , SSWE r and SSWE u by taking the derivative of the loss through back- propagation with respect to the whole set of pa- rameters <ref type="bibr" target="#b8">(Collobert et al., 2011</ref>), and use Ada- Grad ( <ref type="bibr" target="#b11">Duchi et al., 2011</ref>) to update the parame- ters. We empirically set the window size as 3, the embedding length as 50, the length of hidden lay- er as 20 and the learning rate of AdaGrad as 0.1 for all baseline and our models. We learn embed- ding for unigrams, bigrams and trigrams separate- ly with same neural network and same parameter setting. The contexts of unigram (bigram/trigram) are the surrounding unigrams (bigrams/trigrams), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Twitter Sentiment Classification</head><p>We apply sentiment-specific word embedding for Twitter sentiment classification under a supervised learning framework as in previous work <ref type="bibr" target="#b33">(Pang et al., 2002</ref>). Instead of hand-crafting features, we incorporate the continuous representation of word- s and phrases as the feature of a tweet. The senti- ment classifier is built from tweets with manually annotated sentiment polarity.</p><p>We explore min, average and max convolu- tional layers <ref type="bibr" target="#b8">(Collobert et al., 2011;</ref><ref type="bibr" target="#b34">Socher et al., 2011a)</ref>, which have been used as simple and effective methods for compositionality learning in vector-based semantics <ref type="bibr" target="#b27">(Mitchell and Lapata, 2010)</ref>, to obtain the tweet representation. The re- sult is the concatenation of vectors derived from different convolutional layers.</p><formula xml:id="formula_8">z(tw) = [z max (tw), z min (tw), z average (tw)]</formula><p>where z(tw) is the representation of tweet tw and z x (tw) is the results of the convolutional layer x âˆˆ {min, max, average}. Each convolutional layer z x employs the embedding of unigrams, bigrams and trigrams separately and conducts the matrix- vector operation of x on the sequence represented by columns in each lookup table. The output of z x is the concatenation of results obtained from different lookup tables.</p><formula xml:id="formula_9">z x (tw) = [w x L uni tw , w x L bi tw , w x L tri tw ]</formula><p>where w x is the convolutional function of z x , L tw is the concatenated column vectors of the words in the tweet. L uni , L bi and L tri are the lookup tables of the unigram, bigram and trigram embedding, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We conduct experiments to evaluate SSWE by in- corporating it into a supervised learning frame- work for Twitter sentiment classification. We also directly evaluate the effectiveness of the SSWE by measuring the word similarity in the embedding space for sentiment lexicons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Twitter Sentiment Classification</head><p>Experiment Setup and Datasets. We conduct experiments on the latest Twitter sentiment clas- sification benchmark dataset in SemEval 2013 ( <ref type="bibr" target="#b30">Nakov et al., 2013</ref>). The training and develop- ment sets were completely in full to task partici- pants. However, we were unable to download all the training and development sets because some tweets were deleted or not available due to mod- ified authorization status. The test set is directly provided to the participants. The distribution of our dataset is given in <ref type="table">Table 1</ref>. We train sentiment classifier with <ref type="bibr">LibLinear (Fan et al., 2008</ref> Baseline Methods. We compare our method with the following sentiment classification algo- rithms:</p><p>(1) DistSuper: We use the 10 million tweets se- lected by positive and negative emoticons as train- ing data, and build sentiment classifier with Lib- Linear and ngram features ( <ref type="bibr" target="#b16">Go et al., 2009)</ref>.</p><p>(2) SVM: The ngram features and Support Vec- tor Machine are widely used baseline methods to build sentiment classifiers ( <ref type="bibr" target="#b33">Pang et al., 2002</ref>). Li- bLinear is used to train the SVM classifier.</p><p>(3) NBSVM: NBSVM ( <ref type="bibr" target="#b44">Wang and Manning, 2012</ref>) is a state-of-the-art performer on many sen- timent classification datasets, which trades-off be- tween Naive Bayes and NB-enhanced SVM.</p><p>(4) RAE: Recursive Autoencoder ( <ref type="bibr" target="#b36">Socher et al., 2011c</ref>) has been proven effective in many senti- ment analysis tasks by learning compositionality automatically. We run RAE with randomly initial- ized word embedding.</p><p>(5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexi- cons and many manually designed features. We re-implement this system because the codes are not publicly available <ref type="bibr">3</ref> . NRC-ngram refers to the feature set of NRC leaving out ngram features.</p><p>Except for DistSuper, other baseline method- s are conducted in a supervised manner. We do not compare with <ref type="bibr">RNTN (Socher et al., 2013b</ref>) be- cause we cannot efficiently train the RNTN model. The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases. Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the d- ifferences between domains ( <ref type="bibr" target="#b7">Blitzer et al., 2007)</ref>.</p><p>Results and Analysis. <ref type="table" target="#tab_1">Table 2</ref> shows the macro- F1 of the baseline systems as well as the SSWE- based methods on positive/negative sentimen- t classification of tweets. Distant supervision is relatively weak because the noisy-labeled tweet- s are treated as the gold standard, which affects the performance of classifier. The results of bag- of-ngram (uni/bi/tri-gram) features are not satis- fied because the one-hot word representation can- not capture the latent connections between words. NBSVM and RAE perform comparably and have a big gap in comparison with the NRC and SSWE- based methods. The reason is that RAE and NB- SVM learn the representation of tweets from the small-scale manually annotated training set, which cannot well capture the comprehensive linguistic phenomenons of words. NRC implements a variety of features and reaches 84.73% in macro-F1, verifying the impor- tance of a better feature representation for Twit- ter sentiment classification. We achieve 84.98% by using only SSWE u as features without borrow- ing any sentiment lexicons or hand-crafted rules.</p><note type="other">Method Macro-F1 DistSuper + unigram 61.74 DistSuper + uni/bi/tri-gram 63.84 SVM + unigram 74.50 SVM + uni/bi/tri-gram 75.06 NBSVM 75.28 RAE 75.12 NRC (Top System in SemEval) 84.73 NRC -ngram 84.17 SSWE u 84.98 SSWE u +NRC 86.58 SSWE u +NRC-ngram 86.48</note><p>The results indicate that SSWE u automatically learns discriminative features from massive tweets and performs comparable with the state-of-the-art manually designed features. After concatenating SSWE u with the feature set of NRC, the perfor- mance is further improved to 86.58%. We also compare SSWE u with the ngram feature by inte- grating SSWE into NRC-ngram. The concatenated features SSWE u +NRC-ngram (86.48%) outperfor- m the original feature set of NRC (84.73%).</p><p>As a reference, we apply SSWE u on subjec- tive classification of tweets, and obtain 72.17% in macro-F1 by using only SSWE u as feature. Af- ter combining SSWE u with the feature set of NR- C, we improve NRC from 74.86% to 75.39% for subjective classification.</p><p>Comparision between Different Word Embed- ding. We compare sentiment-specific word em- bedding (SSWE h , SSWE r , SSWE u ) with base- line embedding learning algorithms by only us- ing word embedding as features for Twitter sen- timent classification. We use the embedding of u- nigrams, bigrams and trigrams in the experimen- t. The embeddings of C&amp;W <ref type="bibr" target="#b8">(Collobert et al., 2011</ref>), word2vec 4 , WVSA <ref type="bibr" target="#b25">(Maas et al., 2011</ref>) and our models are trained with the same dataset and same parameter setting. We compare with C&amp;W and word2vec as they have been proved effective in many NLP tasks. The trade-off parameter of ReEmb ( <ref type="bibr" target="#b22">Labutov and Lipson, 2013</ref>) is tuned on the development set of SemEval 2013. <ref type="table" target="#tab_3">Table 3</ref> shows the performance on the pos- itive/negative classification of tweets 5 . ReEm- b(C&amp;W) and ReEmb(w2v) stand for the use of embeddings learned from 10 million distant- supervised tweets with C&amp;W and word2vec, re- spectively. Each row of <ref type="table" target="#tab_3">Table 3</ref>   From the first column of <ref type="table" target="#tab_3">Table 3</ref>, we can see that the performance of C&amp;W and word2vec are obvi- ously lower than sentiment-specific word embed- dings by only using unigram embedding as fea- tures. The reason is that C&amp;W and word2vec do not explicitly exploit the sentiment information of the text, resulting in that the words with oppo- site polarity such as good and bad are mapped to close word vectors. When such word embed- dings are fed as features to a Twitter sentimen- t classifier, the discriminative ability of sentiment words are weakened thus the classification perfor- mance is affected. Sentiment-specific word em-beddings (SSWE h , SSWE r , SSWE u ) effectively distinguish words with opposite sentiment polarity and perform best in three settings. SSWE outper- forms MVSA by exploiting more contextual infor- mation in the sentiment predictor function. SSWE outperforms ReEmb by leveraging more senti- ment information from massive distant-supervised tweets. Among three sentiment-specific word em- beddings, SSWE u captures more context informa- tion and yields best performance. SSWE h and SSWE r obtain comparative results.</p><note type="other">represents a word embedding learning algorithm. Each column s- tands for a type of embedding used to compose features of tweets. The column uni+bi denotes the use of unigram and bigram embedding, and the column uni+bi+tri indicates the use of unigram, bigram and trigram embedding. Embedding unigram uni+bi</note><p>From each row of <ref type="table" target="#tab_3">Table 3</ref>, we can see that the bigram and trigram embeddings consistently im- prove the performance of Twitter sentiment classi- fication. The underlying reason is that a phrase, which cannot be accurately represented by uni- gram embedding, is directly encoded into the n- gram embedding as an idiomatic unit. A typical case in sentiment analysis is that the composed phrase and multiword expression may have a dif- ferent sentiment polarity than the individual word- s it contains, such as not <ref type="bibr">[bad]</ref> and [great] deal of (the word in the bracket has different sentiment polarity with the ngram). A very recent study by <ref type="bibr" target="#b26">Mikolov et al. (2013)</ref> also verified the effective- ness of phrase embedding for analogically reason- ing phrases.</p><p>Effect of Î± in SSWE u We tune the hyper- parameter Î± of SSWE u on the development set by using unigram embedding as features. As given in Equation 8, Î± is the weighting score of syntac- tic loss of SSWE u and trades-off the syntactic and sentiment losses. SSWE u is trained from 10 mil- lion distant-supervised tweets.  <ref type="figure" target="#fig_1">Figure 2</ref> shows the macro-F1 of SSWE u on pos- itive/negative classification of tweets with differ- ent Î± on our development set. We can see that SSWE u performs better when Î± is in the range of [0.5, 0.6], which balances the syntactic context and sentiment information. The model with Î±=1 stands for C&amp;W model, which only encodes the syntactic contexts of words. The sharp decline at Î±=1 reflects the importance of sentiment informa- tion in learning word embedding for Twitter senti- ment classification.</p><p>Effect of Distant-supervised Data in SSWE u We investigate how the size of the distant- supervised data affects the performance of SSWE u feature for Twitter sentiment classification. We vary the number of distant-supervised tweets from 1 million to 12 million, increased by 1 million. We set the Î± of SSWE u as 0.5, according to the experiments shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Results of posi- tive/negative classification of tweets on our devel- opment set are given in <ref type="figure" target="#fig_2">Figure 3</ref>. We can see that when more distant-supervised tweets are added, the accuracy of SSWE u con- sistently improves. The underlying reason is that when more tweets are incorporated, the word em- bedding is better estimated as the vocabulary size is larger and the context and sentiment informa- tion are richer. When we have 10 million distant- supervised tweets, the SSWE u feature increases the macro-F1 of positive/negative classification of tweets to 82.94% on our development set. When we have more than 10 million tweets, the per- formance remains stable as the contexts of words have been mostly covered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Similarity of Sentiment Lexicons</head><p>The quality of SSWE has been implicitly evaluat- ed when applied in Twitter sentiment classification in the previous subsection. We explicitly evaluate it in this section through word similarity in the em-bedding space for sentiment lexicons. The evalua- tion metric is the accuracy of polarity consistency between each sentiment word and its top N closest words in the sentiment lexicon,</p><formula xml:id="formula_10">Accuracy = #Lex i=1 N j=1 Î²(w i , c ij ) #Lex Ã— N (10)</formula><p>where #Lex is the number of words in the senti- ment lexicon, w i is the i-th word in the lexicon, c ij is the j-th closest word to w i in the lexicon with co- sine similarity, Î²(w i , c ij ) is an indicator function that is equal to 1 if w i and c ij have the same sen- timent polarity and 0 for the opposite case. The higher accuracy refers to a better polarity consis- tency of words in the sentiment lexicon. We set N as 100 in our experiment.  <ref type="table">Table 4</ref>: Statistics of the sentiment lexicons. Join- t stands for the words that occur in both HL and MPQA with the same sentiment polarity.</p><p>Results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose learning continuous word representations as features for Twitter sen- timent classification under a supervised learning framework. We show that the word embedding learned by traditional neural networks are not ef- fective enough for Twitter sentiment classification. These methods typically only model the contex- t information of words so that they cannot dis- tinguish words with similar context but opposite sentiment polarity (e.g. good and bad). We learn sentiment-specific word embedding (SSWE) by integrating the sentiment information into the loss functions of three neural networks. We train SS- WE with massive distant-supervised tweets select- ed by positive and negative emoticons. The ef- fectiveness of SSWE has been implicitly evaluat- ed by using it as features in sentiment classifica- tion on the benchmark dataset in SemEval 2013, and explicitly verified by measuring word similar- ity in the embedding space for sentiment lexicon- s. Our unified model combining syntactic context of words and sentiment information of sentences yields the best performance in both experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The traditional C&amp;W model and our neural networks (SSWE h and SSWE u ) for learning sentiment-specific word embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Macro-F1 of SSWE u on the development set of SemEval 2013 with different Î±.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Macro-F1 of SSWE u with different size of distant-supervised data on our development set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>) on the training set, tune parameter âˆ’c on the dev set and evaluate on the test set. Evaluation metric is the Macro-F1 of positive and negative categories 2 .</figDesc><table>Positive Negative Neutral Total 
Train 
2,642 
994 
3,436 
7,072 
Dev 
408 
219 
493 
1,120 
Test 
1,570 
601 
1,639 
3,810 

Table 1: Statistics of the SemEval 2013 Twitter 
sentiment classification dataset. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Macro-F1 on positive/negative classifica-
tion of tweets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Macro-F1 on positive/negative classifica-
tion of tweets with different word embeddings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 shows</head><label>5</label><figDesc></figDesc><table>our results com-
pared to other word embedding learning al-
gorithms. The accuracy of random result is 
50% as positive and negative words are ran-
domly occurred in the nearest neighbors of 
each word. Sentiment-specific word embed-
dings (SSWE h , SSWE r , SSWE u ) outperform ex-
isting neural models (C&amp;W, word2vec) by large 
margins. SSWE u performs best in three lexicon-
s. SSWE h and SSWE r have comparable perfor-
mances. Experimental results further demonstrate 
that sentiment-specific word embeddings are able 
to capture the sentiment information of texts and 
distinguish words with opposite sentiment polari-
ty, which are not well solved in traditional neural 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy of the polarity consistency of 
words in different sentiment lexicons. 

models like C&amp;W and word2vec. SSWE outper-
forms MVSA and ReEmb by exploiting more con-
text information of words and sentiment informa-
tion of sentences, respectively. 

</table></figure>

			<note place="foot">t of text. This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors. We address this issue by learning sentimentspecific word embedding (SSWE), which encodes sentiment information in the continuous representation of words. Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g. sentences or tweets) in their loss functions. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.</note>

			<note place="foot" n="1"> We use the emoticons selected by Hu et al. (2013). The positive emoticons are :) : ) :-) :D =), and the negative emoticons are :( : ( :-( .</note>

			<note place="foot" n="2"> We investigate 2-class Twitter sentiment classification (positive/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013.</note>

			<note place="foot" n="3"> For 3-class sentiment classification in SemEval 2013, our re-implementation of NRC achieved 68.3%, 0.7% lower than NRC (69%) due to less training data.</note>

			<note place="foot" n="4"> Available at https://code.google.com/p/word2vec/. We utilize the Skip-gram model because it performs better than CBOW in our experiments. 5 MVSA and ReEmb are not suitable for learning bigram and trigram embedding because their sentiment predictor functions only utilize the unigram embedding.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Yajuan Duan, Shujie Liu, Zhenghua Li, Li Dong, Hong Sun and Lanjun Zhou for their great help. This research was partly supported by National Natural Science Foundation of <ref type="bibr">China (No.61133012, No.61273321, No.61300113</ref>).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust sentiment detection on twitter from biased and noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junlan</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Linguistics</title>
		<meeting>International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="36" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">RÃ©jean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arX- iv:1305.0445</idno>
		<title level="m">Deep learning of representations: Looking forward</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentiment classification based on supervised latent n-gram analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bespalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shokoufandeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Information and Knowledge Management</title>
		<meeting>the Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sentiment classification with supervised sequence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bespalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Shokoufandeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="159" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent dirichlet allocation. the Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enhanced sentiment learning using twitter hashtags and smileys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Linguistics</title>
		<meeting>International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="241" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A holistic lexicon-based approach to opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Web Search and Data Mining</title>
		<meeting>the International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Techniques and applications for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="82" to="89" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for twitter: Annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="12" />
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">CS224N Project Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The role of syntax in vector space models of compositional semantics</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="894" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the tenth ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised sentiment analysis with emotional signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiji</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International World Wide Web Conference</title>
		<meeting>the International World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="607" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceeding of Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Twitter sentiment analysis: The good the bad and the omg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efthymios</forename><surname>Kouloumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International AAAI Conference on Weblogs and Social Media</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Re-embedding words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emoticon smoothed language models for twitter sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kun-Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu-Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Composition in distributional models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1388" to="1429" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nrc-canada: Building the state-ofthe-art in sentiment analysis of tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation</title>
		<meeting>the International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 2: Sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Semantic Evaluation</title>
		<meeting>the International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Twitter as a corpus for sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Paroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Language Resources and Evaluation Conference</title>
		<meeting>Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and trends in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic Compositionality Through Recursive Matrix-Vector Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Lexiconbased methods for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maite</forename><surname>Taboada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Brooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Tofiloski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Voll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="307" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sentiment strength detection for the social web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevan</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paltoglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter D Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Compositional matrix-space models for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ainur</forename><surname>Yessenalina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Moodlens: an emoticon-based sentiment analysis system for chinese tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep learning for chinese word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
