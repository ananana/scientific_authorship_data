<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Type-Sensitive Knowledge Base Inference Without Explicit Type Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-20, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prachi</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Delhi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Delhi</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
							<email>soumen@cse.iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Delhi</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Bombay</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Type-Sensitive Knowledge Base Inference Without Explicit Type Supervision</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="75" to="80"/>
							<date type="published">July 15-20, 2018</date>
						</imprint>
					</monogr>
					<note>75</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>State-of-the-art knowledge base completion (KBC) models predict a score for every known or unknown fact via a latent factorization over entity and relation em-beddings. We observe that when they fail, they often make entity predictions that are incompatible with the type required by the relation. In response, we enhance each base factorization with two type-compatibility terms between entity-relation pairs, and combine the signals in a novel manner. Without explicit supervision from a type catalog, our proposed modification obtains up to 7% MRR gains over base models, and new state-of-the-art results on several datasets. Further analysis reveals that our models better represent the latent types of entities and their embed-dings also predict supervised types better than the embeddings learned by baseline models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge bases (KBs) store facts in the form of relations (r) between subject entity (s) and object entity (o), e.g., Obama, born-in, Hawaii. Since KBs are typically incomplete <ref type="bibr" target="#b0">(Bollacker et al., 2008)</ref>, the task of KB Completion (KBC) attempts to infer new tuples from a given KB. Neural ap- proaches to KBC, e.g., <ref type="bibr">Complex (Trouillon et al., 2016</ref>) and DistMult ( <ref type="bibr">Yang et al., 2015)</ref>, calculate the score f (s, r, o) of a tuple (s, r, o) via a latent factorization over entity and relation embeddings, and use these scores to predict the validity of an unseen tuple.</p><p>A model is evaluated over queries of the form s * , r * , ?. It ranks all entities o in the descend- *Equal contribution.</p><p>ing order of tuple scores f (s * , r * , o), and credit is assigned based on the rank of gold entity o * . Our preliminary analysis of DistMult (DM) and Com- plex (CX) reveals that they make frequent errors by ranking entities that are not compatible with types expected as arguments of r * high. In 19.5% of predictions made by DM on FB15K, the top prediction has a type different from what is ex- pected (see <ref type="table" target="#tab_0">Table 1</ref> for illustrative examples).</p><p>In response, we propose a modification to base models (DM, Complex) by explicitly mod- eling type compatibility. Our modified func- tion f (s, r, o) is the product of three terms: the original tuple score f (s, r, o), subject type- compatibility between r and s, and object type- compatibility between r and o. Our type-sensitive models, TypeDM and TypeComplex, do not ex- pect any additional type-specific supervision - they induce all embeddings using only the origi- nal KB.</p><p>Experiments over three datasets show that all typed models outperform base models by signif- icant margins, obtaining new state-of-the-art re- sults in several cases. We perform additional anal- yses to assess if the learned embeddings indeed capture the type information well. We find that embeddings from typed models can predict known symbolic types better than base models.</p><p>Finally, we note that an older model called E ( <ref type="bibr">Riedel et al., 2013)</ref> can be seen as modeling type compatibilities. Moreover, previous work has explored additive combinations of DM and E ( <ref type="bibr">Garcia-Duran et al., 2015b;</ref><ref type="bibr">Toutanova and Chen, 2015)</ref>. We directly compare against these mod- els and find that, our proposal outperforms both E, DM and their linear combinations.</p><p>We contribute open-source implementations 1 of all models and experiments discussed in this paper for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>We are given an incomplete KB with entities E and relations R. The KB also contains T = {{s, r, o}, a set of known valid tuples, each with subject and object entities s, o ∈ E, and relation r ∈ R. Our goal is to predict the validity of any tuple not present in T . Popular top performing models for this task are <ref type="bibr">Complex and DM.</ref> In Complex, each entity e (resp., relation r) is represented as a complex vector a a a e ∈ C D (resp.,</p><formula xml:id="formula_0">b b b r ∈ C D ). Tuple score f CX (s, r, o) = D d=1 a sd b rd a</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TypeDM and TypeComplex</head><p>Representation: We start with DM as the base model; the Complex case is identical. The first key modification (see <ref type="figure" target="#fig_0">Figure 1</ref>) is that each entity e is now represented by two vectors: u u u e ∈ R K to encode type information, and a a a e ∈ R D to encode information. Typically, K D . The second, concomitant modification is that each relation r is now associated with three vectors: b b b r ∈ R D as before, and also v v v r , w w w r ∈ R K . v v v r and w w w r encode the expected types for subject and object entities. An ideal way to train type embeddings would be to provide canonical type signatures for each relation and entity. Unfortunately, these aspects of realistic KBs are themselves incomplete <ref type="bibr">(Neelakantan and Chang, 2015;</ref><ref type="bibr">Murty et al., 2018)</ref>. Our models train all embeddings using T only and don't rely on any explicit type supervision.</p><p>DM uses (E + R)D model weights for a KB with R relations and E entities, whereas TypeDM uses E(D +K)+R(D +2K). To make compar- isons fair, we set D and K so that the total number of model weights (real or complex) are about the same for base and typed models. Prediction: DM's base prediction score for tu- ple (s, r, o) is a a a s , b b b r , a a a o . We apply a (sigmoid) nonlinearity:</p><formula xml:id="formula_1">f (s, r, o) = σ(a a a s , b b b r , a a a o ),<label>(1)</label></formula><p>and then combine with two additional terms that measure type compatibility between the subject and the relation, and the object and the relation:</p><formula xml:id="formula_2">f (s, r, o) = f (s, r, o) C v v v (s, r) C w w w (o, r), (2)</formula><p>where C x x x (e, r) is a function that measures the compatibility between the type embedding of e for a given argument slot of r:</p><formula xml:id="formula_3">C x x x (e, r) = σ(x x x r · u u u e ) (3)</formula><p>If each of the three terms in Equation 2 is inter- preted as a probability, f (s, r, o) corresponds to a simple logical AND of the three conditions.</p><p>We want f (s, r, o) to be almost 1 for positive instances (tuples known to be in the KG) and close to 0 for negative instances (tuples not in the KG). For a negative instance, one or more of the three terms may be near zero. There is no guidance to the learner on which term to drive down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subject s Relation r Gold Object o Prediction 1 Prediction 2 Howard Leslie Shore follows-religion Jewism (religion) Walk Hard (film) 21 Jump Street (film) Spyglass Entertainment headquarter-located-in El lay (location) The Real World (tv) Contraband (film) Les Fradkin born-in-location New York (location) Federico Fellini (person) Louie De palma (person) Eugene Alden Hackman studied</head><p>Rural Journalism (education) Loudon Snowden Wainwright III (person) The Bourne Legacy (film) Chief Phillips (film) released-in-region Yankee land (location) Akira Isida (person) Presidential Medal of Freedom (award) </p><formula xml:id="formula_4">Pr(o|s, r) = exp(βf (s, r, o)) o exp(βf (s, r, o ))<label>(4)</label></formula><p>Because f ∈ [0, 1] for typed models, we scale it with a hyper-parameter β &gt; 0 (a form of in- verse temperature) to allow Pr(o|s, r) to take val- ues over the full range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> in loss minimization.</p><p>The sum over o in the denominator is sampled based on contrastive sampling, so the left hand side is not a formal probability (exactly as in DM). A similar term is added for Pr(s|r, o). The log- likelihood loss minimizes:</p><formula xml:id="formula_5">− s,r,o∈P</formula><p>log P r(o|s, r; θ) + log P r(s|o, r; θ)</p><p>The summation is over P which is the set of all positive facts. Following <ref type="bibr">Trouillon et al. (2016)</ref>, we also implement the logistic loss s,r,o∈T</p><formula xml:id="formula_7">log 1 + e −Ysrof (s,r,o)<label>(6)</label></formula><p>Here Y sro is 1 if the fact (s, r, o) is true and −1 otherwise. Also, T is the set of all positive facts along with the negative samples. With logis- tic loss, model weights θ are L2-regularized and gradient norm is clipped at 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets: We evaluate on three standard data sets, FB15K, FB15K-237, and YAGO3-10 ( <ref type="bibr" target="#b1">Bordes et al., 2013;</ref><ref type="bibr">Toutanova et al., 2015;</ref><ref type="bibr" target="#b2">Dettmers et al., 2017)</ref>. We retain the exact train, dev and test folds used in previous works. TypeDM and TypeComplex are competitive on the WN18 data set ( <ref type="bibr" target="#b1">Bordes et al., 2013</ref>), but we omit those results, as WN18 has 18 very generic relations (e.g., hy- ponym, hypernym, antonym, meronym), which do not give enough evidence for inducing types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Embedding   all the typed models. To balance total model sizes <ref type="table" target="#tab_2">(Table 2)</ref>, we choose K = 19 dimensions for u u u e , v v v r , w w w r and 180 dimensions for a a a e , b b b r 2 .</p><p>Typed models and E perform best with 400 neg- ative samples per positive tuple while using log- likelihood loss (robust to a larger number of neg- ative facts as opposed to logistic loss, which falls for class imbalance). FB15K and YAGO3-10 use L2 regularization coefficient of 2.0, and it is 5.0 for FB15K-237. Note that the L2 regularization penalty is applied to only those entities and rela- tions that are a part of that batch update, as pro- posed by <ref type="bibr">Trouillon et al. (2016)</ref>. β is set to 20.0 for the typed models, and 1.0 for other models if they use the log-likelihood loss. Entity embeddings are unit normalized at the end of every epoch, for the type models. Also, we find that in TypeDM scal- ing the embeddings of the base model to unit norm performs better than using L2 regularization.</p><p>Results: <ref type="table" target="#tab_3">Table 3</ref> shows that TypeDM and Type- Complex dominate across all data sets. E by it- self is understandably weak, and DM+E does not lift it much. Each typed model improves upon the corresponding base model on all measures, under- scoring the value of type compatibility scores. <ref type="bibr">3</ref> To the best of our knowledge, the results of our typed models are competitive with various reported re- sults for models of similar sizes that do not use any additional information, e.g., soft rules ( <ref type="bibr">Guo et al., 2018)</ref>, or textual corpora ( <ref type="bibr">Toutanova et al., 2015</ref>).</p><p>We also compare against the heuristic genera-tion of type-sensitive negative samples ( <ref type="bibr">Krompaß et al., 2015)</ref>. For this experiment, we train a Com- plex model using this heuristically generated nega- tive set, and use standard evaluation, as in all other models. We find that all the models reported in <ref type="table" target="#tab_3">Ta- ble 3</ref> outperform this approach. </p><formula xml:id="formula_8">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis of Typed Embeddings</head><p>We perform two further analyses to assess whether the embeddings produced by typed models indeed capture type information better. For these exper- iments, we try to correlate (and predict) known symbolic types of an entity using the unsupervised embeddings produced by the models. We take a fine catalog of most frequent 90 freebase types over the 14,951 entities in the FB15k dataset <ref type="bibr">(Xie et al., 2016)</ref>. We exclude /common/topic as it occurs with most entities. On an average each entity has 12 associated types.  belong to one of the 5 types (people, location, organization, film, and sports) from the freebase dataset. These cover 84.88% of FB15K entities. We plot the FB15K entities e using the PCA pro- jection of u u u e and a a a e in <ref type="figure" target="#fig_1">Figure 2</ref>, color-coding their types. We observe that u u u e separates the type clus- ters better than a a a e , suggesting that u u u e vectors in- deed collect type information. We also perform k-means clustering of u u u e and a a a e embeddings of these entities, as available from different models. We report cluster homogeneity and completeness scores <ref type="bibr">(Rosenberg and Hirschberg, 2007</ref>) in Ta- ble 4. Typed models yield superior clusters.</p><p>2. Prediction of Symbolic Types: We train a single-layer network that inputs embeddings from various models and predicts a set of symbolic types from the KB. This tells us the extent to which the embeddings capture KB type informa- tion (that was not provided explicitly during train- ing). <ref type="table" target="#tab_5">Table 4</ref> reports average macro F1 score (5- fold cross validation). Embeddings from TypeDM and TypeComplex are generally better predictors than embeddings learned by Complex, DM and E. u u u e ∈ R 19 is often better than a a a e ∈ R 180 or more, for typed models. DM+E with 199 model weights narrowly beats TypeDM with 19 weights, but re- call that it has poorer KBC scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We propose an unsupervised typing gadget, which enhances top-of-the-line base models for KBC (DistMult, Complex) with two type-compatibility functions, one between r and s and another be- tween r and o. Without explicit supervision from any type catalog, our typed variants (with simi- lar number of parameters as base models) substan- tially outperform base models, obtaining up to 7% MRR improvements and over 10% improvements in the correctness of the top result. To confirm that our models capture type information better, we correlate the embeddings learned without type su- pervision with existing type catalogs. We find that our embeddings indeed separate and predict types better. In future work, combining type-sensitive embeddings with a focus on less frequent relations ( <ref type="bibr">Xie et al., 2017)</ref>, more frequent entities <ref type="bibr" target="#b2">(Dettmers et al., 2017)</ref>, or side information such as inference rules ( <ref type="bibr">Guo et al., 2018;</ref><ref type="bibr">Jain and Mausam, 2016)</ref> or textual corpora ( <ref type="bibr">Toutanova et al., 2015</ref>) may fur- ther increase KBC accuracy. It may also be of in- terest to integrate the typing approach here with the combinations of tensor and matrix factoriza- tion models for KBC ( <ref type="bibr">Jain et al., 2018)</ref>.</p><p>ing two and three-way embeddings models for link prediction in knowledge bases. arXiv preprint arXiv:1506.00999 https://arxiv.org/pdf/1506.00999. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: TypeDM and TypeComplex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Projection of vectors representing entities belonging to frequent KB types{people, location, organisation, film, sports}: a: TypeDM,u u u e ; b: TypeDM,a a a e ; c: TypeComplex,u u u e ; d: DM,a a a e .</figDesc><graphic url="image-3.png" coords="4,306.99,387.94,112.42,84.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Samples of top two DM predictions (having inconsistent types) on FB15K. TypeDM predicts entities of the correct type in top positions in the corresponding examples.</head><label>1</label><figDesc></figDesc><table>Contrastive Sampling: Training data consist of 
positive gold tuples (s, r, o) and negative tuples, 
which are obtained by perturbing each positive tu-
ple by replacing either s or o with a randomly 
sampled s or o . This offers the learning algo-
rithm positive and negative instances. The models 
are trained such that observed tuples have higher 
scores than unobserved ones. 
Loss Functions: We implement two common 
loss objectives. The log-likelihood loss first com-
putes the probability of predicting a response o for 
a query (s, r, ?) as follows: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>models we first perform hyperparam- eter search for size of type embeddings (K) such that total entity embedding size remains 200. We get the best results at K = 20, from among val- ues in {10, 20, 30, 50, 80, 100, 120}. This hyper- parameter search is done for the TypeDM model (which is faster to train than TypeComplex) on FB15k dataset, and the selected split is used for</figDesc><table>Sizes were approximately balanced be-
tween base and typed models (FB15K). 

Metrics: As is common, we regard test instances 
(s, r, ?) as a task of ranking o, with gold o  *  known. 
We report MRR (Mean Reciprocal Rank) and the 
fraction of queries where o  *  is recalled within 
rank 1 and rank 10 (HITS). The filtered evaluation 
(Garcia-Duran et al., 2015a) removes valid train 
or test tuples ranking above (s, r, o  *  ) for scoring 
purposes. 
Hyperparameters: We run AdaGrad for up to 
1000 epochs for all losses, with early stopping on 
the dev fold to prevent overfitting. All the mod-
els generally converge after 300-400 epochs, ex-
cept TypeDM that exhausts 1000 epochs. E, DM, 
DM+E and Complex use 200 dimensional vectors. 
All except E perform best with logistic loss and 
20 negative samples (obtained by randomly cor-
rupting s and r) per positive fact. This is deter-
mined by doing a hyperparameter search on a set 
{10, 20, 50, 100, 200, 400}. 
For typed </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>KBC performance for base, typed, and related formulations. Typed models outperform their 
base models across all datasets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>1 . Clustering Entity/Type Embeddings: For this experiment we subselect entities in FB15k that</head><label>1</label><figDesc></figDesc><table>Method 
Embed Size 
H 
C Type 
-ding 
F1 
TypeDM 
u u ue 
19 
66.72 66.29 81.77 
TypeDM 
a a ae 
180 
57.89 59.67 75.96 
TypeDM 
Both 
199 
66.75 66.29 82.57 
DM 
a a ae 
200 
51.40 48.12 81.34 
TypeComplex 
u u ue 
19 
65.90 62.97 82.70 
TypeComplex 
a a ae 
180x2 50.76 48.57 74.75 
TypeComplex Both 
379 
66.03 63.09 84.14 
Complex 
a a ae 
200x2 51.56 47.20 81.58 
DM+E 
u u ue 
19 
0.48 
2.05 74.66 
DM+E 
a a ae 
180 
49.62 47.24 82.72 
DM+E 
Both 
199 
49.66 47.26 82.68 
E 
a a ae 
200 
39.83 37.62 74.23 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Interpretation of embeddings wrt super-
vised types: cluster homogeneity H, completeness 
C, and type prediction F1 score. 

</table></figure>

			<note place="foot" n="1"> https://github.com/dair-iitd/KBI</note>

			<note place="foot" n="2"> Notice that a typed model has a slightly higher number of parameters for relation embeddings, because it needs to maintain two type embeddings of size K, over and above b b br. Using K = 19 reduced and brought the total number of parameters closer to that of the base model, for a fair direct comparison. The model performance did not differ by much when using either of the options (i.e., K = 19 or 20). 3 For direct comparisons with published work, we choose 200 and 400 parameters per entity for DM and Complex respectively (Complex model has two 200 dimensional embeddings per entity). DM and TypeDM, on increasing the dimensionality to 400, yield MRR scores of 69.79 and 78.91, respectively, for FB15K.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by Google language un-derstanding and knowledge discovery focused re-search grants, a Bloomberg award, an IBM SUR award, and a Visvesvaraya faculty award by Govt. of India to the third author. It is also supported by a TCS Fellowship to the first author. We thank Microsoft Azure sponsorships, IIT Delhi HPC fa-cility and the support of nVidia Corporation for computational resources.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" />
	</analytic>
	<monogr>
		<title level="m">NIPS Conference</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01476</idno>
		<title level="m">Convolutional 2d knowledge graph embeddings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Composing relationships with translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D15-1034" />
	</analytic>
	<monogr>
		<title level="m">EMNLP Conference</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="286" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<title level="m">Antoine Bordes, Nicolas Usunier, and Yves Grandvalet. 2015b. Combin</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
