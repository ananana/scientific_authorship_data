<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Adaptation for Authorship Attribution: Improved Structural Correspondence Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upendra</forename><surname>Sapkota</surname></persName>
							<email>upendra@uab.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Optica y Electrónica</orgName>
								<orgName type="institution" key="instit1">University of Alabama at Birmingham</orgName>
								<orgName type="institution" key="instit2">University of Houston</orgName>
								<orgName type="institution" key="instit3">Instituto Nacional de Astrofísica</orgName>
								<orgName type="institution" key="instit4">University of Alabama at Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
							<email>solorio@cs.uh.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Optica y Electrónica</orgName>
								<orgName type="institution" key="instit1">University of Alabama at Birmingham</orgName>
								<orgName type="institution" key="instit2">University of Houston</orgName>
								<orgName type="institution" key="instit3">Instituto Nacional de Astrofísica</orgName>
								<orgName type="institution" key="instit4">University of Alabama at Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes-Y-Gómez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Optica y Electrónica</orgName>
								<orgName type="institution" key="instit1">University of Alabama at Birmingham</orgName>
								<orgName type="institution" key="instit2">University of Houston</orgName>
								<orgName type="institution" key="instit3">Instituto Nacional de Astrofísica</orgName>
								<orgName type="institution" key="instit4">University of Alabama at Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
							<email>bethard@uab.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Optica y Electrónica</orgName>
								<orgName type="institution" key="instit1">University of Alabama at Birmingham</orgName>
								<orgName type="institution" key="instit2">University of Houston</orgName>
								<orgName type="institution" key="instit3">Instituto Nacional de Astrofísica</orgName>
								<orgName type="institution" key="instit4">University of Alabama at Birmingham</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Adaptation for Authorship Attribution: Improved Structural Correspondence Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2226" to="2235"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present the first domain adaptation model for authorship attribution to leverage unlabeled data. The model includes extensions to structural correspondence learning needed to make it appropriate for the task. For example, we propose a median-based classification instead of the standard binary classification used in previous work. Our results show that punctuation-based character n-grams form excellent pivot features. We also show how singular value decomposition plays a critical role in achieving domain adaptation, and that replacing (in-stead of concatenating) non-pivot features with correspondence features yields better performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Authorship Attribution (AA) can be used for his- torical purposes, such as disentangling the differ- ent authors contributing to a literary work. It can also help in understanding language evolution and change at the individual level, revealing a writer's changes in linguistic patterns over time <ref type="bibr" target="#b5">(Hirst and Feng, 2012)</ref>. Authorship attribution can also help to settle disputes over the original creators of a given piece of text. Or it can help build a prose- cution case against an online abuser, an important application especially considering the rising trends in cyber-bullying and other electronic forms of teen violence <ref type="bibr">1</ref> . The absorbing social media networks, together with the ever increasing use of electronic communications will require robust approaches to authorship attribution that can help to determine with certainty the author of a text, determine the provenance of a written sample, and in sum, help us determine the trustworthiness of electronic data.</p><p>1 http://cyberbullying.org/ One of the scenarios that has received limited attention is cross-domain authorship attribution, when we need to identify the author of a text but all the text with known authors is from a differ- ent topic, genre, or modality. Here we propose to solve the problem of cross-domain authorship attri- bution by adapting the Structural Correspondence Learning (SCL) algorithm proposed by <ref type="bibr" target="#b1">Blitzer et al. (2006)</ref>. We make the following contributions:</p><p>• We introduce the first domain adaptation model for authorship attribution that combines labeled data in a source domain with unla- beled data from a target domain to improve performance on the target domain.</p><p>• We examine two sets of features that have previously been successful in cross-domain authorship attribution, explain how these can be used to select the "pivot" features required by SCL, and show that typed n-gram features (which differentiate between the the in their and the the in breathe) produce simpler mod- els that are just as accurate.</p><p>• We propose a new approach for defining SCL's pivot feature classification task so that it is able to handle count-based features, and show that this median-based approach outper- forms the standard SCL approach.</p><p>• We examine the importance of the dimension- ality reduction step in SCL, and show that the singular value decomposition increases robust- ness even beyond the robustness achieved by SCL's learned feature transformations.</p><p>• We propose an alternative approach to com- bining features within SCL, and show that ex- cluding the non-pivot features from the final classifier generally improves performance.</p><p>Our experimental results show that using stan- dard SCL for this domain adaptation authorship attribution task improves prediction accuracy by only 1% over a model without any domain adap- tation. In contrast, our proposed improvements to SCL reach an accuracy boost of more than 15% over the no domain adaptation model and of 14% over the standard SCL formulation. The extensions to SCL that we propose in this work are likely to yield performance improvements in other tasks where SCL has been successfully applied, such as part-of-speech tagging and sentiment analysis. We plan to investigate this further in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Cross-Domain Authorship Attribution Al- most all previous authorship attribution studies have tackled traditional (single-domain) authorship problems where the distribution of the test data is the same as that of the training data ( <ref type="bibr" target="#b8">Madigan et al., 2005;</ref><ref type="bibr" target="#b15">Stamatatos, 2006;</ref><ref type="bibr" target="#b7">Luyckx and Daelemans, 2008;</ref><ref type="bibr" target="#b3">Escalante et al., 2011</ref>). However, there are a handful of authorship attribution studies that explore cross-domain authorship attribution scenar- ios ( <ref type="bibr" target="#b9">Mikros and Argiri, 2007;</ref><ref type="bibr" target="#b4">Goldstein-Stewart et al., 2009;</ref><ref type="bibr" target="#b13">Schein et al., 2010;</ref><ref type="bibr" target="#b16">Stamatatos, 2013;</ref><ref type="bibr" target="#b11">Sapkota et al., 2014</ref>). Here, following prior work, cross-domain is a cover term for cross-topic, cross-genre, cross-modality, etc., though most work focuses on the cross-topic scenario. <ref type="bibr" target="#b9">Mikros and Argiri (2007)</ref> illustrated that many stylometric variables are actually discriminating topic rather than author. Therefore, the authors suggest their use in authorship attribution should be done with care. However, the study did not attempt to construct authorship attribution models where the source and target domains differ. <ref type="bibr" target="#b4">Goldstein-Stewart et al. (2009)</ref> performed a study on cross-topic authorship attribution by con- catenating the texts of an author from different gen- res on the same topics. Such concatenation allows some cross-topic analysis, but as each test docu- ment contains a mix of genres it is not representa- tive of real world authorship attribution problems.</p><p>Stamatatos (2013) and <ref type="bibr" target="#b11">Sapkota et al. (2014)</ref> ex- plored a wide variety of features, including lexi- cal, stopword, stylistic, and character n-gram, and demonstrated that character n-grams are the most effective features in cross-topic authorship attri- bution. <ref type="bibr" target="#b16">Stamatatos (2013)</ref> concluded that avoid- ing rare features is effective in both intra-topic and cross-topic authorship attribution by training a SVM classifier on one fixed topic and testing on each of the remaining topics. <ref type="bibr" target="#b11">Sapkota et al. (2014)</ref>, rather than fixing a single training topic in advance, considered all possible training/testing topic combinations to investigate cross-topic au- thorship attribution. This showed that training on documents from multiple topics (thematic areas) improves performance in cross-topic authorship attribution ( <ref type="bibr" target="#b11">Sapkota et al., 2014</ref>), even when con- trolling the amount of training data.</p><p>However, none of these studies exploited domain adaptation methods that combine labeled data in a source domain with unlabeled data from a target domain to improve performance on the target do- main. Instead, they focused on identifying relevant features and simply evaluating them when trained on source-domain data and tested on target-domain data. To our knowledge, we are the first to leverage unlabeled data from the target domain to improve authorship attribution.</p><p>Domain Adaptation Domain adaptation is the problem of modifying a model trained on data from a source domain to a different, possibly related, tar- get domain. Given the effort and the cost involved in labeling data for a new target domain, there is a lot of interest in the design of domain adaptation techniques. In NLP related tasks, researchers have explored domain adaptation for part-of-speech tag- ging, parsing, semantic role labeling, word-sense disambiguation, and sentiment analysis <ref type="bibr" target="#b6">(Li, 2012)</ref>. <ref type="bibr" target="#b2">Daumé (2007)</ref> proposed a feature space trans- formation method for domain adaptation based on a simple idea of feature augmentation. The ba- sic idea is to create three versions of each feature from the original problem: the general (domain- independent) version, the source specific version, and the target specific version. While generally suc- cessful, there are some limitations of this method. First, it requires labeled instances in the target do- main. Second, since this method simply dupli- cates each feature in the source domain as domain- independent and domain-specific versions, it is un- able to extract the potential correlations when the features in the two domains are different, but have some hidden correspondences.</p><p>In contrast, structural correspondence learning (SCL) is a feature space transformation method that requires no labeled instances from the tar- get domain, and can capture the hidden correla- tions among different domain-independent features. SCL's basic idea is to use unlabeled data from both the source and target domains to obtain a common feature representation that is meaningful across domains ( <ref type="bibr" target="#b1">Blitzer et al., 2006</ref>). Although the dis- tributions of source and target domain differ, the assumption is that there will still be some general features that share similar characteristics in both domains. SCL has been applied to tasks such as sentiment analysis, dependency parsing, and part- of-speech tagging, but has not yet been explored for the problem of authorship attribution.</p><p>The common feature representation in SCL is created by learning a projection to "pivot" features from all other features. These pivot features are a critical component of the successful use of SCL, and their selection is something that has to be done carefully and specifically to the task at the hand. Tan and Cheng (2009) studied sentiment analy- sis, using frequently occurring sentiment words as pivot features. Similarly, <ref type="bibr" target="#b20">Zhang et al. (2010)</ref> pro- posed a simple and efficient method for selecting pivot features in domain adaptive sentiment analy- sis: choose the frequently occurring words or word- bigrams among domains computed after applying some selection criterion. In dependency parsing, <ref type="bibr" target="#b14">Shimizu and Nakagawa (2007)</ref> chose the presence of a preposition, a determiner, or a helping verb between two tokens as the pivot features. For part- of-speech tagging, <ref type="bibr" target="#b1">Blitzer et al. (2006)</ref> used words that occur more than 50 times in both domains as the pivot features, resulting in mostly function words. In cross-lingual adaptation using SCL, se- mantically related pairs of words from source and target domains were used as pivot features <ref type="bibr" target="#b10">(Prettenhofer and Stein, 2011</ref>). For authorship attribution, we propose two ways of selecting pivot and non- pivot features based on character n-grams.</p><p>Another important aspect of the SCL algorithm is associating a binary classification problem with each pivot feature. The original SCL algorithm assumes that pivot features are binary-valued, so creating a binary classification problem for each pivot feature is trivial: is the value 0 or 1? Most pre- vious work on part-of-speech tagging, sentiment analysis, and dependency parsing also had only binary-valued pivot features. However, for author- ship attribution, all features are count-based, so translation from a pivot feature value to a binary classification problem is not trivial. We propose a median-based solution to this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Structural Correspondence <ref type="bibr">Learning (Blitzer et al., 2006</ref>) uses only unlabeled data to find a common feature representation for a source and a target do- main. The idea is to first manually identify "pivot" features that are likely to have similar behavior across both domains. SCL then learns a transfor- mation from the remaining non-pivot features into the pivot feature space. The result is a new set of features that are derived from all the non-pivot fea- tures, but should be domain independent like the pivot features. A classifier is then trained on the combination of the original and the new features. <ref type="table" target="#tab_0">Table 1</ref> gives the details of the SCL algorithm. First, for each pivot feature, we train a linear clas- sifier to predict the value of that pivot feature using only the non-pivot features. The weight vectors learned for these linear classifiers, ˆ w i , are then con- catenated into a matrix, W , which represents a projection from non-pivot features to pivot features. Singular value decomposition is used to reduce the dimensionality of the projection matrix, yield- ing a reduced-dimensionality projection matrix θ. Finally, a classifier is trained on the combination of the original features and the features generated by applying the reduced-dimensionality projection matrix θ to the non-pivot features x [p:m] .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Standard SCL parameter definitions</head><p>Standard SCL does not define how pivot features are selected; this must be done manually for each new task. However, SCL does provide standard definitions for the loss function (L), the conver- sion to binary values (B i ), the dimensionality of the new correspondence space (d), and the feature combination function (C).</p><p>L is defined as Huber's robust loss:</p><formula xml:id="formula_0">L(a, b) = max(0, 1 − ab) 2 if ab ≥ −1 −4ab otherwise</formula><p>The conversion from pivot feature values to binary classification is defined as:</p><formula xml:id="formula_1">B i (y) = 1 if y &gt; 0 0 otherwise</formula><p>A few different dimensionalities for the reduced feature space have been explored (Prettenhofer and Stein, 2011), but most implementations have fol- lowed the standard SCL description ( <ref type="bibr" target="#b1">Blitzer et al., 2006</ref>) with d defined as:</p><formula xml:id="formula_2">d = 25</formula><p>The feature combination function, C, is defined as simple concatenation, i.e., use all of the old pivot Input:</p><p>• S = {x : x ∈ R m }, the labeled instances from source domain • U = {x : x ∈ R m }, the unlabeled instances from both domains • p and n such that x <ref type="bibr">[0:p]</ref> are the p pivot features and x <ref type="bibr">[p:m]</ref> are n = m − p non-pivot features • f : S → A, the source domain labels, where A is the set of authors • L : R × R → R, a loss function • B i : R → {0, 1} for 0 ≤ i &lt; p, a conversion from a real-valued pivot feature i to binary classification • d, the size of the reduced-dimensionality correspondence space to learn</p><formula xml:id="formula_3">• C : R m × R d → R k , a</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>function for combining the original and new features Output:</head><p>• θ ∈ R n×d , a projection from non-pivot features to the correspondence space • h : R m+d → A, the trained predictor Algorithm:</p><p>1. For each pivot feature i : 0 ≤ i &lt; p, learn prediction weightsˆwweightsˆ weightsˆw i = min</p><formula xml:id="formula_4">w∈R n x∈U L(w x [p:m] , B(x i ))</formula><p>2. Construct a matrix W ∈ R n×p using eachˆweachˆ eachˆw i as a column 3. Apply singular value decomposition W = U ΣV where </p><formula xml:id="formula_5">U ∈ R n×n , Σ ∈ R n×p , V ∈ R p×p 4. Select the reduced-dimensionality projection, θ = U [0:d,:] 5. Train a classifier h from [C(x, x [p:m] θ), f (x) : x ∈ S</formula><formula xml:id="formula_6">C(x, z) = [x; z]</formula><p>We call this the pivot+nonpivot+new setting of C.</p><p>The following sections discuss alternative pa- rameter choices for pivot features, B i , d, and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pivot Features for Authorship Attribution</head><p>The SCL algorithm depends heavily on the pivot features being domain-independent features, and as discussed in Section 2, which features make sense as pivot features varies widely by task. No previous studies have explored structural correspon- dence learning for authorship attribution, so one of the outstanding questions we tackle here is how to identify pivot features. Research has shown that the most discriminative features in attribution and the most robust features across domains are char- acter n-grams <ref type="bibr" target="#b16">(Stamatatos, 2013;</ref><ref type="bibr" target="#b11">Sapkota et al., 2014</ref>). We thus consider two types of character n-grams used in authorship attribution that might make good pivot features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Untyped Character N -grams</head><p>Classical character n-grams are simply the se- quences of characters in the text. For example, given the text:</p><p>The structural correspondence character 3-gram features would look like:</p><p>"The", "he ", "e s", " st", "str", "tru", "ruc", "uct", ...</p><p>We propose to use as pivot features the p most fre- quent character n-grams. For non-pivot features, we use the remaining features from prior work <ref type="bibr" target="#b11">(Sapkota et al., 2014</ref>). These include both the remain- ing (lower frequency) character n-grams, as well as stop-words and bag-of-words lexical features. We call this the untyped formulation of pivot features. <ref type="bibr" target="#b12">Sapkota et al. (2015)</ref> showed that classical charac- ter n-grams lose some information in merging to- gether instances of n-grams like the which could be a prefix (thesis), a suffix (breathe), or a standalone word (the). Therefore, untyped character n-grams were separated into ten distinct categories. Four of the ten categories are related to affixes: prefix, suf- fix, space-prefix, and space-suffix. Three are word- related: whole-word, mid-word, and multi-word. The final three are related to the use of punctuation: beg-punct, mid-punct, and end-punct. For example, the character n-grams from the last section would instead be replaced with: <ref type="bibr" target="#b12">Sapkota et al. (2015)</ref> demonstrated that n-grams starting with a punctuation character (the beg-punct category) and with a punctuation character in the middle (the mid-punct category) were the most ef- fective character n-grams for cross-domain author- ship attribution. We therefore propose to use as pivot features the p/2 most frequent character n- grams from each of the beg-punct and mid-punct categories, yielding in total p pivot features. For non-pivot features, we use all of the remaining features of <ref type="bibr" target="#b12">Sapkota et al. (2015)</ref>. These include both the remaining (lower frequency) beg-punct and mid-punct character n-grams, as well as all of the character n-grams from the remaining eight categories. We call this the typed formulation of pivot features. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Typed Character N -grams</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pivot feature binarization parameters</head><p>Authorship attribution typically relies on count- based features. However, the classic SCL algo- rithm assumes that all pivot features are binary, so that it can train binary classifiers to predict pivot feature values from non-pivot features. We propose a binarization function to produce a binary classifi- cation problem from a count-based pivot feature by testing whether the feature value is above or below the feature's median value in the training data:</p><formula xml:id="formula_7">B i (y) = 1 if y &gt; median({x i : x ∈ S ∪ U }) 0 otherwise</formula><p>The intuition is that for count-based features, "did this pivot feature appear at least once in the text" is not a very informative distinction, especially since the average document has hundreds of words, and pivot features are common. A more informative distinction is "was this pivot feature used more or less often than usual?" and that corresponds to the below-median vs. above-median classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dimensionality reduction parameters</head><p>The reduced dimensionality (d) of the low-rank representation varies depending on the task at hand, though lower dimensionality may be preferred as it will result in faster run times. We empirically compare different choices for d: 25, 50, and 100.</p><p>We also consider the question, how critical is dimensionality reduction? For example, if there <ref type="bibr">2</ref> Because the untyped and typed feature sets are designed to directly replicate <ref type="bibr" target="#b11">Sapkota et al. (2014)</ref> and <ref type="bibr" target="#b12">Sapkota et al. (2015)</ref>, respectively, both include character n-grams, but only untyped includes stop-words and lexical features.  are only p = 100 pivot features, is there any need to run singular-value decomposition? The goal here is to determine if SCL is increasing the robust- ness across domains primarily through transform- ing non-pivot features into pivot-like features, or if the reduced dimensionality from the singular-value decomposition contributes something beyond that.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Feature combination parameters</head><p>It's not really clear why the standard formulation of SCL uses the non-pivot features when training the final classifier. All of the non-pivot features are projected into the pivot feature space in the form of the new correspondence features, and the pivot feature space is, by design, the most domain independent part of the feature space. Thus, it seems reasonable to completely replace the non- pivot features with the new pivot-like features. We therefore consider a pivot+new setting of C:</p><formula xml:id="formula_8">pivot+new: C(x, z) = [x [0:p] ; z]</formula><p>We also consider other settings of C, primarily for understanding how the different pieces of the SCL feature space contribute to the overall model.</p><formula xml:id="formula_9">pivot: C(x, z) = x [0:p] nonpivot: C(x, z) = x [p:m] new: C(x, z) = z pivot+nonpivot: C(x, z) = x</formula><p>Note that the pivot+nonpivot setting corresponds to a model that does not apply SCL at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>To explore cross-domain settings of authorship at- tribution, we need datasets containing documents from a number of authors from different domains (different topics, different genres). We use a corpus that consists of texts published in The Guardian daily newspaper that is actively used by the au- thorship attribution community in cross-domain studies <ref type="bibr" target="#b16">(Stamatatos, 2013;</ref><ref type="bibr" target="#b11">Sapkota et al., 2014;</ref><ref type="bibr" target="#b12">Sapkota et al., 2015)</ref>. The Guardian corpus con- tains opinion articles written by 13 authors in four different topics: World, U.K., Society, and Poli- tics. Following prior work, to make the collection balanced across authors, we choose at most ten documents per author for each of the four topics. <ref type="table" target="#tab_2">Table 2</ref> presents some statistics about the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Settings</head><p>We trained support vector machine (SVM) classi- fiers using the Weka implementation <ref type="bibr" target="#b19">(Witten and Frank, 2005</ref>) with default parameters. For the un- typed features, we used character 3-grams appear- ing at least 5 times in the training data, a list of 643 predefined stop-words, and the 3,500 most frequent non-stopword words as the lexical features. For the typed features, we used the top 3,500 most frequent 3-grams occurring at least five times in the training data for each of the 10 character n-gram categories.</p><p>In both cases, we selected p = 100 pivot features as described in Section 3.2.</p><p>We measured performance in terms of accuracy across all possible topic pairings. That is, we paired each of the 4 topics in the Guardian corpus with each of the 3 remaining topics: train on Politics, test on Society; train on Politics, test on UK; train on Politics, test on World; etc. For each such model, we allowed SCL to learn feature correspondences from the labeled data of the 1 training topic and the unlabeled data of the 1 test topic. This resulted in 12 pairings of training/testing topics. We report both accuracy on the individual pairings and an overall average of the 12 accuracies.</p><p>We compare performance against two state-of- the-art baselines: <ref type="bibr" target="#b11">Sapkota et al. (2014)</ref> and <ref type="bibr" target="#b12">Sapkota et al. (2015)</ref>, as described in Section 3.2, and whose features are denoted as untyped and typed, respectively. We replicate these models by using the pivot+nonpivot setting of C, i.e., not including any of the new SCL-based features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>The following sections explore the results of our innovations in different areas: pivot features, fea- ture binarizations, dimensionality reduction, and feature combination. For each section, we hold the other parameters constant and vary only the one parameter of interest. Thus, where not otherwise specified, we set parameters to the best values we observed in our experiments: we set the feature set to typed, the binarization B i (y) to the median, <ref type="table">Table 3</ref>: Accuracy of untyped and typed feature sets. The difference between the averages is not statistically significant (p=0.927).</p><note type="other">Dataset untyped typed Politics-Society 61.29 67.74 Politics-UK 66.67 63.33 Politics-World 58.97 64.10 Society-Politics 62.96 62.96 Society-UK 72.50 72.50 Society-World 56.62 48.08 UK-Politics 68.75 60.71 UK-Society 66.13 67.74 UK-World 57.27 58.97 World-Politics 62.50 59.82 World-Society 61.29 62.90 World-UK 46.67 54.44 Average 61.80 61.94</note><p>the reduced dimensionality d to 50, and the fea- ture combination C(x, z) to pivot+new (i.e., we use the old pivot features alongside the new cor- respondence features). All reports of statistical significance are based on paired, two-tailed t-tests over the 12 different topic pairings. <ref type="table">Table 3</ref> compares the untyped feature set to the typed feature set. Both feature sets perform rea- sonably well, and substantially better than a model without SCL, where the performance of untyped is 56.43 and typed is 53.62 (see the pivot+nonpivot columns of <ref type="table" target="#tab_7">Table 6 and Table 7</ref>, discussed in Sec- tion 6.4). Recall that the typed formulation in- cludes only character n-gram features, while the untyped formulation includes stopwords and lexi- cal features as well. Thus, given their very similar performance in <ref type="table">Table 3</ref>, typed being slightly better, we select the simpler typed feature formulation for the remaining experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Untyped vs. Typed features</head><p>6.2 Greater-than-zero vs. Median Binarization <ref type="table">Table 4</ref> compares choices for B i (y), the function for converting a pivot feature value into a binary classification problem. In every single train/test scenario, and for both untyped and typed feature sets, our proposed median-based binarization func- tion yielded performance greater than or equal to that of the traditional SCL greater-than-zero bina- rization function. This confirms our hypothesis that count-based features were inadequately modeled  <ref type="table">Table 4</ref>: Accuracy of greater-than-zero and me- dian formulations of the B i (y) binarization func- tion. Median is significantly better than greater- than-zero in both untyped (p=0.0007) and typed (p=0.003  in standard SCL and that the median-based bina- rization function improves the modeling of such features. <ref type="table" target="#tab_5">Table 5</ref> compares different choices for the dimen- sionality reduction parameter d, as well as the possibility of not performing any dimensionality reduction at all ("No-SVD"). While each value of d yields the best performance on some of the train/test scenarios, d = 50 achieves the highest average accuracy (61.94). Removing the SVD en- tirely generally performs worse, and though on a small number of train/test scenarios it outperforms d = 25 and d = 100, it is always worse than d = 50. This shows that SCL's feature correspondences alone are not sufficient to achieve domain adap- tation. Without the SVD, performance is barely above a model without SCL: 54.16 vs. 53.62 (see Section 6.4). Much of the benefit appears to be coming from the SVD's basis-shift, since d = 100 outperforms no-SVD by more than 5 points 3 , while d = 50 only outperforms d = 100 by 2 points. These results are consistent with SCL's origins in al- ternating structural optimization ( <ref type="bibr" target="#b0">Ando and Zhang, 2005)</ref>, where SVD is derived as a necessary step for identifying a shared low-dimensional subspace. <ref type="table" target="#tab_7">Table 6 and Table 7</ref> compare the performance of dif- ferent choices for the feature combination function C(x, z) on untyped and typed features, respec- tively. Our proposed pivot+new combination func- tion, which replaces the non-pivot features with the new correspondence features, performs better on average than the two state-of-the-art baselines with no domain adaptation (pivot+nonpivot) and than the two state-of-the-art baselines augmented with classic SCL (pivot+nonpivot+new): 61.80 vs. 56.43 and 56.93 for untyped, and 61.94 vs. 53.62 and 54.23 for typed). These 5-8 point performance gains confirm the utility of our proposed pivot+new combination function, which replaces the old non- pivot features with the new correspondence fea- tures. These gains are consistent with <ref type="bibr" target="#b1">(Blitzer et al., 2006</ref>), who included both pivot and non-pivot features, but found that they had to give pivot fea- tures a weight "five times that of the <ref type="bibr">[non-pivot]</ref> features" to see improved performance.</p><note type="other">Society-UK 72.50 72.50 71.00 72.50 Society-World 51.92 56.62 46.00 48.08 UK-Politics 59.82 68.75 60.00 60.71 UK-Society 59.68 66.13 64.52 67.74 UK-World 47.86 57.27 57.27 58.97 World-Politics 56.25 62.50 56.50 59.82 World-Society 50.00 61.29 61.52 62.90 World-UK 42.22 46.67 50.00 54.44 Average 56.11 61.80 59.83 61.94</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Dimensionality Reduction Choices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Replacing vs. Concatenating Features</head><p>While our approach is better on average, in some individual scenarios, it performs worse than clas- sic SCL or no domain adaptation. For example, on Politics-Society, Politics-UK, and World-UK, using typed features, pivot+new performs worse than no domain adaptation (pivot+nonpivot). Our results suggest a rule for predicting when this degra- dation will happen: pivot+new will outperform   <ref type="table">Table 7</ref>: Accuracy of different typed feature combinations. The best performance for each dataset is in bold. The performance of pivot+new is significantly better than pivot+nonpivot (p=0.041) but not significantly different from pivot+nonpivot+new (p=0.059).</p><p>both pivot+nonpivot and pivot+nonpivot+new iff the new features alone outperform the nonpivot features alone. This rule holds in all 12 of 12 train/test scenarios for untyped features and 11 of 12 scenarios for typed features (failing on only World-Society). Intuitively, if the new correspon- dence features that result from SCL aren't better than the features they were meant to replace, then it is unlikely that they will result in performance gains. This might happen if the pivot features are not strong enough predictors, either because they have been selected poorly or because there are too few of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>To the best of our knowledge, we are the first to introduce a domain adaption model for authorship attribution that combines labeled data in a source domain with unlabeled data from a target domain to improve performance on the target domain. We proposed several extensions to the popular struc- tural correspondence learning (SCL) algorithm for domain adaptation to make it more amenable to tasks like authorship attribution. The SCL algo- rithm requires the manual identification of domain independent pivot features for each task, so we proposed two feature formulations using charac-ter n-grams as the pivot features, and showed that both yielded state-of-the-art performance. We also showed that for the binary classification task that is used by SCL to learn the feature correspondences, replacing the traditional greater-than-zero classifi- cation task with a median-based classification task allowed the model to better handle our count-based features. We explored the dimensionality reduc- tion step of SCL and showed that singular value decomposition (SVD) over the feature correspon- dence matrix is critical to achieving high perfor- mance. Finally, we introduced a new approach to combining the original features with the learned correspondence features, and showed that replacing (rather than concatenating) the non-pivot features with the correspondence features generally yields better performance.</p><p>In the future, we would like to extend this work in several ways. First, though our median-based approach was successful in converting pivot feature values to binary classification problems, learning a regression model might be an even better approach for count-based features. Second, since the SVD basis-shift seems to be the source of much of the gains, we would like to explore replacing the SVD with other algorithms, such as independent com- ponent analysis. Finally, we would like to explore further our finding that the performance of the over- all model seems to be predicted by the difference in performance between the non-pivot features and the new correspondence features, especially to see if this can be predicted at training time rather than as a post-hoc analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The structural correspondence learning (SCL) algorithm 

features, all the old non-pivot features, and all the 
new correspondence features: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Statistics of the Guardian dataset.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Dataset untyped typed &gt;0 &gt;med &gt;0 &gt;med Politics-Society 58.06 61.29 61.29 67.74 Politics-UK 66.67 66.67 63.33 63.33 Politics-World 55.56 58.97 63.81 64.10 Society-Politics 61.81 62.96 62.67 62.96</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy of different choices for dimen-
sionality reduction with typed features. The pat-
tern is similar for untyped. d = 50 is significantly 
better than no SVD (p=0.0009), but not signifi-
cantly different from d = 25 (p=0.291) or d = 100 
(p=0.211). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Accuracy of different untyped feature combinations. The best performance for each dataset is 
in bold. The performance of pivot+new is not significantly different from pivot+nonpivot (p=0.258) or 
pivot+nonpivot+new (p=0.305). 

Dataset 
pivot nonpivot new pivot+nonpivot pivot+nonpivot+new pivot+new 
Politics-Society 48.39 70.97 59.68 
72.58 
72.58 
67.74 
Politics-UK 
52.22 68.89 66.67 
71.11 
72.22 
63.33 
Politics-World 46.15 61.54 61.54 
63.25 
64.10 
64.10 
Society-Politics 55.56 48.15 
61.11 
48.15 
50.00 
62.96 
Society-UK 
65.00 45.00 
65.00 
45.00 
45.00 
72.50 
Society-World 38.46 46.15 53.85 
44.23 
46.15 
48.08 
UK-Politics 
48.21 44.64 55.36 
45.54 
45.54 
60.71 
UK-Society 
51.61 41.94 
66.13 
41.94 
41.94 
67.74 
UK-World 
44.44 33.33 
45.30 
35.90 
35.90 
58.97 
World-Politics 50.89 51.79 61.39 
57.14 
57.14 
59.82 
World-Society 54.84 59.68 43.55 
59.68 
61.29 
62.9 
World-UK 
44.44 56.67 50.00 
58.89 
58.89 
54.44 
Average 
50.02 52.40 
57.47 
53.62 
54.23 
61.94 

</table></figure>

			<note place="foot">&quot;whole-word:The&quot;, &quot;space-suffix:he &quot;, &quot;multi-word:e s&quot;, &quot;space-prefix: st&quot;, &quot;prefix:str&quot;, &quot;mid-word:tru&quot;, &quot;mid-word:ruc&quot;, &quot;mid-word:uct&quot;, ...</note>

			<note place="foot" n="3"> Recall that p = 100, so d = 100 means the full matrix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>This work was supported in part by CONACYT Project number 247870, and by the National Sci-ence Foundation award number 1462141.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubota</forename><surname>Rie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual meeting-association for computational linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="256" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Local histograms of character n-grams for authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montes-Y Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="288" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Person identification from text and speech genre samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein-Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ransom</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberta</forename><forename type="middle">Evans</forename><surname>Sabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;09</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Changes in style in authors with alzheimer&apos;s disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><forename type="middle">Wei</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">English Studies</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Literature survey: Domain adaptation algorithms for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Authorship attribution and verification with many authors and limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Luyckx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Daelemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics<address><addrLine>Manchester, UK, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Author identification on the large scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Argamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fradkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CSNA/Interface 05</title>
		<meeting>CSNA/Interface 05</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Investigating topic influence in authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><forename type="middle">K</forename><surname>Mikros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Argiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection</title>
		<meeting>the International Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="29" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crosslingual adaptation using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno>13:1-13:22</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crosstopic authorship attribution: Will out-of-topic data help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upendra</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1228" to="1237" />
		</imprint>
		<respStmt>
			<orgName>August. Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Not all character ngrams are created equal: A study in authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upendra</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Author attribution evaluation with novel topic cross-validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">I</forename><surname>Schein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnnie</forename><forename type="middle">F</forename><surname>Caver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randale</forename><forename type="middle">J</forename><surname>Honaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">H</forename><surname>Martell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDIR &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Structural correspondence learning for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuyuki</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</title>
		<meeting>the CoNLL Shared Task Session of EMNLP-CoNLL<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="1166" to="1169" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Authorship attribution based on feature set subspacing ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Artificial Intelligence tools</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="823" to="838" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the robustness of authorship attribution based on character n-gram features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Stamatatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Law &amp; Policy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="421" to="439" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving SCL model for sentiment-transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songbo</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The</title>
		<meeting>Human Language Technologies: The</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers</title>
		<meeting><address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kauffmann</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A new method of selecting pivot features for structural correspondence learning in domain adaptive sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youli</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Database Technology and Applications (DBTA)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
	<note>2nd International Workshop on</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
