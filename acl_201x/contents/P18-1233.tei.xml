<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:27+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Domain Sentiment Classification with Target Domain Specific Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018. 2505</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlong</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-Domain Sentiment Classification with Target Domain Specific Information</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2505" to="2513"/>
							<date type="published">July 15-20, 2018. 2018. 2505</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The task of adopting a model with good performance to a target domain that is different from the source domain used for training has received considerable attention in sentiment analysis. Most existing approaches mainly focus on learning representations that are domain-invariant in both the source and target domains. Few of them pay attention to domain specific information, which should also be informative. In this work, we propose a method to simultaneously extract domain specific and invariant representations and train a classifier on each of the representation, respectively. And we introduce a few target domain labeled data for learning domain-specific information. To effectively utilize the target domain labeled data, we train the domain-invariant representation based classifier with both the source and target domain labeled data and train the domain-specific representation based classifier with only the target domain labeled data. These two classifiers then boost each other in a co-training style. Extensive sentiment analysis experiments demonstrated that the proposed method could achieve better performance than state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment classification aims to automatically predict sentiment polarity of user generated sen- timent data like movie reviews. The exponential increase in the availability of online reviews and recommendations makes it an interesting topic in research and industrial areas. However, reviews</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Book Review</head><p>Kitchen Review can span so many different domains that it is difficult to gather annotated training data for all of them. This has motivated much research on cross- domain sentiment classification which transfers the knowledge from label rich domain (source domain) to the label few domain (target domain). In recent years, the most popular cross-domain sentiment classification approach is to extract domain invariant features, whose distribution in the source domain is close to that in the target domain. <ref type="bibr" target="#b10">(Glorot et al., 2011;</ref><ref type="bibr" target="#b8">Fernando et al., 2013;</ref><ref type="bibr" target="#b12">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b1">Aljundi et al., 2015;</ref><ref type="bibr">Baochen Sun, 2015;</ref><ref type="bibr" target="#b13">Long et al., 2015;</ref><ref type="bibr" target="#b9">Ganin et al., 2016;</ref><ref type="bibr" target="#b18">Zellinger et al., 2017)</ref>. And based on this representation, it trains a classifier with the source rich labeled data. Specifically, for data of the source domain X s and data of the target domain X t , it trains a feature generator G(·) with restriction P (G(X s )) ≈ P (G(X t )). And the classifier is trained on G(X s ) with the source labels Y s . The main difference of these approaches is the mechanism to incorporate the restriction on G(·) into the system. The major limitation of this framework is that it losses the domain specific information. As depicted in Figure1, even if it can perfectly extract the domain invariant features (e.g., excellent), it will loss some strong indicators (e.g., delicious, fast) of the target Kitchen domain. We believe that it can achieve greater improvement if it can effectively make use of this information.</p><p>Thus, in this work, we try to explore a path to use the target domain specific information with as few as possible target labeled data. Specifically, we first introduce a novel method to extract the domain invariant and domain specific features of target domain data. Then, we treat these two representations as two different views of the target domain data and accordingly train a domain invariant classifier and a target domain specific classifier, respectively. Because the domain invari- ant representation is compatible with both source data and target data, we train the domain invariant classifier with both source and target labeled data. And for the target domain specific classifier, we train it with target labeled data only. Based on these two classifiers, we perform co-training on target unlabeled data, which can further improve the usage of target data in a bootstrap style.</p><p>In summary, the contributions of this paper include: (i) This is the first work to explore the usage of target domain specific information in cross-domain sentiment classification task. (ii) We propose a novel to extract the domain spe- cific representation of target domain data, which encodes the individual characteristics of the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Domain adaptation aims to generalize a classifier that is trained on a source domain, for which typically plenty of labeled data is available, to a target domain, for which labeled data is scarce. In supervised domain adaptation, cross-domain classifiers are learnt by using labeled source samples and a small number of labeled target samples ( <ref type="bibr" target="#b11">Hoffman et al., 2014)</ref>. A common practice is training the cross-domain classifiers with the labeled source data and then fine-tuning the classifier with the target labeled data <ref type="bibr" target="#b15">(Pan and Yang, 2010)</ref>. Meanwhile, some unsupervised and semi-supervised cross domain methods ( <ref type="bibr" target="#b9">Ganin et al., 2016;</ref><ref type="bibr" target="#b14">Louizos et al., 2015;</ref><ref type="bibr" target="#b18">Zellinger et al., 2017)</ref> are proposed by combining the transfer of classifiers with the match of distributions. These methods focus on extracting the domain- invariant features with the help of unlabeled data.</p><p>Specifically, <ref type="bibr" target="#b9">Ganin et al., (2016)</ref> incorporated an adversarial framework to perform this task. It trained the feature generator to minimize the classification loss and simutaneously deceive the discriminator, which is trained to distinguish the domain of the input data coming from. <ref type="bibr" target="#b14">Louizos et al., (2015)</ref> used the Maximum Mean Discrepancy ( <ref type="bibr" target="#b5">Borgwardt et al., 2006</ref>) regularizer to constrain the feature generator to extract the domain in- variant features. And similarly, <ref type="bibr" target="#b18">Zellinger et al., (2017)</ref> proposed the central moment discrepancy (CMD) metric for the role of domain regularizer. The above methods either treat it no difference between domain specific information and domain invariant information or just ignore the domain specific information during in the process of learning adaptive classifiers.</p><p>One of the most related work is the DSN model ( <ref type="bibr" target="#b6">Bousmalis et al., 2016)</ref>. It proposed to extract the domain specific and the domain invariant representations, simultaneously. However, It does not explored the usage of the domain specific information. Its classifier was still only trained on the domain invariant representation. This work differs from it in the following two aspects. First, we make use of the source and target unlabeled data to extract domain specific information, in- stead of relying on the orthogonality constraint between the extra representation and the domain invariant counterpart. It is achieved by forcing the distribution of the source examples and that of the target examples in the domain specific space to be different. We argue that this can avoid the po- tential problem of the orthogonality constraint in that the domain specific representation can be well predicted by the domain invariant representation, while simultaneously meeting the orthogonality constraint. For example, let X = (0, Z) be the domain invariant representation and Y = (Z, 0) be the domain specific representation, then X can be uniquely determined by Y , while in the meanwhile X ⊥ Y . Second, we apply a co- training framework to make use of the domain specific representation, rather than simply treating it as a regularizer for extracting the domain invariant representation.</p><p>Another related work is the CODA model <ref type="bibr" target="#b7">(Chen et al., 2011</ref>  <ref type="figure">Figure 2</ref>: The general architecture of the proposed model. The source data X s and target data X t are mapped to a domain invariant representation and a target domain specific representation by feature maps E c and E t , respectively. In the space of the domain invariant representation, the distributions of source data H s inv and target data H t common are forced to be similar by minimizing a certain distance L sim . In contrast, in the space of the target domain specific representation, the distributions of source data H s spec and target data H t spec are forced to be different by minimizing the distance L dif f . Based on the domain invariant representation, a classifier F c is trained with the source rich labeled data and some of the target labeled data. In addition, based on the target domain specific representation, a classifier F t is trained with the target labeled data only. These two classifiers teach each other in a co-training framework based on the target unlabeled data U t .</p><p>parts, it randomly separated the features space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We consider the following domain adaptation setting.</p><p>The source domain consists of a set of n s fully labeled points</p><formula xml:id="formula_0">D s = {(x s 1 , y s 1 ), · · · , (x s ns , y s ns )} ⊂ R d × Y drawn from the distribution P s (X, Y). And the target data is divided into n l (n l n s ) labeled points D l t = {(x t 1 , y t 1 ), · · · , (x t n l , y t n l } ⊂ R d × Y from the distribution P t (X, Y) and n u (n u n l ) unlabeled points D u t = {(x t n l +1 , y t n l +1 ), · · · , (x t n l +nu , y t n l +nu } ⊂ R d from the marginal distribution P t (X).</formula><p>The goal is to build a classifier for the target domain data using the source domain data and a few labeled target domain data.</p><p>In the following section, we first introduce the CMD metric, which is used to measure the probability distribution discrepancy between two random variables. Then, we describe our method to extract the domain specific and domain invari- ant representations of target domain examples, using the CMD-based regularizer. Finally, we show how to combine these two representations using a co-training framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Central Moment Discrepancy (CMD)</head><p>The CMD metric was proposed by <ref type="bibr" target="#b18">Zellinger et al.(2017)</ref> to measure the discrepancy between the probability distributions of two (high- dimensional) random variables. It is one of the state-of-the-art metrics and is used as a domain regularizer for domain adaptation. Here, we introduce its definition as a domain regularizer.</p><p>Definition 1 (CMD regularizer). Let X and Y be bounded random samples with respective probability distributions p and q on the interval</p><formula xml:id="formula_1">[a, b] N . The CMD regularizer CM D K is defined by CM D K (X, Y ) = 1 |b − a| E(X) − E(Y ) 2 + 1 |b − a| k K k=2 C k (X) − C k (Y ) 2 , (1)</formula><p>where E(X) = 1 |X| x∈X x is the empirical expectation vector computed on the sample X and</p><formula xml:id="formula_2">C k (X) = E( N i=1 (X i − E(X i )) r i r i ≥0, N i r i =k ,</formula><p>is the vector of all k th order sample central moments of the coordinates of X.</p><p>An intuitive understanding of this metric is that if two probability distributions are similar, their central moment of each order should be close.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extract Domain Invariant and Domain Specific Representations</head><p>In this work, we aim to extract a domain invari- ant representation, as well as a domain specific counterpart, for each target example. This makes our work different from most of the existing works, which only focus on the domain invariant representation. The general architecture of the proposed model is illustrated in <ref type="figure">Figure 2</ref>. Data are mapped into a domain invariant hidden space and target domain specific hidden space using two different mappers E t and E c , respectively:</p><formula xml:id="formula_3">H s spec = E t (X s ; θ t e ) H t spec = E t (X t ; θ t e ) H s inv = E c (X s ; θ c e ) H t inv = E c (X t ; θ c e ).<label>(2)</label></formula><p>Here, E t refers to the domain invariant mapper and E c is the target domain specific mapper. θ t e and θ c e denote their corresponding parameters. The subscript e denotes encode. Based on the hidden presentations H t inv and H t spec , we build an auto- encoder for the target domain examples:</p><formula xml:id="formula_4">ˆ X t = D t (H t inv , H t spec ; θ t d ),<label>(3)</label></formula><p>with respect to parameters θ t d , where the subscript d denotes decode. The corresponding reconstruc- tion loss is defined by the mean square error:</p><formula xml:id="formula_5">L recon = 1 n t nt i 1 k ||X i t − ˆ X t i || 2 2 ,<label>(4)</label></formula><p>where k is the dimension of the input feature vector, and X i t denotes the i th example of the target domain data. Note that in this work, only target examples are passed to the auto-encoder because we only want to extract target domain specific information.</p><p>For E c , we hope that it only encodes features shared by both the source and target domains. From the distribution view, we hope that the distributions of the mapped outputs, by E c , of source and target data are similar. To this end, we apply the CMD regularizer onto the hidden representation of source data H s inv and that of target data H t inv . The corresponding loss is defined by:</p><formula xml:id="formula_6">L sim = CM D K (H s inv , H t inv ).<label>(5)</label></formula><p>Minimizing this loss will force the distribution of H s inv and H t inv to be similar, which in turn en-</p><note type="other">courages E c to encode domain invariant features. And for the domain specific encoder E t , we hope that it only encodes features dominated by the target domain. Ideally, these features should commonly appear in the target domain while hardly appear in the source domain. We argue that this can also be obtained by forcing the distribution of these features in the target domain to differ from that in the source domain, because the target specific auto-encoder D t should filter out features that hardly appear in the target domain while commonly appear in the source domain. Based on this intuition, we apply a signal flipped CMD regularizer onto the mapped representation of source data H s spec and that of target data H t spec . The corresponding loss is defined by:</note><formula xml:id="formula_7">L dif f = −CM D K (H s spec , H t spec ).<label>(6)</label></formula><p>Minimizing this loss encourages the distribution of H s spec to differ from that of H t spec , which in turn encourage E t to encode domain specific features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Co-Training with Domain Invariant and Domain Specific Representations</head><p>The co-training algorithm assumes that the data set is presented in two separate views, and two classifiers are trained for each view. In each iteration, some unlabeled examples that are con- fidently predicted according to exactly one of the two classifiers are moved to the training set. In this way, one classifier provides the predicted labels to the unlabeled examples, on which the other classifier may be uncertain. In this work, we treat the domain invariant representation and the domain specific represen- tation as the two separate views of target domain examples. Based on the domain invariant repre- sentation, we train a domain invariant classifier, F c , with respect to parameters θ c . In addition, based on the domain specific representation, we train a domain specific classifier, F t , with respect to parameters θ t .</p><p>Because the distribution of the source examples is compatible with that of the target examples in input:</p><p>L s : labeled source domain examples L t : labeled target domain examples</p><formula xml:id="formula_8">U t : unlabeled target domain examples H s inv : Invariant representation of L s H t inv</formula><note type="other">: Invariant representation of L t H t spec : Specific representation of L t repeat Train classifier F c with L s and L t based on H s inv and H t inv ; Apply classifier F c to label U t ;</note><p>Select p positive and n negative the most confidently predicted examples U c t from U t ; Train classifier F t with L t based on H t spec ; Apply classifier F t to label U t ; Select p positive and n negative the most confidently predicted examples U t t from U t ; Remove examples U c t ∪ U t t from U t ; Add examples U c t ∪ U t t and their corresponding labels to L t ; until best performance obtained on the developing data set; Algorithm 1: Co-Training for Domain Adaptation the domain invariant hidden space, we use both the source rich labels and a few target labels to train the classifier F c . To train the classifier F t , only the target labels are used. The entire procedure is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Learning</head><p>The training of this model is divided into two parts with one for the domain invariant classifier, F c , and another one for the domain specific classifier, F t . For F c , the goal of training is to minimize the following loss with respect to parameters Θ = {θ c e , θ c e , θ t d , θ c }:</p><formula xml:id="formula_9">L = L recon (θ c e , θ t e , θ t d ) + αL c (θ c e , θ c ) + γL sim (θ c e ) + λL dif f (θ t e ),<label>(7)</label></formula><p>where α, γ, and λ are weights that control the interaction of the loss terms. L(θ) means that loss, L, is optimized on the parameters θ during training. And L c denotes the classification loss on the domain invariant representation, which is defined by the negative log-likelihood of the ground truth class for examples of both source and target domains:</p><formula xml:id="formula_10">L c = 1 n s + l t ns i=1 −Y i s log F c (Y i s |E c (L i s )) + 1 n s + l t lt i=1 −Y i t log F c (Y i t |E c (L i t )),<label>(8)</label></formula><p>where Y i s is the one-hot encoding of the class label for the i th source example, Y i t is that for the i th labeled target example, and l t denotes the dynamic number of target labeled data in each iteration.</p><p>For F t , the goal of training is to minimize the following loss with respect to parameters Θ = {θ c e ,</p><formula xml:id="formula_11">θ t e , θ t d , θ t }: L = L recon (θ c e , θ t e , θ t d ) + βL t (θ t e , θ t ) + γL sim (θ c e ) + λL dif f (θ t e ),<label>(9)</label></formula><p>where γ and λ are the same weights as those for the classifier F c , and β is the weight that controls the portion of classification loss, L t , on the domain specific representation, which is defined by the negative log-likelihood of the ground truth class for examples of the target domain only:</p><formula xml:id="formula_12">L t = 1 l t lt i=1 −Y i t log F t (Y i t |E t (L i t ))<label>(10)</label></formula><p>4 Experiment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Domain adaptation for sentiment classification has been widely studied in the NLP community.</p><p>The major experiments were performed on the benchmark made of reviews of Amazon products gathered by . This data set 1 contains Amazon product reviews from four different domains: Books, DVD, Electronics, and Kitchen appliances from Amazon.com. Each review was originally associated with a rating of 1- 5 stars. For simplicity, we are only concerned with whether or not a review is positive (higher than 3 stars) or negative (3 stars or lower). Reviews are encoded in 5,000 dimensional tf-idf feature vec- tors of bag-of-words unigrams and bigrams. From this data, we constructed 12 cross-domain binary classification tasks. Each domain adaptation task consists of 2,000 labeled source examples, <ref type="table">Semi-supervised Transfer  SO  ST  CMD  DSN</ref> CMD-ft DSN-ft CODA CoCMD (p-value) B→D 81.7±0.2 81.6±0.4 82.6±0.3 82.8±0.4 82.7±0.1 82.7±0.6 81.9±0.4 83.1±0.1(.003) B→E 74.0±0.6 75.8±0.2 81.5±0.6 81.9±0.5 82.4±0.6 82.3±0.8 77.5±2.0 83.0±0.6(.061) B→K 76.4±1.0 78.2±0.6 84.4±0.3 84.4±0.6 84.7±0.5 84.8±0.9 80.4±0.8 85.3±0.7(.039) D→B 79.5±0.3 80.0±0.4 80.7±0.6 80.1±1.3 81.0±0.7 81.1±1.2 80.6±0.3 81.8±0.5(.022) D→E 75.6±0.7 77.0±0.3 82.2±0.5 81.4±1.1 82.5±0.7 81.3±1.2 79.4±0.7 83.4±0.6(.019) D→K 79.5±0.4 80.4±0.6 84.8±0.2 83.3±0.7 84.5±0.9 83.8±0.8 82.4±0.5 85.5±0.8(.055) E→B 72.3±1.5 74.7±0.4 74.9±0.6 75.1±0.4 76.2±0.6 76.3±1.4 73.6±0.7 76.9±0.6(.094) E→D 74.2±0.6 75.4±0.4 77.4±0.3 77.1±0.3 77.7±0.7 77.1±1.1 75.9±0.2 78.3±0.1(.079) E→K 85.6±0.6 85.7±0.7 86.4±0.9 87.2±0.7 86.7±0.3 87.1±0.9 86.1±0.4 87.3±0.4(.093) K→B 73.1±0.1 73.8±0.3 75.8±0.3 76.4±0.5 76.4±0.5 76.2±0.3 74.3±1.0 77.2±0.4(.016) K→D 75.2±0.7 76.6±0.9 77.7±0.4 78.0±1.4 78.8±0.4 78.5±0.5 77.5±0.4 79.6±0.5(.039) K→E 85.4±1.0 85.3±1.6 86.7±0.6 86.7±0.7 87.3±0.3 87.2±0.4 86.4±0.5 87.2±0.4(.512) <ref type="table" target="#tab_1">Table 1</ref>: Average prediction accuracy with 5 runs on target domain testing data set. The left group of models refer to previous state-of-the-art methods and the right group of models refer to the proposed model and some of its variants. We list the p-values of the T-test between CoCMD and CMD-ft for more intuitive understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S→T Supervised Learning Unsupervised Transfer</head><p>2,000 unlabeled target examples, and 50 labeled target examples for training. To fine-tune the hyper-parameters, we randomly select 500 target examples as developing data set, leaving 2,500- 5,500 examples for testing. All of the compared methods and CoCMD share this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Methods</head><p>CoCMD is systematically compared with: 1) neural network classifier without any domain adaptation trained on labeled source data only (SO); 2) neural network classifier without any domain adaptation trained on the union of labeled source and target data (ST); 3) unsupervised central moment discrepancy trained with labeled source data only (CMD) ( <ref type="bibr" target="#b18">Zellinger et al., 2017</ref>); 4) unsupervised domain separation network (DSN) (Bousmalis et al., 2016); 5) semi-supervised CMD trained on labeled source data and then fine- tuned on labeled target data (CMD-ft); 6) semi- supervised DSN trained on labeled source data and then fine-tuned on labeled target data (DSN- ft); 7) semi-supervised Co-training for domain adaptation (CODA) <ref type="figure" target="#fig_0">(Chen et al., 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Detail</head><p>CoCMD was imeplented with a similar architec- ture to that of <ref type="bibr" target="#b9">Ganin et al., (2016)</ref> and <ref type="bibr" target="#b18">Zellinger et al., (2017)</ref>, with one dense hidden layer with 50 hidden nodes and sigmoid activation functions. The classifiers consist of a softmax layer with two dimensional outputs. And the decoder was implemented with a multilayer perceptron (MLP) with one dense hidden layer, tanh activation functions, and relu output functions.</p><p>Model optimization was performed using the RmsProp (Tieleman and Hinton, 2012) update rule with learning rate set to 0.005 for all of the tasks.Hyper-parameter K of the CMD regularizer was set to 3 for all of the tasks, according to the experiment result of <ref type="bibr" target="#b18">Zellinger et al. (2017)</ref>. For the hyper-parameters α, β, γ, and λ, we took the values that achieve the best performance on the developing data set via a grid search {0.01, 0.1, 1, 10, 100}. However, instead of build- ing grids on α, β, γ, and λ all at the same time, we first fine-tuned the values of α and β with the values of γ and λ fixed at 1. After that, we fine- tuned the values of γ and λ with α and β fixed at the best values obtained at last step. Though, this practice may miss the best combination of these hyper-parameters, it can greatly reduce the time consuming for fine-tuning and still obtain acceptable results. And for each iteration of the co-training, we set p = n = 5.  If organizing the domain B and D into a group and organizing the domain E and K into another group, we can observe that the domain adaptation methods achieve greater improvement on the standard classifiers over cross-group tasks (e.g., B → K) than over within-group tasks (e.g., B → D). Similar observation can also be observed by comparing ST with SO, CMD with CMD-ft, and DSN with DSN-ft. The possible explanation is that domains within the same group are more close. Thus adapting over within group tasks is easier than adapting over cross group tasks, if without any domain adaptation regularizer. In addition, we can also observe that CoCMD achieve relatively greater improvement on CMD baseline over the cross-group tasks that over the within-group tasks. We argue that this is because domains in the same group contain relatively less domain individual characteristic. While for domains cross the groups, the domain specific information usually takes a larger share of all of the information. Because the additional part of our proposed method compared to the CMD baseline, is built on the domain specific information, the improvement should be relatively less for within- group tasks. Further analysis of the proposed model in the next section empirically proves this explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Result</head><formula xml:id="formula_13">(b) books → dvd (c) books → dvd (f) electronics → kitchen (g) electronics → kitchen (i) books → kitchen (j) books → kitchen</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Model Analysis</head><p>In this section, we look into how similar two domains are to each other in the space of domain invariant representation and domain specific rep- resentation. A-distance Study: Some of previous works proposed to make use of a proxy of the A- distance <ref type="bibr" target="#b3">(Ben-David et al., 2007</ref>  <ref type="figure">Figure 4</ref>: Proxy A-distance between domains of the Amazon benchmark for the 4 different tasks.</p><p>the distance of two domains. The proxy was defined by 2(1 − 2), where is the generalization error of a linear SVM classifier trained on the binary classification problem to distinguish inputs between the source and target domains. <ref type="figure">Figure  4</ref> shows the results of each pair of domains. We observe several trends: Firstly, the proxy A-distance of within-group domain pairs (i.e., BD and EK) is consistently smaller than that of the cross-group domain pairs (i.e., BK and DE) on all of the hidden spaces. Secondly, the proxy A-distance on the domain specific space is consistently larger than its corresponding value on the hidden space of SO model, as expected. While the proxy A-distance value on domain invariant space is generally smaller than its corresponding value on the hidden space of SO model, except for BK domain pair. A possible explanation is that the balance of classification loss and domain discrepancy loss makes there is still some target domain specific information in the domain invariant space, introduced by the target unlabeled data.</p><p>Visualization: For more intuitive understanding of the behaviour of the proposed model, we further perform a visualization of the domain invariant representation and the domain specific representation, respectively. For this purpose, we reduce the dimension of the hidden space to 2 using principle component analysis (PCA) ( <ref type="bibr" target="#b17">Wold et al., 1987)</ref>. Due to space constraints we choose three tasks: two within-group tasks (B→D and E→K) and a cross-group task (B→K). For comparison, we also display the distribution of each domain in the hidden space of the SO model. The results are shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Pictures of the first column in <ref type="figure" target="#fig_1">Figure 3</ref> show the original distribution of the source and target examples in the hidden space of SO model. As can be seen, there is a great overlap between the distributions of the domain B and the domain D domains and between the distributions of the domain E and the domain K. While there is quite a gap between the distribution of the domain B and the domain K. This strengthens our argument that within-group domains share relatively more common information than cross-group domains. Pictures of the second column show the distri- bution of the source and target examples in the domain invariant hidden space of the proposed model. From these pictures we can see that the distributions of the source and target data are quite similar in this presentation. This demonstrates the effectiveness of the CMD regularizer for ex- tracting domain invariant representation. Pictures of the third column show the distribution of the source and target examples in the domain specific hidden space of the proposed model. As can be seen from these pictures, examples of the source and target domains are separated very well. This demonstrates the effectiveness of our proposed method for extracting domain specific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we investigated the importance of domain specific information for domain adap- tation. In contrast with most of the previous methods, which pay more attention to domain invariant information, we showed that domain specific information could also be beneficially used in the domain adaptation task with a small amount of in-domain labeled data. Specifically, we proposed a novel method, based on the CMD metric, to simultaneously extract domain invariant feature and domain specific feature for target domain data. With these two different features, we performed co-training with labeled data from the source domain and a small amount of labeled data from the target domain. Sentiment analysis experiments demonstrated the effectiveness of this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by National Natural</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Top indicators extracted with logistic regression for Book and Kitchen domains. The overlap between the two ellipses denotes the shared features between these two domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The distribution of source and target data in the hidden space of different representations. The red points denote the source examples and the blue ones denote the target examples. The pictures of each row correspond to the B→D, E→K, and B→K task. The pictures of each column correspond to the hidden space, H e c , of the source-only model, the domain invariant representation, and the target specific representation of the proposed model.</figDesc><graphic url="image-9.png" coords="7,358.22,258.27,118.42,78.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 showsbooks → dvd (e) electronics → kitchen (h) books → kitchen</head><label>1</label><figDesc>the average classification accuracy of our proposed model and the baselines over all 12 domain adaptation tasks. We can first observe that the proposed model CoCMD outperforms the compared methods over almost all of the</figDesc><table>Source-only 
CoCMD: Invariant 
CoCMD: Specific 

(a) Source-only 
CoCMD: Invariant 
CoCMD: Specific 

Source-only 
CoCMD: Invariant 
CoCMD: Specific 

</table></figure>

			<note place="foot" n="1"> https://www.cs.jhu.edu/ mdredze/datasets/sentiment/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>and STCSM (No.16JC1420401</idno>
	</analytic>
	<monogr>
		<title level="j">Science Foundation of China</title>
		<imprint>
			<date type="published" when="61472088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Landmarks-based kernelized subspace alignment for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Muselet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Jiashi Feng Kate Saenko Baochen Sun</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6254-domain-separation-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Asymmetric and category invariant feature transformations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="28" to="41" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>arX- iv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain invariant transfer kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaguang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S Yu</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1519" to="1532" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00830</idno>
		<title level="m">The variational fair autoencoder</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Principal component analysis. Chemometrics and intelligent laboratory systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svante</forename><surname>Wold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Esbensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Geladi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Central moment discrepancy (cmd) for domain-invariant representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08811</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
