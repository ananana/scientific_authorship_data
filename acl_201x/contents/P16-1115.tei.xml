<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Resolving References to Objects in Photographs using the Words-As-Classifiers Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dialogue Systems Group // CITEC // Faculty of Linguistics and Literary Studies</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Zarrieß</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dialogue Systems Group // CITEC // Faculty of Linguistics and Literary Studies</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Kennington</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dialogue Systems Group // CITEC // Faculty of Linguistics and Literary Studies</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Resolving References to Objects in Photographs using the Words-As-Classifiers Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1213" to="1223"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A common use of language is to refer to visually present objects. Modelling it in computers requires modelling the link between language and perception. The &quot;words as classifiers&quot; model of grounded semantics views words as classifiers of perceptual contexts, and composes the meaning of a phrase through composition of the denotations of its component words. It was recently shown to perform well in a game-playing scenario with a small number of object types. We apply it to two large sets of real-world photographs that contain a much larger variety of object types and for which referring expressions are available. Using a pre-trained convolu-tional neural network to extract image region features, and augmenting these with positional information, we show that the model achieves performance competitive with the state of the art in a reference resolution task (given expression, find bounding box of its referent), while, as we argue, being conceptually simpler and more flexible .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A common use of language is to refer to objects in the shared environment of speaker and addressee. Being able to simulate this is of particular impor- tance for verbal human/robot interfaces (HRI), and the task has consequently received some attention in this field ( <ref type="bibr" target="#b18">Matuszek et al., 2012;</ref><ref type="bibr" target="#b27">Tellex et al., 2011;</ref><ref type="bibr" target="#b13">Krishnamurthy and Kollar, 2013</ref>).</p><p>Here, we study a somewhat simpler precursor task, namely that of resolution of reference to objects in static images (photographs), but use a larger set of object types than is usually done in HRI work (&gt; 300, see below). More formally, the task is to retrieve, given a referring expression e and an image I, the region bb * of the image that is most likely to contain the referent of the expres- sion. As candidate regions, we use both manually annotated regions as well as automatically com- puted ones.</p><p>As our starting point, we use the "words-as- classifiers" model recently proposed by <ref type="bibr" target="#b12">Kennington and Schlangen (2015)</ref>. It has before only been tested in a small domain and with specially de- signed features; here, we apply it to real-world photographs and use learned representations from a convolutional neural network ( <ref type="bibr">Szegedy et al., 2015)</ref>. We learn models for between 400 and 1,200 words, depending on the training data set. As we show, the model performs competitive with the state of the art ( <ref type="bibr" target="#b10">Hu et al., 2016;</ref><ref type="bibr" target="#b17">Mao et al., 2016)</ref> on the same data sets.</p><p>Our background interest in situated interaction makes it important for us that the approach we use is 'dialogue ready'; and it is, in the sense that it supports incremental processing (giving re- sults while the incoming utterance is going on) and incremental learning (being able to improve per- formance from interactive feedback). However, in this paper we focus purely on 'batch', non- interactive performance. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The idea of connecting words to what they de- note in the real world via perceptual features goes back at least to <ref type="bibr" target="#b8">Harnad (1990)</ref>, who coined "The Symbol Grounding Problem": " <ref type="bibr">[H]</ref>ow can the se- mantic interpretation of a formal symbol system be made intrinsic to the system, rather than just par- asitic on the meanings in our heads?" The pro-posed solution was to link 'categorial representa- tions' with "learned and innate feature detectors that pick out the invariant features of object and event categories from their sensory projections". This suggestion has variously been taken up in computational work. An early example is Deb Roy's work from the early 2000s ( <ref type="bibr" target="#b22">Roy, 2002;</ref><ref type="bibr" target="#b23">Roy, 2005</ref>). In ( ), computer vision techniques are used to detect ob- ject boundaries in a video feed, and to compute colour features (mean colour pixel value), posi- tional features, and features encoding the relative spatial configuration of objects. These features are then associated in a learning process with cer- tain words, resulting in an association of colour features with colour words, spatial features with prepositions, etc., and based on this, these words can be interpreted with reference to the scene cur- rently presented to the video feed.</p><p>Of more recent work, that of <ref type="bibr" target="#b18">Matuszek et al. (2012)</ref> is closely related to the approach we take. The task in this work is to compute (sets of) refer- ents, given a (depth) image of a scene containing simple geometric shapes and a natural language expression. In keeping with the formal semantics tradition, a layer of logical form representation is assumed; it is not constructed via syntactic parsing rules, however, but by a learned mapping (seman- tic parsing). The non-logical constants of this rep- resentation then are interpreted by linking them to classifiers that work on perceptual features (rep- resenting shape and colour of objects). Interest- ingly, both mapping processes are trained jointly, and hence the links between classifiers and non- logical constants on the one hand, and non-logical constants and lexemes on the other are induced from data. In the work presented here, we take a simpler approach that forgoes the level of semantic representation and directly links lexemes and per- ceptions, but does not yet learn the composition.</p><p>Most closely related on the formal side is re- cent work by <ref type="bibr" target="#b14">Larsson (2015)</ref>, which offers a very direct implementation of the 'words as classi- fiers' idea (couched in terms of type theory with records (TTR; <ref type="bibr" target="#b0">(Cooper and Ginzburg, 2015)</ref>) and not model-theoretic semantics). In this approach, some lexical entries are enriched with classifiers that can judge, given a representation of an object, how applicable the term is to it. The paper also describes how these classifiers could be trained (or adapted) in interaction. The model is only speci- fied theoretically, however, with hand-crafted clas- sifiers for a small set of words, and not tested with real data.</p><p>The second area to mention here is the recently very active one of image-to-text generation, which has been spurred on by the availability of large datasets and competitions structured around them. The task here typically is to generate a descrip- tion (a caption) for a given image. A frequently taken approach is to use a convolutional neural network (CNN) to map the image to a dense vec- tor (which we do as well, as we will describe be- low), and then condition a neural language model (typically, an LSTM) on this to produce an output string ( <ref type="bibr" target="#b29">Vinyals et al., 2015;</ref><ref type="bibr" target="#b2">Devlin et al., 2015)</ref>.  modify this approach somewhat, by using what they call "word detectors" first to specifically propose words for image regions, out of which the caption is then generated. This has some similarity to our word models as described below, but again is tailored more towards genera- tion. <ref type="bibr">Socher et al. (2014)</ref> present a more composi- tional variant of this type of approach where sen- tence representations are composed along the de- pendency parse of the sentence. The representa- tion of the root node is then mapped into a multi- modal space in which distance between sentence and image representation can be used to guide im- age retrieval, which is the task in that paper. Our approach, in contrast, composes on the level of de- notations and not that of representation.</p><p>Two very recent papers carry this type of ap- proach over to the problem of resolving references to objects in images. Both ( <ref type="bibr" target="#b9">Hu et al., 2015)</ref> and ( <ref type="bibr" target="#b16">Mao et al., 2015</ref>) use CNNs to encode image in- formation (and interestingly, both combine, in dif- ferent ways, information from the candidate re- gion with more global information about the im- age as a whole), on which they condition an LSTM to get a prediction score for fit of candidate region and referring expression. As we will discuss be- low, our approach has some similarities, but can be seen as being more compositional, as the expres- sion score is more clearly composed out of indi- vidual word scores (with rule-driven composition, however). We will directly compare our results to those reported in these papers, as we were able to use the same datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The "Words-As-Classifiers" Model</head><p>We now briefly review (and slightly reformu- late) the model introduced by <ref type="bibr" target="#b12">Kennington and Schlangen (2015)</ref>. It has several components:</p><p>A Model of Word Meanings Let w be a word whose meaning is to be modelled, and let x be a representation of an object in terms of its vi- sual features. The core ingredient then is a clas- sifier then takes this representation and returns a score f w (x), indicating the "appropriateness" of the word for denoting the object.</p><p>Noting a (loose) correspondence to <ref type="bibr">Montague's (1974)</ref> intensional semantics, where the intension of a word is a function from possible worlds to extensions <ref type="bibr" target="#b6">(Gamut, 1991)</ref>, the intensional mean- ing of w is then defined as the classifier itself, a function from a representation of an object to an "appropriateness score": 2</p><formula xml:id="formula_0">[[w]] obj = λx.fw(x) (1) (Where [[.]</formula><p>] is a function returning the meaning of its argument, and x is a feature vecture as given by f obj , the function that computes the representa- tion for a given object.)</p><p>The extension of a word in a given (here, vi- sual) discourse universe W can then be modelled as a probability distribution ranging over all can- didate objects in the given domain, resulting from the application of the word intension to each object (x i is the feature vector for object i, normalize() vectorized normalisation, and I a random variable ranging over the k candidates):</p><formula xml:id="formula_1">[[w]] W obj = normalize(([[w]] obj (x1), . . . , [[w]] obj (x k ))) = normalize((fw(x1), . . . , fw(x k ))) = P (I|w) (2)</formula><p>Composition Composition of word meanings into phrase meanings in this approach is governed by rules that are tied to syntactic constructions. In the following, we only use simple multiplicative composition for nominal constructions:</p><formula xml:id="formula_2">[[[nomw1, . . . , w k ]]] W = [[NOM]] W [[w1, . . . , w k ]] W = • /N ([[w1]] W , . . . , [[w k ]] W ) (3)</formula><p>where • /N is defined as</p><formula xml:id="formula_3">• /N ([[w1]] W , . . . , [[w k ]] W ) = P•(I|w1, . . . , w k ) with P•(I = i|w1, . . . , w k ) = 1 Z (P (I = i|w1) * · · · * P (I = i|w k )) for i ∈ I (4)</formula><p>(Z takes care that the result is normalized over all candidate objects.)</p><p>Selection To arrive at the desired extension of a full referring expression-an individual object, in our case-, one additional element is needed, and this is contributed by the determiner. For uniquely referring expressions ("the red cross"), what is re- quired is to pick the most likely candidate from the distribution:</p><formula xml:id="formula_4">[[the]] = λx. arg max Dom(x) x (5) [[[the] [nomw1, . . . , w k ]]] W = arg max i∈W [ [[[nomw1, . . . , w k ]]] W ] (6)</formula><p>In other words, the prediction of an expression such as "the brown shirt guy on right" is com- puted by first getting the responses of the classi- fiers corresponding to the words, individually for each object. I.e., the classifier for "brown" is ap- plied to objects o 1 , . . . , o n . This yields a vec- tor of responses (of dimensionality n, the num- ber of candidate objects); similarly for all other words. These vectors are then multiplied, and the predicted object is the maximal component of the resulting vector. <ref type="figure" target="#fig_0">Figure 1</ref> gives a schematic overview of the model as implemented here, in- cluding the feature extraction process. , with region masks from SAIAPR TC-12 (middle); "brown shirt guy on right" is a referring expression in REFERITGAME for the region singled out on the right done manually and provide close maskings of the objects. This extended dataset is also known as "SAIAPR TC-12" (for "segmented and annotated IAPR TC-12").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E ¥¥.t</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image with bounding boxes of objects (R, R', …) Regions con- verted to 224 x 224 pixels</head><p>The third component is provided by <ref type="bibr" target="#b11">Kazemzadeh et al. (2014)</ref>, who collected a large number of expressions referring to (pre- segmented) objects from these images, using a crowd-sourcing approach where two players were paired and a director needed to refer to a prede- termined object to a matcher, who then selected it. (An example is given in <ref type="figure">Figure 2</ref> (right).) This corpus contains 120k referring expressions, covering nearly all of the 99.5k regions from SAIAPR TC-12. <ref type="bibr">3</ref> The average length of a referring expression from this corpus is 3.4 tokens. The 500k token realise 10,340 types, with 5785 hapax legomena. The most frequent tokens (other than articles and prepositions) are "left" and "right", with 22k occurrences. (In the following, we will refer to this corpus as <ref type="bibr">REFERIT.)</ref> This combination of segmented images and re- ferring expressions has recently been used by <ref type="bibr" target="#b9">Hu et al. (2015)</ref> for learning to resolve references, as we do here. The authors also tested their method on region proposals computed using the EdgeBox algorithm <ref type="bibr" target="#b31">(Zitnick and Dollár, 2014</ref>). They kindly provided us with this region proposal data (100 best proposals per image), and we compare our results on these region proposals with theirs be- low. The authors split the dataset evenly into 10k images (and their corresponding referring expres- sions) for training and 10k for testing. As we needed more training data, we made a 90/10 split, ensuring that all our test images are from their test split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSCOCO / GoogleRefExp / ReferItGame</head><p>The second dataset is based on the "Microsoft Common Objects in Context" collection ( <ref type="bibr" target="#b15">Lin et al., 2014)</ref>, which contains over 300k images with object segmentations (of objects from 80 pre- specified categories), object labels, and image cap- tions. <ref type="figure" target="#fig_1">Figure 3</ref> shows some examples of images containing objects of type "person". This dataset was augmented by <ref type="bibr" target="#b16">Mao et al. (2015)</ref> with what they call 'unambiguous object descriptions', using a subset of 27k images that contained between 2 and 4 instances of the same object type within the same image. The authors collected and validated 100k descriptions in a crowd-sourced approach as well, but unlike in the ReferItGame setup, describers and validators were not connected live in a game setting. <ref type="bibr">4</ref> The average length of the descriptions is 8.3 token. The 790k token in the corpus realise 14k types, with 6950 hapax legomena. The most frequent tokens other than articles and prepositions are "man" (15k oc- currences) and "white" (12k). (In the following, we will refer to this corpus as GREXP.)</p><p>The authors also computed automatic region proposals for these images, using the multibox method of <ref type="bibr" target="#b3">Erhan et al. (2014)</ref> and classifying those using a model trained on MSCOCO categories, re- taining on average only 8 per image. These region proposals are on average of a much higher quality than those we have available for the other dataset.</p><p>As mentioned in ( <ref type="bibr" target="#b16">Mao et al., 2015)</ref>, Tamara Berg and colleagues have at the same time used their ReferItGame paradigm to collect referring expressions for MSCOCO images as well. Upon request, Berg and colleagues also kindly provided us with this data-140k referring expressions, for 20k images, average length 3.5 token, 500k to- ken altogether, 10.3k types, 5785 hapax legom- ena; most frequent also "left" (33k occurrences) It is interesting to note the differences in the expressions from REFCOCO and GREXP, the lat- ter on average being almost 5 token longer. <ref type="figure" target="#fig_1">Fig- ure 3</ref> gives representative examples. We can spec- ulate that the different task descriptions ("refer to this object" vs. "produce an unambiguous descrip- tion") and the different settings (live to a partner vs. offline, only validated later) may have caused this. As we will see below, the GREXP descriptions did indeed cause more problems to our approach, which is meant for reference in interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training the Word/Object Classifiers</head><p>The basis of the approach we use are the classi- fiers that link words and images. These need to be trained from data; more specifically, from pair- ings of image regions and referring expressions, as provided by the corpora described in the previous section.</p><p>Representing Image Regions The first step is to represent the information from the image re- gions. We use a deep convolutional neural net- work, "GoogLeNet" ( <ref type="bibr">Szegedy et al., 2015)</ref>, that was trained on data from the Large Scale Visual Recognition Challenge 2014 (ILSVRC2014) from the ImageNet corpus <ref type="bibr" target="#b1">(Deng et al., 2009</ref>) to extract features. <ref type="bibr">5</ref> It was optimised to recognise categories from that challenge, which are different from those occurring in either SAIAPR or COCO, but in any case we only use the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We aug- ment this with 7 features that encode information about the region relative to the image: the (rela- tive) coordinates of two corners, its (relative) area, distance to the center, and orientation of the im- age. The full representation hence is a vector of 1031 features. (See also <ref type="figure" target="#fig_0">Figure 1</ref> above.)</p><p>Selecting candidate words How do we select the words for which we train perceptual classi- fiers? There is a technical consideration to be made here and a semantic one. The technical con- sideration is that we need sufficient training data for the classifiers, and so can only practically train classifiers for words that occur often enough in the training corpus. We set a threshold here of a min- imum of 40 occurences in the training corpus, de- termined empirically on the validation set to pro- vide a good tradeoff between vocabulary coverage and number of training instances.</p><p>The semantic consideration is that intuitively, the approach does not seem appropriate for all types of words; where it might make sense for attributes and category names to be modelled as image classifiers, it does less so for prepositions and other function words. Nevertheless, for now, we make the assumption that all words in a refer- ring expression contribute information to the vi- sual identification of its referent. We discuss the consequences of this decision below.</p><p>This assumption is violated in a different way in phrases that refer via a landmark, such as in "the thing next to the woman with the blue shirt". Here we cannot assume for example that the referent region provides a good instance of "blue" (since it is not the target object in the region that is de- scribed as blue), and so we exclude such phrases from the training set (by looking for a small set of expressions such as "left of", "behind", etc.; see appendix for a full list). This reduces the train- ing portions of REFERIT, REFCOCO and GREXP to 86%, 95%, and 82% of their original size, respec- tively (counting referring expressions, not tokens).</p><p>Now that we have decided on the set of words for which to train classifiers, how do we assemble the training data?</p><p>Positive Instances Getting positive instances from the corpus is straightforward: We pair each word in a referring expression with the represen- tation of the region it refers to. That is, if the word "left" occurs 20,000 times in expressions in the training corpus, we have 20,000 positive instances for training its classifier.</p><p>Negative Instances Acquiring negative in- stances is less straightforward. The corpus does not record inappropriate uses of a word, or 'negative referring expressions' (as in "this is not a red chair"). To create negative instances, we make a second assumption which again is not generally correct, namely that when a word was never in the corpus used to refer to an object, this object can serve as a negative example for that word/object classifier. In the experiments reported below, we randomly selected 5 image regions from the training corpus whose referring expressions (if there were any) did not contain the word in question. <ref type="bibr">6</ref> The classifiers Following this regime, we train binary logistic regression classifiers (with 1 regu- larisation) on the visual object features representa- tions, for all words that occurred at least 40 times in the respective training corpus. <ref type="bibr">7</ref> To summarise, we train separate binary classifiers for each word (not making any a-priori distinction between function words and others, or attribute la- bels and category labels), giving them the task to predict how likely it would be that the word they represent would be used to refer to the image re- gion they are given. All classifiers are presented during training with data sets with the same bal- ance of positive and negative examples (here, a fixed ratio of 1 positive to 5 negative). Hence, the classifiers themselves do not reflect any word fre- quency effects; our claim (to be validated in future <ref type="bibr">6</ref> This approach is inspired by the negative sampling tech- nique of <ref type="bibr" target="#b19">Mikolov et al. (2013)</ref> for training textual word em- beddings. <ref type="bibr">7</ref> We used the implementation in the scikit learn package ( <ref type="bibr" target="#b20">Pedregosa et al., 2011</ref> work) is that any potential effects of this type are better modelled separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>The task in our experiments is the following: Given an image I together with bounding boxes of regions (bb 1 , . . . , bb n ) within it, and a referring expression e, predict which of these regions con- tains the referent of the expression.</p><p>By Corpus We start with training and testing models for all three corpora (REFERIT, REFCOCO, GREXP) separately. But first, we establish some baselines. The first is just randomly picking one of the candidate regions. The second is a 1-rule clas- sifier that picks the largest region. The respective accuracies on the corpora are as follows: REFERIT 0.20/0.19; REFCOCO 0.16/0.23; GREXP 0.19/0.20. Training on the training sets of REFERIT, REF- COCO and GREX with the regime described above (min. 40 occurrences) gives us classifiers for 429, 503, and 682 words, respectively. <ref type="table">Table 1</ref> shows the evaluation on the respective test parts: accu- racy (acc), mean reciprocal rank (mrr) and for how much of the expression, on average, a word classifier is present (arc). '&gt;0' shows how much of the testcorpus is left if expressions are filtered out for which not even a single word is the model (which we evaluate by default as false), and accu- racy for that reduced set. The 'NR' rows give the same numbers for reduced test sets in which all relational expressions have been removed; '%tst' shows how much of a reduction that is relative to the full testset. The rows with the citations give the best reported results from the literature. <ref type="bibr">8</ref> As this shows, in most cases we come close, but do not quite reach these results. The distance is the biggest for GREXP with its much longer ex- pressions. As discussed above, not only are the descriptions longer on average in this corpus, the vocabulary size is also much higher. Many of the descriptions contain action descriptions ("the man smiling at the woman"), which do not seem to be as helpful to our model. Overall, the ex- pressions in this corpus do appear to be more like 'mini-captions' describing the region rather than referring expressions that efficiently single it out among the set of distractors; our model tries to capture the latter.</p><p>Combining Corpora A nice effect of our setup is that we can freely mix the corpora for train- ing, as image regions are represented in the same way regardless of source corpus, and we can com- bine occurrences of a word across corpora. We tested combining the testsets of REFERIT and RE- FCOCO (RI+RC in the Table below), REFCOCO and GREXP (RC+GR), and all three (REFERIT, REF- COCO, and GREXP; RI+RC+GR), yielding mod- els for 793, 933, 1215 words, respectively (with the same "min. 40 occurrences" criterion). For all testsets, the results were at least stable compared to <ref type="table">Table 1</ref>, for some they improved. For reasons of space, we only show the improvements here.  Computed Region Proposals Here, we cannot expect the system to retrieve exactly the ground truth bounding box, since we cannot expect the set of automatically computed regions to contain it. We follow <ref type="bibr" target="#b16">Mao et al. (2015)</ref> in using intersection over union (IoU) as metric (the size of the inter- sective area between candidate and ground truth bounding box normalised by the size of the union) and taking an IoU ≥ 0.5 of the top candidate as a threshold for success (P@1). As a more relaxed metric, we also count for the SAIAPR proposals (of which there are 100 per image) as success when at least one among the top 10 candidates exceeds this IoU threshold (R@10). (For MSCOCO, there are only slightly above 5 proposals per image on aver- age, so computing this more relaxed measure does not make sense.) The random baseline (RND) is computed by applying the P@1 criterion to a ran- domly picked region proposal. (That it is higher than 1/#regions for SAIAPR shows that the regions cluster around objects.)  <ref type="table">Table 3</ref>: Results on region proposals</p><p>With the higher quality proposals provided for the MSCOCO data, and the shorter, more prototyp- ical referring expressions from REFCOCO, we nar- rowly beat the reported results. (Again, note that we use a different split that ensures separation on the level of images between training and test.) ( <ref type="bibr" target="#b9">Hu et al., 2015</ref>) performs relatively better on the re- gion proposals (the gap is wider), on GREXP, we come relatively closer using these proposals. We can speculate that using automatically computed boxes of a lower selectivity (REFERIT) shifts the balance between needing to actually recognise the image and getting information from the shape and position of the box (our positional features; see Section 5).</p><p>Ablation Experiments To get an idea about what the classifiers actually pick up on, we trained variants given only the positional features (POS columns below in <ref type="table" target="#tab_4">Table 4</ref>) and only object fea- tures (NOPOS columns). We also applied a vari- ant of the model with only the top 20 classifiers (in terms of number of positive training examples; TOP20). We only show accuracy here, and repeat the relevant numbers from <ref type="table">Table 1</ref>   This table shows an interesting pattern. To a large extent, the object image features and the po- sitional features seem to carry redundant informa- tion, with the latter on their own performing better than the former on their own. The full model, how- ever, still gains something from the combination of the feature types. The top-20 classifiers (and consequently, top 20 most frequent words) alone reach decent performance (the numbers are shown for the full test set here; if reduced to only utter- ances where at least one word is known, the num- bers rise, but the reduction of the testset is much more severe than for the full models with much larger vocabulary).  Error Analysis <ref type="figure" target="#fig_5">Figure 4</ref> shows the accuracy of the model split by length of the referring expres- sion (top lines; lower lines show the proportion of expression of this length in the whole corpus). The pattern is similar for all corpora (but less pro- nounced for GREXP): shorter utterances fare bet- ter.</p><p>Manual inspection of the errors made by the system further corroborates the suspicion that composition as done here neglects too much of the internal structure of the expression. An example from REFERIT where we get a wrong prediction is "second person from left". The model clearly does not have a notion of counting, and here it wrongly selects the leftmost person. In a similar vein, we gave results above for a testset where spatial rela- tions where removed, but other forms of relation (e.g., "child sitting on womans lap") that weren't modelled still remain in the corpus.</p><p>We see as an advantage of the model that we can inspect words individually. Given the per- formance of short utterances, we can conclude that the word/object classifiers themselves per- form reasonably well. This seems to be somewhat independent of the number of training examples they received. <ref type="figure" target="#fig_6">Figure 5</ref> shows, for REFERIT, # training instances (x-axis) vs. average accuracy on the validation set, for the whole vocabulary. As this shows, the classifiers tend to get better with more training instances, but there are good ones even with very little training material. ) This is, to a degree, as expected: our assumption behind training clas- sifiers for all ocurring words and not pre-filtering based on their part-of-speech or prior hypotheses about visual relevance was that words that can oc- cur in all kinds of visual contexts will lead to clas- sifiers whose contributions cancel out across all candidate objects in a scene.</p><p>However, the mean average precision of the classifiers for colour words is also relatively low at 0.6 (std 0.08), for positional words ("left", "right", "center", etc.) it is 0.54 (std 0.1). This might sug- gest that the features we take from the CNN might indeed be more appropriate for tasks close to what they were originally trained on, namely category and not attribute prediction. We will explore this in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have shown that the "words-as-classifiers" model scales up to a larger set of object types with a much larger variety in appearance (SAIAPR and MSCOCO); to a larger vocabulary and much less restricted expressions (REFERIT, REFCOCO, GR- EXP); and to use of automatically learned feature types (from a CNN). It achieves results that are comparable to those of more complex models.</p><p>We see as advantage that the model we use is "transparent" and modular. Its basis, the word/object classifiers, ties in more directly with more standard approaches to semantic analysis and composition. Here, we have disregarded much of the internal structure of the expressions. But there is a clear path for bringing it back in, by defining other composition types for other con- struction types and different word models for other word types. <ref type="bibr" target="#b12">Kennington and Schlangen (2015)</ref> do this for spatial relations in their simpler do- main; for our domain, new and more richly anno- tated data such as VISUALgenome looks promis- ing for learning a wide variety of relations. <ref type="bibr">9</ref> The use of denotations / extensions might make pos- sible transfer of methods from extensional seman- tics, e.g. for the addition of operators such as nega- tion or generalised quantifiers. The design of the model, as mentioned in the introduction, makes it amenable for use in interactive systems that learn; we are currently exploring this avenue. Lastly, the word/object classifiers also show promise in the reverse task, generation of referring expressions <ref type="bibr" target="#b30">(Zarrieß and Schlangen, 2016)</ref>.</p><p>All this is future work. In its current state- besides, we believe, strongly motivating this fu- ture work-, we hope that the model can also serve as a strong baseline to other future approaches to reference resolution, as it is conceptually simple and easy to implement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the model</figDesc><graphic url="image-1.png" coords="3,303.23,425.73,74.79,57.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples from MSCOCO and "right" (32k). (In the following, we will call this corpus REFCOCO.) In our experiments, we use the training/validation/test splits on the images suggested by Berg et al., as the splits provided by Mao et al. (2015) are on the level of objects and have some overlap in images. It is interesting to note the differences in the expressions from REFCOCO and GREXP, the latter on average being almost 5 token longer. Figure 3 gives representative examples. We can speculate that the different task descriptions ("refer to this object" vs. "produce an unambiguous description") and the different settings (live to a partner vs. offline, only validated later) may have caused this. As we will see below, the GREXP descriptions did indeed cause more problems to our approach, which is meant for reference in interaction.</figDesc><graphic url="image-7.png" coords="5,72.00,180.06,152.79,104.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5</head><label></label><figDesc>http://www.image-net.org/challenges/ LSVRC/2014/. We use the sklearn-theano (http://sklearn-theano. github.io/feature_extraction/index.html# feature-extraction) port of the Caffe replication and re-training (https://github.com/BVLC/caffe/ tree/master/models/bvlc_googlenet) of this network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy by expression length (top 3 lines); percentage of expressions with this length (lower 3 lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average accuracy vs. # train instanc. Mean average precision (i.e., area under the precision / recall curve) over all classifiers (exemplarily computed for the RI+RC set, 793 words) is 0.73 (std 0.15). Interestingly, the 155 classifiers in the top range (average precision over 0.85) are almost all for concrete nouns; the 128 worst performing ones (below 0.60) are mostly other parts of speech. (See appendix.) This is, to a degree, as expected: our assumption behind training classifiers for all ocurring words and not pre-filtering based on their part-of-speech or prior hypotheses about visual relevance was that words that can occur in all kinds of visual contexts will lead to classifiers whose contributions cancel out across all candidate objects in a scene. However, the mean average precision of the classifiers for colour words is also relatively low at 0.6 (std 0.08), for positional words ("left", "right", "center", etc.) it is 0.54 (std 0.1). This might suggest that the features we take from the CNN might indeed be more appropriate for tasks close to what they were originally trained on, namely category and not attribute prediction. We will explore this in future work.</figDesc><graphic url="image-8.png" coords="8,327.74,102.99,174.62,120.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Results, combined corpora</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Results with reduced models</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> The code for reproducing the results reported in this paper can be found at https://github.com/ dsg-bielefeld/image_wac.</note>

			<note place="foot" n="2"> (Larsson, 2015) develops this intension/extension distinction in more detail for his formalisation.</note>

			<note place="foot" n="3"> The IAPR TC-12 and SAIAPR TC-12 data is available from http://imageclef.org; REFERITGAME from http://tamaraberg.com/referitgame.</note>

			<note place="foot" n="4"> The data is available from https://github.com/ mjhucla/Google_Refexp_toolbox.</note>

			<note place="foot" n="8"> Using a different split than (Mao et al., 2015), as their train/test set overlaps on the level of images.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank <ref type="bibr" target="#b10">Hu et al. (2016)</ref>, <ref type="bibr" target="#b17">Mao et al. (2016)</ref> and Tamara Berg for giving us access to their data. Thanks are also due to the anonymous reviewers for their very insightful comments. We acknowl-edge support by the Cluster of Excellence "Cogni-tive Interaction Technology" (CITEC; EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG), and by the DUEL project, also funded by DFG (grant SCHL 845/5-1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Supplemental Material</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Type theory with records for natural language semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ginzburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Contemporary Semantic Theory</title>
		<editor>Shalom Lappin and Chris Fox</editor>
		<imprint>
			<publisher>WileyBlackwell</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR09</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models for image captioning: The quirks and what works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="100" to="105" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable Object Detection Using Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2155" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The segmented and annotated IAPR TC-12 benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo Jair</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>López-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><forename type="middle">F</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Enrique</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Villaseñor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grubinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="419" to="428" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Logic, Language and Meaning: Intensional Logic and Logical Grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T F</forename><surname>Gamut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>Chicago University Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The IAPR TC-12 benchmark: a new evaluation resource for visual information systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC 2006)</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC 2006)<address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The symbol grounding problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stevan</forename><surname>Harnad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="346" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1511.04164</idno>
		<title level="m">Natural language object retrieval. ArXiv / CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR 2016</title>
		<meeting>CVPR 2016<address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to Objects in Photographs of Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahar</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="787" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple learning and compositional application of perceptually grounded word meanings for incremental reference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Kennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="292" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jointly learning to parse and perceive: Connecting natural language to the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="193" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Formal semantics for perceptual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Staffan Larsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of logic and computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="369" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision ECCV</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>abs/1511.02283</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ArXiv / CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR 2016</title>
		<meeting>CVPR 2016<address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Joint Model of Language and Perception for Grounded Attribute Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013 (NIPS 2013)</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A trainable spoken language understanding system for visual object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gorniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Juster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Speech and Language Processing</title>
		<meeting>the International Conference on Speech and Language Processing<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Colorado</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning visually-grounded words and syntax for a scene description task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grounding words in perception and action: Computational insights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciene</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="389" to="396" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grounded Compositional Semantics for Finding and Describing Images with Sentences</title>
	</analytic>
	<monogr>
		<title level="m">Transactions of the ACL (TACL)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashis</forename><forename type="middle">Gopal</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1507" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Formal Philosophy: Selected Papers of Richard Montague</title>
		<editor>Richmond H. Thomason</editor>
		<imprint>
			<date type="published" when="1974" />
			<publisher>Yale University Press</publisher>
			<pubPlace>New Haven and London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Easy things first: Installments improve referring expression generation for objects in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Zarrieß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">cactus&apos;, &apos;sun&apos;, &apos;smoke&apos;, &apos;llama&apos;, &apos;fruit&apos;, &apos;ruins&apos;, &apos;waterfall&apos;, &apos;nightstand&apos;, &apos;books&apos;, &apos;night&apos;, &apos;coke&apos;, &apos;skirt&apos;, &apos;leaf&apos;, &apos;wheel&apos;, &apos;label&apos;, &apos;pot&apos;, &apos;animals&apos;, &apos;cup&apos;, &apos;tablecloth&apos;, &apos;pillar&apos;, &apos;flag&apos;, &apos;field&apos;, &apos;monkey&apos;, &apos;bowl&apos;, &apos;curtain&apos;, &apos;plate&apos;, &apos;van&apos;, &apos;surfboard&apos;, &apos;bottle&apos;, &apos;fish&apos;, &apos;umbrella&apos;, &apos;bus&apos;, &apos;shirtless&apos;, &apos;train&apos;, &apos;bed&apos;, &apos;painting&apos;, &apos;lamp&apos;, &apos;metal&apos;, &apos;paper&apos;, &apos;sky&apos;, &apos;luggage&apos;, &apos;player&apos;, &apos;face&apos;, &apos;going&apos;, &apos;desk&apos;, &apos;ship&apos;, &apos;raft&apos;, &apos;lying&apos;, &apos;vehicle&apos;, &apos;trunk&apos;, &apos;couch&apos;, &apos;palm&apos;, &apos;dress&apos;, &apos;doors&apos;, &apos;fountain&apos;, &apos;column&apos;, &apos;cars&apos;, &apos;flowers&apos;, &apos;tire&apos;, &apos;plane&apos;, &apos;against&apos;, &apos;bunch&apos;, &apos;car&apos;, &apos;shelf&apos;, &apos;bunk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>&amp;apos;court&amp;apos;, &amp;apos;riding&amp;apos;, &amp;apos;penguin&amp;apos;, &amp;apos;balloon&amp;apos;, &amp;apos;ball&amp;apos;, &amp;apos;mug&amp;apos;, &amp;apos;turtle&amp;apos;, &amp;apos;tennis&amp;apos;, &amp;apos;beer&amp;apos;, &amp;apos;seal&amp;apos;, &amp;apos;cow&amp;apos;, &amp;apos;bird&amp;apos;, &amp;apos;horse&amp;apos;, &amp;apos;drink&amp;apos;, &amp;apos;koala&amp;apos;, &amp;apos;sheep</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Average Precision See Section 6. Classifiers with average precision over 0.85: [&apos;giraffe&apos;, &apos;coffee&apos;. boat&apos;, &apos;dog&apos;, &apos;vase&apos;, &apos;animal&apos;, &apos;pack&apos;, &apos;anyone&apos;, &apos;clock&apos;, &apos;glass&apos;, &apos;tile&apos;, &apos;window&apos;, &apos;chair&apos;, &apos;phone&apos;, &apos;across&apos;, &apos;cake&apos;, &apos;branches&apos;, &apos;bicycle&apos;, &apos;snow&apos;, &apos;windows&apos;, &apos;book&apos;, &apos;curtains&apos;, &apos;bear&apos;, &apos;guitar&apos;, &apos;dish&apos;, &apos;both&apos;, &apos;tower&apos;, &apos;truck&apos;, &apos;bridge&apos;, &apos;creepy&apos;, &apos;cloud&apos;, &apos;suit&apos;, &apos;stool&apos;, &apos;tv&apos;, &apos;flower&apos;, &apos;seat&apos;, &apos;buildings&apos;, &apos;shoes&apos;, &apos;bread&apos;, &apos;hut&apos;, &apos;donkey&apos;, &apos;had&apos;, &apos;were&apos;, &apos;fire&apos;, &apos;food&apos;, &apos;turned&apos;, &apos;mountains&apos;, &apos;city&apos;, &apos;range&apos;, &apos;inside&apos;, &apos;carpet&apos;, &apos;beach&apos;, &apos;walls&apos;, &apos;ice&apos;, &apos;crowd&apos;, &apos;mirror&apos;, &apos;brush&apos;, &apos;road&apos;, &apos;anything&apos;, &apos;blanket&apos;, &apos;clouds&apos;, &apos;island&apos;, &apos;building&apos;, &apos;door&apos;, &apos;4th&apos;, &apos;stripes&apos;, &apos;bottles&apos;, &apos;cross&apos;, &apos;gold&apos;, &apos;smiling&apos;, &apos;pillow&apos;</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">looks&apos;, &apos;very&apos;, &apos;second&apos;, &apos;its&apos;, &apos;dat&apos;, &apos;stack&apos;, &apos;dudes&apos;, &apos;men&apos;, &apos;him&apos;, &apos;arm&apos;, &apos;smaller&apos;, &apos;half&apos;, &apos;piece&apos;, &apos;out&apos;, &apos;item&apos;, &apos;line&apos;, &apos;stuff&apos;, &apos;he&apos;, &apos;spot&apos;, &apos;green&apos;, &apos;head&apos;, &apos;see&apos;, &apos;be&apos;, &apos;black&apos;, &apos;think&apos;, &apos;leg&apos;, &apos;way&apos;, &apos;women&apos;, &apos;furthest&apos;, &apos;rt&apos;, &apos;most&apos;, &apos;big&apos;, &apos;grey&apos;, &apos;only&apos;, &apos;like&apos;, &apos;corner&apos;, &apos;picture&apos;, &apos;shoulder&apos;, &apos;no&apos;, &apos;spiders&apos;, &apos;n&apos;, &apos;has&apos;, &apos;his&apos;, &apos;we&apos;, &apos;bit&apos;, &apos;spider&apos;, &apos;guys&apos;, &apos;2&apos;, &apos;portion&apos;, &apos;are&apos;, &apos;section&apos;, &apos;us&apos;, &apos;towards&apos;, &apos;sorry&apos;, &apos;where</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><surname>Bright&amp;apos;, &amp;apos;lol&amp;apos;, &amp;apos;blue&amp;apos;, &amp;apos;her&amp;apos;, &amp;apos;yes&amp;apos;, &amp;apos;blk</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Classifiers with average precision below 0.6: [&apos;shadow&apos;, &quot;woman&apos;s&quot;, &apos;was&apos;. gray&apos;, &apos;image&apos;, &apos;but&apos;, &apos;something&apos;, &apos;center&apos;, &apos;i&apos;, &apos;closest&apos;, &apos;first&apos;, &apos;middle&apos;, &apos;those&apos;, &apos;edge&apos;, &apos;there&apos;, &apos;or&apos;, &apos;white&apos;, &apos;-&apos;, &apos;little&apos;, &apos;them&apos;, &apos;barely&apos;, &apos;brown&apos;, &apos;all&apos;, &apos;mid&apos;, &apos;is&apos;, &apos;thing&apos;, &apos;dark&apos;, &apos;by&apos;, &apos;back&apos;, &apos;with&apos;, &apos;other&apos;, &apos;near&apos;, &apos;two&apos;, &apos;screen&apos;, &apos;so&apos;, &apos;front&apos;, &apos;you&apos;, &apos;photo&apos;, &apos;up&apos;, &apos;one&apos;, &apos;it&apos;, &apos;space&apos;, &apos;okay&apos;, &apos;side&apos;, &apos;click&apos;, &apos;part&apos;, &apos;pic&apos;, &apos;at&apos;, &apos;that&apos;, &apos;area&apos;, &apos;directly&apos;, &apos;in&apos;, &apos;on&apos;, &apos;and&apos;, &apos;to&apos;, &apos;just&apos;, &apos;of&apos;</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
