<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aspect Based Sentiment Analysis with Gated Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Sciences</orgName>
								<orgName type="institution">Florida International University</orgName>
								<address>
									<settlement>Miami</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Sciences</orgName>
								<orgName type="institution">Florida International University</orgName>
								<address>
									<settlement>Miami</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Aspect Based Sentiment Analysis with Gated Convolutional Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2514" to="2523"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2514</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Second , the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Opinion mining and sentiment analysis <ref type="bibr" target="#b20">(Pang and Lee, 2008</ref>) on user-generated reviews can pro- vide valuable information for providers and con- sumers. Instead of predicting the overall sen-timent polarity, fine-grained aspect based senti- ment analysis (ABSA) ( <ref type="bibr" target="#b17">Liu and Zhang, 2012</ref>) is proposed to better understand reviews than tradi- tional sentiment analysis. Specifically, we are in- terested in the sentiment polarity of aspect cate- gories or target entities in the text. Sometimes, it is coupled with aspect term extractions ( <ref type="bibr" target="#b34">Xue et al., 2017)</ref>. A number of models have been developed for ABSA, but there are two different subtasks, namely aspect-category sentiment anal- ysis (ACSA) and aspect-term sentiment analysis (ATSA). The goal of ACSA is to predict the sen- timent polarity with regard to the given aspect, which is one of a few predefined categories. On the other hand, the goal of ATSA is to identify the sentiment polarity concerning the target enti- ties that appear in the text instead, which could be a multi-word phrase or a single word. The num- ber of distinct words contributing to aspect terms could be more than a thousand. For example, in the sentence "Average to good Thai food, but terri- ble delivery.", ATSA would ask the sentiment po- larity towards the entity Thai food; while ACSA would ask the sentiment polarity toward the aspect service, even though the word service does not ap- pear in the sentence.</p><p>Many existing models use LSTM lay- ers <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) to distill sentiment information from embedding vectors, and apply attention mechanisms <ref type="bibr" target="#b0">(Bahdanau et al., 2014</ref>) to enforce models to focus on the text spans related to the given aspect/entity. Such models include Attention-based LSTM with Aspect Embedding (ATAE-LSTM) ( <ref type="bibr" target="#b31">Wang et al., 2016b</ref>) for ACSA; Target-Dependent Sentiment Classification (TD-LSTM) ( <ref type="bibr" target="#b27">Tang et al., 2016a</ref>), Gated Neural Networks ( <ref type="bibr" target="#b35">Zhang et al., 2016)</ref> and Recurrent Attention Memory Network (RAM) <ref type="bibr" target="#b1">(Chen et al., 2017</ref>) for ATSA. Attention mechanisms has been successfully used in many NLP tasks. It first computes the alignment scores between context vectors and target vector; then carry out a weighted sum with the scores and the context vectors. However, the context vectors have to encode both the aspect and sentiment information, and the alignment scores are applied across all feature dimensions regardless of the dif- ferences between these two types of information. Both LSTM and attention layer are very time- consuming during training. LSTM processes one token in a step. Attention layer involves exponen- tial operation and normalization of all alignment scores of all the words in the sentence ( <ref type="bibr" target="#b31">Wang et al., 2016b</ref>). Moreover, some models needs the positional information between words and targets to produce weighted LSTM ( <ref type="bibr" target="#b1">Chen et al., 2017)</ref>, which can be unreliable in noisy review text. Certainly, it is possible to achieve higher accuracy by building more and more complicated LSTM cells and sophisticated attention mechanisms; but one has to hold more parameters in memory, get more hyper-parameters to tune and spend more time in training. In this paper, we propose a fast and effective neural network for ACSA and ATSA based on convolutions and gating mechanisms, which has much less training time than LSTM based networks, but with better accuracy.</p><p>For ACSA task, our model has two separate convolutional layers on the top of the embedding layer, whose outputs are combined by novel gat- ing units. Convolutional layers with multiple fil- ters can efficiently extract n-gram features at many granularities on each receptive field. The pro- posed gating units have two nonlinear gates, each of which is connected to one convolutional layer. With the given aspect information, they can selec- tively extract aspect-specific sentiment informa- tion for sentiment prediction. For example, in the sentence "Average to good Thai food, but terrible delivery.", when the aspect food is provided, the gating units automatically ignore the negative sen- timent of aspect delivery from the second clause, and only output the positive sentiment from the first clause. Since each component of the proposed model could be easily parallelized, it has much less training time than the models based on LSTM and attention mechanisms. For ATSA task, where the aspect terms consist of multiple words, we ex- tend our model to include another convolutional layer for the target expressions. We evaluate our models on the SemEval datasets, which contains restaurants and laptops reviews with labels on as- pect level. To the best of our knowledge, no CNN- based model has been proposed for aspect based sentiment analysis so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We present the relevant studies into following two categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Networks</head><p>Recently, neural networks have gained much pop- ularity on sentiment analysis or sentence classifi- cation task. Tree-based recursive neural networks such as Recursive Neural Tensor Network ( <ref type="bibr" target="#b25">Socher et al., 2013)</ref> and Tree-LSTM ( <ref type="bibr" target="#b26">Tai et al., 2015)</ref>, make use of syntactic interpretation of the sen- tence structure, but these methods suffer from time inefficiency and parsing errors on review text. Recurrent Neural Networks (RNNs) such as LSTM <ref type="bibr" target="#b9">(Hochreiter and Schmidhuber, 1997</ref>) and GRU ( <ref type="bibr" target="#b2">Chung et al., 2014</ref>) have been used for sen- timent analysis on data instances having variable length ( <ref type="bibr" target="#b28">Tang et al., 2015;</ref><ref type="bibr" target="#b33">Xu et al., 2016;</ref><ref type="bibr" target="#b14">Lai et al., 2015)</ref>. There are also many models that use convolutional neural networks (CNNs) <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr" target="#b11">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b12">Kim, 2014;</ref><ref type="bibr" target="#b4">Conneau et al., 2016</ref>) in NLP, which also prove that convolution operations can capture composi- tional structure of texts with rich semantic infor- mation without laborious feature engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Aspect based Sentiment Analysis</head><p>There is abundant research work on aspect based sentiment analysis. Actually, the name ABSA is used to describe two different subtasks in the lit- erature. We classify the existing work into two main categories based on the descriptions of senti- ment analysis tasks in <ref type="bibr">SemEval 2014</ref><ref type="bibr">Task 4 (Pontiki et al., 2014</ref>): Aspect-Term Sentiment Analysis and Aspect-Category Sentiment Analysis.</p><p>Aspect-Term Sentiment Analysis. In the first category, sentiment analysis is performed toward the aspect terms that are labeled in the given sen- tence. A large body of literature tries to utilize the relation or position between the target words and the surrounding context words either by using the tree structure of dependency or by simply counting the number of words between them as a relevance information <ref type="bibr" target="#b1">(Chen et al., 2017)</ref>.</p><p>Recursive neural networks ( <ref type="bibr" target="#b15">Lakkaraju et al., 2014;</ref><ref type="bibr" target="#b6">Dong et al., 2014;</ref><ref type="bibr" target="#b30">Wang et al., 2016a</ref>) rely on external syntactic parsers, which can be very inaccurate and slow on noisy texts like tweets and reviews, which may result in inferior performance.</p><p>Recurrent neural networks are commonly used in many NLP tasks as well as in ABSA prob- lem. TD-LSTM ( <ref type="bibr" target="#b27">Tang et al., 2016a</ref>) and gated neural networks ( <ref type="bibr" target="#b35">Zhang et al., 2016</ref>) use two or three LSTM networks to model the left and right contexts of the given target individually. A fully- connected layer with gating units predicts the sen- timent polarity with the outputs of LSTM layers.</p><p>Memory network ( <ref type="bibr" target="#b32">Weston et al., 2014</ref>) coupled with multiple-hop attention attempts to explicitly focus only on the most informative context area to infer the sentiment polarity towards the tar- get word ( <ref type="bibr" target="#b29">Tang et al., 2016b;</ref><ref type="bibr" target="#b1">Chen et al., 2017)</ref>. Nonetheless, memory network simply bases its knowledge bank on the embedding vectors of in- dividual words <ref type="bibr" target="#b29">(Tang et al., 2016b)</ref>, which makes itself hard to learn the opinion word enclosed in more complicated contexts. The performance is improved by using LSTM, attention layer and feature engineering with word distance between surrounding words and target words to produce target-specific memory <ref type="bibr" target="#b1">(Chen et al., 2017)</ref>.</p><p>Aspect-Category Sentiment Analysis. In this category, the model is asked to predict the sen- timent polarity toward a predefined aspect cate- gory. Attention-based LSTM with Aspect Embed- ding ( <ref type="bibr" target="#b31">Wang et al., 2016b</ref>) uses the embedding vec- tors of aspect words to selectively attend the re- gions of the representations generated by LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gated Convolutional Network with Aspect Embedding</head><p>In this section, we present a new model for ACSA and ATSA, namely Gated Convolutional network with Aspect Embedding (GCAE), which is more efficient and simpler than recurrent network based models ( <ref type="bibr" target="#b31">Wang et al., 2016b;</ref><ref type="bibr" target="#b27">Tang et al., 2016a;</ref><ref type="bibr" target="#b18">Ma et al., 2017;</ref><ref type="bibr" target="#b1">Chen et al., 2017)</ref>. Recurrent neu- ral networks sequentially compose hidden vectors</p><formula xml:id="formula_0">h i = f (h i−1 )</formula><p>, which does not enable paralleliza- tion over inputs. In the attention layer, softmax normalization also has to wait for all the alignment scores computed by a similarity function. Hence, they cannot take advantage of highly-parallelized modern hardware and libraries. Our model is built on convolutional layers and gating units. Each convolutional filter computes n-gram features at different granularities from the embedding vectors at each position individually. The gating units on top of the convolutional layers at each position are also independent from each other. Therefore, our model is more suitable to parallel computing. Moreover, our model is equipped with two kinds of effective filtering mechanisms: the gating units on top of the convolutional layers and the max pooling layer, both of which can accurately gen- erate and select aspect-related sentiment features. We first briefly review the vanilla CNN for text classification <ref type="bibr" target="#b12">(Kim, 2014)</ref>. The model achieves state-of-the-art performance on many standard sentiment classification datasets ( <ref type="bibr" target="#b16">Le et al., 2017</ref>).</p><p>The CNN model consists of an embedding layer, a one-dimension convolutional layer and a max-pooling layer. The embedding layer takes the indices w i ∈ {1, 2, . . . , V } of the input words and outputs the corresponding embedding vec- tors v i ∈ R D . D denotes the dimension size of the embedding vectors. V is the size of the word vocabulary. The embedding layer is usu- ally initialized with pre-trained embeddings such as GloVe ( <ref type="bibr" target="#b21">Pennington et al., 2014</ref>), then they are fine-tuned during the training stage. The one- dimension convolutional layer convolves the in- puts with multiple convolutional kernels of differ- ent widths. Each kernel corresponds a linguistic feature detector which extracts a specific pattern of n-gram at various granularities ( <ref type="bibr" target="#b11">Kalchbrenner et al., 2014</ref>). Specifically, the input sentence is represented by a matrix through the embedding layer,</p><formula xml:id="formula_1">X = [v 1 , v 2 , . . . , v L ],</formula><p>where L is the length of the sentence with padding. A convolutional fil- ter W c ∈ R D×k maps k words in the receptive field to a single feature c. As we slide the filter across the whole sentence, we obtain a sequence of new features c = [c 1 , c 2 , . . . , c L ].</p><formula xml:id="formula_2">c i = f (X i:i+K * W c + b c ) ,<label>(1)</label></formula><p>where b c ∈ R is the bias, f is a non-linear acti- vation function such as tanh function, * denotes convolution operation. If there are n k filters of the same width k, the output features form a ma- trix C ∈ R n k ×L k . For each convolutional filter, the max-over-time pooling layer takes the maxi- mal value among the generated convolutional fea- tures, resulting in a fixed-size vector whose size is equal to the number of filters n k . Finally, a soft- max layer uses the vector to predict the sentiment polarity of the input sentence. Gated Tanh-ReLU Units (GTRU) with aspect em- bedding are connected to two convolutional neu- rons at each position t. Specifically, we compute the features c i as</p><formula xml:id="formula_3">a i = relu(X i:i+k * W a + V a v a + b a ) (2) s i = tanh(X i:i+k * W s + b s ) (3) c i = s i × a i ,<label>(4)</label></formula><p>where v a is the embedding vector of the given as- pect category in ACSA or computed by another CNN over aspect terms in ATSA. The two convo- lutions in Equation 2 and 3 are the same as the convolution in the vanilla CNN, but the convo- lutional features a i receives additional aspect in- formation v a with ReLU activation function. In other words, s i and a i are responsible for generat- ing sentiment features and aspect features respec- tively. The above max-over-time pooling layer generates a fixed-size vector e ∈ R d k , which keeps the most salient sentiment features of the whole sentence. The final fully-connected layer with softmax function uses the vector e to pre- dict the sentiment polarityˆypolarityˆ polarityˆy. The model is trained by minimizing the cross-entropy loss between the ground-truth y and the predicted valuê y for all data samples.</p><formula xml:id="formula_4">L = − i j y j i logˆylogˆ logˆy j i ,<label>(5)</label></formula><p>where i is the index of a data sample, j is the index of a sentiment class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Gating Mechanisms</head><p>The proposed Gated Tanh-ReLU Units control the path through which the sentiment information flows towards the pooling layer. The gating mech- anisms have proven to be effective in LSTM. In as- pect based sentiment analysis, it is very common that different aspects with different sentiments ap- pear in one sentence. The ReLU gate in Equation 2 does not have upper bound on positive inputs but strictly zero on negative inputs. Therefore, it can output a similarity score according to the relevance between the given aspect information v a and the aspect feature a i at position t. If this score is zero, the sentiment features s i would be blocked at the gate; otherwise, its magnitude would be amplified accordingly. The max-over-time pooling further removes the sentiment features which are not sig- nificant over the whole sentence.</p><p>In language modeling ( <ref type="bibr" target="#b10">Kalchbrenner et al., 2016;</ref><ref type="bibr">van den Oord et al., 2016;</ref><ref type="bibr" target="#b8">Gehring et al., 2017)</ref>, Gated Tanh Units (GTU) and Gated Linear Units (GLU) have shown effectiveness of gating mechanisms. GTU is rep- resented by tanh(X * W + b) × σ(X * V + c), in which the sigmoid gates control features for pre- dicting the next word in a stacked convolutional block. To overcome the gradient vanishing prob- lem of GTU, GLU uses (X * W+b)×σ(X * V+c) instead, so that the gradients would not be down- scaled to propagate through many stacked convo- lutional layers. However, a neural network that has only one convolutional layer would not suf- fer from gradient vanish problem during training. We show that on text classification problem, our GTRU is more effective than these two gating units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GCAE on ATSA</head><p>ATSA task is defined to predict the sentiment po- larity of the aspect terms in the given sentence. We simply extend GCAE by adding a small con- volutional layer on aspect terms, as shown in <ref type="figure" target="#fig_1">Fig- ure 2.</ref> In ACSA, the aspect information controlling the flow of sentiment features in GTRU is from one aspect word; while in ATSA, such informa- tion is provided by a small CNN on aspect terms while retains the ability of parallel computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets and Experiment Preparation</head><p>We conduct experiments on public datasets from SemEval workshops ( <ref type="bibr" target="#b22">Pontiki et al., 2014</ref>), which consist of customer reviews about restaurants and laptops. Some existing work ( <ref type="bibr" target="#b31">Wang et al., 2016b;</ref><ref type="bibr" target="#b18">Ma et al., 2017;</ref><ref type="bibr" target="#b1">Chen et al., 2017</ref>) removed "con- flict" labels from four sentiment labels, which makes their results incomparable to those from the workshop report <ref type="bibr" target="#b13">(Kiritchenko et al., 2014</ref>). We reimplemented the compared methods, and used hyper-parameter settings described in these refer- ences. The sentences which have different sentiment labels for different aspects or targets in the sen- tence are more common in review data than in standard sentiment classification benchmark. The sentence in <ref type="table">Table 1</ref> shows the reviewer's different attitude towards two aspects: food and delivery. Therefore, to access how the models perform on review sentences more accurately, we create small but difficult datasets, which are made up of the sentences having opposite or different sentiments on different aspects/targets. In <ref type="table">Table 1</ref>, the two identical sentences but with different sentiment la- bels are both included in the dataset. If a sentence has 4 aspect targets, this sentence would have 4 copies in the data set, each of which is associated with different target and sentiment label.</p><p>For ACSA task, we conduct experiments on restaurant review data of SemEval 2014 Task 4. There are 5 aspects: food, price, service, ambi- ence, and misc; 4 sentiment polarities: positive, negative, neutral, and conflict. By merging restau- rant reviews of three years 2014 -2016, we obtain a larger dataset called "Restaurant-Large". Incom- patibilities of data are fixed during merging. We replace conflict labels with neutral labels in the 2014 dataset. In the 2015 and 2016 datasets, there could be multiple pairs of "aspect terms" and "as- pect category" in one sentence. For each sentence, let p denote the number of positive labels minus the number of negative labels. We assign a sen- tence a positive label if p &gt; 0, a negative label if p &lt; 0, or a neutral label if p = 0. After removing duplicates, the statistics are show in <ref type="table" target="#tab_0">Table 2</ref>. The resulting dataset has 8 aspects: restaurant, food, drinks, ambience, service, price, misc and loca- tion.</p><p>For ATSA task, we use restaurant reviews and laptop reviews from SemEval 2014 Task 4. On each dataset, we duplicate each sentence n a times, which is equal to the number of associated aspect categories (ACSA) or aspect terms (ATSA) <ref type="bibr">(Ruder et al., 2016b,a)</ref>. The statistics of the datasets are shown in <ref type="table" target="#tab_0">Table 2</ref>.</p><p>The sizes of hard data sets are also shown in Ta- ble 2. The test set is designed to measure whether a model can detect multiple different sentiment polarities in one sentence toward different enti- ties. Without such sentences, a classifier for over- all sentiment classification might be good enough Sentence aspect category/term sentiment label Average to good Thai food, but terrible delivery. food positive Average to good Thai food, but terrible delivery. delivery negative <ref type="table">Table 1</ref>: Two example sentences in one hard test set of restaurant review dataset of SemEval 2014.</p><p>for the sentences associated with only one senti- ment label.</p><p>In our experiments, word embedding vectors are initialized with 300-dimension GloVe vectors which are pre-trained on unlabeled data of 840 bil- lion tokens ( <ref type="bibr" target="#b21">Pennington et al., 2014</ref>). Words out of the vocabulary of GloVe are randomly initialized with a uniform distribution U (−0.25, 0.25). We use Adagrad ( <ref type="bibr" target="#b7">Duchi et al., 2011</ref>) with a batch size of 32 instances, default learning rate of 1e−2, and maximal epochs of 30. We only fine tune early stopping with 5-fold cross validation on training datasets. All neural models are implemented in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Compared Methods</head><p>To comprehensively evaluate the performance of GCAE, we compare our model against the follow- ing models.</p><p>NRC-Canada ( <ref type="bibr" target="#b13">Kiritchenko et al., 2014</ref>) is the top method in SemEval 2014 Task 4 for ACSA and ATSA task. SVM is trained with extensive fea- ture engineering: various types of n-grams, POS tags, and lexicon features. The sentiment lexicons improve the performance significantly, but it re- quires large scale labeled data: 183 thousand Yelp reviews, 124 thousand Amazon laptop reviews, 56 million tweets, and 3 sentiment lexicons labeled manually.</p><p>CNN <ref type="bibr" target="#b12">(Kim, 2014</ref>) is widely used on text clas- sification task. It cannot directly capture aspect- specific sentiment information on ACSA task, but it provides a very strong baseline for sentiment classification. We set the widths of filters to 3, 4, 5 with 100 features each.</p><p>TD-LSTM (Tang et al., 2016a) uses two LSTM networks to model the preceding and following contexts of the target to generate target-dependent representation for sentiment prediction.</p><p>ATAE-LSTM ( <ref type="bibr" target="#b31">Wang et al., 2016b</ref>) is an attention-based LSTM for ACSA task. It appends the given aspect embedding with each word em- bedding as the input of LSTM, and has an atten- tion layer above the LSTM layer.</p><p>IAN (Ma et al., 2017) stands for interactive attention network for ATSA task, which is also based on LSTM and attention mechanisms. RAM ( <ref type="bibr" target="#b1">Chen et al., 2017</ref>) is a recurrent atten- tion network for ATSA task, which uses LSTM and multiple attention mechanisms.</p><p>GCN stands for gated convolutional neural net- work, in which GTRU does not have the aspect embedding as an additional input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">ACSA</head><p>Following the SemEval workshop, we report the overall accuracy of all competing models over the test datasets of restaurant reviews as well as the hard test datasets. Every experiment is repeated five times. The mean and the standard deviation are reported in <ref type="table" target="#tab_2">Table 4</ref>.</p><p>LSTM based model ATAE-LSTM has the worst performance of all neural networks. Aspect-based sentiment analysis is to extract the sentiment in- formation closely related to the given aspect. It is important to separate aspect information and sen- timent information from the extracted information of sentences. The context vectors generated by LSTM have to convey the two kinds of informa- tion at the same time. Moreover, the attention scores generated by the similarity scoring function are for the entire context vector. GCAE improves the performance by 1.1% to 2.5% compared with ATAE-LSTM. First, our model incorporates GTRU to control the sentiment information flow according to the given aspect in- formation at each dimension of the context vec- tors. The element-wise gating mechanism works at fine granularity instead of exerting an alignment score to all the dimensions of the context vectors in the attention layer of other models. Second, GCAE does not generate a single context vector, but two vectors for aspect and sentiment features respectively, so that aspect and sentiment informa- tion is unraveled. By comparing the performance on the hard test datasets against CNN, it is easy to see the convolutional layer of GCAE is able to differentiate the sentiments of multiple entities.</p><p>Convolutional neural networks CNN and GCN <ref type="table" target="#tab_0">Positive  Negative  Neutral  Conflict  Train Test Train Test Train Test Train Test  Restaurant-Large  2710 1505 1198 680 757  241 - - Restaurant-Large-Hard 182  92  178  81  107  61  - - Restaurant-2014  2179 657  839  222 500  94  195  52  Restaurant-2014-Hard 139  32  136  26  50  12  40</ref> 19  <ref type="table">Table 3</ref>: Statistics of the datasets for ATSA task.</p><p>are not designed for aspect based sentiment anal- ysis, but their performance exceeds that of ATAE- LSTM.</p><p>The performance of SVM ( <ref type="bibr" target="#b13">Kiritchenko et al., 2014</ref>) depends on the availability of the features it can use. Without the large amount of sentiment lexicons, SVM perform worse than neural meth- ods. With multiple sentiment lexicons, the perfor- mance is increased by 7.6%. This inspires us to work on leveraging sentiment lexicons in neural networks in the future.</p><p>The hard test datasets consist of replicated sen- tences with different sentiments towards differ- ent aspects. The models which cannot utilize the given aspect information such as CNN and GCN perform poorly as expected, but GCAE has higher accuracy than other neural network mod- els. GCAE achieves 4% higher accuracy than ATAE-LSTM on Restaurant-Large and 5% higher on SemEval-2014 on ACSA task. However, GCN, which does not have aspect modeling part, has higher score than GCAE on the original restaurant dataset. It suggests that GCN performs better than GCAE when there is only one sentiment label in the given sentence, but not on the hard test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">ATSA</head><p>We apply the extended version of GCAE on ATSA task. On this task, the aspect terms are marked in the sentences and usually consist of multi- ple words. We compare IAN ( <ref type="bibr" target="#b18">Ma et al., 2017</ref>), RAM ( <ref type="bibr" target="#b1">Chen et al., 2017)</ref>, TD-LSTM ( <ref type="bibr" target="#b27">Tang et al., 2016a</ref>), ATAE-LSTM ( <ref type="bibr" target="#b31">Wang et al., 2016b)</ref>, and our GCAE model in <ref type="table" target="#tab_3">Table 5</ref>. The models other than GCAE is based on LSTM and attention mechanisms. IAN has better performance than TD-LSTM and ATAE-LSTM, because two atten- tion layers guides the representation learning of the context and the entity interactively. RAM also achieves good accuracy by combining multiple at- tentions with a recurrent neural network, but it needs more training time as shown in the follow- ing section. On the hard test dataset, GCAE has 1% higher accuracy than RAM on restaurant data and 1.7% higher on laptop data. GCAE uses the outputs of the small CNN over aspect terms to guide the composition of the sentiment features through the ReLU gate. Because of the gating mechanisms and the convolutional layer over as- pect terms, GCAE outperforms other neural mod- els and basic SVM. Again, large scale sentiment lexicons bring significant improvement to SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Training Time</head><p>We record the training time of all models un- til convergence on a validation set on a desktop machine with a 1080 Ti GPU, as shown in Ta- ble <ref type="bibr">6</ref>. LSTM based models take more training time than convolutional models. On ATSA task, because of multiple attention layers in IAN and RAM, they need even more time to finish the training. GCAE is much faster than other neural models, because neither convolutional operation nor GTRU has time dependency compared with LSTM and attention layer. Therefore, it is easier for hardware and libraries to parallel the comput-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Restaurant- <ref type="table" target="#tab_0">Large  Restaurant 2014  Test  Hard Test  Test  Hard Test  SVM*  - -</ref>      ing process. Since the performance of SVM is re- trieved from the original paper, we are not able to compare the training time of SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Gating Mechanisms</head><p>In this section, we compare GLU (X * W + b) × σ(X * W a + Vv a + b a ) ( ), GTU tanh(X * W + b) × σ(X * W a + Vv a + b a ) (van den <ref type="bibr">Oord et al., 2016)</ref>, and GTRU used in GCAE. <ref type="table" target="#tab_5">Table 7</ref> shows that all of three gating units achieve relatively high accuracy on restau- rant datasets. GTRU outperforms the other gates. It has a convolutional layer generating aspect fea- tures via ReLU activation function, which controls the magnitude of the sentiment signals according to the given aspect information. On the other hand, the sigmoid function in GTU and GLU has the up- per bound +1, which may not be able to distill sentiment features effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Visualization</head><p>In this section, we take a concrete review sen- tence as an example to illustrate how the proposed gate GTRU works. It is more difficult to visualize the weights generated by the gates than the atten- tion weights in other neural networks. The atten- tion weight score is a global score over the words and the vector dimensions; whereas in our model, there are N word × N filter × N dimension gate outputs. Therefore, we train a small model with only one filter which is only three word wide. Then, for each word, we sum the N dimension outputs of the ReLU gates. After normalization, we plot the val- ues on each word in <ref type="figure" target="#fig_2">Figure 3</ref>. Given different aspect targets, the ReLU gates would control the magnitude of the outputs of the tanh gates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>In this paper, we proposed an efficient convolu- tional neural network with gating mechanisms for ACSA and ATSA tasks. GTRU can effectively control the sentiment flow according to the given aspect information, and two convolutional layers model the aspect and sentiment information sep- arately. We prove the performance improvement compared with other neural models by extensive experiments on SemEval datasets. How to lever- age large-scale sentiment lexicons in neural net- works would be our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 Figure 1 :</head><label>11</label><figDesc>Figure 1: Illustration of our model GCAE for ACSA task. A pair of convolutional neuron computes features for a pair of gates: tanh gate and ReLU gate. The ReLU gate receives the given aspect information to control the propagation of sentiment features. The outputs of two gates are element-wisely multiplied for the max pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of model GCAE for ATSA task. It has an additional convolutional layer on aspect terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The outputs of the ReLU gates in GTRU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the datasets for ACSA task. The hard dataset is only made up of sentences having 
multiple aspect labels associated with multiple sentiments. 

Positive 
Negative 
Neutral 
Conflict 
Train Test Train Test Train Test Train Test 
Restaurant 
2164 728 805 
196 633 
196 91 
14 
Restaurant-Hard 379 
92 
323 
62 
293 
83 
43 
8 
Laptop 
987 
341 866 
128 460 
169 45 
16 
Laptop-Hard 
159 
31 
147 
25 
173 
49 
17 
3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The accuracy of all models on test sets and on the subsets made up of test sentences that have 
multiple sentiments and multiple aspect terms. Restaurant-Large dataset is created by merging all the 
restaurant reviews of SemEval workshops within three years. '*': the results with SVM are retrieved 
from NRC-Canada (Kiritchenko et al., 2014). 

Models 
Restaurant 
Laptop 
Test 
Hard Test 
Test 
Hard Test 
SVM* 
77.13 
-
63.61 
-
SVM + lexicons* 80.16 
-
70.49 
-
TD-LSTM 
73.44±1.17 56.48±2.46 62.23±0.92 46.11±1.89 
ATAE-LSTM 
73.74±3.01 50.98±2.27 64.38±4.52 40.39±1.30 
IAN 
76.34±0.27 55.16±1.97 68.49±0.57 44.51±0.48 
RAM 
76.97±0.64 55.85±1.60 68.48±0.85 45.37±2.03 
GCAE 
77.28±0.32 56.73±0.56 69.14±0.32 47.06±2.45 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The accuracy of ATSA subtask on SemEval 2014 Task 4. '*': the results with SVM are retrieved 
from NRC-Canada (Kiritchenko et al., 2014) 

Model 
ATSA 
ATAE 
25.28 
IAN 
82.87 
RAM 
64.16 
TD-LSTM 19.39 
GCAE 
3.33 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>The time to converge in seconds on ATSA 
task. 

Gates 
Restaurant-Large 
Restaurant 2014 
Test Hard Test Test Hard Test 
GTU 
84.62 60.25 
79.31 51.93 
GLU 
84.74 59.82 
79.12 50.80 
GTRU 85.92 70.75 
79.35 50.55 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>The accuracy of different gating units on 
restaurant reviews on ACSA task. 

</table></figure>

			<note place="foot" n="1"> The code and data is available at https://github. com/wxue004cs/GCAE</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs-1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent Attention Network on Memory for Aspect Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural Language Processing (Almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language Modeling with Gated Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>John C Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1610.10099</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Machine Translation in Linear Time. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">NRC-Canada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@COLING</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<title level="m">Recurrent Convolutional Neural Networks for Text Classification. AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aspect specific sentiment analysis using hierarchical deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Representation Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Do Convolutional Networks need to be Deep for Text Classification ? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Hoa T Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Cerisara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denis</surname></persName>
		</author>
		<idno>abs/1707.04108</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A Survey of Opinion Mining and Sentiment Analysis. Mining Text Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="415" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interactive Attention Networks for Aspect-Level Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4068" to="4074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Oriol Vinyals, and Alex Graves. 2016. Conditional Image Generation with PixelCNN Decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends R in Information Retrieva</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@COLING</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John G</forename><surname>Breslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="999" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">INSIGHT-1 at SemEval-2016 Task 5-Deep Learning for Multilingual Aspect-based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John G</forename><surname>Breslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="330" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Effective LSTMs for Target-Dependent Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Document Modeling with Gated Recurrent Neural Network for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aspect Level Sentiment Classification with Deep Memory Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokui</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for Aspectlevel Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs-1410.3916</idno>
		<title level="m">Memory Networks. In ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1660" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mtna: A neural multi-task model for aspect category classification and aspect term extraction on restaurant reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wubai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gated Neural Networks for Targeted Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy-Tin</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3087" to="3093" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
