<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to Train Dependency Parsers with Inexact Search for Joint Sentence Boundary Detection and Parsing of Entire Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Falé Nska</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Seeker</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Maschinelle Sprachverarbeitung</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How to Train Dependency Parsers with Inexact Search for Joint Sentence Boundary Detection and Parsing of Entire Documents</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1924" to="1934"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We cast sentence boundary detection and syntactic parsing as a joint problem, so an entire text document forms a training instance for transition-based dependency parsing. When trained with an early update or max-violation strategy for inexact search, we observe that only a tiny part of these very long training instances is ever exploited. We demonstrate this effect by extending the ArcStandard transition system with swap for the joint prediction task. When we use an alternative update strategy , our models are considerably better on both tasks and train in substantially less time compared to models trained with early update/max-violation. A comparison between a standard pipeline and our joint model furthermore empirically shows the usefulness of syntactic information on the task of sentence boundary detection.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although punctuation mostly provides reliable cues for segmenting longer texts into sentence units, human readers are able to exploit their un- derstanding of the syntactic and semantic structure to (re-)segment input in the absence of such cues.</p><p>When working with carefully copy-edited text documents, sentence boundary detection can be viewed as a minor preprocessing task in Natu- ral Language Processing, solvable with very high accuracy. However, when dealing with the out- put of automatic speech recognition or "noisier" texts such as blogs and emails, non-trivial sentence segmentation issues do occur. <ref type="bibr" target="#b12">Dridan and Oepen (2013)</ref>, for example, show how much impact fully automatic preprocessing can have on parsing qual- ity for well-edited and less-edited text.</p><p>Two possible strategies to approach this prob- lem are (i) to exploit other cues for sentence boundaries, such as prosodic phrasing and intona- tion in speech (e.g., <ref type="bibr" target="#b19">Kolář et al. (2006)</ref>) or format- ting cues in text documents ( <ref type="bibr" target="#b26">Read et al., 2012)</ref>, and (ii) to emulate the human ability to exploit syntactic competence for segmentation. We focus here on the latter, which has received little atten- tion, and propose to cast sentence boundary detec- tion and syntactic (dependency) parsing as a joint problem, such that segmentations that would give rise to suboptimal syntactic structures can be dis- carded early on.</p><p>A joint model for parsing and sentence bound- ary detection by definition operates on documents rather than single sentences, as is the standard case for parsing. The task is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, which shows the beginning of a document in the Switchboard corpus, a collection of transcribed telephone dialogues. The parser must predict the syntactic structure of the three sentences as well as the start points of each sentence. <ref type="bibr">1</ref> The simple fact that documents are consider- ably longer than sentences, often by orders of magnitude, creates some interesting challenges for a joint system. First of all, the decoder needs to handle long inputs efficiently. This problem is easily solved by using transition-based decoders, which excel in this kind of setting due to their in- cremental approach and their low theoretical com- plexity. Specifically, we use a transition-based de- coder that extends the Swap transition system of <ref type="bibr" target="#b25">Nivre (2009)</ref> in order to introduce sentence bound- aries during the parsing process. The parser per- forms inexact search for the optimal structure by you said you have four cats i have four cats how old are they . . . The beginning of a sample document from the Switchboard corpus. Tokens that start a sentence are underlined. The task is to predict syntactic structure and sentence boundaries jointly.</p><p>maintaining a beam of several candidate deriva- tions throughout the parsing process. We will show in this paper that, besides effi- cient decoding, a second, equally significant chal- lenge lies in the way such a parser is trained. Nor- mally, beam-search transition-based parsers are trained with structured perceptrons using either early update <ref type="bibr" target="#b29">(Zhang and Clark, 2008;</ref><ref type="bibr" target="#b7">Collins and Roark, 2004</ref>) or max-violation updates <ref type="bibr" target="#b17">(Huang et al., 2012</ref>). Yet our analysis demonstrates that nei- ther of these update strategies is appropriate for training on very long input sequences as they dis- card a large portion of the training data. 2 A sig- nificant part of the training data is therefore never used to train the model. As a remedy to this prob- lem, we instead use an adaptation of the update strategy in <ref type="bibr" target="#b0">Björkelund and Kuhn (2014)</ref>. They ap- ply early update in a coreference resolution system and observe that the task is inherently so difficult that the correct item practically never stays in the beam. So early updates are unable to exploit the full instances during training. They propose to ap- ply the updates iteratively on the same document until the full document has been observed. In our case, i.e. when parsing entire documents, the prob- lem is similar in that early updates do not reach the point where the learning algorithm exploits the full training data within reasonable time. Train- ing instead with the iterative update strategy gives us significantly better models in substantially less training time.</p><p>The second contribution in this paper is to demonstrate empirically that syntactic information can make up to a large extent for missing or un- reliable cues from punctuation. The joint system implements this hypothesis and allows us to test the influence of syntactic information on the pre-diction of sentence boundaries as compared to a pipeline baseline where both tasks are performed independently of each other. For our analysis, we use the Wall Street Journal as the standard benchmark set and as a representative for copy- edited text. We also use the Switchboard cor- pus of transcribed dialogues as a representative for data where punctuation cannot give clues to a sentence boundary predictor (other types of data that may show this property to varying degrees are web content data, e.g. forum posts or chat protocols, or (especially historical) manuscripts). While the Switchboard corpus gives us a realis- tic scenario for a setting with unreliable punctua- tion, the syntactic complexity of telephone conver- sations is rather low compared to the Wall Street Journal. Therefore, as a controlled experiment for assessing how far syntactic competence alone can take us if we stop trusting punctuation and capitalization entirely, we perform joint sentence boundary detection/parsing on a lower-cased, no- punctuation version of the Wall Street Journal. In this setting, where the parser must rely on syntac- tic information alone to predict sentence bound- aries, syntactic information makes a difference of 10 percentage point absolute for the sentence boundary detection task, and two points for la- beled parsing accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Transition system</head><p>We start from the ArcStandard system extended with a swap transition to handle non-projective arcs <ref type="bibr" target="#b25">(Nivre, 2009)</ref>. We add a transition SB (for sentence boundary) that flags the front of the buffer as the beginning of a new sentence. SB blocks the SHIFT transition until the stack has been reduced and a tree has been constructed, which prevents the system from introducing arcs between separate sentences. To track the predicted sentence boundaries, we augment the configura- tions with a set S to hold the predicted sentence boundaries. Conceptually this leads to a represen- tation where a document has a single artificial root</p><formula xml:id="formula_0">Transition Preconditions LEFTARC (σ|s1|s0, β, A, S) ⇒ (σ|s0, β, A ∪ {s0 → s1}, S) s1 = 0 RIGHTARC (σ|s1|s0, β, A, S) ⇒ (σ|s1, β, A ∪ {s1 → s0}, S) SHIFT (σ, b0|β, A, S) ⇒ (σ|b0, β, A, S) b0 = LAST(S) ∨ |σ| = 1 ∨ SWAPPED(β) SWAP (σ|s1|s0, β, A, S) ⇒ (σ|s0, s1|β, A, S) s1 &lt; s0 SB (σ, b0|β, A, S) ⇒ (σ, b0|β, A, S ∪ {b0}) LAST(S) &lt; b0 ∧ ¬SWAPPED(β)</formula><p>Figure 2: Transition system. σ|s 1 |s 0 denotes the stack with s 0 and s 1 on top, b 0 |β denotes the buffer with b 0 in front. LAST(S) denotes the most recent sentence boundary, and SWAPPED(β) is true iff the buffer contains swapped items.</p><p>node that replaces the artificial root nodes for in- dividual sentences.</p><p>The transition types of the system are shown in <ref type="figure">Figure 2</ref>. The configurations consist of four data structures: the stack σ, the input buffer β, the set of constructed arcs A, and the set of sen- tence boundaries S. LEFTARC, RIGHTARC, and SWAP have the same semantics and preconditions as in <ref type="bibr" target="#b25">Nivre (2009)</ref>. We modify the preconditions of SHIFT in order to block shifts when necessary. Whether shift is allowed can be categorized into three cases subject to the most recently predicted sentence boundary:</p><p>• If LAST(S) &lt; b 0 : The last predicted sen- tence boundary has already been shifted onto the stack. At this point, the system is building a new sentence and has not yet decided where it ends. SHIFT is therefore allowed.</p><p>• If LAST(S) &gt; b 0 : This situation can only oc- cur if the system predicted a sentence boundary and subsequently made a SWAP. LAST(S) then denotes the end of the current sentence and is deeper in the buffer than b 0 . Thus SHIFT is al- lowed since b 0 belongs to the current sentence.</p><p>• If LAST(S) = b 0 : The system must complete the current sentence by reducing the stack be- fore it can continue shifting and SHIFT is gen- erally not allowed, with two exceptions. (1) If the stack consists only of the root (i.e., |σ| = 1), the current sentence has been completed and the system is ready to begin parsing the next one.</p><p>(2) If b 0 denotes the beginning of a new sen- tence, but it has been swapped back onto the buffer, then it belongs to the same sentence as the tokens currently on the stack.</p><p>The preconditions for SB are straightforward. It is only allowed if the current b 0 is ahead of the most recently predicted sentence boundary. Addi- tionally, the transition is not allowed if b 0 has been swapped out from the stack. If it were, then b 0 would be part of the following sentence and sen- tences would no longer be continuous.</p><p>Extending the transition system to also handle sentence boundaries does not affect the compu- tational complexity. While the swap transition system has worst case O(n 2 ) complexity, Nivre (2009) shows that swaps are rare enough that the system maintains a linear time complexity on aver- age. A naive implementation of the configurations that make the arc set A and the sentence boundary set S explicit could result in configurations that re- quire linear time for copying during beam search. <ref type="bibr" target="#b14">Goldberg et al. (2013)</ref> show how this problem can be circumvented in the case of a sentence-based parser. Instead of making the arc set explicit, the arcs are reconstructed after parsing by following back-pointers to previous states. Only a small set of arcs required for feature extraction are saved in the states. We note that the same trick can be ap- plied to avoid keeping an explicit representation of S since the system only needs to know the last predicted sentence boundary.</p><p>Snt 1 sh sh la sh sh la sh sh sb early la ra ra ra sb late Snt 2 sh sh la sh sh sb early la ra ra sb late Snt 3 sh sh la sh la sh sb early ra ra sb late <ref type="table">Table 1</ref>: Transition sequences including sentence boundary transitions for the example in <ref type="figure" target="#fig_0">Figure 1</ref>.  oracle transitions of each sentence from <ref type="figure" target="#fig_0">Figure 1</ref>. During preliminary experiments we compared the two alternatives and found that the early version performed better than the late. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning</head><p>We focus on training structured perceptrons for search-based dependency parsing. Here, the score of a parse is defined as the scalar product of a weight vector w and a global feature vector Φ. The feature vector in turn is defined as the sum of lo- cal feature vectors φ, corresponding to the features extracted for a single transition t given a configu- ration c. During prediction we thus aim to obtainˆy</p><formula xml:id="formula_1">obtainˆ obtainˆy = arg max y∈Y Φ(y) · w = arg max y∈Y (t,c)∈TRSEQ(y) φ(t, c) · w (1)</formula><p>where TRSEQ represents the sequence of configu- rations and transitions executed to obtain the tree y. As the space of possible transition sequences is too large to search exhaustively, we use beam search for approximate search.</p><p>Early Update. Using approximate search while training structured perceptrons is complicated by the fact that the correct solution may actually ob- tain the highest score given the current model, but it was pruned off by the search procedure and therefore never considered. <ref type="bibr" target="#b7">Collins and Roark (2004)</ref> solve this by halting search as soon as the correct solution is pruned and then making an early update on the partial sequences. Intuitively this makes sense, since once the correct solution is no longer reachable, it makes no sense to continue searching.</p><p>Max-violation updates. <ref type="bibr" target="#b17">Huang et al. (2012)</ref> note that early updates require a considerable number of training iterations as it often discards a big portion of the training data. Moreover, they show that updates covering a greater subsequence can also be valid and define the max-violation up- date. Specifically, max-violation updates extend the beam beyond early updates and apply updates where the maximum difference in scores as de- fined by Equation (1) between the correct solution and the best prediction (i.e., the maximal viola- tion) is used for update. They show that this leads to faster convergence compared to early update.</p><p>The curse of long sequences. Neither early nor max-violation updates commit to using the full training sequence for updates. In standard sentence-level tasks such as part-of-speech tag- ging or sentence-based dependency parsing these updates suffice and reasonably quickly reach a level where all or almost all of the training se- quences are used for training. An entire document, however, may be composed of tens or hundreds of sentences, leading to transition sequences that are orders of magnitude longer.</p><p>To illustrate the difference between sentence- level and document-level parsing <ref type="figure" target="#fig_2">Figure 3</ref> shows plots of the average percentage of the gold train- ing sequences that are being used as training pro- gresses on the Switchboard training set.</p><p>The left plot shows the parser trained on sen-tences, the right one when it is trained on doc- uments (where it also has to predict sentence boundaries). On the sentence level we see that both update strategies quite quickly reach close to 100%, i.e., they see more or less complete tran- sition sequences during training. On the docu- ment level the picture is considerably different. The average length of seen transition sequences never even goes above 50%. In other words, more than half of the training data is never used. Early update shows a slow increase over time, presum- ably because the parser sees a bit more of every training instance at every iteration and therefore advances. However, max violation starts off us- ing much more training data than early update, but then drops and settles around 20%. This illustrates the fact that max violation does not commit to ex- ploiting more training data, but rather selects the update which constitutes the maximum violation irrespective of how much of the instance is being used. Empirically, even though the percentage of used training data decreases over iterations, max violation is still profiting from more iterations in the document-level task (cf. <ref type="figure" target="#fig_4">Figure 4</ref> in Section 4).</p><p>Delayed LaSO. To solve the problem with the discarded training data, we follow Björkelund and Kuhn (2014) and apply the DLASO 4 update. This idea builds on early update, but crucially differs in the sense that the remainder of a training sequence is not discarded when a mistake is made. Rather, the corresponding update is stored and the beam is reseeded with the correct solution. This enables the learning algorithm to exploit the full training data while still making sound updates (or, using the terminology of <ref type="bibr" target="#b17">Huang et al. (2012)</ref>, a number of updates that are all violations). Pseudocode for DLASO is shown in Algo- rithm 1. Similar to early update it performs beam search until the correct item falls off the beam (lines 9-12). Here, early update would halt, update the weights w and move on to the next instance. Instead, DLASO computes the corresponding up- date, i.e., a change in the w, and stores it away. It then resets the beam to the correct solution c i , and continues beam search. This procedure is repeated until the end of a sequence, with a fi- nal check for correctness after search has finished (line 15). After a complete pass-through of the training instance an update is made if any updates were recorded during beam search (lines 17-18). <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Delayed Learning as Search Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 DLaSO</head><p>Input: Training data D = {(xi, yi)} n i=1 , epochs T , beam size B. Output: Weight vector w.</p><p>1: w = 0 2: for t ∈ 1..T do 3:</p><formula xml:id="formula_2">for (x, y) ∈ D do 4:</formula><p>c0..n = ORACLE(y) 5: Beam = {c0} 6: Updates = {} 7:</p><p>for i ∈ 1 .. (n − 1) do 8: Beam = EXPANDANDFILTER(Beam, B) 9:</p><p>if ci ∈ Beam then 10: ˆ y = BEST(Beam) 11: Updates = Updates ∪ CALCUPDATE(ci, ˆ y) 12:</p><p>Beam = {ci} 13:</p><formula xml:id="formula_3">Beam = EXPANDANDFILTER(Beam, B) 14: ˆ y = BEST(Beam) 15: if cn = ˆ y then 16: Updates = Updates ∪ CALCUPDATE(cn, ˆ y) 17:</formula><p>if |Updates| &gt; 0 then 18: w = APPLYUPDATES(w, Updates) 19: return w</p><p>The DLASO update is closely related to LASO <ref type="bibr" target="#b10">(Daumé III and Marcu, 2005</ref>), but differs in that it delays the updates until the full instance has been decoded. <ref type="bibr" target="#b0">Björkelund and Kuhn (2014)</ref> show that the difference is important, as it prevents the learning algorithm from getting feedback within instances. Without the delay the learner can bias the weights for rare (e.g., lexicalized) features that occur within a single instance which renders the learning setting quite different from test time in- ference where no such feedback is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>Data sets. We experiment with two parts of the English Penn Treebank ( <ref type="bibr" target="#b21">Marcus et al., 1993</ref>). We use the Wall Street Journal (WSJ) as an exam- ple of copy-edited newspaper-quality texts with proper punctuation and capitalized sentences. We also use the Switchboard portion which consists of (transcribed) telephone conversations between strangers. Following previous work on Switch- board we lowercase all text and remove punctu- ation and disfluency markups.</p><p>We use sections 2-21 of the WSJ for training, 24 as development set and 23 as test set. For Switch- board we follow <ref type="bibr" target="#b6">Charniak and Johnson (2001)</ref>. We convert both data sets to Stanford dependen- cies with the Stanford dependency converter (de <ref type="bibr" target="#b11">Marneffe et al., 2006</ref>). We predict part-of-speech tags with the CRF tagger MARMOT <ref type="bibr">(Müller et al., 2013)</ref> and annotate the training sets via 10-fold jackknifing. Depending on the experimental sce-nario we use MARMOT in two different settings - standard sentence-level where we train and apply it on sentences, and document-level where a whole document is fed to the tagger, implicitly treating it as a single very long sentence.</p><p>Sentence boundary detection. We work with two well-established sentence boundary detection baselines. Following <ref type="bibr" target="#b26">(Read et al., 2012</ref>) we use the tokenizer from the Stanford CoreNLP <ref type="bibr" target="#b20">(Manning et al., 2014</ref>) and the sentence boundary de- tector from OpenNLP 5 which has been shown to achieve state-of-the-art results on WSJ. We eval- uate the performance of sentence boundary detec- tion on the token level using F-measure (F 1 ). <ref type="bibr">6</ref> Typical sentence boundary detectors such as CORENLP or OPENNLP focus on punctuation marks and are therefore inapplicable to data like Switchboard that does not originally include punc- tuation. In such cases CRF taggers are commonly selected as baselines, e.g. for punctuation predic- tion experiments ( <ref type="bibr" target="#b31">Zhang et al., 2013a</ref>). We there- fore introduce a third baseline using MARMOT. For this, we augment the POS tags with informa- tion to indicate if a token starts a new sentence or not. We prepare the training data accordingly and train the document-level sequence labeler on them. <ref type="table" target="#tab_1">Table 2</ref> shows the accuracies of all base- line systems on the development sets. For WSJ all three algorithms achieve similar results which shows that MARMOT is a competitive baseline. As can be seen, predicting sentence boundaries for the Switchboard dataset is a more difficult task than for well-formatted text like the WSJ.  Parser implementation. Our parser imple- ments the labeled version of the transition system described in Section 2 with a default beam size of 20. We use the oracle by  to create transition sequences for each sentence of a document, and then concatenate them with SB transitions that occur as early as possible (cf. Section 2). The feature set is based on previous work ( <ref type="bibr" target="#b30">Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b1">Bohnet and Kuhn, 2012;</ref><ref type="bibr" target="#b4">Bohnet et al., 2013)</ref> and was developed for a sentence-based parser for the WSJ. We made ini- tial experiments trying to introduce new features aimed at capturing sentence boundaries such as trying to model verb subcategorization or sentence length, however none of these proved useful com- pared to the baseline feature set. Following the line of work by Bohnet et al., we use the passive- aggressive algorithm <ref type="bibr" target="#b9">(Crammer et al., 2006</ref>) in- stead of the vanilla perceptron, parameter averag- ing <ref type="bibr" target="#b8">(Collins, 2002)</ref>, and a hash function to map features <ref type="bibr" target="#b5">(Bohnet, 2010)</ref>. <ref type="bibr">7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Comparison of training methods. <ref type="figure" target="#fig_4">Figure 4</ref> shows learning curves of the different training al- gorithms where sentence boundary F 1 and parsing accuracy LAS are plotted as a function of training iterations. The plots show performance for early update, max-violation, and DLASO updates. In addition, a greedy version of the parser is also in- cluded. The greedy parser uses a plain averaged perceptron classifier that is trained on all the train- ing data. The straight dashed line corresponds to the MARMOT baseline. While the greedy parser, DLASO, and the MARMOT baseline all exploit the full train- ing data during training, early update and max- violation do not (as shown in Section 3). This fact has a direct impact on the performance of these systems. DLASO reaches a plateau rather quickly, whereas even after 100 iterations, early update and max-violation perform considerably worse. <ref type="bibr">8</ref> We also see that the greedy parser quickly reaches an optimum and then starts degrading, presum- ably due to overfitting. It is noteworthy, however, that max-violation needs something between 40 to 60 iterations until it reaches a level similar to the greedy parsers optimal value. This effect is quite different from single sentence parsing scenarios, where it is known that beam search parsers easily outperform the greedy counterparts, requiring not nearly as many training iterations.   Increasing the beam size. Intuitively, using a bigger beam size might alleviate the problem of discarded training data and enable max-violation to exploit more training data. <ref type="figure" target="#fig_5">Figure 5</ref> shows the sentence boundary F 1 as a function of training it- erations for different beam sizes for DLASO and max-violation. For DLASO, we see that a big- ger beam provides a slight improvement. Max- violation shows a greater correlation between greater beam size and improved F 1 . However, even with a beam of size 100 max-violation is nowhere near DLASO. In theory a beam size or- ders of magnitude greater may rival DLASO but as the beam size directly influences the time com- plexity of the parser, this is not a viable option.</p><p>Does syntax help? One of the underlying as- sumptions of the joint model is our expectation that access to syntactic information should support the model in finding the sentence boundaries. We have already seen that the joint parser outperforms the MARMOT baseline by a big margin in terms of sentence boundary F 1 <ref type="figure" target="#fig_4">(Figure 4)</ref>. However, the comparison is not entirely fair as the two systems use different feature sets and learning algorithms.</p><p>To properly measure the effect of syntactic information on the sentence boundary detection task, we therefore trained another model for the joint system on a treebank where we replaced the gold-standard trees with trivial trees that connect the last token of each sentence to the root node, and everything in between as a left-branching chain. We dub this setting NOSYNTAX and it al- lows us to use exactly the same machine learning for a fair comparison between a system that has access to syntax and one without.</p><p>As the syntactic complexity in Switchboard is rather low, we compare these two systems also on a version of the WSJ where we removed all punctuation and lower-cased all words, effec- tively making it identical to the Switchboard set- ting (henceforth WSJ * ). <ref type="figure" target="#fig_6">Figure 6</ref> shows sentence boundary F 1 over training iterations when training with and without access to syntax. On both data sets, the system with access to syntax stays con- sistently above the other system. The striking dif- ference between the data sets is that syntactic in- formation has a much bigger impact on WSJ * than on Switchboard, which we attribute to the higher syntactic complexity of newswire text. Overall the comparison shows clearly that syntactic structure provides useful information to the task of sentence boundary detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Final results</head><p>Sentence boundary detection. We optimize the number of iterations on the dev sets: for the joint model we take the iteration with the highest av- erage between F 1 and LAS, NOSYNTAX is tuned according to F 1 . <ref type="table" target="#tab_3">Table 3</ref> gives the performance of the sentence boundary detectors on test sets. <ref type="bibr">9</ref> On WSJ all systems are close to 98 and this high number once again affirms that the task of segmenting newspaper-quality text does not leave much space for improvement. Although the pars- ing models outperform MARMOT, the improve- ments in F 1 are not significant.</p><p>In contrast, all systems fare considerably worse on WSJ * which confirms that the orthographic clues in newspaper text suffice to segment the sen- tences properly. Although NOSYNTAX outper- forms MARMOT, the difference is not significant. However, when real syntax is used (JOINT) we see a huge improvement in F 1 -10 points absolute - which is significantly better than both NOSYNTAX and MARMOT.</p><p>On Switchboard MARMOT is much lower and both parsing models outperform it significantly. Surprisingly the NOSYNTAX system achieves a very high result beating the baseline significantly by almost 4.5 points. The usage of syntax in the JOINT model raises this gain to 4.8 points. <ref type="bibr">9</ref> We test for significance using the Wilcoxon signed-rank test with p &lt; 0.01. † and ‡ denote significant increases over MARMOT and NOSYNTAX, respectively. * denotes signifi- cant increases over JOINT <ref type="table" target="#tab_4">(Table 4)</ref>  Parsing. In order to evaluate the joint model on the parsing task separately we compare it to pipeline setups. We train a basic parser on sin- gle sentences using gold standard sentence bound- aries, predicted POS tags and max-violation up- dates (GOLD). The number of training iterations is tuned to optimize LAS on the dev set. This parser is used as the second stage in the pipeline models. Additionally, we also build a pipeline where we use JOINT only as a sentence segmenter and then parse once again (denoted JOINT-REPARSED).  For WSJ * and Switchboard the picture is much different. Compared to GOLD, all systems show considerable drops in accuracy which asserts that errors from the sentence boundary detection task propagate to the parser and worsen the parser ac- curacy. On Switchboard the parsers yield signifi- cantly better results than MARMOT. The best re- sult is obtained after reparsing and this is also sig- nificantly better than any other system. Although there is a slight drop in accuracy between NOSYN- TAX and JOINT, this difference is not significant.</p><p>The results on WSJ * show that not only does syntax help to improve sentence segmentation, it does so to a degree that parsing results deterio- rate when simpler sentence boundary detectors are used. Here, both JOINT and JOINT-REPARSED obtain significantly better parsing accuracies than the systems that do not have access to syntax during sentence boundary prediction. Although JOINT-REPARSED performs a bit worse, the dif- ference compared to JOINT is not significant. <ref type="bibr" target="#b29">Zhang and Clark (2008)</ref> first showed how to train transition-based parsers with the structured per- ceptron <ref type="bibr" target="#b8">(Collins, 2002</ref>) using beam search and early update ( <ref type="bibr" target="#b7">Collins and Roark, 2004</ref>). It has since become the de facto standard way of training search-based transition-based dependency parsers <ref type="bibr" target="#b16">(Huang and Sagae, 2010;</ref><ref type="bibr" target="#b30">Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b4">Bohnet et al., 2013)</ref>. <ref type="bibr" target="#b17">Huang et al. (2012)</ref> showed how max-violation leads to faster convergence for transition-based parsers and max-violation up- dates have subsequently been applied to other tasks such as machine translation ( <ref type="bibr" target="#b28">Yu et al., 2013)</ref> and semantic parsing ( <ref type="bibr" target="#b35">Zhao and Huang, 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head><p>Boundary Detection. Sentence boundary detection has attracted only mod- est attention by the research community even though it is a component in every real-world NLP application. Previous work is divided into rule-based, e.g., <ref type="bibr">CoreNLP (Manning et al., 2014)</ref>, and machine learning approaches (e.g., OpenNLP, a re-implementation of <ref type="bibr" target="#b27">Reynar and Ratnaparkhi (1997)</ref>'s MxTerminator). The task is often sim- plified to the task of period disambiguation <ref type="bibr" target="#b18">(Kiss and Strunk, 2006</ref>), which only works on text that uses punctuation consistently. The current state of the art uses sequence labelers, e.g., a CRF ( <ref type="bibr" target="#b13">Evang et al., 2013;</ref><ref type="bibr" target="#b12">Dridan and Oepen, 2013)</ref>. For a broad survey of methodology and tools, we refer the reader to <ref type="bibr" target="#b26">Read et al. (2012)</ref>.</p><p>Joint models. Solving several tasks jointly has lately been popular in transition-based parsing, e.g., combining parsing with POS tagging <ref type="bibr" target="#b15">(Hatori et al., 2011;</ref><ref type="bibr" target="#b3">Bohnet and Nivre, 2012</ref>) and tok- enization ( <ref type="bibr" target="#b33">Zhang et al., 2013b;</ref><ref type="bibr" target="#b34">Zhang et al., 2014</ref>). Joint approaches avoid error propagation between the subtasks and often lead to overall better mod- els, especially for the lower level tasks that sud- denly have access to syntactic information.</p><p>Our transition system is inspired by the work of <ref type="bibr" target="#b31">Zhang et al. (2013a)</ref>. They present a projective transition-based parser that jointly predicts punc- tuation and syntax. Their ArcEager transition sys- tem <ref type="bibr" target="#b24">(Nivre, 2003)</ref> includes an additional transition that introduces punctuation similar to our SB tran- sition. They also use beam search and circumvent the problem of long training sequences by chop- ping up the training data into pseudo-documents of at most 10 sentences. As we have shown, this solution works because the training instances are not long enough to hurt the performance. How- ever, while this is possible for parsing, other tasks may not be able to chop up their training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have demonstrated that training a structured perceptron for inexact search on very long input sequences ignores significant portions of the train- ing data when using early update or max-violation. We then showed how this effect can be avoided by applying a different update strategy, DLASO, which leads to considerably better models in sig- nificantly less time. This effect only occurs when the training instances are very long, e.g., on whole documents, but not when training on single sen- tences. We also showed that the lower perfor- mance of early update and max-violation cannot be compensated for by increasing beam size or number of iterations. We compared our system for joint sentence boundary detection and dependency parsing to competitive pipeline systems showing that syntax can provide valuable information to sentence boundary prediction when punctuation and capitalization is not available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The beginning of a sample document from the Switchboard corpus. Tokens that start a sentence are underlined. The task is to predict syntactic structure and sentence boundaries jointly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average length of training sequences used during training for early update and max violation.</figDesc><graphic url="image-2.png" coords="4,307.24,73.76,204.10,137.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of different update strategies on the Switchboard development set.</figDesc><graphic url="image-5.png" coords="7,72.00,284.37,218.27,147.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The effect of increasing beam size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The effect of syntactic information on sentence boundary prediction, on dev sets.</figDesc><graphic url="image-7.png" coords="8,72.00,165.80,218.27,78.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results (F 1 ) for baselines for sentence 
boundary detection on dev sets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>.</head><label></label><figDesc></figDesc><table>WSJ 
Switchboard WSJ  *  

MARMOT 97.64 
71.87 
53.02 
NOSYNTAX 98.21 
76.31  † 
55.15 
JOINT 98.21 
76.65  † 
65.34  † ‡ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Sentence boundary detection results (F 1 ) 
on test sets. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 shows</head><label>4</label><figDesc>the results on the test sets. For WSJ, where sentence segmentation is almost triv- ial, we see only minor drops in LAS between GOLD and the systems that use predicted sentence boundaries. Among the systems that use predicted boundaries, no differences are significant.</figDesc><table>WSJ 
Switchboard 
WSJ  *  

GOLD 90.22 
84.99 
88.71 

MARMOT 89.81 
78.93 
83.37 
NOSYNTAX 89.95 
80.30  † 
83.61 

JOINT 89.71 
79.97  † 
85.66  † ‡ 
JOINT-REPARSED 89.93 
80.61  † ‡ *  
85.38  † ‡ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Parsing results (LAS) on test sets for dif-
ferent sentence boundaries. 

</table></figure>

			<note place="foot" n="1"> We chose this example for its brevity. For this particular example, the task of sentence boundary prediction could be solved easily with speaker information since the second sentence is from another speaker&apos;s turn. The interesting cases involve sentence segmentation within syntactically complex turns of a single speaker.</note>

			<note place="foot" n="2"> We make one simplifying assumption in our experimental setup by assuming gold tokenization. Tokenization is often taken for granted, mostly because it is a fairly easy task in English. For a realistic setting, tokenization would have to be predicted as well, but since we are interested in the effect of long sequences on training, we do not complicate our setting by including tokenization.</note>

			<note place="foot">Oracle. Since each sentence constitutes its own subtree under the root node, a regular sentencebased oracle can be used to derive the oracle transition sequence for complete documents. Specifically, we apply the sentence-based oracle to each sentence, and then join the sequences with a SB transition in between. In the oracle sequences derived this way we can either apply the SB transition as late as possible, requiring the system to completely reduce the stack before introducing a sentence boundary. Alternatively, sentence boundaries can be introduced as early as possible, i.e., applying the SB transition as soon as b 0 starts a new sentence. Table 1 shows this difference in the</note>

			<note place="foot" n="3"> One could also imagine leaving the decision of when to apply SB latent and let the machine learning decide. However, preliminary experiments again suggested that this strategy was inferior to the earliest possible point.</note>

			<note place="foot" n="5"> http://opennlp.apache.org 6 A true positive is defined as a token that was correctly predicted to begin a new sentence.</note>

			<note place="foot" n="7"> We make the source code of the parser available on the first author&apos;s website. 8 The x-axis is cut at 100 iterations for simplicity. Although early update and max-violation still are growing at this point, the overall effect does not change-even after 200 iterations the DLASO update outperforms the other two by at least two points absolute.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the German Research Foundation (DFG) in project D8 of SFB 732. We also thank spaCy GmbH for making it possible for the third author to finish his contribution to this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Structured Perceptrons for Coreference Resolution with Latent Antecedents and Non-local Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The best of bothworlds-a graph-based completion model for transition-based parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">France</forename><surname>Avignon</surname></persName>
		</author>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1455" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint morphological and syntactic analysis for richly inflected languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Boguslavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="415" to="428" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Top accuracy and fast dependency parsing is not a contradiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics<address><addrLine>Beijing, China, August. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Edit detection and parsing for transcribed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incremental Parsing with the Perceptron Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume</title>
		<meeting>the 42nd Meeting of the Association for Computational Linguistics (ACL&apos;04), Main Volume<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shai Shalev-Shwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Reseach</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006-03" />
		</imprint>
	</monogr>
	<note>Online passive-aggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning as search optimization: approximate large margin methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Document parsing: Towards realistic syntactic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Dridan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Parsing Technologies</title>
		<meeting>the 13th International Conference on Parsing Technologies<address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Elephant: Sequence labeling for word and sentence segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1422" to="1426" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient implementation of beam-search incremental parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="628" to="633" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incremental joint pos tagging and dependency parsing in chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Hatori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Joint Conference on Natural Language Processing</title>
		<meeting>5th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1216" to="1224" />
		</imprint>
	</monogr>
	<note>Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic programming for linear-time incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="1077" to="1086" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structured Perceptron with Inexact Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suphan</forename><surname>Fayong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised multilingual sentence boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tibor</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Strunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="525" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using prosody for automatic sentence segmentation of multi-party meetings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jáchym</forename><surname>Kolář</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text, Speech and Dialogue</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="629" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COMPUTATIONAL LINGUISTICS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient higher-order CRFs for morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An improved oracle for dependency parsing with online reordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Kuhlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies (IWPT&apos;09)</title>
		<meeting>the 11th International Conference on Parsing Technologies (IWPT&apos;09)<address><addrLine>Paris, France, October</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="73" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Parsing Technologies (IWPT)</title>
		<meeting>the 8th International Workshop on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing in expected linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sentence boundary detection: A long solved problem?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Dridan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Oepen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">Jørgen</forename><surname>Solberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mumbai, India, December. The COLING 2012 Organizing Committee</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="985" to="994" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012: Posters</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to identifying sentence boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Reynar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Applied Natural Language Processing</title>
		<meeting>the Fifth Conference on Applied Natural Language Processing<address><addrLine>Washington, DC, USA, March</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="16" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Max-violation perceptron and forced decoding for scalable MT training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1112" to="1123" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Tale of Two Parsers: Investigating and Combining Graph-based and Transition-based Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="562" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Punctuation prediction with transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st</title>
		<meeting>the 51st</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="752" to="760" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Chinese parsing exploiting characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Character-level chinese dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1326" to="1336" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Type-driven incremental semantic parsing with polymorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1416" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
