<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatically Generating Rhythmic Verse with Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hopkins</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
						</author>
						<title level="a" type="main">Automatically Generating Rhythmic Verse with Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="168" to="178"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1016</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discrimi-native weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale ex-trinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machine-generated poem to be the most human-like amongst all evaluated.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Poetry is an advanced form of linguistic commu- nication, in which a message is conveyed that sat- isfies both aesthetic and semantic constraints. As poetry is one of the most expressive forms of lan- guage, the automatic creation of texts recognis- able as poetry is difficult. In addition to requiring an understanding of many aspects of language in- cluding phonetic patterns such as rhyme, rhythm and alliteration, poetry composition also requires a deep understanding of the meaning of language.</p><p>Poetry generation can be divided into two sub- tasks, namely the problem of content, which is concerned with a poem's semantics, and the prob- lem of form, which is concerned with the aesthetic rules that a poem follows. These rules may de- scribe aspects of the literary devices used, and are usually highly prescriptive. Examples of different forms of poetry are limericks, ballads and sonnets. Limericks, for example, are characterised by their strict rhyme scheme (AABBA), their rhythm (two unstressed syllables followed by one stressed syl- lable) and their shorter third and fourth lines. Cre- ating such poetry requires not only an understand- ing of the language itself, but also of how it sounds when spoken aloud.</p><p>Statistical text generation usually requires the construction of a generative language model that explicitly learns the probability of any given word given previous context. Neural language models ( <ref type="bibr" target="#b25">Schwenk and Gauvain, 2005;</ref><ref type="bibr" target="#b2">Bengio et al., 2006</ref>) have garnered signficant research interest for their ability to learn complex syntactic and seman- tic representations of natural language <ref type="bibr" target="#b20">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b27">Sutskever et al., 2014;</ref><ref type="bibr" target="#b4">Cho et al., 2014;</ref><ref type="bibr" target="#b13">Kim et al., 2015)</ref>. Poetry generation is an interesting application, since performing this task automatically requires the creation of models that not only focus on what is being written (content), but also on how it is being written (form).</p><p>We experiment with two novel methodologies for solving this task. The first involves training a model to learn an implicit representation of con- tent and form through the use of a phonological encoding. The second involves training a gener- ative language model to represent content, which is then constrained by a discriminative pronuncia- tion model, representing form. This second model is of particular interest because poetry with arbi- trary rhyme, rhythm, repetition and themes can be generated by tuning the pronunciation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Automatic poetry generation is an important task due to the significant challenges involved. Most systems that have been proposed can loosely be categorised as rule-based expert systems, or statis- tical approaches.</p><p>Rule-based poetry generation attempts include case-based reasoning <ref type="bibr" target="#b8">(Gervás, 2000)</ref>, template- based generation <ref type="bibr" target="#b5">(Colton et al., 2012</ref>), constraint satisfaction ( <ref type="bibr" target="#b28">Toivanen et al., 2013;</ref><ref type="bibr" target="#b1">Barbieri et al., 2012</ref>) and text mining <ref type="bibr" target="#b23">(Netzer et al., 2009</ref>). These approaches are often inspired by how humans might generate poetry. Statistical approaches, conversely, make no as- sumptions about the creative process. Instead, they attempt to extract statistical patterns from ex- isting poetry corpora in order to construct a lan- guage model, which can then be used to gener- ate new poetic variants ( <ref type="bibr" target="#b32">Yi et al., 2016;</ref><ref type="bibr" target="#b11">Greene et al., 2010)</ref>. Neural language models have been increasingly applied to the task of poetry gener- ation. The work of <ref type="bibr" target="#b33">Zhang and Lapata (2014)</ref> is one such example, where they were able to outper- form all other classical Chinese poetry generation systems with both manual and automatic evalua- tion. <ref type="bibr" target="#b9">Ghazvininejad et al. (2016)</ref> and <ref type="bibr" target="#b10">Goyal et al. (2016)</ref> apply neural language models with regu- larising finite state machines. However, in the for- mer case the rhythm of the output cannot be de- fined at sample time, and in the latter case the fi- nite state machine is not trained on rhythm at all, as it is trained on dialogue acts. <ref type="bibr" target="#b18">McGregor et al. (2016)</ref> construct a phonological model for gener- ating prosodic texts, however there is no attempt to embed semantics into this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Phonetic-level Model</head><p>Our first model is a pure neural language model, trained on a phonetic encoding of poetry in or- der to represent both form and content. Phonetic encodings of language represent information as sequences of around 40 basic acoustic symbols. Training on phonetic symbols allows the model to learn effective representations of pronunciation, including rhyme and rhythm.</p><p>However, just training on a large corpus of po- etry data is not enough. Specifically, two problems need to be overcome. 1) Phonetic encoding re- sults in information loss: words that have the same pronunciation (homophones) cannot be perfectly reconstructed from the corresponding phonemes. This means that we require an additional proba- bilistic model in order to determine the most likely word given a sequence of phonemes. 2) The va- riety of poetry and poetic devices one can use- e.g., rhyme, rhythm, repetition-means that po- ems sampled from a model trained on all poetry would be unlikely to maintain internal consistency of meter and rhyme. It is therefore important to train the model on poetry which has its own inter- nal consistency.</p><p>Thus, the model comprises three steps: translit- erating an orthographic sequence to its phonetic representation, training a neural language model on the phonetic encoding, and decoding the gen- erated sequence back from phonemes to ortho- graphic symbols.</p><p>Phonetic encoding To solve the first step, we apply a combination of word lookups from the CMU pronunciation dictionary <ref type="bibr" target="#b30">(Weide, 2005</ref>) with letter-to-sound rules for handling out-of- vocabulary words. These rules are based on the CART techniques described by <ref type="bibr" target="#b3">Black et al. (1998)</ref>, and are represented with a simple Finite State Transducer 1 . The number of letters and number of phones in a word are rarely a one-to-one match: letters may match with up to three phones. In ad- dition, virtually all letters can, in some contexts, map to zero phones, which is known as 'wild' or epsilon. Expectation Maximisation is used to compute the probability of a single letter match- ing a single phone, which is maximised through the application of Dynamic Time <ref type="bibr">Warping (Myers et al., 1980)</ref> to determine the most likely position of epsilon characters.</p><p>Although this approach offers full coverage over the training corpus-even for abbreviated words like ask'd and archaic words like re- newest-it has several limitations. Irregularities in the English language result in difficulty deter- mining general letter-to-sound rules that can man- age words with unusual pronunciations such as "colonel" and "receipt" 2 .</p><p>In addition to transliterating words into phoneme sequences, we also represent word break characters as a specific symbol. This makes decipherment, when converting back into an or- thographic representation, much easier. Phonetic transliteration allows us to construct a phonetic poetry corpus comprising 1,046,536 phonemes.</p><p>Neural language model We train a Long-Short Term Memory network <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997</ref>) on the phonetic representation of our poetry corpus. The model is trained using stochas- tic gradient descent to predict the next phoneme given a sequence of phonemes. Specifically, we maximize a multinomial logistic regression ob- jective over the final softmax prediction. Each phoneme is represented as a 256-dimensional em- bedding, and the model consists of two hidden layers of size 256. We apply backpropagation- through-time <ref type="bibr" target="#b31">(Werbos, 1990)</ref> for 150 timesteps, which roughly equates to four lines of poetry in sonnet form. This allows the network to learn features like rhyme even when spread over mul- tiple lines. Training is preemptively stopped at 25 epochs to prevent overfitting.</p><p>Orthographic decoding When decoding from phonemes back to orthographic symbols, the goal is to compute the most likely word correspond- ing to a sequence of phonemes. That is, we com- pute the most probable hypothesis word W given a phoneme sequence ρ:</p><formula xml:id="formula_0">arg max i P ( W i | ρ )<label>(1)</label></formula><p>We can consider the phonetic encoding of plain- text to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings. The problem of homophonic decipherment has received significant research at- tention in the past; with approaches utilising Ex- pectation Maximisation ( <ref type="bibr" target="#b14">Knight et al., 2006</ref>), In- teger Programming ( <ref type="bibr" target="#b24">Ravi and Knight, 2009</ref>) and A* search <ref type="bibr" target="#b6">(Corlett and Penn, 2010)</ref>. Transliteration from phonetic to an ortho- graphic representation is done by constructing a Hidden Markov Model using the CMU pronunci- ation dictionary <ref type="bibr" target="#b30">(Weide, 2005</ref>) and an n-gram lan- guage model. We calculate the transition proba- bilities (using the n-gram model) and the emission matrix (using the CMU pronunciation dictionary) to determine pronunciations that correspond to a single word. All pronunciations are naively con- sidered equiprobable. We perform Viterbi decod- ing to find the most likely sequence of words. This means finding the most likely word w t+1 given a And humble and their fit flees are wits size but that one made and made thy step me lies ------------- Cool light the golden dark in any way the birds a shade a laughter turn away ------------- Then adding wastes retreating white as thine She watched what eyes are breathing awe what shine ------------- But sometimes shines so covered how the beak Alone in pleasant skies no more to seek previous word sequence (w t−n , ..., w t ).</p><formula xml:id="formula_1">arg max w t+1 P ( w t+1 | w 1 , ... , w t )<label>(2)</label></formula><p>If a phonetic sequence does not map to any word, we apply the heuristic of artificially breaking the sequence up into two subsequences at index n, such that n maximises the n-gram frequency of the subsequences.</p><p>Output A popular form of poetry with strict in- ternal structure is the sonnet. Popularised in En- glish by Shakespeare, the sonnet is characterised by a strict rhyme scheme and exactly fourteen lines of Iambic Pentameter ( <ref type="bibr" target="#b11">Greene et al., 2010)</ref>. Since the 17,134 word tokens in Shakespeare's 153 sonnets are insufficient to train an effective model, we augment this corpus with poetry taken from the website sonnets.org, yielding a training set of 288,326 words and 1,563,457 characters. An example of the output when training on this sonnets corpus is provided in <ref type="figure" target="#fig_0">Figure 1</ref>. Not only is it mostly in strict Iambic Pentameter, but the gram- mar of the output is mostly correct and the poetry contains rhyme. adequate size even exists. Even when such poetic corpora are available, a new model must be trained for each type of poetry. This precludes tweaking the form of the output, which is important when generating poetry automatically.</p><p>We now explore an alternative approach. In- stead of attempting to represent both form and content in a single model, we construct a pipeline containing a generative language model represent- ing content, and a discriminative model represent- ing form. This allows us to represent the problem of creating poetry as a constraint satisfaction prob- lem, where we can modify constraints to restrict the types of poetry we generate.</p><p>Character Language Model Rather than train a model on data representing features of both con- tent and form, we now use a simple character-level model <ref type="bibr" target="#b26">(Sutskever et al., 2011</ref>) focused solely on content. This approach offers several benefits over the word-level models that are prevalent in the lit- erature. Namely, their more compact vocabulary allows for more efficient training; they can learn common prefixes and suffixes to allow us to sam- ple words that are not present in the training cor- pus and can learn effective language representa- tions from relatively small corpora; and they can handle archaic and incorrect spellings of words.</p><p>As we no longer need the model to explicitly represent the form of generated poetry, we can loosen our constraints when choosing a training corpus. Instead of relying on poetry only in sonnet form, we can instead construct a generic corpus of poetry taken from online sources. This corpus is composed of 7.56 million words and 34.34 mil- lion characters, taken largely from 20 th Century poetry books found online. The increase in cor- pus size facilitates a corresponding increase in the number of permissible model parameters. This al- lows us to train a 3-layer LSTM model with 2048- dimensional hidden layers, with embeddings in 128 dimensions. The model was trained to pre- dict the next character given a sequence of charac- ters, using stochastic gradient descent. We attenu- ate the learning rate over time, and by 20 epochs the model converges.</p><p>Rhythm Modeling Although a character-level language model trained on a corpus of generic po- etry allows us to generate interesting text, internal irregularities and noise in the training data prevent the model from learning important features such as rhythm. Hence, we require an additional classi- fier to constrain our model by either accepting or rejecting sampled lines based on the presence or absence of these features. As the presence of me- ter (rhythm) is the most characteristic feature of poetry, it therefore must be our primary focus.</p><p>Pronunciation dictionaries have often been used to determine the syllabic stresses of words ( <ref type="bibr" target="#b5">Colton et al., 2012;</ref><ref type="bibr" target="#b17">Manurung et al., 2000;</ref><ref type="bibr" target="#b21">Misztal and Indurkhya, 2014</ref></p><note type="other">), but suffer from some limitations for constructing a classifier. All word pronunci- ations are considered equiprobable, including ar- chaic and uncommon pronunciations, and pronun- ciations are provided context free, despite the im- portance of context for pronunciation 3 . Further- more, they are constructed from American En- glish, meaning that British English may be mis- classified.</note><p>These issues are circumvented by applying lightly supervised learning to determine the con- textual stress pattern of any word. That is, we ex- ploit the latent structure in our corpus of sonnet poetry, namely, the fact that sonnets are composed of lines in rigid Iambic Pentameter, and are there- fore exactly ten syllables long with alternating syl- labic stress. This allows us to derive a syllable- stress distribution. Although we use the sonnets corpus for this, it is important to note that any cor- pus with such a latent structure could be used.</p><p>We represent each line of poetry as a cascade of Weighted Finite State Transducers (WFST). A WFST is a finite-state automaton that maps be- tween two sets of symbols. It is defined as an eight-tuple where ⟨Q, Σ, ρ, I, F, ∆, λ, p⟩:</p><p>Q : A set of states Σ : An input alphabet of symbols ρ : An output alphabet of symbols I : A set of initial states F : A set of final states, or sinks ∆ : A transition function mapping pairs of states and symbols to sets of states λ : A set of weights for initial states P : A set of weights for final states A WFST assigns a probability (or weight, in the general case) to each path through it, going from an initial state to an end state. Every path corre- sponds to an input and output label sequence, and there can be many such paths for each sequence. WFSTs are often used in a cascade, where a number of machines are executed in series, such that the output tape of one machine is the input tape for the next. Formally, a cascade is repre- sented by the functional composition of several machines.</p><formula xml:id="formula_2">W (x, z) = A(x|y) • B(y|z) • C(z)<label>(3)</label></formula><p>Where W (x, z) is defined as the ⊕ sum of the path probabilities through the cascade, and x and z are an input sequence and output sequence respec- tively. In the real semiring (where the product of probabilities are taken in series, and the sum of the probabilities are taken in parallel), we can rewrite the definition of weighted composition to produce the following:</p><formula xml:id="formula_3">W (x, z) = ⊕ y A(x | y) ⊗ B(y | z) ⊗ C(z)<label>(4)</label></formula><p>As we are dealing with probabilities, this can be rewritten as:</p><formula xml:id="formula_4">P (x, z) = ∑ y P (x | y)P (y | z)P (z)<label>(5)</label></formula><p>We can perform Expectation Maximisation over the poetry corpus to obtain a probabilistic classi- fier which enables us to determine the most likely stress patterns for each word. Every word is rep- resented by a single transducer. In each cascade, a sequence of input words is mapped onto a sequence of stress patterns ⟨×, /⟩ where each pattern is between 1 and 5 syllables in length <ref type="bibr">4</ref> . We initially set all transition proba- bilities equally, as we make no assumptions about the stress distributions in our training set. We then iterate over each line of the sonnet corpus, using Expectation Maximisation to train the cas- cades. In practice, there are several de facto vari- ations of Iambic meter which are permissible, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We train the rhythm classifier by converging the cascades to whatever output is the most likely given the line. <ref type="bibr">4</ref> Words of more than 5 syllables comprise less than 0.1% of the lexicon <ref type="bibr" target="#b0">(Aoyama and Constable, 1998)</ref>. Constraining the model To generate poetry us- ing this model, we sample sequences of charac- ters from the character-level language model. To impose rhythm constrains on the language model, we first represent these sampled characters at the word level and pool sampled characters into word tokens in an intermediary buffer. We then apply the separately trained word-level WFSTs to con- struct a cascade of this buffer and perform Viterbi decoding over the cascade. This defines the distri- bution of stress-patterns over our word tokens.</p><formula xml:id="formula_5">× / × / × / × / × / / × × / × / × / × / × / × / × / × / × / × / × × / × / × / × / ×</formula><p>We can represent this cascade as a probabilistic classifier, and accept or reject the buffered output based on how closely it conforms to the desired meter. While sampling sequences of words from this model, the entire generated sequence is passed to the classifier each time a new word is sampled. The pronunciation model then returns the proba- bility that the entire line is within the specified me- ter. If a new word is rejected by the classifier, the state of the network is rolled back to the last for- mulaically acceptable state of the line, removing the rejected word from memory. The constraint on rhythm can be controlled by adjusting the accept- ability threshold of the classifier. By increasing the threshold, output focuses on form over content. Conversely, decreasing the criterion puts greater emphasis on content. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Themes and Poetic devices</head><p>It is important for any generative poetry model to include themes and poetic devices. One way to achieve this would be by constructing a corpus that exhibits the desired themes and devices. To create a themed corpus about 'love', for instance, we would aggregate love poetry to train the model, which would thus learn an implicit representation of love. However, this forces us to generate poetry according to discrete themes and styles from pre- trained models, requiring a new training corpus for each model. In other words, we would suffer from similar limitations as with the phonetic-level model, in that we require a dedicated corpus. Al- ternatively, we can manipulate the language model by boosting character probabilities at sample time to increase the probability of sampling thematic words like 'love'. This approach is more robust, and provides us with more control over the final output, including the capacity to vary the inclusion of poetic devices in the output.</p><p>Themes In order to introduce thematic content, we heuristically boost the probability of sampling words that are semantically related to a theme word from the language model. First, we com- pile a list of similar words to a key theme word by retrieving its semantic neighbours from a distribu- tional semantic model ( <ref type="bibr" target="#b19">Mikolov et al., 2013</ref>). For example, the theme winter might include thematic words frozen, cold, snow and frosty. We represent these semantic neighbours at the character level, and heuristically boost their probability by multi- plying the sampling probability of these character strings by their cosine similarity to the key word, plus a constant. Thus, the likelihood of sampling a thematically related word is artificially increased, while still constraining the model rhythmically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Errors per line 1 2 3 4 Total</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Phonetic Model 11 2 3 1 28 Character Model + WFST 6 5 1 1 23 Character Model 3 8 7 7 68</head><p>Table 1: Number of lines with n errors from a set of 50 lines generated by each of the three models.</p><p>Poetic devices A similar method may be used for poetic devices such as assonance, consonance and alliteration. Since these devices can be or- thographically described by the repetition of iden- tical sequences of characters, we can apply the same heuristic to boost the probability of sampling character strings that have previously been sam- pled. That is, to sample a line with many instances of alliteration (multiple words with the same ini- tial sound) we record the historical frequencies of characters sampled at the beginning of each previ- ous word. After a word break character, we boost the probability that those characters will be sam- pled again in the softmax. We only keep track of frequencies for a fixed number of time steps. By increasing or decreasing the size of this window, we can manipulate the prevalence of alliteration. Variations of this approach are applied to invoke consonance (by boosting intra-word consonants) and assonance (by boosting intra-word vowels). An example of two sampled lines with high de- grees of alliteration, assonance and consonance is given in <ref type="figure" target="#fig_4">Figure 4c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In order to examine how effective our methodolo- gies for generating poetry are, we evaluate the pro- posed models in two ways. First, we perform an intrinsic evaluation where we examine the quality of the models and the generated poetry. Second, we perform an extrinsic evaluation where we eval- uate the generated output using human annotators, and compare it to human-generated poetry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Intrinsic evaluation</head><p>To evaluate the ability of both models to gen- erate formulaic poetry that adheres to rhythmic rules, we compared sets of fifty sampled lines from each model. The first set was sampled from the phonetic-level model trained on Iambic poetry. The second set was sampled from the character- level model, constrained to Iambic form. For com-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Line Coverage</head><p>Wikipedia 64.84% 83.35% 97.53% Sonnets 85.95% 80.32% 99.36% <ref type="table">Table 2</ref>: Error when transliterating text into phonemes and reconstructing back into text.</p><p>parison, and to act as a baseline, we also sampled from the unconstrained character model. We created gold-standard syllabic classifica- tions by recording each line spoken-aloud, and marking each syllable as either stressed or un- stressed. We then compared these observations to loose Iambic Pentameter (containing all four variants), to determine how many syllabic mis- classifications existed on each line. This was done by speaking each line aloud, and noting where the speaker put stresses.</p><p>As <ref type="table">Table 1</ref> shows, the constrained character level model generated the most formulaic poetry. Results from this model show that 70% of lines had zero mistakes, with frequency obeying an in- verse power-law relationship with the number of errors. We can see that the phonetic model per- formed similarly, but produced more subtle mis- takes than the constrained character model: many of the errors were single mistakes in an otherwise correct line of poetry.</p><p>In order to investigate this further, we examined to what extent these errors are due to translitera- tion (i.e., the phonetic encoding and orthographic decoding steps). <ref type="table">Table 2</ref> shows the reconstruction accuracy per word and per line when transliterat- ing either Wikipedia or Sonnets to phonemes us- ing the CMU pronunciation dictionary and subse- quently reconstructing English text using the n- gram model <ref type="bibr">5</ref> . Word accuracy reflects the fre- quency of perfect reconstruction, whereas per line tri-gram similarity <ref type="bibr" target="#b15">(Kondrak, 2005</ref>) reflects the overall reconstruction. Coverage captures the per- centage of in-vocabulary items. The relatively low per-word accuracy achieved on the Wikipedia cor- pus is likely due to the high frequency of out-of- vocabulary words. The results show that a signifi- cant number of errors in the phonetic-level model are likely to be caused by transliteration mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Extrinsic evaluation</head><p>We conducted an indistinguishability study with a selection of automatically generated poetry and human poetry. As extrinsic evaluations are expen- sive and the phonetic model was unlikely to do well (as illustrated in <ref type="figure" target="#fig_4">Figure 4e</ref>: the model gener- ates good Iambic form, but not very good English), we only evaluate on the constrained character- level model. Poetry was generated with a variety of themes and poetic devices (see supplementary material).</p><p>The aim of the study was to determine whether participants could distinguish between human and machine-generated poetry, and if so to what ex- tent. A set of 70 participants (of whom 61 were English native speakers) were each shown a selection of randomly chosen poetry segments, and were invited to classify them as either hu- man or generated. Participants were recruited from friends and people within poetry communi- ties within the University of Cambridge, with an age range of 17 to 80, and a mean age of 29. Our participants were not financially incentivised, per- ceiving the evaluation as an intellectual challenge.</p><p>In addition to the classification task, each partic- ipant was also invited to rate each poem on a 1-5 scale with respect to three criteria, namely read- ability, form and evocation (how much emotion did a poem elicit). We naively consider the over- all quality of a poem to be the mean of these three measures. We used a custom web-based environ- ment, built specifically for this evaluation <ref type="bibr">6</ref> , which is illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>. Based on human judg- ments, we can determine whether the models pre- sented in this work can produce poetry of a similar quality to humans.</p><p>To select appropriate human poetry that could be meaningfully compared with the machine- generated poetry, we performed a comprehension test on all poems used in the evaluation, using the Dale-Chall readability formula <ref type="bibr" target="#b7">(Dale and Chall, 1948)</ref>. This formula represents readability as a function of the complexity of the input words. We selected nine machine-generated poems with a high readability score. The generated poems produced an average score of 7.11, indicating that readers over 15 years of age should easily be able to comprehend them.</p><p>For our human poems, we focused explicitly on poetry where greater consideration is placed on (a) The crow crooked on more beautiful and free, He journeyed off into the quarter sea. his radiant ribs girdled empty and very - least beautiful as dignified to see.</p><p>(c) Man with the broken blood blue glass and gold. Cheap chatter chants to be a lover do.</p><p>(e) The son still streams and strength and spirit. The ridden souls of which the fills of.</p><p>(b) Is that people like things (are the way we to figure it out) and I thought of you reading and then is your show or you know we will finish along will you play.   prosodic elements like rhythm and rhyme than se- mantic content (known as "nonsense verse"). We randomly selected 30 poems belonging to that cat- egory from the website poetrysoup.com, of which eight were selected for the final comparison based on their comparable readability score. The se- lected poems were segmented into passages of be- tween four and six lines, to match the length of the generated poetry segments. An example of such a segment is shown in <ref type="figure" target="#fig_4">Figure 4d</ref>. The human poems had an average score of 7.52, requiring a similar level of English aptitude to the generated texts.</p><p>The performance of each human poem, along- side the aggregated scores of the generated poems, is illustrated in <ref type="table">Table 3</ref>. For the human poems, our group of participants guessed correctly that they were human 51.4% of the time. For the gen- erated poems, our participants guessed correctly 46.2% of the time that they were machine gener- ated. To determine whether our results were statis- tically significant, we performed a Chi 2 test. This resulted in a p-value of 0.718. This indicates that our participants were unable to tell the difference between human and generated poetry in any sig- nificant way. Although our participants generally considered the human poems to be of marginally higher quality than our generated poetry, they were unable to effectively distinguish between them. Interestingly, our results seem to suggest that our participants consider the generated poems to be more 'human-like' than those actually written by humans. In addition, the poem with the highest overall quality rating is a machine generated one. This shows that our approach was effective at gen- erating high-quality rhythmic verse.</p><p>It should be noted that the poems that were most 'human-like' and most aesthetic respectively were generated by the neural character model. Gener- ally the set of poetry produced by the neural char- acter model was slightly less readable and emo- tive than the human poetry, but had above average form. All generated poems included in this evalu- ation can be found in the supplementary material, and our code is made available online <ref type="bibr">7</ref>   <ref type="table">Table 3</ref>: Proportion of people classifying each poem as 'human', as well as the relative qualitative scores of each poem as deviations from the mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Our contributions are twofold. First, we devel- oped a neural language model trained on a pho- netic transliteration of poetic form and content. Although example output looked promising, this model was limited by its inability to generalise to novel forms of verse. We then proposed a more ro- bust model trained on unformed poetic text, whose output form is constrained at sample time. This approach offers greater control over the style of the generated poetry than the earlier method, and facilitates themes and poetic devices. An indistinguishability test, where participants were asked to classify a randomly selected set of human "nonsense verse" and machine-generated poetry, showed generated poetry to be indistin- guishable from that written by humans. In ad- dition, the poems that were deemed most 'hu- manlike' and most aesthetic were both machine- generated.</p><p>In future work, it would be useful to investigate models based on morphemes, rather than char- acters, which offers potentially superior perfor- mance for complex and rare words ( <ref type="bibr" target="#b16">Luong et al., 2013)</ref>, which are common in poetry.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example output of the phonetic-level model trained on Iambic Pentameter poetry (grammatical errors are emphasised).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Permissible variations of Iambic Pentameter in Shakespeare's sonnets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two approaches for generating themed poetry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>How dreary to be somebody, How public like a frog To tell one's name the livelong day To an admiring bog.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of automatically generated and human generated poetry. (a) Character-level model-Strict rhythm regularisation-Iambic-No Theme. (b) Character-level model-Strict rhythm regularisation-Anapest. (c) Character-level model-Boosted alliteration/assonance. (d) Emily Dickinson-I'm nobody, who are you? (e) Phonetic-level model-Nonsensical Iambic lines.</figDesc><graphic url="image-1.png" coords="8,72.00,349.38,226.78,113.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The experimental environment for asking participants to distinguish between automatically generated and human poetry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Poet 

Title 
Human Readability Emotion Form 

Generated 
Best 
0.66 
0.60 
-0.77 
0.90 

G. M. Hopkins Carrion Comfort 
0.62 
-1.09 
1.39 
-1.55 

J. Thornton 
Delivery of Death 0.60 
0.26 
-1.38 
-0.65 

Generated 
Mean 
0.54 
-0.28 
-0.30 
0.23 

M. Yvonne 
Intricate Weave 
0.53 
2.38 
0.94 
-1.67 

E. Dickinson 
I'm Nobody 
0.52 
-0.46 
0.92 
0.44 

G. M. Hopkins The Silver Jubilee 0.52 
0.71 
-0.33 
0.65 

R. Dryden 
Mac Flecknoe 
0.51 
-0.01 
0.35 
-0.78 

A. Tennyson 
Beautiful City 
0.48 
-1.05 
0.97 
-1.26 

W. Shakespeare A Fairy Song 
0.45 
0.65 
1.30 
1.18 

</table></figure>

			<note place="foot" n="1"> Implemented using FreeTTS (Walker et al., 2010) 2 An evaluation of models in American English, British English, German and French was undertaken by Black et al. (1998), who reported an externally validated per token accuracy on British English as low as 67%. Although no experiments were carried out on corpora of early-modern English, it is likely that this accuracy would be significantly lower.</note>

			<note place="foot" n="4"> Constrained Character-level Model As the example shows, phonetic-level language models are effective at learning poetic form, despite small training sets and relatively few parameters. However, the fact that they require training data with internal poetic consistency implies that they do not generalise to other forms of poetry. That is, in order to generate poetry in Dactylic Hexameter (for example), a phonetic model must be trained on a corpus of Dactylic poetry. Not only is this impractical, but in many cases no corpus of</note>

			<note place="foot" n="3"> For example, the independent probability of stressing the single syllable word at is 40%, but this increases to 91% when the following word is the (Greene et al., 2010)</note>

			<note place="foot" n="5"> Obviously, calculating this value for the character-level model makes no sense, since no transliteration occurs in that case.</note>

			<note place="foot" n="6"> http://neuralpoetry.getforge.io/</note>

			<note place="foot" n="7"> https://github.com/JackHopkins/ACLPoetry</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Word length frequency and distribution in english: Observations, theory, and implications for the construction of verse lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Aoyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Constable</surname></persName>
		</author>
		<idno>cmp-lg/9808004</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Markov constraints for generating lyrics with style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Pachet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko Degli</forename><surname>Esposti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th European Conference on Artificial Intelligence</title>
		<meeting>the 20th European Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Sébastien</forename><surname>Senécal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovations in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="137" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Issues in building general letter to sound rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Pagel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Full face poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Colton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Computational Creativity</title>
		<meeting>the Third International Conference on Computational Creativity</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An exact a* method for deciphering letter-substitution ciphers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Corlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Penn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1040" to="1047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A formula for predicting readability: Instructions. Educational research bulletin pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Dale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1948" />
			<biblScope unit="page" from="37" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wasp: Evaluation of different strategies for the automatic generation of spanish verse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Gervás</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AISB-00 Symposium on Creative &amp; Cultural Aspects of AI</title>
		<meeting>the AISB-00 Symposium on Creative &amp; Cultural Aspects of AI</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="93" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating topical poetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1183" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural language generation through character-based rnns with finite-state prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic analysis of rhythmic poetry with applications to generation and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erica</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tugba</forename><surname>Bodrumlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="524" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06615</idno>
		<title level="m">Character-aware neural language models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised analysis for decipherment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishit</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Main conference poster sessions. Association for Computational Linguistics</title>
		<meeting>the COLING/ACL on Main conference poster sessions. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">N-gram similarity and distance. In String processing and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="115" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards a computational model of poetry generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisar</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Thompson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
		<respStmt>
			<orgName>The University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Process based evaluation of computer generated poetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Purver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The INLG 2016 Workshop on Computational Creativity in Natural Language Generation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>and Geraint Wiggins. page 51</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Poetry generation system with an emotional personality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Misztal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bipin</forename><surname>Indurkhya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Computational Creativity</title>
		<meeting>the Fourth International Conference on Computational Creativity</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Performance tradeoffs in dynamic time warping algorithms for isolated word recognition. Acoustics, Speech and Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cory</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">E</forename><surname>Lawrence R Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="623" to="635" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gaiku: Generating haiku with word associations norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Approaches to Linguistic Creativity. Association for Computational Linguistics</title>
		<meeting>the Workshop on Computational Approaches to Linguistic Creativity. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning phoneme mappings for transliteration without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training neural network language models on very large corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Harnessing constraint programming for poetry composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Jukka M Toivanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannu</forename><surname>Järvisalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toivonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Conference on Computational Creativity</title>
		<meeting>the Fourth International Conference on Computational Creativity</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>page 160</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Freetts 1.2: A speech synthesizer written entirely in the java programming language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Lamere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Kwok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The carnegie mellon pronouncing dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weide</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>cmudict. 0.6</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul J Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Generating chinese classical poems with rnn encoderdecoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01537</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Chinese poetry generation with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
