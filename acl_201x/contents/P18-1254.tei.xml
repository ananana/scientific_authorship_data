<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finding Syntax in Human Encephalography with Beam Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Brennan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♦</forename><forename type="middle">♠</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>UK</roleName><surname>London</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Linguistics</orgName>
								<orgName type="department" key="dep3">Department of Linguistics</orgName>
								<orgName type="institution" key="instit1">University of Oxford</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Finding Syntax in Human Encephalography with Beam Search</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2727" to="2736"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recurrent neural network grammars (RNNGs) are generative models of (tree, string) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophys-iological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computational psycholinguistics has "always been...the thing that computational linguistics stood the greatest chance of providing to hu- manity" <ref type="bibr" target="#b22">(Kay, 2005)</ref>. Within this broad area, cognitively-plausible parsing models are of par- ticular interest. They are mechanistic computa- tional models that, at some level, do the same task people do in the course of ordinary language com- prehension. As such, they offer a way to gain in- sight into the operation of the human sentence pro- cessing mechanism (for a review see <ref type="bibr" target="#b19">Hale, 2017)</ref>.</p><p>As <ref type="bibr" target="#b23">Keller (2010)</ref> suggests, a promising place to look for such insights is at the intersection of (a) incremental processing, (b) broad coverage, and (c) neural signals from the human brain.</p><p>The contribution of the present paper is situ- ated precisely at this intersection. It combines a probabilistic generative grammar (RNNG; <ref type="bibr" target="#b13">Dyer et al., 2016</ref>) with a parsing procedure that uses this grammar to manage a collection of syntac- tic derivations as it advances from one word to the next ( <ref type="bibr">, cf. Roark, 2004</ref>). Via well-known complexity metrics, the interme- diate states of this procedure yield quantitative predictions about language comprehension diffi- culty. Juxtaposing these predictions against data from human encephalography (EEG), we find that they reliably derive several amplitude effects in- cluding the P600, which is known to be associated with syntactic processing (e.g. <ref type="bibr" target="#b32">Osterhout and Holcomb, 1992)</ref>.</p><p>Comparison with language models based on long short term memory networks (LSTM, e.g. <ref type="bibr" target="#b20">Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b29">Mikolov, 2012;</ref><ref type="bibr" target="#b17">Graves, 2012)</ref> shows that these effects are specific to the RNNG. A further analysis pinpoints one of these effects to RNNGs' syntactic com- position mechanism. These positive findings re- frame earlier null results regarding the syntax- sensitivity of human processing <ref type="bibr" target="#b14">(Frank et al., 2015)</ref>. They extend work with eyetracking (e.g. <ref type="bibr" target="#b35">Roark et al., 2009;</ref><ref type="bibr" target="#b11">Demberg et al., 2013</ref>) and neu- roimaging ( <ref type="bibr" target="#b7">Brennan et al., 2016;</ref><ref type="bibr" target="#b1">Bachrach, 2008)</ref> to higher temporal resolution. 1 Perhaps most sig- nificantly, they establish a general correspondence between a computational model and electrophysi- ological responses to naturalistic language.</p><p>Following this Introduction, section 2 presents recurrent neural network grammars, emphasiz- ing their suitability for incremental parsing. Sections 3 then reviews a previously-proposed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p e n a n e w p h r a s e g e n e r a t e a w o r d c lo s e o ff p h r a s e</head><p>Prob(action) beam search procedure for them. Section 4 goes on to introduce the novel application of this procedure to human data via incremental com- plexity metrics. Section 5 explains how these the- oretical predictions are specifically brought to bear on EEG data using regression. Sections 6 and 7 elaborate on the model comparison men- tioned above and report the results in a way that isolates the operative element. Section 8 discusses these results in relation to established computa- tional models. The conclusion, to anticipate sec- tion 9, is that syntactic processing can be found in naturalistic speech stimuli if ambiguity resolution is modeled as beam search.</p><formula xml:id="formula_0">NP VP PP S … … Prob(nonterminal)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recurrent neural network grammars</head><p>for incremental processing 2016) are probabilistic models that generate trees. The probability of a tree is decomposed via the chain rule in terms of derivational action- probabilities that are conditioned upon previ- ous actions i.e. they are history-based gram- mars ( <ref type="bibr" target="#b2">Black et al., 1993</ref>). In the vanilla version of RNNG, these steps follow a depth-first traversal of the developing phrase structure tree. This en- tails that daughters are announced bottom-up one by one as they are completed, rather than being predicted at the same time as the mother.</p><p>Each step of this generative story depends on the state of a stack, depicted inside the gray box in <ref type="figure" target="#fig_1">Figure 1</ref>. This stack is "neuralized" such that each stack entry corresponds to a numerical vec- tor. At each stage of derivation, a single vector summarizing the entire stack is available in the form of the final state of a neural sequence model. This is implemented using the stack LSTMs of <ref type="bibr" target="#b12">Dyer et al. (2015)</ref>. These stack-summary vec- tors (central rectangle in <ref type="figure" target="#fig_1">Figure 1</ref>) allow RNNGs to be sensitive to aspects of the left context that would be masked by independence assumptions in a probabilistic context-free grammar. In the present paper, these stack-summaries serve as in- put to a multi-layer perceptron whose output is converted via softmax into a categorical distribu- tion over three possible parser actions: open a new constituent, close off the latest constituent, or generate a word. A hard decision is made, and if the first or last option is selected, then the same vector-valued stack-summary is again used, via multilayer perceptrons, to decide which specific nonterminal to open, or which specific word to generate.</p><p>Phrase-closing actions trigger a syntactic com- position function (depicted in <ref type="figure" target="#fig_2">Figure 2</ref>) which squeezes a sequence of subtree vectors into one single vector. This happens by applying a bidi- rectional LSTM to the list of daughter vectors, prepended with the vector for the mother category following §4.1 of <ref type="bibr" target="#b13">Dyer et al. (2016)</ref>.</p><p>The parameters of all these components are adaptively adjusted using backpropagation at training time, minimizing the cross entropy rela- tive to a corpus of trees. At testing time, we parse incrementally using beam search as described be- low in section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word-synchronous beam search</head><p>Beam search is one way of addressing the search problem that arises with generative gram- mars -constructive accounts of language that are sometimes said to "strongly generate" sentences. Strong generation in this sense simply means that they derive both an observable word-string as well as a hidden tree structure. Probabilistic grammars are joint models of these two aspects. By contrast, parsers are programs intended to infer a good tree from a given word-string. In incremental pars- ing with history-based models this inference task is particularly challenging, because a decision that looks wise at one point may end up looking foolish in light of future words. Beam search addresses this challenge by retaining a collection called the "beam" of parser states at each word. These states are rated by a score that is related to the probabil- ity of a partial derivation, allowing an incremen- tal parser to hedge its bets against temporary am- biguity. If the score of one analysis suddenly plummets after seeing some word, there may still be others within the beam that are not so drasti- cally affected. This idea of ranked parallelism has become central in psycholinguistic modeling (see e.g. <ref type="bibr" target="#b16">Gibson, 1991;</ref><ref type="bibr" target="#b31">Narayanan and Jurafsky, 1998;</ref><ref type="bibr" target="#b3">Boston et al., 2011)</ref>.</p><p>As  observe, the most straightforward application of beam search to generative models like RNNG does not perform well. This is because lexical actions, which ad- vance the analysis onwards to successive words, are assigned such low probabilities compared to structural actions which do not advance to the next word. This imbalance is inevitable in a probability model that strongly generates sen- tences, and it causes naive beam-searchers to get bogged down, proposing more and more phrase structure rather than moving on through the sentence. To address it,  pro- pose a word-synchronous variant of beam search. This variant keeps searching through structural ac- tions until "enough" high-scoring parser states fi- nally take a lexical action, arriving in synchrony at the next word of the sentence. Their procedure is written out as Algorithm 1.</p><p>Algorithm 1 Word-synchronous beam search with fast-tracking. After  1: thisword ← input beam 2: nextword ← ∅ 3: while |nextword| &lt; k do 4:</p><p>fringe ← successors of all states s ∈ thisword via any parsing action <ref type="bibr">5:</ref> prune fringe to top k</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>thisword ← ∅</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>for each parser state s ∈ fringe do 8:</p><p>if s came via a lexical action then end for 14: end while 15: return nextword pruned to top k word k</p><p>In Algorithm 1 the beam is held in a set-valued variable called nextword. Beam search continues until this set's cardinality exceeds the designated action beam size, k. If the beam still isn't large enough (line 3) then the search process explores one more action by going around the while-loop again. Each time through the loop, lexical ac- tions compete against structural actions for a place among the top k (line 5). The imbalance men- tioned above makes this competition fierce, and on many loop iterations nextword may not grow by much. Once there are enough parser states, another threshold called the word beam k word kicks in (line 15). This other threshold sets the number of analyses that are handed off to the next invocation of the algorithm. In the study reported here the word beam remains at the default setting suggested by Stern and col- leagues, k/10.  go on to offer a modifica- tion of the basic procedure called "fast tracking" which improves performance, particularly when the action beam k is small. Under fast tracking, an additional step is added between lines 4 and 5 of k=100 k=200 k=400 k=600 k=800 k=1000 k=2000   <ref type="table">Table 1</ref>: Penn Treebank development section bracketing accuracies (F1) under Word-Synchronous beam search. These figures show that an incremental parser for RNNG can perform well on a stan- dard benchmark. "ppl" indicates the perplexity of over both trees and strings for the trained model on the development set, averaged over words In all cases the word beam is set to a tenth of the action beam, i.e. k word = k/10.</p><p>Algorithm 1 such that some small number k f t of parser states are promoted directly into nextword. These states are required to come via a lexical ac- tion, but in the absence of fast tracking they quite possibly would have failed the thresholding step in line 5. <ref type="table">Table 1</ref> shows Penn Treebank accuracies for this word-synchronous beam search procedure, as applied to RNNG. As expected, accuracy goes up as the parser considers more and more analyses. Above k = 200, the RNNG+beam search combi- nation outperforms a conditional model based on greedy decoding (88.9). This demonstration re- emphasizes the point, made by Brants and Crocker (2000) among others, that cognitively-plausible incremental processing can be achieved without loss of parsing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Complexity metrics</head><p>In order to relate computational models to mea- sured human responses, some sort of auxiliary hy- pothesis or linking rule is required. In the domain of language, these are traditionally referred to as complexity metrics because of the way they quan- tify the "processing complexity" of particular sen- tences. When a metric offers a prediction on each successive word, it is an incremental complex- ity metric. <ref type="table" target="#tab_2">Table 2</ref> characterizes four incremental com- plexity metrics that are all obtained from inter- mediate states of Algorithm 1. The metric de- noted DISTANCE is the most classic; it is inspired by the count of "transitions made or attempted" in <ref type="bibr" target="#b21">Kaplan (1972)</ref>. It quantifies syntactic work by counting the number of parser actions explored by Algorithm 1 between each word i.e. the number of times around the while-loop on line 3. The information theoretical quantities SURPRISAL and ENTROPY came into more widespread use later.</p><p>They quantify unexpectedness and uncertainty, re- spectively, about alternative syntactic analyses at a given point within a sentence. Hale (2016) re- views their applicability across many different lan- guages, psycholinguistic measurement techniques and grammatical models. Recent work proposes possible relationships between these two metrics, at the empirical as well as theoretical level (van <ref type="bibr" target="#b37">Schijndel and Schuler, 2017;</ref><ref type="bibr" target="#b9">Cho et al., 2018</ref>  The SURPRISAL metric was computed over the word beam i.e. the k word highest-scoring partial syntactic analyses at each successive word. In an attempt to obtain a more faithful estimate, EN- TROPY and its first-difference are computed over nextword itself, whose size varies but is typically much larger than k word .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Regression models of naturalistic EEG</head><p>Electroencephalography (EEG) is an experimen- tal technique that measures very small volt- age fluctuations on the scalp. For a review emphasizing its implications vis-´ a-vis computa- tional models, see <ref type="bibr" target="#b30">Murphy et al. (2018)</ref>.</p><p>We analyzed EEG recordings from 33 par- ticipants as they passively listened to a spoken recitation of the first chapter of Alice's Adventures in Wonderland. <ref type="bibr">2</ref> This au- ditory stimulus was delivered via earphones in an isolated booth. All participants scored significantly better than chance on a post-session 8-question comprehension quiz. An additional ten datasets were excluded for not meeting this behavioral criterion, six due to excessive noise, and three due to experimenter error. All partic- ipants provided written informed consent under the oversight of the University of Michigan HSBS Institutional Review Board (#HUM00081060) and were compensated $15/h. <ref type="bibr">3</ref> Data were recorded at 500 Hz from 61 ac- tive electrodes (impedences &lt; 25 kΩ) and divided into 2129 epochs, spanning -0.3-1 s around the onset of each word in the story. Ocular artifacts were removed using ICA, and remaining epochs with excessive noise were excluded. The data were filtered from 0.5-40 Hz, baseline corrected against a 100 ms pre-word interval, and separated into epochs for content words and epochs for func- tion words because of interactions between pars- ing variables of interest and word-class ( <ref type="bibr" target="#b35">Roark et al., 2009)</ref>.</p><p>Linear regression was used per-participant, at each time-point and electrode, to iden- tify content-word EEG amplitudes that corre- late with complexity metrics derived from the RNNG+beam search combination via the com- plexity metrics in <ref type="table" target="#tab_2">Table 2</ref>. We refer to these time series as Target predictors.</p><p>Each Target predictor was included in its own model, along with several Control predictors that are known to influence sentence processing: sen- tence order, word-order in sentence, log word fre- quency <ref type="bibr" target="#b27">(Lund and Burgess, 1996)</ref>, frequency of the previous and subsequent word, and acoustic sound power averaged over the first 50 ms of the epoch.</p><p>All predictors were mean-centered. We also constructed null regression models in which the rows of the design matrix were randomly per- muted. <ref type="bibr">4</ref> β coefficients for each effect were tested against these null models at the group level across all electrodes from 0-1 seconds post-onset, using a non-parametric cluster-based permutation test to correct for multiple comparisons across electrodes and time-points <ref type="bibr" target="#b28">(Maris and Oostenveld, 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Language models for literary stimuli</head><p>We compare the fit against EEG data for models that are trained on the same amount of textual data but differ in the explicitness of their syntactic rep- resentations.</p><p>At the low end of this scale is the LSTM lan- guage model. Models of this type treat sentences as a sequence of words, leaving it up to backprop- agation to decide whether or not to encode syntac- tic properties in a learned history vector ( <ref type="bibr" target="#b26">Linzen et al., 2016)</ref>. We use SURPRISAL from the LSTM as a baseline.</p><p>RNNGs are higher on this scale because they explicitly build a phrase structure tree using a sym- bolic stack. We consider as well a degraded ver- sion, RNNG −comp which lacks the composi- tion mechanism shown in <ref type="figure" target="#fig_2">Figure 2</ref>. This de- graded version replaces the stack with initial sub- strings of bracket expressions, following <ref type="bibr" target="#b10">Choe and Charniak (2016)</ref>; <ref type="bibr" target="#b40">Vinyals et al. (2015)</ref>. An exam- ple would be the length 7 string shown below (S (NP the hungry cat ) N P (VP Here, vertical lines separate symbols whose vec- tor encoding would be considered separately by RNNG −comp . In this degraded representation, the noun phrase is not composed explicitly. It takes up five symbols rather than one. The balanced paren- theses (NP and ) NP are rather like instructions for some subsequent agent who might later perform the kind of syntactic composition that occurs on- line in RNNGs, albeit in an implicit manner.</p><p>In all cases, these language mod- els were trained on chapters 2-12 of Alice's Adventures in Wonderland. This com- prises 24941 words. The stimulus that participants saw during EEG data collection, for which the metrics in <ref type="table" target="#tab_2">Table 2</ref> are calculated, was chapter 1 of the same book, comprising 2169 words.</p><p>RNNGs were trained to match the output trees provided by the Stanford parser ( <ref type="bibr" target="#b24">Klein and Manning, 2003)</ref>. These trees conform to the Penn Treebank annotation standard but do not explicitly mark long-distance dependency or in- clude any empty categories. They seem to ade- quately represent basic syntactic properties such as clausal embedding and direct objecthood; nev- ertheless we did not undertake any manual correc- tion.</p><p>During RNNG training, the first chapter was used as a development set, proceeding until the per-word perplexity over all parser actions on this set reached a minimum, 180. This performance was obtained with a RNNG whose state vector was 170 units wide. The corresponding LSTM language model state vector had 256 units; it reached a per-word perplexity of 90.2. Of course the RNNG estimates the joint probability of both trees and words, so these two perplexity levels are not directly comparable. Hyperparameter settings were determined by grid search in a region near the one which yielded good performance on the Penn Treebank benchmark reported on <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><p>To explore the suitability of the RNNG + beam search combination as a cognitive model of language processing difficulty, we fitted regres- sion models as described above in section 5 for each of the metrics in <ref type="table" target="#tab_2">Table 2</ref>. We considered six beam sizes k = {100, 200, 400, 600, 800, 1000}. <ref type="table">Table 3</ref> summarizes statistical significance levels reached by these Target predictors; no other com- binations reached statistical significance.  <ref type="table">Table 3</ref>: Statistical significance of fitted Target predictors in Whole-Head analysis. p cluster val- ues are minima for each Target with respect to a Monte Carlo cluster-based permutation test <ref type="bibr" target="#b28">(Maris and Oostenveld, 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Whole-Head analysis</head><p>Surprisal from the LSTM sequence model did not reliably predict EEG amplitude at any timepoint or electrode. The DISTANCE predictor did derive a central positivity around 600 ms post-word onset as shown in <ref type="figure" target="#fig_7">Figure 3a</ref>. SURPRISAL predicted an early frontal positivity around 250 ms, shown in <ref type="figure" target="#fig_7">Figure 3b</ref>. ENTROPY and ENTROPY ∆ seemed to drive effects that were similarly early and frontal, although negative-going (not depicted); the effect for ENTROPY ∆ localized to just the left side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Region of Interest analysis</head><p>We compared RNNG to its degraded cousin, RNNG −comp , in three regions of interest shown in <ref type="figure" target="#fig_8">Figure 4</ref>. These regions are defined by a se- lection of electrodes and a time window whose zero-point corresponds to the onset of the spo- ken word in the naturalistic speech stimulus. Re- gions "N400" and "P600" are well-known in EEG research, while "ANT" is motivated by find- ings with a PCFG baseline reported by <ref type="bibr" target="#b5">Brennan and Hale (2018)</ref>.</p><p>Single-trial data were averaged across elec- trodes and time-points within each region and fit with a linear mixed-effects model with fixed ef- fects as described below and random intercepts by-subjects ( <ref type="bibr" target="#b0">Alday et al., 2017)</ref>. We used a step- wise likelihood-ratio test to evaluate whether indi- vidual Target predictors from the RNNG signifi- cantly improved over RNNG −comp , and whether a RNNG −comp model significantly improve a base- line regression model. The baseline regression model, denoted ∅, contains the Control predictors described in section 5 and SURPRISAL from the LSTM sequence model. Targets represent each of the eight reliable whole-head effects detailed in <ref type="table">Table 3</ref>. These 24 tests (eight effects by three regions) motivate a Bonferroni correction of α = 0.002 = 0.05/24.</p><p>Statistically significant results obtained for DIS- TANCE from RNNG −comp in the P600 region and for SURPRISAL for RNNG in the ANT region. No significant results were observed in the N400 re- gion. These results are detailed in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>Since beam search explores analyses in descend- ing order of probability, DISTANCE and SUR- PRISAL ought to be yoked, and indeed they are correlated at r = 0.33 or greater across all of the beam sizes k that we considered in this study. However they are reliably associated with differ- ent EEG effects. SURPRISAL manifests at ante- rior electrodes relatively early. This seems to be a different effect from that observed by <ref type="bibr" target="#b14">Frank et al. (2015)</ref>. Frank and colleagues relate N400 ampli-   tude to word surprisals from an Elman-net, anal- ogous to the LSTM sequence model evaluated in this work. Their study found no effects of syntax-based predictors over and above sequen- tial ones. In particular, no effects emerged in the 500-700 ms window, where one might have ex- pected a P600. The present results, by contrast, show that an explicitly syntactic model can derive the P600 quite generally via DISTANCE. The ab- sence of an N400 effect in this analysis could be attributable to the choice of electrodes, or perhaps the modality of the stimulus narrative, i.e. spoken versus read. The model comparisons in <ref type="table">Table 4</ref> indicate that the early peak, but not the later one, is attributable to the RNNG's composition function. <ref type="bibr" target="#b10">Choe and Charniak's (2016)</ref> "parsing as language model- ing" scheme potentially could explain the P600- like wave, but it would not account for the ear- lier peak. This earlier peak is the one derived by the RNNG under SURPRISAL, but only when the RNNG includes the composition mechanism de- picted in <ref type="figure" target="#fig_2">Figure 2</ref>.</p><p>This pattern of results suggests an approach to the overall modeling task. In this approach, both grammar and processing strategy remain the same, and alternative complexity metrics, such as SUR- PRISAL and DISTANCE, serve to interpret the uni- fied model at different times or places within the brain. This inverts the approach of <ref type="bibr" target="#b8">Brouwer et al. (2017)</ref> and <ref type="bibr">Wehbe et al. (2014)</ref> who interpret dif- ferent layers of the same neural net using the same complexity metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Recurrent neural net grammars indeed learn some- thing about natural language syntax, and what they learn corresponds to indices of human lan- guage processing difficulty that are manifested in electroencephalography. This correspondence, between computational model and human elec- trophysiological response, follows from a sys- tem that lacks an initial stage of purely string- based processing. Previous work was "two-stage" in the sense that the generative model served to RNNG −comp &gt; ∅ RNNG &gt; RNNG −comp χ 2 df p χ 2 df p DISTANCE, "P600" region k = 200 13.409 1 0.00025 4.198 1 0.04047 k = 400 15.842 1 &lt;0.0001</p><p>3.853 1 0.04966 k = 600 13.955 1 0.00019 3.371 1 0.06635 SURPRISAL, "ANT" region k = 100 3.671 1 0.05537 13.167 1 0.00028 k = 200 3.993 1 0.04570 10.860 1 0.00098 k = 400 3.902 1 0.04824 10.189 1 0.00141 ENTROPY ∆, "ANT" region k = 400 10.141 1 0.00145 5.273 1 0.02165 <ref type="table">Table 4</ref>: Likelihood-ratio tests indicate that regression models with predictors derived from RNNGs with syntactic composition (see <ref type="figure" target="#fig_2">Figure 2</ref>) do a better job than their degraded counterparts in accounting for the early peak in region "ANT" (right-hand columns). Similar comparisons in the "P600" region show that the model improves, but the improvement does not reach the α = 0.002 significance threshold imposed by our Bonferroni correction (bold-faced text). RNNGs lacking syntactic composition do improve over a baseline model (∅) containing lexical predictors and an LSTM baseline (left-hand columns).</p><p>rerank proposals from a conditional model ( <ref type="bibr" target="#b13">Dyer et al., 2016)</ref>. If this one-stage model is cog- nitively plausible, then its simplicity undercuts arguments for string-based perceptual strategies such as the Noun-Verb-Noun heuristic (for a text- book presentation see <ref type="bibr" target="#b39">Townsend and Bever, 2001</ref>). Perhaps, as Phillips (2013) suggests, these are un- necessary in an adequate cognitive model. Cer- tainly, the road is now open for more fine-grained investigations of the order and timing of individ- ual parsing operations within the human sentence processing mechanism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Recurrent neural network grammar configuration used in this paper. The absence of a lookahead buffer is significant, because it forces parsing to be incremental. Completed constituents such as [ NP the hungry cat ] are represented on the stack by numerical vectors that are the output of the syntactic composition function depicted in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RNNG composition function traverses daughter embeddings u, v and w, representing the entire tree with a single vector x. This Figure is reproduced from (Dyer et al., 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fried</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>LSTM not significant SURPRISAL k = 100 p cluster = 0.027 DISTANCE k = 200 p cluster = 0.012 SURPRISAL k = 200 p cluster = 0.003 DISTANCE k = 400 p cluster = 0.002 SURPRISAL k = 400 p cluster = 0.049 ENTROPY ∆ k = 400 p cluster = 0.026 DISTANCE k = 600 p cluster = 0.012 ENTROPY k = 600 p cluster = 0.014</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc>a) DISTANCE derives a P600 at k = 200. (b) SURPRISAL derives an early response at k = 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Plotted values are fitted regression coefficients and 95% confidence intervals, statistically significant in the dark-shaded region with respect to a permutation test following Maris and Oostenveld (2007). The zero point represents the onset of a spoken word. Insets show electrodes with significant effects along with grand-averaged coefficient values across the significant time intervals. The diagram averages over all content words in the first chapter of Alice's Adventures in Wonderland.</figDesc><graphic url="image-1.png" coords="7,72.00,62.81,226.77,169.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Regions of interest. The first region on the left, named "N400", comprises centralposterior electrodes during a time window 300500 ms post-onset. The middle region, "P600" includes posterior electrodes 600-700 ms postonset. The rightmost region "ANT" consists of just anterior electrodes 200-400 ms post-onset.</figDesc><graphic url="image-3.png" coords="7,66.59,347.47,233.65,83.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Complexity Metrics</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Magnetoencephalography also offers high temporal resolution and as such this work fits into a tradition that includes Wehbe et al. (2014), van Schijndel et al. (2015), Wingfield et al. (2017) and Brennan and Pylkkänen (2017).</note>

			<note place="foot" n="2"> https://tinyurl.com/alicedata 3 A separate analysis of these data appears in Brennan and Hale (2018); datasets are available from JRB. 4 Temporal auto-correlation across epochs could impact model fits. Content-words are spaced 1 s apart on average and a spot-check of the residuals from these linear models indicates that they do not show temporal auto-correlation: AR(1) &lt; 0.1 across subjects, time-points, and electrodes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the National Science Foundation under Grants No. 1607441 and No. 1607251. We thank Max Cantor and Rachel Eby for helping with data col-lection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Electrophysiology reveals the neural dynamics of naturalistic auditory language processing: event-related potentials reflect continuous model updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><forename type="middle">M</forename><surname>Alday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Schlesewsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ina</forename><surname>Bornkessel-Schlesewsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Imaging neural correlates of syntactic complexity in a naturalistic context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Bachrach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Ph.D. thesis, MIT</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards history-based grammars: Using richer models for probabilistic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ezra</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafrerty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Magerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parallel processing and sentence comprehension difficulty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marisa</forename><surname>Ferrara Boston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shravan</forename><surname>Vasishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhold</forename><surname>Kliegl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Cognitive Processes</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="349" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic parsing and psychological plausibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Crocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 18th International Conference on Computational Linguistics COLING-2000</title>
		<meeting>18th International Conference on Computational Linguistics COLING-2000<address><addrLine>Saarbrücken/Luxembourg/Nancy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hierarchical structure guides rapid linguistic predictions during naturalistic listening. Forthcoming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Hale</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MEG evidence for incremental sentence composition in the anterior temporal lobe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liina</forename><surname>Pylkkänen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">S6</biblScope>
			<biblScope unit="page" from="1515" to="1531" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abstract linguistic structure correlates with temporal activity during naturalistic comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">P</forename><surname>Stabler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">E</forename><surname>Van Wagenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Ming</forename><surname>Luh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="page" from="81" to="94" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A neurocomputational model of the N400 and the P600 in language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harm</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Crocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noortje</forename><forename type="middle">J</forename><surname>Venhuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C J</forename><surname>Hoeks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1318" to="1352" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic encoding of structural uncertainty in gradient symbols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pyeong Whan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Goldrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018)</title>
		<meeting>the 8th Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parsing as language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2331" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Incremental, predictive parsing with psycholinguistically motivated tree-adjoining grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vera</forename><surname>Demberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1025" to="1066" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>California</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The ERP response to the amount of information conveyed by words in sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">L</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leun</forename><forename type="middle">J</forename><surname>Otten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Galli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriella</forename><surname>Vigliocco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and Language</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving neural parsing by disentangling model combination and reranking effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="161" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A Computational Theory of Human Linguistic Processing: Memory Limitations and Processing Breakdown</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Information-theoretical complexity metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Linguistics Compass</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="397" to="412" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Models of human sentence comprehension in computational psycholinguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Oxford Research Encyclopedia of Linguistics</title>
		<editor>Mark Aronoff</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Augmented transition networks as psychological models of sentence comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="77" to="100" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ACL lifetime achievement award: A life of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cognitively plausible models of human language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="60" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What do recurrent neural network grammars learn about syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Assessing the ability of LSTMs to learn syntax-sensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Producing high-dimensional semantic spaces from lexical cooccurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curt</forename><surname>Burgess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="203" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nonparametric statistical testing of EEG-and MEG-data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Maris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Oostenveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Statistical Language Models Based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoding language from the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alona</forename><surname>Fyshe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language, Cognition, and Computational Models</title>
		<editor>Thierry Poibeau and Aline Editors Villavicencio</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian models of human sentence processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srini</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 20th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madson</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Eventrelated brain potentials elicited by syntactic anomaly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Osterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><forename type="middle">J</forename><surname>Holcomb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="785" to="806" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parser &amp; grammar relations: We don&apos;t understand everything twice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Montserrat Sanz, Itziar Laka, and Michael K. Tanenhaus, editors, Language Down the Garden Path: The Cognitive and Biological Basis of Linguistic Structures</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="294" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust garden path parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Pallier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="324" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Evidence of syntactic working memory usage in MEG data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Van Schijndel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CMCL 2015</title>
		<meeting>CMCL 2015<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Approximations of predictive entropy correlate with reading times</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Van Schijndel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Schuler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CogSci 2017</title>
		<meeting>CogSci 2017<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cognitive Science Society</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Effective inference for generative neural parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1695" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Sentence comprehension : the integration of habits and rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Townsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
