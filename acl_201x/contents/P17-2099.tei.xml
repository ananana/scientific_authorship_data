<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representing Sentences as Low-Rank Subspaces</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Mu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
						</author>
						<title level="a" type="main">Representing Sentences as Low-Rank Subspaces</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="629" to="634"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2099</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences-the word representations of a given sentence (on average 10.23 words in all Se-mEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation , we represent a sentence by the low-rank subspace spanned by its word vectors. Such an unsupervised representation is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15% on average.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Real-valued word representations have brought a fresh approach to classical problems in NLP, rec- ognized for their ability to capture linguistic reg- ularities: similar words tend to have similar rep- resentations; similar word pairs tend to have sim- ilar difference vectors ( <ref type="bibr">Bengio et al., 2003;</ref><ref type="bibr">Mnih and Hinton, 2007;</ref><ref type="bibr">Mikolov et al., 2010;</ref><ref type="bibr">Collobert et al., 2011;</ref><ref type="bibr">Huang et al., 2012;</ref><ref type="bibr">Dhillon et al., 2012;</ref><ref type="bibr">Mikolov et al., 2013;</ref><ref type="bibr" target="#b5">Pennington et al., 2014;</ref><ref type="bibr">Levy and Goldberg, 2014;</ref><ref type="bibr">Arora et al., 2015;</ref><ref type="bibr" target="#b8">Stratos et al., 2015)</ref>. Going beyond words, sentences capture much of the semantic informa- tion. Given the success of lexical representations, a natural question of great topical interest is how to extend the power of distributional representations to sentences.</p><p>There are currently two approaches to represent sentences. A sentence contains rich syntactic in- formation and can be modeled through sophisti- cated neural networks (e.g., convolutional neural networks <ref type="bibr">(Kim, 2014;</ref><ref type="bibr">Kalchbrenner et al., 2014</ref>), recurrent neural networks ( <ref type="bibr" target="#b9">Sutskever et al., 2014;</ref><ref type="bibr">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b11">Kiros et al., 2015;</ref><ref type="bibr">Hill et al., 2016</ref>) and recursive neural networks <ref type="bibr" target="#b6">(Socher et al., 2013)</ref>). Another simple and common ap- proach ignores the latent structure of sentences: a prototypical approach is to represent a sentence by summing or averaging over the vectors of the words in this sentence ( <ref type="bibr" target="#b10">Wieting et al., 2015;</ref><ref type="bibr" target="#b0">Adi et al., 2016;</ref><ref type="bibr">Kenter et al., 2016)</ref>. Recently, <ref type="bibr" target="#b10">Wieting et al. (2015)</ref>; <ref type="bibr" target="#b0">Adi et al. (2016)</ref> reveal that even though the latter approach ignores all syntactic information, it is simple, straightfor- ward, and remarkably robust at capturing the sen- tential semantics. Such an approach successfully outperforms the neural network based approaches on textual similarity tasks in both supervised and unsupervised settings.</p><p>We follow the latter approach but depart from representing sentences in a vector space as in these prior works; we present a novel Grassmannian property of sentences. The geometry is motivated by <ref type="bibr">(Gong et al., 2017;</ref><ref type="bibr" target="#b4">Mu et al., 2016)</ref> where an interesting phenomenon is observed -the local context of a given word/phrase can be well rep- resented by a low rank subspace. We propose to generalize this observation to sentences: not only do the word vectors in a snippet of a sentence (i.e., a context for a given word defined as several words surrounding it) lie in a low-rank subspace, but the entire sentence (on average 10.23 words in all Se- mEval datasets with standard deviation 4.84) fol- lows this geometric property as well:</p><p>Geometry of Sentences: The word rep- resentations lie in a low-rank subspaces (rank 3-5) for all words in a target sen-tence.</p><p>The observation indicates that the subspace con- tains most of the information about this sentence, and therefore motivates a sentence representation method: the sentences should be represented in the space of subspaces (i.e., the Grassmannian mani- fold) instead of a vector space; formally:</p><p>Sentence Representation: A sentence can be represented by a low-rank sub- space spanned by its word representa- tions.</p><p>Analogous to word representations of similar words being similar vectors, the principle of sen- tence representations is: similar sentences should have similar subspaces. Two questions arise: (a) how to define the similarity between sentences and (b) how to define the similarity between sub- spaces.</p><p>The first question has been already addressed by the popular semantic textual similarity (STS) tasks. Unlike textual entailment (which aims at in- ferring directional relation between two sentences) and paraphrasing (which is a binary classifica- tion problem), STS provides a unified framework of measuring the degree of semantic equivalence <ref type="bibr">(Agirre et al., 2012</ref><ref type="bibr">(Agirre et al., , 2013</ref><ref type="bibr" target="#b2">(Agirre et al., , 2014</ref><ref type="bibr" target="#b1">(Agirre et al., , 2015</ref>) in a contin- uous fashion. Motivated by the cosine similarity between vectors being a good index for word sim- ilarity, we generalize this metric to subspaces: the similarity between subspaces defined in this paper is the 2 -norm of the singular values between two subspaces; note that the singular values are in fact the cosine of the principal angles.</p><p>The key justification for our approach comes from empirical results that outperform state-of- the-art in some cases, and being comparable in others. In summary, representing sentences by subspaces outperforms representing sentences by averaged word vectors (by 14% on average) and sophisticated neural networks (by 15%) on 19 dif- ferent STS datasets, ranging over different do- mains (News, WordNet definition, and Twitter).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Geometry of Sentences</head><p>Assembling successful distributional word repre- sentations (for example, <ref type="bibr">GloVe (Pennington et al., 2014)</ref>) into sentence representations is an active research topic. Different from previous studies (for example, doc2vec <ref type="bibr">(Mikolov et al., 2013)</ref>, skip-thought vectors ( <ref type="bibr" target="#b11">Kiros et al., 2015</ref>), Siamese CBOW ( <ref type="bibr">Kenter et al., 2016)</ref>), our main contri- bution is to represent sentences using non-vector space representations: a sentence can be well rep- resented by the subspace spanned by the con- text word vectors -such a method naturally builds on any word representation method. Due to the widespread use of word2vec and GloVe, we use their publicly available word representa- tions -word2vec( <ref type="bibr">Mikolov et al., 2013</ref>) trained us- ing Google News 1 and GloVe ( <ref type="bibr" target="#b5">Pennington et al., 2014</ref>) trained using Common Crawl 2 -to test our observations.</p><p>Observation Let v(w) ∈ R d be the d- dimensional word representation for a given word w ∈ V , and s = (w 1 , . . . , w n ) be a given sen- tence. Consider the following sentence where n = 32:</p><p>They would not tell me if there was any pension left here, and would only tell me if there was (and how much there was) if they saw I was entitled to it.</p><p>After stacking the (non-functional) word vectors v(w) to form a d × n matrix, we observe that most energy (80% for GloVe and 72% for word2vec) of such a matrix is contained in a rank-N subspace, where N is much smaller than n (for comparison, we choose N to be 4 and therefore N/n ≈ 13%). <ref type="figure">Figure 1</ref> provides a visual representation of this geometric phenomenon, where we have projected the d-dimensional word representations into 3- dimensional vectors and use these 3-dimensional word vectors to get the subspace for this sentence (we set N = 2 here for visualization), and plot the subspaces as 2-dimensional planes.</p><p>Geometry of Sentences The example above generalizes to a vast majority of the sentences: the word representations of a given sentence roughly reside in a low-rank subspace, which can be ex- tracted by principal component analysis (PCA).</p><p>Verification We empirically validate this ge- ometric phenomenon by collecting 53,396 sen- tences from the SemEval STS share tasks ( <ref type="bibr">Agirre et al., 2012</ref><ref type="bibr">Agirre et al., , 2013</ref><ref type="bibr" target="#b2">Agirre et al., , 2014</ref><ref type="bibr" target="#b1">Agirre et al., , 2015</ref> and plotting the fraction of energy being captured by the top N components of PCA in <ref type="figure" target="#fig_0">Figure 2</ref> for N = 3, 4, 5,</p><note type="other">5 4 3 2 1 0 1 2 3 3 2 1 0 1 2 3 3 2 1 0 1 2 3 GloVe word2vec Figure 1: Geometry of sentences.</note><p>from where we can observe that on average 70% of the energy is captured by a rank-3 subspace, and 80% for a rank-4 subspace and 90% for rank-5 subspace. For comparison, the fraction of energy of random sentences (generated i.i.d. from the un- igram distribution) are also plotted in <ref type="figure" target="#fig_0">Figure 2</ref>. Representation The observation above moti- vates our sentence representation algorithm: since the words in a sentence concentrate on a few di- rections, the subspace spanned by these directions could in principle be a proper representation for this sentence. The direction and subspace in turn can be extracted via PCA as in Algorithm 1.</p><p>Similarity Metric The principle of sentence representations is that similar sentences should have similar representations. In this case, we expect the similarity between subspaces to be a good index for the semantic similarity of sen- tences. In our paper, we define the similarity be- tween subspaces as follows: let u 1 (s), ..., u N (s) be the N orthonormal basis for a sentence s. Af- ter stacking the N vectors in a d × N matrix Algorithm 1: The algorithm for sentence rep- resentations.</p><p>Input : a sentence s, word embeddings v(·), and a PCA rank N .</p><formula xml:id="formula_0">u 1 , ..., u N ← PCA(v(w ), w ∈ s), S ← N n=1</formula><p>: α n u n , α n ∈ R Output: N orthonormal basis u 1 , ..., u N and a subspace S.</p><p>U (s) = (u 1 (s), ..., u N (s)), we define the corre- sponding cosine similarity as, CosSim(s 1 , s 2 ) = N t=1 σ 2 t , where σ t is the t-th singular value of U (s 1 ) T U (s 2 ).</p><p>Note that σ t = cos(θ t ) where θ t is the t-th "principle angle" between two subspaces. Such a metric is naturally related to the cosine similar- ity between vectors, which has been empirically validated to be a good measure of word similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section we evaluate our sentence represen- tations empirically on the STS tasks. The objec- tive of this task is to test the degree to which the al- gorithm can capture the semantic equivalence be- tween two sentences. For example, the similarity between "a kid jumping a ledge with a bike" and "a black and white cat playing with a blanket" is 0 and the similarity between "a cat standing on tree branches" and "a black and white cat is high up on tree branches" is 3.6. The algorithm is then evalu- ated in terms of Pearson's correlation between the predicted score and the human judgment.</p><p>Datasets We test the performances of our algo- rithm on 19 different datasets, which include Se- mEval STS share tasks ( <ref type="bibr">Agirre et al., 2012</ref><ref type="bibr">Agirre et al., , 2013</ref><ref type="bibr" target="#b2">Agirre et al., , 2014</ref><ref type="bibr" target="#b1">Agirre et al., , 2015</ref>, sourced from multiple domains (for example, News, WordNet definitions and Twitter).</p><p>Baselines and Preliminaries Our main compar- isons are with algorithms that perform unsuper- vised sentence representation: average of word representations (i.e., avg. (of GloVe and skip- gram) where we use the average of word vec- tors), doc2vec (D2V) ( <ref type="bibr">Le and Mikolov, 2014</ref>) and sophisticated neural networks (i.e., skip-thought vectors (ST) ( <ref type="bibr" target="#b11">Kiros et al., 2015)</ref>, Siamese CBOW (SC) ( <ref type="bibr">Kenter et al., 2016)</ref>). In order to enable a fair comparison, we use the Toronto Book Cor- pus ( <ref type="bibr" target="#b11">Zhu et al., 2015</ref>) to train word embeddings. In our experiment, we adapt the same setting as in ( <ref type="bibr">Kenter et al., 2016</ref>) where we use skip-gram ( <ref type="bibr">Mikolov et al., 2013) of and</ref><ref type="bibr">GloVe (Pennington et al., 2014</ref>) to train a 300-dimensional word vec- tors for the words that occur 5 times or more in the training corpus. The rank of subspaces is set to be 4 for both word2vec and GloVe.</p><p>Results The detailed results are reported in Ta- ble 1, from where we can observe two phenom- ena: (a) representing sentences by its averaged word vectors provides a strong baseline and the performances are remarkably stable across differ- ent datasets; (b) our subspace-based method out- performs the average-based method by 14% on av- erage and the neural network based approaches by 15%. This suggests that representing sentences by subspaces maintains more information than sim- ply taking the average, and is more robust than highly-tuned sophisticated models.</p><p>When we average over the words, the average vector is biased because of many irrelevant words (for example, function words) in a given sentence. Therefore, given a longer sentence, the effect of useful word vectors become smaller and thus the average vector is less reliable at capturing the se- mantics. On the other hand, the subspace repre- sentation is immune to this phenomenon: the word vectors capturing the semantics of the sentence tend to concentrate on a few directions which dominate the subspace representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper presents a novel unsupervised sentence representation leveraging the Grassmannian ge- ometry of word representations. While the cur- rent approach relies on the pre-trained word rep- resentations, the joint learning of both word and sentence representations and in conjunction with supervised datasets such the Paraphrase Database (PPDB) ( <ref type="bibr">Ganitkevitch et al., 2013</ref>) is left to fu- ture research. Also interesting is the exploration of neural network architectures that operate on sub- spaces (as opposed to vectors), allowing for down- stream evaluations of our novel representation.  <ref type="table">Table 1</ref>: Pearson's correlation (x100) on SemEval textual similarity task using 19 different datasets, where l is the average sentence length of each dataset. Results that are better than the baselines are marked with underlines and the best results are in bold.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Fraction of energy captured by the top principal components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>HLT . Association for Computational Lin- guistics, Atlanta, Georgia, pages 758-764. http://cs.jhu.edu/ ccb/publications/ppdb.pdf.</head><label>HLT</label><figDesc></figDesc><table>Hongyu Gong, Suma Bhat, and Pramod Viswanath. 
2017. Geometry of compositionality. Association 
for Advancement of Artificial Intelligence (AAAI) . 

Felix Hill, Kyunghyun Cho, and Anna Korhonen. 
2016. 
Learning distributed representations of 
sentences from unlabelled data. arXiv preprint 
arXiv:1602.03483 . 

Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word 
representations via global context and multiple word 
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1. Association for Com-
putational Linguistics, pages 873-882. 

Nal Kalchbrenner, Edward Grefenstette, and Phil 
Blunsom. 2014. 
A convolutional neural net-
work for modelling sentences. 
arXiv preprint 
arXiv:1404.2188 . 

Tom Kenter, Alexey Borisov, and Maarten de Rijke. 
2016. Siamese cbow: Optimizing word embed-
dings for sentence representations. arXiv preprint 
arXiv:1606.04640 . 

Yoon Kim. 2014. 
Convolutional neural net-
works for sentence classification. arXiv preprint 
arXiv:1408.5882 . 

Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, 
Richard Zemel, Raquel Urtasun, Antonio Torralba, 
and Sanja Fidler. 2015. Skip-thought vectors. In 
Advances in neural information processing systems. 
pages 3294-3302. 

Quoc V Le and Tomas Mikolov. 2014. Distributed rep-
resentations of sentences and documents. In ICML. 
volume 14, pages 1188-1196. 

Omer Levy and Yoav Goldberg. 2014. Neural word 
embedding as implicit matrix factorization. In Ad-
vances in neural information processing systems. 
pages 2177-2185. 

Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word 
representations in vector space. arXiv preprint 
arXiv:1301.3781 . 

Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan 
Cernock`Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Inter-
speech. volume 2, page 3. 

Andriy Mnih and Geoffrey Hinton. 2007. Three new 
graphical models for statistical language modelling. 
In Proceedings of the 24th international conference 
on Machine learning. ACM, pages 641-648. dataset 

l 
ST 
SC 
D2V 
GloVe 
skip-gram 
avg. 
subspace 
avg. 
subspace 

2012 

MSRpar 
17.70 5.60 43.79 14.85 46.18 
40.74 
16.82 
35.71 
MSRvid 
6.63 58.07 45.22 19.82 63.75 
73.90 
58.28 
63.19 
OnWn 
7.57 60.45 64.44 35.73 56.72 
63.21 
42.22 
58.43 
SMTeuroparl 
10.70 42.03 45.03 36.18 52.51 
45.83 
37.99 
45.35 
SMTnews 
11.72 39.11 39.02 52.78 38.99 
45.73 
23.44 
37.73 

2013 

FNWN 
19.90 31.24 23.22 51.07 39.29 
41.03 
19.35 
26.43 
OnWn 
7.17 24.18 49.85 49.26 52.48 
72.03 
58.30 
56.52 
headlines 
7.21 38.61 65.34 28.90 49.07 
66.13 
41.53 
62.84 

2014 

OnWn 
7.74 48.82 60.73 60.84 60.15 
76.28 
55.38 
67.13 
deft-forum 
8.38 37.36 40.82 22.63 22.75 
42.60 
32.87 
45.30 
deft-news 
15.78 46.17 59.13 18.93 62.91 
64.40 
38.72 
53.62 
headlines 
7.43 40.31 63.64 24.31 46.00 
62.42 
36.46 
61.44 
images 
9.12 42.57 64.97 39.92 55.19 
73.38 
45.17 
71.84 
tweet-news 
10.03 51.38 73.15 33.56 60.45 
74.29 
44.16 
73.87 

2015 

answer-forum 
15.03 27.84 21.81 28.59 31.39 
69.50 
34.83 
57.62 
answer-studetns 10.44 26.61 36.71 11.14 48.46 
63.43 
43.85 
59.01 
belief 
13.00 45.84 47.69 30.58 44.73 
69.65 
49.24 
64.48 
headlines 
7.50 12.48 21.51 22.64 44.80 
65.67 
44.23 
68.02 
images 
9.12 21.00 25.60 34.14 66.40 
80.12 
56.47 
70.53 

</table></figure>

			<note place="foot" n="1"> https://code.google.com/archive/p/ word2vec/ 2 http://nlp.stanford.edu/projects/ glove/</note>

			<note place="foot" n="1"> Compute the first N principle components of samples v(w ), w ∈ c,</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fine-grained analysis of sentence embeddings using auxiliary prediction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Kermany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Lavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04207</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international workshop on semantic evaluation</title>
		<meeting>the 9th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international workshop on semantic evaluation</title>
		<meeting>the 8th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2013. sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalezagirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">* SEM 2013: The Second Joint Conference on Lexical and Computational Semantics</title>
		<imprint>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suma</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramod</forename><surname>Viswanath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07569</idno>
		<title level="m">Geometry of polysemy</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Citeseer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-based word embeddings from decompositions of count matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1282" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08198</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1506.06724</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
