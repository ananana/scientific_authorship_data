<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Open Web Platform for Rule-Based Speech-to-Sign Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>August 7-12, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manny</forename><surname>Rayner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Geneva</orgName>
								<orgName type="institution" key="instit2">FTI/TIM</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierrette</forename><surname>Bouillon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Geneva</orgName>
								<orgName type="institution" key="instit2">FTI/TIM</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Gerlach</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Geneva</orgName>
								<orgName type="institution" key="instit2">FTI/TIM</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Strasly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Geneva</orgName>
								<orgName type="institution" key="instit2">FTI/TIM</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Tsourakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Geneva</orgName>
								<orgName type="institution" key="instit2">FTI/TIM</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Ebling</surname></persName>
							<email>ebling@ifi.uzh.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computational Linguistics</orgName>
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Open Web Platform for Rule-Based Speech-to-Sign Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="162" to="168"/>
							<date type="published">August 7-12, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present an open web platform for developing , compiling, and running rule-based speech to sign language translation applications. Speech recognition is performed using the Nuance Recognizer 10.2 toolkit, and signed output, including both manual and non-manual components, is rendered using the JASigning avatar system. The platform is designed to make the component technologies readily accessible to sign language experts who are not necessarily computer scientists. Translation grammars are written in a version of Synchronous Context-Free Grammar adapted to the peculiarities of sign language. All processing is carried out on a remote server, with content uploaded and accessed through a web interface. Initial experiences show that simple translation grammars can be implemented on a timescale of a few hours to a few days and produce signed output readily com-prehensible to Deaf informants. Overall, the platform drastically lowers the barrier to entry for researchers interested in building applications that generate high-quality signed language.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While a considerable amount of linguistic research has been carried out on sign languages to date, work in automatic sign language processing is still in its infancy. Automatic sign language process- ing comprises applications such as sign language recognition, sign language synthesis, and sign lan- guage translation <ref type="bibr" target="#b17">(Sáfár and Glauert, 2012)</ref>. For all of these applications, drawing on the expertise of native signers, sign language linguists and sign language interpreters is crucial. These different types of sign language experts may exhibit varying degrees of computer literacy. In the past, their con- tribution to the development of systems that au- tomatically translate into sign language has been restricted mostly to the provision of transcribed and/or annotated sign language data.</p><p>In this paper, we report on the development and evaluation of a platform that allows sign language experts with modest computational skills to play a more active role in sign language machine trans- lation. The platform enables these users to inde- pendently develop and run applications translating speech into synthesized sign language through a web interface. Synthesized sign language is pre- sented by means of a signing avatar. To the best of our knowledge, our platform is the first to facilitate low-threshold speech-to-sign translation, opening up various possible use cases, e.g. that of com- municating with a Deaf customer in a public ser- vice setting like a hospital, train station or bank. <ref type="bibr">1</ref> By pursuing a rule-based translation approach, the platform also offers new possibilities for empiri- cal investigation of sign language linguistics: the linguist can concretely implement a fragment of a hypothesized sign language grammar, sign a range of generated utterances through the avatar, and ob- tain judgements from Deaf informants.</p><p>The remainder of this paper is structured as fol- lows. Section 2 presents background and related work. Section 3 describes the architecture of the speech-to-sign platform. Section 4 reports on a preliminary evaluation of the usability of the plat- form and of translations produced by the platform. Section 5 offers a conclusion and an outlook on fu- ture research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and related work</head><p>There has been surprisingly little work to date on speech to sign language translation. The best- performing system reported in the literature still appears to be TESSA ( <ref type="bibr" target="#b2">Cox et al., 2002</ref>), which translated English speech into British Sign Lan- guage (BSL) in a tightly constrained post office counter service domain, using coverage captured in 370 English phrasal patterns with associated BSL translations. The system was evaluated in a realistic setting in a British post office, with three post office clerks on the hearing side of the dialogues and six Deaf subjects playing the role of customers, and performed creditably. Another substantial project is the one described by <ref type="bibr">SanSegundo et al. (2008)</ref>, which translated Spanish speech into Spanish Sign Language; this, however, does not appear to have reached the stage of be- ing able to achieve reasonable coverage even of a small domain, and the evaluation described in the paper is restricted to comprehensibility of signs from the manual alphabet. <ref type="bibr">2</ref> It is reasonable to ask why so little attention has been devoted to what many people would agree is an important and interesting problem, especially given the early success of TESSA. Our own ex- periences, and those of other researchers we have talked to, suggest that the critical problem is the high barrier to entry: in order to build a speech- to-sign system, it is necessary to be able to com- bine components for speech recognition, transla- tion and sign language animation. The first two technologies are now well-understood, and good platforms are readily available. Sign language animation is still, however, a niche subject, and the practical problems involved in obtaining us- able sign language animation components are non- trivial. The fact that San- <ref type="bibr" target="#b18">Segundo et al. (2008)</ref> chose to develop their own animation component speaks eloquently about the difficulties involved.</p><p>There are three approaches to sign language an- imation: hand-crafted animation, motion captur- ing and synthesis from form notation <ref type="bibr" target="#b8">(Glauert, 2013)</ref>. Hand-crafted animation consists of manu- ally modeling and posing an avatar character. This procedure typically yields high-quality results but is very labor-intensive. A signing avatar may also be animated based on information obtained from motion capturing, which involves recording a hu- man's signing. Although sign language anima- tions obtained through motion capturing also tend to be of good quality, the major drawback of this approach is the long calibration time and extensive postprocessing required.</p><p>Synthesis from form notation permits construc- tion of a fully-fledged animation system that al- lows synthesis of any signed form that can be de- scribed through the associated notation. Avatar signing synthesized from form notation is the most flexible in that it is able to render dynamic content, e.g. display the sign language output of a machine translation system, present the contents of a sign language wiki or an e-learning application, visual- ize lexicon entries or present public transportation information ( <ref type="bibr" target="#b5">Efthimiou et al., 2012;</ref><ref type="bibr" target="#b12">Kipp et al., 2011</ref>). At the same time, this approach to sign language animation typically results in the lowest quality: controlling the appearance of all possible sign forms that may be produced from a given no- tation is virtually impossible.</p><p>The most comprehensive existing sign language animation system based on synthesis from form notation is undoubtedly JASigning ( <ref type="bibr" target="#b7">Elliott et al., 2008;</ref><ref type="bibr" target="#b10">Jennings et al., 2010</ref>), a distant descen- dant of the avatar system used in TESSA which was further developed over the course of the eS- IGN and DictaSign European Framework projects. JASigning performs synthesis from SiGML ( <ref type="bibr" target="#b6">Elliott et al., 2000</ref>), an XML-based representation of the physical form of signs based on the well- understood Hamburg Notation System for Sign Languages (HamNoSys) ( <ref type="bibr" target="#b14">Prillwitz et al., 1989)</ref>. HamNoSys can be converted into SiGML in a straightforward fashion. Unfortunately, despite its many good and indeed unique properties, JASign- ing is a piece of research software that in practice has posed an insurmountable challenge to most linguists without a computer science background.</p><p>The basic purpose of the Lite Speech2Sign project can now be summarised in a sentence: we wished to package JASigning together with a state-of-the-art commercial speech recognition platform and a basic machine translation frame- work in a way that makes the combination easily usable by sign language linguists who are not soft- ware engineers. In the rest of the paper, we de- scribe the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>163</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Lite Speech2Sign platform</head><p>The fact that the Lite Speech2Sign platform is in- tended primarily for use by sign language experts who may only have modest skills in computer sci- ence has dictated several key design decisions. In particular, 1) the formalism used is simple and minimal and 2) no software need be installed on the local machine: all processing (compilation, de- ployment, testing) is performed on a remote server accessed through the web interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Runtime functionality and formalism</head><p>At runtime, the basic processing flow is speech → source language text → "sign table" → SiGML → signed animation. Input speech, source lan- guage text and signed animation have their ob- vious meanings, and we have already introduced SiGML in the preceding section. At the input end of the pipeline, speech recognition is carried out using the Nuance Recognizer 10.2 platform, equipped with domain-specific language models compiled from the grammar. At the output end, SiGML is converted into signed animation form using the JASigning avatar system.</p><p>The "sign table", the level which joins all these pieces together, is an intermediate representa- tion modelled on the diagrams typically used in theoretical sign language linguistics to represent signed utterances. A sign table is, concretely, a matrix whose rows represent the different paral- lel channels of signed language output (manual activities, gaze, head movements, mouth move- ments, etc). The only obligatory row is the one for manual activities, which consists of a sequence of "glosses", each gloss referring to one manual ac- tivity. There is one column for each gloss/manual activity in the signed utterance.</p><p>The usefulness of this representation is depen- dent on the appropriateness of the assumption that sign language is timed so that each non-manual activity can be assumed synchronous with some manual activity. This has been shown to be true for non-manual activities that serve linguistic func- tions. Non-manual activities that serve purely af- fective purposes, e.g., expressing anger or disgust, are known to start slightly earlier than the sur- rounding manual activities ( <ref type="bibr" target="#b16">Reilly and Anderson, 2002;</ref><ref type="bibr" target="#b21">Wilbur, 2000</ref>). A restriction imposed by the low-level SiGML representation is that non- manual activities cannot be extended across sev- eral manual activities in a straightforward way;  however, workarounds have been introduced for this <ref type="bibr">(Ebling and Glauert, 2015)</ref>. Experience with SiGML has shown that it is capable of support- ing signed animation of satisfactory quality <ref type="bibr" target="#b19">(Smith and Nolan, 2015)</ref>.</p><p>The core translation formalism is a version of Synchronous Context Free Grammar (SCFG; <ref type="bibr" target="#b0">(Aho and Ullman, 1969;</ref><ref type="bibr" target="#b1">Chiang, 2005)</ref>) adapted to the peculiarities of sign language translation. A complete toy application definition is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The top-level Utterance rule trans- lates French expressions of the form Je m'appelle NAME ("I am called NAME") to Swiss French Sign Language (LSF-CH) expressions of a form that can be glossed as MOI S_APPELER NAME together with accompanying non-manual com- ponents; for example, the manual activity MOI (signed by pointing at one's chest) is here per- formed together with a head nod, raised eyebrows, widened eyes, and a series of mouth movements approximating the shapes used to say "mwe". The two TrPhrase rules translate the names "Claude" and "Marie" into fingerspelled forms with accompanying mouthings.</p><p>The mapping between the sign table and SiGML levels is specified using three other types of declarations, defined in the resource lexica listed in the initial include lines. 1) Glosses are associated with strings of HamNoSys sym- bols; in this case, the resource lexicon used is lsf_ch.csv, a CSV spreadsheet whose columns are glosses and HNS strings for LSF-CH signs. 2) Symbols in the non-manual rows (Head, Gaze, etc) are mapped into the set of SiGML tags supported by the avatar, according to the decla- rations in the sign-language-independent resource file visicast.txt.</p><p>3) The Mouthing line is treated specially. Two types of mouthings are sup- ported: "mouth pictures", approximate mouthings of phonemes, are written as SAMPA <ref type="bibr" target="#b20">(Wells, 1997)</ref> strings (e.g. mwe is a SAMPA string). It is also possible to use the repertoire of "mouth gestures" (mouth movements not related to spoken language words, produced with teeth, jaw, lips, cheeks, or tongue) supported by the avatar, again using definitions taken from the visicast.txt re- source file. For example, L23 denotes pursed lips <ref type="bibr" target="#b9">(Hanke, 2001)</ref>.</p><p>The Domain unit at the top defines the name of the translation app, the source language 3 and sign language channels, and the type of web client used to display it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compile-and deploy-time functionality</head><p>The compilation process takes application de- scriptions like the one above as input and trans- forms them first into SCFG grammars, then into GrXML grammars 4 , and finally into runnable Nu- ance recognition grammars. The compiler also produces tables of metadata listing associations between symbols and HamNoSys, SAMPA, and SiGML constants.</p><p>Two main challenges needed to be addressed when designing the compile-time functionality. The first was to make the process of developing, uploading, compiling, and deploying web-based speech applications simple to invoke, so that these operations could be performed without detailed understanding of the underlying technology. The second was to support development on a shared server; here, it is critical to ensure that a developer who uploads bad content is not able to break the system for other users.</p><p>At an abstract level, the architecture is as fol- lows. Content is divided into separate "names- paces", with each developer controlling one or more namespaces; a namespace in turn contains one or more translation apps. At the source level, each namespace is a self-contained directory, and each app a self-contained subdirectory.</p><p>From the developer's point of view, the whole upload/compile/deploy cycle reduces to a simple progression across a dashboard with four tabs la- beled "Select", "Compile", "Test", and "Release". The developer starts the upload/compile/deploy cycle by uploading one or more namespace direc- tories over an FTP client and choosing one of them from the "Select" tab.</p><p>The platform contains three separate servers, respectively called compilation, staging, and de- ployment. After selecting the app on the first tab, the developer moves to the second one and presses the "Compile" button to invoke the com- pilation server. Successful compilation results in a Nuance grammar recognition module and a set of namespace-specific table entries; a separate Nuance recognition grammar is created for each namespace. As part of the compilation process, a set of files is also created which list undefined constants. These can be downloaded over the FTP connection and are structured so as to make it easy for the developer to fill in missing entries and add the new content to the resource files.</p><p>When the app has compiled, the developer pro- ceeds to the third, "Staging" tab, and presses the "Test" button. This initiates a process which copies the compiled recognition grammar, table entries and metadata to appropriate places on the staging server and registers the grammar as avail- able for use by the recognition engine, after which the developer can interactively test the application through the web interface. It is important that only copying actions are performed by the "Staging" server; experience shows that recompiling appli- cations can often lead to problems if the compiler changes after an application is uploaded.</p><p>When the developer is satisfied with the appli- cation, they move to the fourth tab and press the "Release" button. This carries out a second set of copying operations which transfer the application to the deployment server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Initial experiences with the platform</head><p>The Lite Speech2Sign platform is undergoing ini- tial testing; during this process, we have con- structed half a dozen toy apps for the transla- tion directions French → LSF-CH and German → Swiss German Sign Language, and one mod- erately substantial app for French → LSF-CH. Grammars written so far all have a flat structure.</p><p>Our central claims regarding the platform are that it greatly simplifies the process of building a speech-to-sign application and allows rapid con- struction of apps which produce signed language of adequate quality. To give some substance to these statements, we tracked the construction of a small French → LSF-CH medical questionnaire app and performed a short evaluation. The app was built by a sign language expert whose main qualifications are in sign language interpretation. The expert began by discussing the corpus with Deaf native signers, to obtain video-recorded ma- terial on which to base development. They then implemented rules and HNS entries, uploaded, de- bugged, and deployed the content, and used the deployed system to perform the evaluation.</p><p>Rule-writing typically required on the order of ten to fifteen minutes per rule, using a method of repeatedly playing the recorded video and enter- ing first the gloss line and then the accompany- ing non-manual lines. Uploading, debugging, and deployment of the app was completely straight- forward and took approximately one hour. The most time-consuming part of the process was im- plementing HNS entries for signs missing from the current LSF-CH HNS lexicon. The time re- quired per entry varied a great deal depending on the sign's complexity, but was typically on the or- der of half an hour to two hours. This part of the task will of course become less important as the HNS lexicon resource becomes more complete.</p><p>The evaluation was carried out with five Deaf subjects and based on recommendations for sign language animation evaluation studies by <ref type="bibr" target="#b11">Kacorri et al. (2015)</ref>. Each subject was first given a short demographic questionnaire. Subjects were then asked to watch seven outputs from the app and echo them back, either in signed or mouthed form, to check the comprensibility of the app's signed output. They then answered a second short ques- tionnaire which asked for their overall impres- sions. The result was encouraging: although none of the subjects felt the signing was truly fluent and human-like (a frequent comment was "artificial"), they all considered it grammatically correct and perfectly comprehensible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and further directions</head><p>Although the Lite Speech2Sign platform is de- signed to appear very simple and most of its run- time processing is carried out by the third-party JASigning and Nuance components, it represents a non-trivial engineering effort. The value it adds is that it allows sign language linguists who may have only modest computational skills to build translation applications that produce synthesized signed language, using a tool whose basic func- tioning can be mastered in two or three weeks. By including speech recognition, these applications can potentially be useful in real situations. In a research context, the platform opens up new possibilities for investigation of the grammar of signed languages. If the linguist wishes to inves- tigate the productivity of a hypothesized syntac- tic rule, they can quickly implement a grammar fragment and produce a set of related signed utter- ances, all signed uniformly using the avatar. Our initial experiences, as described in Section 4, sug- gest that rendering quality is sufficient to obtain useful signer judgements.</p><p>Full documentation for Lite Speech2Sign is available <ref type="bibr" target="#b15">(Rayner, 2016)</ref>. The platform is cur- rently in alpha testing; we plan to open it up for general use during Q3 2016. People interested in obtaining an account may do so by mailing one of the authors of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Toy speech2sign application definition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>include lsf_ch.csv include visicast.txt</figDesc><table>Domain 
Name toy1 
Client speech2sign_client 
SourceLanguage french 
TargetLanguages gloss head gaze \ 
eyebrows aperture mouthing 
EndDomain 

Utterance 
Source 
je m'appelle $$name 
Gloss 
MOI 
S_APPELER $$name 
Head 
Nod 
Neutral 
Neutral 
Gaze 
Neutral Neutral 
Neutral 
Eyebrows Up 
Up 
Up 
Aperture Wide 
Wide 
Wide 
Mouthing mwe 
appel 
$$name 
EndUtterance 

TrPhrase $$name 
Source 
claude 
Gloss 
C L A U D E 
Mouthing C L a u: d e 
EndTrPhrase 

TrPhrase $$name 
Source 
marie 
Gloss 
M 
A R I E 
Mouthing L23 a R i e 
EndTrPhrase 

</table></figure>

			<note place="foot" n="1"> We follow the widely recognized convention of using the upper-cased word Deaf to describe members of the linguistic community of sign language users and, in contrast, the lower-cased word deaf to describe the audiological state of a hearing loss (Morgan and Woll, 2002). 162</note>

			<note place="foot" n="2"> Sign languages make use of a communication form known as the manual alphabet (or, finger alphabet), in which the letters of a spoken language word are fingerspelled, i.e., dedicated signs are used for each letter of the word.</note>

			<note place="foot" n="3"> Any recognition language supported by Nuance Recognizer 10.2 can potentially be used as a source language; the current version of the platform is loaded with language packs for English, French, German, Italian, Japanese and Slovenian. 4 GrXML is an open standard for writing speech recognition grammars.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank John Glauert of the School of Computing Sciences, UEA, for his invaluable help with JASigning, and Nuance Inc for gener-ously making their software available to us for re-search purposes.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Properties of syntax directed translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><forename type="middle">V</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="319" to="334" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tessa, a system to aid communication with deaf people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Tryggvason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Nakisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Tutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Abbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth international ACM conference on Assistive technologies</title>
		<meeting>the fifth international ACM conference on Assistive technologies</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="205" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Building a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Ebling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Glauert</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<idno>s10209-015-0408-1</idno>
		<ptr target="http://dx.doi.org/10.1007/" />
		<title level="m">Swiss German Sign Language avatar with JASigning and evaluating it among the Deaf community. Universal Access in the Information Society</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>last accessed November 20</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Annelies Braffort, Christophe Collet, Petros Maragos, and François Lefebvre-Albaret. The DictaSign Wiki: Enabling web communication for the Deaf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Efthimiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stavroula-Evita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Fotinea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Glauert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Computers Helping People with Special Needs (ICCHP)</title>
		<meeting>the 13th International Conference on Computers Helping People with Special Needs (ICCHP)<address><addrLine>Linz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="205" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The development of language processing support for the ViSiCAST project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glauert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Kennaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth international ACM conference on Assistive technologies</title>
		<meeting>the fourth international ACM conference on Assistive technologies</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Linguistic modelling and language-processing technologies for avatar-based sign language presentation. Universal Access in the Information Society</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glauert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Kennaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Safar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="375" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Lecture held at the University of Zurich</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Glauert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-10-09" />
		</imprint>
	</monogr>
	<note>Animating sign language for Deaf people. unpublished</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ViSiCAST project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hanke</surname></persName>
		</author>
		<ptr target="http://www.visicast.cmp.uea.ac.uk/Papers/ViSiCAST_D5-1v017rev2.pdf(lastaccessed" />
	</analytic>
	<monogr>
		<title level="m">ViSiCAST Deliverable D5-1: Interface definitions</title>
		<imprint>
			<date type="published" when="2001-11-20" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Requirements for a signing avatar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vince</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kennaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Glauert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th LREC Workshop on the Representation and Processing of Sign Languages</title>
		<meeting>the 4th LREC Workshop on the Representation and Processing of Sign Languages<address><addrLine>La Valetta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Demographic and experiential factors influencing acceptance of sign language animation by Deaf users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernisa</forename><surname>Kacorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Ebling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kasmira</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mackenzie</forename><surname>Willard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International ACM SIGACCESS Conference on Computers &amp; Accessibility</title>
		<meeting>the 17th International ACM SIGACCESS Conference on Computers &amp; Accessibility</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sign language avatars: Animation and comprehensibility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Heloir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Intelligent Virtual Agents (IVA)</title>
		<meeting>the 11th International Conference on Intelligent Virtual Agents (IVA)<address><addrLine>Reykjavík, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="113" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The development of complex sentences in British Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bencie</forename><surname>Woll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Directions in Sign Language Acquisition: Trends in Language Acquisition Research</title>
		<editor>Gary Morgan and Bencie Woll</editor>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="255" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">HamNoSys: Version 2.0: An Introductory Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegmund</forename><surname>Prillwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Leven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Zienert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Henning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>Signum, Hamburg, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Using the Regulus Lite Speech2Sign Platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manny</forename><surname>Rayner</surname></persName>
		</author>
		<ptr target="http://www.issco.unige.ch/en/research/projects/Speech2SignDoc/build/html/index.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Online documentation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">FACES: The acquisition of non-manual morphology in ASL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Directions in Sign Language Acquisition</title>
		<editor>G. Morgan and B. Woll</editor>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="159" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computer modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Sáfár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Glauert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sign Language: An International Handbook</title>
		<editor>Roland Pfau, Markus Steinbach, and Bencie Woll</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1075" to="1101" />
		</imprint>
		<respStmt>
			<orgName>De Gruyter Mouton</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Proposing a speech to gesture translation architecture for Spanish deaf people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rubén</forename><surname>San-Segundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Manuel</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Macías-Guarasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Córdoba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José Manuel</forename><surname>Ferreiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="523" to="538" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Emotional facial expressions in synthesised sign language avatars: A manual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Nolan</surname></persName>
		</author>
		<idno>s10209-015-0410-7</idno>
		<ptr target="http://dx.doi.org/10.1007/" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Universal Access in the Information Society. last accessed November 20</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SAMPA computer readable phonetic alphabet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Wells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Standards and Resources for Spoken Language Systems</title>
		<editor>D. Gibbon, R. Moore, and R. Winski</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>De Gruyter Mouton</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Phonological and prosodic layering of nonmanuals in American Sign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronnie</forename><forename type="middle">B</forename><surname>Wilbur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Signs of Language Revisited</title>
		<editor>Karen Emmorey and Harlan Lane</editor>
		<meeting><address><addrLine>Erlbaum, Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="215" to="244" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
