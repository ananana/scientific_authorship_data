<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Purely End-to-end System for Multi-speaker Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Seki</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories (MERL)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Toyohashi University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories (MERL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Le Roux</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories (MERL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Mitsubishi Electric Research Laboratories (MERL)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Purely End-to-end System for Multi-speaker Speech Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2620" to="2630"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2620</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, there has been growing interest in multi-speaker speech recognition, where the utterances of multiple speakers are recognized from their mixture. Promising techniques have been proposed for this task, but earlier works have required additional training data such as isolated source signals or senone alignments for effective learning. In this paper, we propose a new sequence-to-sequence framework to directly decode multiple label sequences from a single speech sequence by unifying source separation and speech recognition functions in an end-to-end manner. We further propose a new objective function to improve the contrast between the hidden vectors to avoid generating similar hypotheses. Experimental results show that the model is directly able to learn a mapping from a speech mixture to multiple label sequences, achieving 83.1% relative improvement compared to a model trained without the proposed objective. Interestingly, the results are comparable to those produced by previous end-to-end works featuring explicit separation and recognition modules.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Conventional automatic speech recognition (ASR) systems recognize a single utterance given a speech signal, in a one-to-one transformation. However, restricting the use of ASR systems to sit- uations with only a single speaker limits their ap- plicability. Recently, there has been growing inter- * This work was done while H. Seki, Ph.D. candidate at Toyohashi University of Technology, Japan, was an intern at MERL. est in single-channel multi-speaker speech recog- nition, which aims at generating multiple tran- scriptions from a single-channel mixture of mul- tiple speakers' speech ( <ref type="bibr" target="#b6">Cooke et al., 2009)</ref>.</p><p>To achieve this goal, several previous works have considered a two-step procedure in which the mixed speech is first separated, and recognition is then performed on each separated speech sig- nal ( <ref type="bibr" target="#b13">Isik et al., 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2017)</ref>. Dramatic advances have recently been made in speech separation, via the deep clustering framework ( <ref type="bibr" target="#b13">Isik et al., 2016)</ref>, hereafter referred to as DPCL. DPCL trains a deep neural network to map each time-frequency (T-F) unit to a high-dimensional embedding vector such that the embeddings for the T-F unit pairs dominated by the same speaker are close to each other, while those for pairs dom- inated by different speakers are farther away. The speaker assignment of each T-F unit can thus be inferred from the embeddings by simple cluster- ing algorithms, to produce masks that isolate each speaker. The original method using k-means clus- tering ( ) was extended to al- low end-to-end training by unfolding the cluster- ing steps using a permutation-free mask inference objective <ref type="bibr" target="#b13">(Isik et al., 2016</ref>). An alternative ap- proach is to perform direct mask inference using the permutation-free objective function with net- works that directly estimate the labels for a fixed number of sources. Direct mask inference was first used in  as a baseline method, but without showing good performance. This ap- proach was revisited in  and <ref type="bibr" target="#b15">Kolbaek et al. (2017)</ref> under the name permutation- invariant training (PIT). Combination of such single-channel speaker-independent multi-speaker speech separation systems with ASR was first con- sidered in <ref type="bibr" target="#b13">Isik et al. (2016)</ref> using a conventional Gaussian Mixture Model/Hidden Markov Model (GMM/HMM) system. Combination with an end- to-end ASR system was recently proposed in <ref type="bibr" target="#b21">(Settle et al., 2018)</ref>. Both these approaches either trained or pre-trained the source separation and ASR networks separately, making use of mixtures and their corresponding isolated clean source ref- erences. While the latter approach could in princi- ple be trained without references for the isolated speech signals, the authors found it difficult to train from scratch in that case. This ability can nonetheless be used when adapting a pre-trained network to new data without such references.</p><p>In contrast with this two-stage approach, <ref type="bibr" target="#b20">Qian et al. (2017)</ref> considered direct optimization of a deep-learning-based ASR recognizer without an explicit separation module. The network is opti- mized based on a permutation-free objective de- fined using the cross-entropy between the system's hypotheses and reference labels. The best per- mutation between hypotheses and reference labels in terms of cross-entropy is selected and used for backpropagation. However, this method still re- quires reference labels in the form of senone align- ments, which have to be obtained on the clean iso- lated sources using a single-speaker ASR system. As a result, this approach still requires the original separated sources. As a general caveat, generation of multiple hypotheses in such a system requires the number of speakers handled by the neural net- work architecture to be determined before train- ing. However, <ref type="bibr" target="#b20">Qian et al. (2017)</ref> reported that the recognition of two-speaker mixtures using a model trained for three-speaker mixtures showed almost identical performance with that of a model trained on two-speaker mixtures. Therefore, it may be possible in practice to determine an upper bound on the number of speakers. <ref type="bibr" target="#b2">Chen et al. (2018)</ref> proposed a progressive training procedure for a hybrid system with ex- plicit separation motivated by curriculum learn- ing. They also proposed self-transfer learning and multi-output sequence discriminative training methods for fully exploiting pairwise speech and preventing competing hypotheses, respectively.</p><p>In this paper, we propose to circumvent the need for the corresponding isolated speech sources when training on a set of mixtures, by using an end-to-end multi-speaker speech recognition with- out an explicit speech separation stage. In sep- aration based systems, the spectrogram is seg- mented into complementary regions according to sources, which generally ensures that different ut- terances are recognized for each speaker. Without this complementarity constraint, our direct multi- speaker recognition system could be susceptible to redundant recognition of the same utterance. In order to prevent degenerate solutions in which the generated hypotheses are similar to each other, we introduce a new objective function that enhances contrast between the network's representations of each source. We also propose a training procedure to provide permutation invariance with low com- putational cost, by taking advantage of the joint CTC/attention-based encoder-decoder network ar- chitecture proposed in ( <ref type="bibr" target="#b10">Hori et al., 2017a)</ref>. <ref type="bibr">Experimental results</ref> show that the proposed model is able to directly convert an input speech mix- ture into multiple label sequences without requir- ing any explicit intermediate representations. In particular no frame-level training labels, such as phonetic alignments or corresponding unmixed speech, are required. We evaluate our model on spontaneous English and Japanese tasks and ob- tain comparable results to the DPCL based method with explicit separation ( <ref type="bibr" target="#b21">Settle et al., 2018</ref>). </p><formula xml:id="formula_0">O = (o t ∈ R D |t = 1, . . . , T )</formula><p>, and the past label history. The probability of the n-th label y n is computed by conditioning on the past history y 1:n−1 :</p><formula xml:id="formula_1">p att (Y |O) = N n=1 p att (y n |O, y 1:n−1 ).<label>(1)</label></formula><p>The model is composed of two main sub-modules, an encoder network and a decoder network. The encoder network transforms the input feature vec- tor sequence into a high-level representation H = (h l ∈ R C |l = 1, . . . , L). The decoder net- work emits labels based on the label history y and a context vector c calculated using an atten- tion mechanism which weights and sums the C- dimensional sequence of representation H with at- tention weight a. A hidden state e of the decoder is updated based on the previous state, the previous context vector, and the emitted label. This mecha- nism is summarized as follows:</p><formula xml:id="formula_2">H = Encoder(O),<label>(2)</label></formula><p>y n ∼ Decoder(c n , y n−1 ), (3) c n , a n = Attention(a n−1 , e n , H), (4) e n = Update(e n−1 , c n−1 , y n−1 ). <ref type="formula">(5)</ref> At inference time, the previously emitted labels are used. At training time, they are replaced by the reference label sequence R = (r 1 , . . . , r N ) in a teacher-forcing fashion, leading to conditional probability p att (Y R |O), where Y R denotes the out- put label sequence variable in this condition. The detailed definitions of Attention and Update are described in Section A of the supplementary mate- rial. The encoder and decoder networks are trained to maximize the conditional probability of the ref- erence label sequence R using backpropagation:</p><formula xml:id="formula_3">L att = Loss att (Y R , R) − log p att (Y R = R|O),<label>(6)</label></formula><p>where Loss att is the cross-entropy loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Joint CTC/attention-based encoder-decoder network</head><p>The joint CTC/attention approach ( <ref type="bibr" target="#b14">Kim et al., 2017;</ref><ref type="bibr" target="#b10">Hori et al., 2017a</ref>), uses the connection- ist temporal classification (CTC) objective func- tion ( <ref type="bibr" target="#b8">Graves et al., 2006</ref>) as an auxiliary task to train the network. CTC formulates the condi- tional probability by introducing a framewise la- bel sequence Z consisting of a label set U and an additional blank symbol defined as Z = {z l ∈ U ∪ {'blank'}|l = 1, · · · , L}:</p><formula xml:id="formula_4">p ctc (Y |O) = Z L l=1 p(z l |z l−1 , Y )p(z l |O),<label>(7)</label></formula><p>where p(z l |z l−1 , Y ) represents monotonic align- ment constraints in CTC and p(z l |O) is the frame- level label probability computed by</p><formula xml:id="formula_5">p(z l |O) = Softmax(Linear(h l )),<label>(8)</label></formula><p>where h l is the hidden representation generated by an encoder network, here taken to be the en- coder of the attention-based encoder-decoder net- work defined in Eq. <ref type="formula" target="#formula_2">(2)</ref>, and Linear(·) is the final linear layer of the CTC to match the number of labels. Unlike the attention model, the forward- backward algorithm of CTC enforces monotonic alignment between the input speech and the out- put label sequences during training and decod- ing. We adopt the joint CTC/attention-based encoder-decoder network as the monotonic align- ment helps the separation and extraction of high- level representation. The CTC loss is calculated as:</p><formula xml:id="formula_6">L ctc = Loss ctc (Y, R) − log p ctc (Y = R|O).<label>(9)</label></formula><p>The CTC loss and the attention-based encoder- decoder loss are combined with an interpolation weight λ ∈ [0, 1]:</p><formula xml:id="formula_7">L mtl = λL ctc + (1 − λ)L att .<label>(10)</label></formula><p>Both CTC and encoder-decoder networks are also used in the inference step. The final hypothe- sis is a sequence that maximizes a weighted condi- tional probability of CTC in Eq. <ref type="formula" target="#formula_4">( 7)</ref> and attention- based encoder decoder network in Eq. <ref type="formula" target="#formula_1">(1)</ref>:</p><formula xml:id="formula_8">ˆ Y = arg max Y γ log p ctc (Y |O) + (1 − γ) log p att (Y |O) ,<label>(11)</label></formula><p>where γ ∈ [0, 1] is an interpolation weight.</p><p>3 Multi-speaker end-to-end ASR</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Permutation-free training</head><p>In situations where the correspondence between the outputs of an algorithm and the references is an arbitrary permutation, neural network training faces a permutation problem. This problem was first addressed by deep clustering , which circumvented it in the case of source separation by comparing the relationships between pairs of network outputs to those between pairs of labels. As a baseline for deep clustering,  also proposed another approach to ad- dress the permutation problem, based on an ob- jective which considers all permutations of refer- ences when computing the error with the network estimates. This objective was later used in <ref type="bibr" target="#b13">Isik et al. (2016)</ref> and . In the latter, it was referred to as permutation-invariant training. This permutation-free training scheme extends the usual one-to-one mapping of outputs and la- bels for backpropagation to one-to-many by se- lecting the proper permutation of hypotheses and references, thus allowing the network to generate multiple independent hypotheses from a single- channel speech mixture. When a speech mixture contains speech uttered by S speakers simulta- neously, the network generates S label sequence variables Y s = (y s 1 , . . . , y s Ns ) with N s labels from the T -frame sequence of D-dimensional input fea- ture vectors, O = (o t ∈ R D |t = 1, . . . , T ):</p><formula xml:id="formula_9">Y s ∼ g s (O), s = 1, . . . , S,<label>(12)</label></formula><p>where the transformations g s are implemented as neural networks which typically share some com- ponents with each other. In the training stage, all possible permutations of the S sequences R s = (r s 1 , . . . , r s N s ) of N s reference labels are consid- ered (considering permutations on the hypotheses would be equivalent), and the one leading to min- imum loss is adopted for backpropagation. Let P denote the set of permutations on {1, . . . , S}. The final loss L is defined as</p><formula xml:id="formula_10">L = min π∈P S s=1 Loss(Y s , R π(s) ),<label>(13)</label></formula><p>where π(s) is the s-th element of a permutation π. For example, for two speakers, P includes two permutations (1, 2) and (2, 1), and the loss is de- fined as:</p><formula xml:id="formula_11">L = min(Loss(Y 1 , R 1 ) + Loss(Y 2 , R 2 ), Loss(Y 1 , R 2 ) + Loss(Y 2 , R 1 )).<label>(14)</label></formula><p>Figure 1 shows an overview of the proposed end-to-end multi-speaker ASR system. In the fol- lowing Section 3.2, we describe an extension of encoder network for the generation of multiple hidden representations. We further introduce a permutation assignment mechanism for reducing the computation cost in Section 3.3, and an ad- ditional loss function L KL for promoting the dif- ference between hidden representations in Sec- tion 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">End-to-end permutation-free training</head><p>To make the network output multiple hypotheses, we consider a stacked architecture that combines both shared and unshared (or specific) neural net- work modules. The particular architecture we con- sider in this paper splits the encoder network into three stages: the first stage, also referred to as mixture encoder, processes the input mixture and outputs an intermediate feature sequence H; that sequence is then processed by S independent en- coder sub-networks which do not share param- eters, also referred to as speaker-differentiating (SD) encoders, leading to S feature sequences H s ; at the last stage, each feature sequence H s is inde- pendently processed by the same network, also re- ferred to as recognition encoder, leading to S final high-level representations G s .</p><p>Let u ∈ {1 . . . , S} denote an output index (cor- responding to the transcription of the speech by one of the speakers), and v ∈ {1 . . . , S} de- note a reference index. Denoting by Encoder Mix the mixture encoder, Encoder u SD the u-th speaker- differentiating encoder, and Encoder Rec the recognition encoder, an input sequence O corre- sponding to an input mixture can be processed by the encoder network as follows:</p><formula xml:id="formula_12">H = Encoder Mix (O),<label>(15)</label></formula><formula xml:id="formula_13">H u = Encoder u SD (H),<label>(16)</label></formula><formula xml:id="formula_14">G u = Encoder Rec (H u ).<label>(17)</label></formula><p>The motivation for designing such an architecture can be explained as follows, following analogies with the architectures in ( <ref type="bibr" target="#b13">Isik et al., 2016)</ref> and <ref type="bibr" target="#b21">(Settle et al., 2018</ref>) where separation and recog-nition are performed explicitly in separate steps: the first stage in Eq. <ref type="formula" target="#formula_1">(15)</ref> corresponds to a speech separation module which creates embedding vec- tors that can be used to distinguish between the multiple sources; the speaker-differentiating sec- ond stage in Eq. (16) uses the first stage's output to disentangle each speaker's speech content from the mixture, and prepare it for recognition; the fi- nal stage in Eq. <ref type="formula" target="#formula_1">(17)</ref> corresponds to an acoustic model that encodes the single-speaker speech for final decoding. The decoder network computes the conditional probabilities for each speaker from the S outputs of the encoder network. In general, the decoder network uses the reference label R as a history to generate the attention weights during training, in a teacher-forcing fashion. However, in the above permutation-free training scheme, the reference label to be attributed to a particular output is not determined until the loss function is computed, so we here need to run the attention decoder for all reference labels. We thus need to consider the con- ditional probability of the decoder output variable Y u,v for each output G u of the encoder network under the assumption that the reference label for that output is R v :</p><formula xml:id="formula_15">p att (Y u,v |O) = n p att (y u,v n |O, y u,v 1:n−1 ), (18) c u,v n , a u,v n = Attention(a u,v n−1 , e u,v n , G u ), (19) e u,v n = Update(e u,v n−1 , c u,v n−1 , r v n−1 ),<label>(20)</label></formula><formula xml:id="formula_16">y u,v n ∼ Decoder(c u,v n , r v n−1 ).<label>(21)</label></formula><p>The final loss is then calculated by considering all permutations of the reference labels as follows:</p><formula xml:id="formula_17">L att = min π∈P s Loss att (Y s,π(s) , R π(s) ).<label>(22)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reduction of permutation cost</head><p>In order to reduce the computational cost, we fixed the permutation of the reference labels based on the minimization of the CTC loss alone, and used the same permutation for the attention mechanism as well. This is an advantage of using a joint CTC/attention based end-to-end speech recogni- tion. Permutation is performed only for the CTC loss by assuming synchronous output where the permutation is decided by the output of CTC:</p><formula xml:id="formula_18">ˆ π = arg min π∈P s Loss ctc (Y s , R π(s) ),<label>(23)</label></formula><p>where Y u is the output sequence variable corre- sponding to encoder output G u . Attention-based decoding is then performed on the same hidden representations G u , using teacher forcing with the labels determined by the permutationˆπpermutationˆ permutationˆπ that mini- mizes the CTC loss:</p><formula xml:id="formula_19">p att (Y u,ˆ π(u) |O) = n p att (y u,ˆ π(u) n |O, y u,ˆ π(u) 1:n−1 ), c u,ˆ π(u) n , a u,ˆ π(u) n = Attention(a u,ˆ π(u) n−1 , e u,ˆ π(u) n , G u ), e u,ˆ π(u) n = Update(e u,ˆ π(u) n−1 , c u,ˆ π(u) n−1 , r ˆ π(u) n−1 ), y u,ˆ π(u) n ∼ Decoder(c u,ˆ π(u) n , r ˆ π(u) n−1 ).</formula><p>This corresponds to the "permutation assignment" in <ref type="figure" target="#fig_1">Fig. 1</ref>. In contrast with Eq. <ref type="formula" target="#formula_1">(18)</ref>, we only need to run the attention-based decoding once for each output G u of the encoder network. The final loss is defined as the sum of two objective functions with interpolation λ:</p><formula xml:id="formula_20">L mtl = λL ctc + (1 − λ)L att ,<label>(24)</label></formula><formula xml:id="formula_21">L ctc = s Loss ctc (Y s , R ˆ π(s) ),<label>(25)</label></formula><formula xml:id="formula_22">L att = s Loss att (Y s,ˆ π(s) , R ˆ π(s) ).<label>(26)</label></formula><p>At inference time, because both CTC and attention-based decoding are performed on the same encoder output G u and should thus pertain to the same speaker, their scores can be incorpo- rated as follows:</p><formula xml:id="formula_23">ˆ Y u = arg max Y u γ log p ctc (Y u |G u ) + (1 − γ) log p att (Y u |G u ) ,<label>(27)</label></formula><p>where p ctc (Y u |G u ) and p att (Y u |G u ) are obtained with the same encoder output G u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Promoting separation of hidden vectors</head><p>A single decoder network is used to output mul- tiple label sequences by independently decoding the multiple hidden vectors generated by the en- coder network. In order for the decoder to gener- ate multiple different label sequences the encoder needs to generate sufficiently differentiated hidden vector sequences for each speaker. We propose to encourage this contrast among hidden vectors by introducing in the objective function a new term based on the negative symmetric Kullback-Leibler (KL) divergence. In the particular case of two- speaker mixtures, we consider the following ad- ditional loss function:</p><formula xml:id="formula_24">L KL = −η l KL( ¯ G 1 (l) || ¯ G 2 (l)) + KL( ¯ G 2 (l) || ¯ G 1 (l)) ,<label>(28)</label></formula><p>where η is a small constant value, and</p><formula xml:id="formula_25">¯ G u = (softmax(G u (l)) | l = 1, . . . , L)</formula><p>is ob- tained from the hidden vector sequence G u at the output of the recognition encoder Encoder Rec as in <ref type="figure" target="#fig_1">Fig. 1</ref> by applying an additional frame-wise softmax operation in order to obtain a quantity amenable to a probability distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Split of hidden vector for multiple hypotheses</head><p>Since the network maps acoustic features to la- bel sequences directly, we consider various archi- tectures to perform implicit separation and recog- nition effectively. As a baseline system, we use the concatenation of a VGG-motivated CNN net- work ( <ref type="bibr" target="#b22">Simonyan and Zisserman, 2014</ref>) (referred to as VGG) and a bi-directional long short-term memory (BLSTM) network as the encoder net- work. For the splitting point in the hidden vector computation, we consider two architectural varia- tions as follows:</p><p>• Split by BLSTM: The hidden vector is split at the level of the BLSTM network. 1) the VGG network generates a single hidden vector H; 2) H is fed into S independent BLSTMs whose parameters are not shared with each other; 3) the output of each independent BLSTM H u , u = 1, . . . , S, is further separately fed into a unique BLSTM, the same for all outputs. Each step corresponds to Eqs. (15), (16), and (17).</p><p>• Split by VGG: The hidden vector is split at the level of the VGG network. The number of filters at the last convolution layer is multiplied by the number of mixtures S in order to split the out- put into S hidden vectors (as in Eq. <ref type="formula" target="#formula_1">(16)</ref>). The layers prior to the last VGG layer correspond to the network in Eq. <ref type="formula" target="#formula_1">(15)</ref>, while the subsequent BLSTM layers implement the network in (17).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>We used English and Japanese speech corpora, WSJ (Wall street journal) (Consortium, 1994; <ref type="table">Table 1</ref>: Duration (hours) of unmixed and mixed corpora. The mixed corpora are generated by Al- gorithm 1 in Section B of the supplementary ma- terial, using the training, development, and evalu- ation set respectively.  <ref type="bibr" target="#b7">Garofalo et al., 2007)</ref> and CSJ (Corpus of spon- taneous Japanese) <ref type="bibr" target="#b16">(Maekawa, 2003)</ref>. To show the effectiveness of the proposed models, we gener- ated mixed speech signals from these corpora to simulate single-channel overlapped multi-speaker recording, and evaluated the recognition perfor- mance using the mixed speech data. For WSJ, we used WSJ1 SI284 for training, Dev93 for develop- ment, and Eval92 for evaluation. For CSJ, we fol- lowed the Kaldi recipe ( <ref type="bibr" target="#b17">Moriya et al., 2015)</ref> and used the full set of academic and simulated pre- sentations for training, and the standard test sets 1, 2, and 3 for evaluation. We created new corpora by mixing two utter- ances with different speakers sampled from exist- ing corpora. The detailed algorithm is presented in Section B of the supplementary material. The sampled pairs of two utterances are mixed at vari- ous signal-to-noise ratios (SNR) between 0 dB and 5 dB with a random starting point for the overlap. Duration of original unmixed and generated mixed corpora are summarized in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Network architecture</head><p>As input feature, we used 80-dimensional log Mel filterbank coefficients with pitch features and their delta and delta delta features (83 × 3 = 249- dimension) extracted using Kaldi tools <ref type="bibr" target="#b19">(Povey et al., 2011</ref>). The input feature is normalized to zero mean and unit variance. As a baseline sys- tem, we used a stack of a 6-layer VGG network and a 7-layer BLSTM as the encoder network. Each BLSTM layer has 320 cells in each direc- tion, and is followed by a linear projection layer with 320 units to combine the forward and back- ward LSTM outputs. The decoder network has an 1-layer LSTM with 320 cells. As described in Section 3.5, we adopted two types of encoder ar- chitectures for multi-speaker speech recognition. The network architectures are summarized in Ta- ble 2. The split-by-VGG network had speaker differentiating encoders with a convolution layer  <ref type="formula">(5)</ref> (and the following maxpooling layer). The split- by-BLSTM network had speaker differentiating encoders with two BLSTM layers. The architec- tures were adjusted to have the same number of layers. We used characters as output labels. The number of characters for WSJ was set to 49 includ- ing alphabets and special tokens (e.g., characters for space and unknown). The number of charac- ters for CSJ was set to 3,315 including Japanese Kanji/Hiragana/Katakana characters and special tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Optimization</head><p>The network was initialized randomly from uni- form distribution in the range -0.1 to 0.1. We used the AdaDelta algorithm (Zeiler, 2012) with gradient clipping ( <ref type="bibr" target="#b18">Pascanu et al., 2013</ref>) for opti- mization. We initialized the AdaDelta hyperpa- rameters as ρ = 0.95 and = 1 −8 . is de- cayed by half when the loss on the development set degrades. The networks were implemented with Chainer ( <ref type="bibr" target="#b23">Tokui et al., 2015)</ref> and ChainerMN <ref type="bibr" target="#b0">(Akiba et al., 2017)</ref>. The optimization of the networks was done by synchronous data parallelism with 4 GPUs for WSJ and 8 GPUs for CSJ. The networks were first trained on single- speaker speech, and then retrained with mixed speech. When training on unmixed speech, only one side of the network only (with a single speaker differentiating encoder) is optimized to output the label sequence of the single speaker. Note that only character labels are used, and there is no need for clean source reference corresponding to the mixed speech. When moving to mixed speech, the other speaker-differentiating encoders are ini- tialized using the already trained one by copying the parameters with random perturbation, w = w × (1 + Uniform(−0.1, 0.1)) for each param- eter w. The interpolation value λ for the multiple objectives in Eqs. <ref type="formula" target="#formula_1">(10)</ref> and <ref type="formula" target="#formula_2">(24)</ref> was set to 0.1 for WSJ and to 0.5 for CSJ. Lastly, the model is re- trained with the additional negative KL divergence loss in Eq. <ref type="formula" target="#formula_2">(28)</ref> with η = 0.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Decoding</head><p>In the inference stage, we combined a pre- trained RNNLM (recurrent neural network lan- guage model) in parallel with the CTC and de- coder network. Their label probabilities were lin- early combined in the log domain during beam search to find the most likely hypothesis. For the WSJ task, we used both character and word level RNNLMs <ref type="figure" target="#fig_1">(Hori et al., 2017b)</ref>, where the charac- ter model had a 1-layer LSTM with 800 cells and an output layer for 49 characters. The word model had a 1-layer LSTM with 1000 cells and an output layer for 20,000 words, i.e., the vocabulary size was 20,000. Both models were trained with the WSJ text corpus. For the CSJ task, we used a char- acter level RNNLM ( <ref type="bibr" target="#b12">Hori et al., 2017c</ref>), which had a 1-layer LSTM with 1000 cells and an out- put layer for 3,315 characters. The model parame- ters were trained with the transcript of the training set in CSJ. We added language model probabilities with an interpolation factor of 0.6 for character- level RNNLM and 1.2 for word-level RNNLM. The beam width for decoding was set to 20 in all the experiments. Interpolation γ in Eqs. <ref type="formula" target="#formula_1">(11)</ref> and <ref type="bibr">(27)</ref> was set to 0.4 for WSJ and 0.5 for CSJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Evaluation of unmixed speech</head><p>First, we examined the performance of the base- line joint CTC/attention-based encoder-decoder network with the original unmixed speech data. <ref type="table" target="#tab_2">Table 3</ref> shows the character error rates <ref type="bibr">(CERs)</ref>, where the baseline model showed 2.6% on WSJ and 7.8% on CSJ. Since the model was trained and evaluated with unmixed speech data, these CERs are considered lower bounds for the CERs in the succeeding experiments with mixed speech data. <ref type="table" target="#tab_3">Table 4</ref> shows the CERs of the generated mixed speech from the WSJ corpus. The first col- umn indicates the position of split as mentioned in Section 3.5. The second, third and forth columns indicate CERs of the high energy speaker (HIGH E. SPK.), the low energy speaker (LOW E. SPK.), and the average (AVG.), respectively. The baseline model has very high CERs because  it was trained as a single-speaker speech recog- nizer without permutation-free training, and it can only output one hypothesis for each mixed speech. In this case, the CERs were calculated by du- plicating the generated hypothesis and comparing the duplicated hypotheses with the correspond- ing references. The proposed models, i.e., split- by-VGG and split-by-BLSTM networks, obtained significantly lower CERs than the baseline CERs, the split-by-BLSTM model in particular achieving 14.0% CER. This is an 83.1% relative reduction from the baseline model. The CER was further re- duced to 13.7% by retraining the split-by-BLSTM model with the negative KL loss, a 2.1% rela- tive reduction from the network without retrain- ing. This result implies that the proposed negative KL loss provides better separation by actively im- proving the contrast between the hidden vectors of each speaker. Examples of recognition results are shown in Section C of the supplementary ma- terial. Finally, we profiled the computation time for the permutations based on the decoder network and on CTC. Permutation based on CTC was 16.3 times faster than that based on the decoder net- work, in terms of the time required to determine the best match permutation given the encoder net- work's output in Eq. (17). <ref type="table" target="#tab_4">Table 5</ref> shows the CERs for the mixed speech from the CSJ corpus. Similarly to the WSJ ex- periments, our proposed model significantly re- duced the CER from the baseline, where the aver- age CER was 14.9% and the reduction ratio from the baseline was 83.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Evaluation of mixed speech</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Visualization of hidden vectors</head><p>We show a visualization of the encoder networks outputs in <ref type="figure" target="#fig_2">Fig. 2</ref> to illustrate the effect of the neg- ative KL loss function. Principal component anal- ysis (PCA) was applied to the hidden vectors on the vertical axis. <ref type="figure" target="#fig_2">Figures 2(a) and 2(b)</ref> show the hidden vectors generated by the split-by-BLSTM model without the negative KL divergence loss for an example mixture of two speakers. We can observe different activation patterns showing that the hidden vectors were successfully separated to the individual utterances in the mixed speech, al- though some activity from one speaker can be seen as leaking into the other. <ref type="figure" target="#fig_2">Figures 2(c) and 2(d)</ref> show the hidden vectors generated after retrain- ing with the negative KL divergence loss. We can more clearly observe the different patterns and boundaries of activation and deactivation of hid- den vectors. The negative KL loss appears to reg- ularize the separation process, and even seems to help in finding the end-points of the speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Comparison with earlier work</head><p>We first compared the recognition performance with a hybrid (non end-to-end) system including DPCL-based speech separation and a Kaldi-based ASR system. It was evaluated under the same evaluation data and metric as in <ref type="bibr" target="#b13">(Isik et al., 2016</ref>) based on the WSJ corpus. However, there are dif- ferences in the size of training data and the op- tions in decoding step. Therefore, it is not a fully matched condition. Results are shown in <ref type="table" target="#tab_5">Table 6</ref>. The word error rate (WER) reported in ( <ref type="bibr" target="#b13">Isik et al., 2016</ref>) is 30.8%, which was obtained with jointly trained DPCL and second-stage speech enhance- ment networks. The proposed end-to-end ASR gives an 8.4% relative reduction in WER even though our model does not require any explicit frame-level labels such as phonetic alignment, or clean signal reference, and does not use a phonetic lexicon for training. Although this is an unfair comparison, our purely end-to-end system outper- formed a hybrid system for multi-speaker speech recognition.</p><p>Next, we compared our method with an end- to-end explicit separation and recognition net- work ( <ref type="bibr" target="#b21">Settle et al., 2018)</ref>. We retrained our model previously trained on our WSJ-based corpus using the training data generated by <ref type="bibr" target="#b21">Settle et al. (2018)</ref>, because the direct optimization from scratch on their data caused poor recognition performance due to data size. Other experimental conditions are shared with the earlier work. Interestingly, our method showed comparable performance to the end-to-end explicit separation and recognition network, without having to pre-train using clean signal training references. It remains to be seen if this parity of performance holds in other tasks and conditions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Several previous works have considered an ex- plicit two-step procedure <ref type="bibr" target="#b13">Isik et al., 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2017</ref><ref type="bibr" target="#b2">Chen et al., , 2018</ref>. In contrast with our work which uses a sin- gle objective function for ASR, they introduced an objective function to guide the separation of mixed speech. <ref type="bibr" target="#b20">Qian et al. (2017)</ref> trained a multi-speaker speech recognizer using permutation-free training without explicit objective function for separation. In contrast with our work which uses an end-to- end architecture, their objective function relies on a senone posterior probability obtained by align- ing unmixed speech and text using a model trained as a recognizer for single-speaker speech. Com- pared with <ref type="bibr" target="#b20">(Qian et al., 2017)</ref>, our method di- rectly maps a speech mixture to multiple character sequences and eliminates the need for the corre- sponding isolated speech sources for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we proposed an end-to-end multi- speaker speech recognizer based on permutation- free training and a new objective function pro- moting the separation of hidden vectors in order to generate multiple hypotheses. In an encoder- decoder network framework, teacher forcing at the decoder network under multiple refer- ences increases computational cost if implemented naively. We avoided this problem by employing a joint CTC/attention-based encoder-decoder net- work.</p><p>Experimental results showed that the model is able to directly convert an input speech mixture into multiple label sequences under the end-to-end framework without the need for any explicit inter- mediate representation including phonetic align- ment information or pairwise unmixed speech. We also compared our model with a method based on explicit separation using deep clustering, and showed comparable result. Future work includes data collection and evaluation in a real world scenario since the data used in our experiments are simulated mixed speech, which is already ex- tremely challenging but still leaves some acous- tic aspects, such as Lombard effects and real room impulse responses, that need to be alleviated for further performance improvement. In addition, further study is required in terms of increasing the number of speakers that can be simultane- ously recognized, and further comparison with the separation-based approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2</head><label></label><figDesc>Single-speaker end-to-end ASR 2.1 Attention-based encoder-decoder network An attention-based encoder-decoder net- work (Bahdanau et al., 2016) predicts a target label sequence Y = (y 1 , . . . , y N ) without requir- ing intermediate representation from a T -frame sequence of D-dimensional input feature vectors,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: End-to-end multi-speaker speech recognition. We propose to use the permutation-free training for CTC and attention loss functions Loss ctc and Loss att , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of the two hidden vector sequences at the output of the split-by-BLSTM encoder on a two-speaker mixture. (a,b): Generated by the model without the negative KL loss. (c,d): Generated by the model with the negative KL loss.</figDesc><graphic url="image-309.png" coords="9,135.78,62.81,325.98,201.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Network architectures for the en- coder network. The number of layers is indi- cated in parentheses. Encoder Mix , Encoder u SD , and Encoder Rec correspond to Eqs. (15), (16), and (17).</head><label>2</label><figDesc></figDesc><table>SPLIT BY EncoderMix 
Encoder u 

SD 

EncoderRec 

NO 

VGG (6) 
-
BLSTM (7) 
VGG 
VGG (4) 
VGG (2) 
BLSTM (7) 
BLSTM 
VGG (6) 
BLSTM (2) BLSTM </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Evaluation of unmixed speech without multi-speaker training.</head><label>3</label><figDesc></figDesc><table>TASK AVG. 
WSJ 
2.6 
CSJ 
7.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : CER (%) of mixed speech for WSJ.</head><label>4</label><figDesc></figDesc><table>SPLIT 
HIGH E. SPK. LOW E. SPK. AVG. 
NO (BASELINE) 
86.4 
79.5 
83.0 
VGG 
17.4 
15.6 
16.5 
BLSTM 
14.6 
13.3 
14.0 
+ KL LOSS 
14.0 
13.3 
13.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : CER (%) of mixed speech for CSJ.</head><label>5</label><figDesc></figDesc><table>SPLIT 
HIGH E. SPK. LOW E. SPK. AVG. 
NO (BASELINE) 
93.3 
92.1 
92.7 
BLSTM 
11.0 
18.8 
14.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison with conventional ap-
proaches 

METHOD 
WER (%) 
DPCL + ASR (ISIK ET AL., 2016) 
30.8 
Proposed end-to-end ASR 
28.2 
METHOD 
CER (%) 
END-TO-END DPCL + ASR (CHAR LM) 
(SETTLE ET AL., 2018) 
13.2 
Proposed end-to-end ASR (char LM) 
14.0 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ChainerMN: Scalable Distributed Deep Learning Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on ML Systems in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>Workshop on ML Systems in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Endto-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
	<note>Philemon Brakel, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Progressive joint modeling in unsupervised single-channel overlapped speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhehuai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="184" to="196" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Jan K Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">CSR-II (wsj1) complete. Linguistic Data Consortium, Philadelphia</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">94</biblScope>
		</imprint>
	</monogr>
	<note>Linguistic Data Consortium</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monaural speech separation and recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven J</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rennie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">CSR-I (wsj0) complete. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Garofalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pallett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="93" to="99" />
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning (ICML)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>John R Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint CTC/attention decoding for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL): Human Language Technologies: long papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (ACL): Human Language Technologies: long papers</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-level language modeling and decoding for open vocabulary end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>ASRU</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Advances in joint CTC-Attention based end-to-end speech recognition with a deep CNN encoder and RNN-LM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>William</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="page" from="949" to="953" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Singlechannel multi-speaker separation using deep clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="545" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint CTC-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Corpus of Spontaneous Japanese: Its design and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kikuo</forename><surname>Maekawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &amp; IEEE Workshop on Spontaneous Speech Processing and Recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaldi recipe for Japanese spontaneous speech recognition and its evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takafumi</forename><surname>Moriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takahiro</forename><surname>Shinozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autumn Meeting of ASJ</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">On the difficulty of training recurrent neural networks. International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>ASRU</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Single-channel multi-talker speech recognition with permutation invariant training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuankai</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06527</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end multi-speaker speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4819" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning Systems (LearningSys) in NIPS</title>
		<meeting>Workshop on Machine Learning Systems (LearningSys) in NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><surname>Kolbk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
