<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Augmentation for Low-Resource Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marzieh</forename><surname>Fadaee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
						</author>
						<title level="a" type="main">Data Augmentation for Low-Resource Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="567" to="573"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-2090</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision , we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In computer vision, data augmentation techniques are widely used to increase robustness and im- prove learning of objects with a limited number of training examples. In image processing the train- ing data is augmented by, for instance, horizon- tally flipping, random cropping, tilting, and al- tering the RGB channels of the original images ( <ref type="bibr" target="#b7">Krizhevsky et al., 2012;</ref><ref type="bibr">Chatfield et al., 2014</ref>). Since the content of the new image is still the same, the label of the original image is preserved (see top of <ref type="figure" target="#fig_0">Figure 1</ref>). While data augmentation has become a standard technique to train deep net- works for image processing, it is not a common practice in training networks for NLP tasks such as Machine Translation.</p><p>Neural Machine Translation (NMT) ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b14">Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Cho et al., 2014</ref>) is a sequence-to-sequence architecture where an encoder builds up a representation of the source sentence and a decoder, using the previous A boy is holding a bat.</p><p>A boy is holding a backpack.</p><p>A boy is holding a bat.</p><p>A boy is holding a bat. Ein Junge hält einen Schläger.</p><p>Ein Junge hält einen Rucksack. LSTM hidden states and an attention mechanism, generates the target translation. To train a model with reliable parameter estima- tions, these networks require numerous instances of sentence translation pairs with words occurring in diverse contexts, which is typically not avail- able in low-resource language pairs. As a result NMT falls short of reaching state-of-the-art per- formances for these language pairs ( <ref type="bibr" target="#b15">Zoph et al., 2016)</ref>. The solution is to either manually annotate more data or perform unsupervised data augmen- tation. Since manual annotation of data is time- consuming, data augmentation for low-resource language pairs is a more viable approach. Re- cently <ref type="bibr" target="#b12">Sennrich et al. (2016a)</ref> proposed a method to back-translate sentences from monolingual data and augment the bitext with the resulting pseudo parallel corpora.</p><p>In this paper, we propose a simple yet effective approach, translation data augmentation (TDA), that augments the training data by altering existing sentences in the parallel corpus, similar in spirit to the data augmentation approaches in computer vi- sion (see <ref type="figure" target="#fig_0">Figure 1</ref>). In order for the augmentation process in this scenario to be label-preserving, any change to a sentence in one language must pre-serve the meaning of the sentence, requiring sen- tential paraphrasing systems which are not avail- able for many language pairs. Instead, we propose a weaker notion of label preservation that allows to alter both source and target sentences at the same time as long as they remain translations of each other.</p><p>While our approach allows us to augment data in numerous ways, we focus on augment- ing instances involving low-frequency words, be- cause the parameter estimation of rare words is challenging, and further exacerbated in a low- resource setting. We simulate a low-resource set- ting as done in the literature ( <ref type="bibr" target="#b10">Marton et al., 2009;</ref><ref type="bibr" target="#b3">Duong et al., 2015)</ref> and obtain substantial im- provements for translating EnglishÑGerman and GermanÑEnglish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Translation Data Augmentation</head><p>Given a source and target sentence pair (S,T), we want to alter it in a way that preserves the semantic equivalence between S and T while diversifying as much as possible the training examples. A number of ways to do this can be envisaged, as for example paraphrasing (parts of) S or T. Paraphrasing, how- ever, is by itself a difficult task and is not guaran- teed to bring useful new information into the train- ing data. We choose instead to focus on a subset of the vocabulary that we know to be poorly modeled by our baseline NMT system, namely words that occur rarely in the parallel corpus. Thus, the goal of our data augmentation technique is to provide novel contexts for rare words. To achieve this we search for contexts where a common word can be replaced by a rare word and consequently replace its corresponding word in the other language by that rare word's translation:</p><p>original pair augmented pair S : s1, ..., si, ..., sn S 1 : s1, ..., s 1 i , ..., sn T : t1, ..., tj, ..., tm T 1 : t1, ..., t 1 j , ..., tm where t j is a translation of s i and word-aligned to s i . Plausible substitutions are those that result in a fluent and grammatical sentence but do not neces- sarily maintain its semantic content. As an exam- ple, the rare word motorbike can be substituted in different contexts:</p><p>Sentence Implausible substitutions need to be ruled out dur- ing data augmentation. To this end, rather than re- lying on linguistic resources which are not avail- able for many languages, we rely on LSTM lan- guage models (LM) (Hochreiter and Schmidhu- ber, 1997; <ref type="bibr" target="#b6">Jozefowicz et al., 2015</ref>) trained on large amounts of monolingual data in both forward and backward directions. Our data augmentation method involves the fol- lowing steps:</p><p>Targeted words selection: Following common practice, our NMT system limits its vocabulary V to the v most common words observed in the train- ing corpus. We select the words in V that have fewer than R occurrences and use this as our tar- geted rare word list V R .</p><p>Rare word substitution: If the LM suggests a rare substitution in a particular context, we replace that word and add the new sentence to the training data. Formally, given a sentence pair pS, T q and a position i in S we compute the probability distri- bution over V by the forward and backward LMs and select rare word substitutions C as follows:</p><formula xml:id="formula_0">Ý Ñ C " ts 1 i P V R : topK P ForwardLM S ps 1 i | s i´1 1 qu Ð Ý C " ts 1 i P V R : topK P BackwardLM S ps 1 i | s i`1 n qu C " ts 1 i | s 1 i P Ý Ñ C ^ s 1 i P Ð Ý C u</formula><p>where topK returns the K words with highest con- ditional probability according to the context. The selected substitutions s 1 i , are used to replace the original word and generate a new sentence.</p><p>Translation selection: Using automatic word alignments 1 trained over the bitext, we replace the translation of word s i in T by the translation of its substitution s 1 i . Following a common practice in statistical MT, the optimal translation t 1 j is chosen by multiplying direct and inverse lexical transla- tion probabilities with the LM probability of the translation in context:</p><formula xml:id="formula_1">t 1 j " arg max tPtransps 1 i q P ps 1 i | tqP pt | s 1 i qP LM T pt | t j´1 1 q</formula><p>If no translation candidate is found because the word is unaligned or because the LM probability is less than a certain threshold, the augmented sen- tence is discarded. This reduces the risk of gener- ating sentence pairs that are semantically or syn- tactically incorrect.</p><p>Sampling: We loop over the original parallel corpus multiple times, sampling substitution po- sitions, i, in each sentence and making sure that each rare word gets augmented at most N times so that a large number of rare words can be affected. We stop when no new sentences are generated in one pass of the training data. <ref type="table" target="#tab_1">Table 1</ref> provides some examples resulting from our augmentation procedure. While using a large LM to substitute words with rare words mostly re- sults in grammatical sentences, this does not mean that the meaning of the original sentence is pre- served. Note that meaning preservation is not an objective of our approach.</p><p>Two translation data augmentation (TDA) se- tups are considered: only one word per sentence can be replaced (TDA r"1 ), or multiple words per sentence can be replaced, with the condition that any two replaced words are at least five positions apart (TDA rě1 ). The latter incurs a higher risk of introducing noisy sentences but has the poten- tial to positively affect more rare words within the same amount of augmented data. We evaluate both setups in the following section. <ref type="bibr">En</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this section we evaluate the utility of our ap- proach in a simulated low-resource NMT scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and experimental setup</head><p>To simulate a low-resource setting we randomly sample 10% of the EnglishØGerman WMT15 training data and report results on newstest <ref type="bibr" target="#b15">, and 2016</ref><ref type="bibr">(Bojar et al., 2016</ref>). For reference we also provide the result of our baseline system on the full data. As NMT system we use a 4-layer attention- based encoder-decoder model as described in (Lu-</p><note type="other">ong et al., 2015) trained with hidden dimension 1000, batch size 80 for 20 epochs. In all experi- ments the NMT vocabulary is limited to the most common 30K words in both languages. Note that data augmentation does not introduce new words to the vocabulary. In all experiments we prepro- cess source and target language data with Byte- pair encoding (BPE) (Sennrich et al., 2016b) using 30K merge operations. In the augmentation exper- iments BPE is performed after data augmentation.</note><p>For the LMs needed for data augmentation, we train 2-layer LSTM networks in forward and back- ward directions on the monolingual data provided for the same task (3.5B and 0.9B tokens in En- glish and German respectively) with embedding size 64 and hidden size 128. We set the rare word threshold R to 100, top K words to 1000 and max- imum number N of augmentations per rare word to 500. In all experiments we use the English LM for the rare word substitutions, and the German LM to choose the optimal word translation in con- text. Since our approach is not label preserving we only perform augmentation during training and do not alter source sentences during testing.</p><p>We also compare our approach to <ref type="bibr" target="#b12">Sennrich et al. (2016a)</ref> by back-translating monolingual data and adding it to the parallel training data. Specifically, we back-translate sentences from the target side of WMT'15 that are not included in our low-resource baseline with two settings: keeping a one-to-one ratio of back-translated versus original data (1 : 1) following the authors' suggestion, or using three times more back-translated data (3 : 1).</p><p>We measure translation quality by single- reference case-insensitive BLEU ( <ref type="bibr" target="#b11">Papineni et al., 2002</ref>) computed with the multi-bleu.perl script from Moses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>All translation results are displayed in <ref type="table">Table 2</ref>. As expected, the low-resource baseline performs much worse than the full data system, re-iterating <ref type="table" target="#tab_1">Model  Data  test2014  test2015  test2016  test2014  test2015  test2016  Full data (ceiling) 3.9M 21.1</ref> 22.0 26.9 17.0 18.5 21.7 Baseline 371K 10.6 11.3 13.1 8.2 9.2 11.0 Back-translation1:1 731K 11.4 (+0.8) Ĳ 12.2 (+0.9) Ĳ 14.6 (+1.5) Ĳ 9.0 (+0.8) Ĳ 10.4 (+1.2) Ĳ 12.0 (+1.0) Ĳ Back-translation3:1 1.5M 11.2 (+0.6) 11.2 (-0.1) 13.3 (+0.2) 7.8 (-0.4) 9.4 (+0.2) 10.7 (-0.3) TDAr"1 4.5M 11.9 (+1.3) Ĳ,-13.4 (+2.1) Ĳ,Ĳ 15.2 (+2.1) Ĳ,Ĳ 10.4 (+2.2) Ĳ,Ĳ 11.2 (+2.0) Ĳ,Ĳ 13.5 (+2.5) Ĳ,Ĳ TDArě1 6M 12.6 (+2.0) Ĳ,Ĳ 13.7 (+2.4) Ĳ,Ĳ 15.4 (+2.3) Ĳ,Ĳ 10.7 (+2.5) Ĳ,Ĳ 11.5 (+2.3) Ĳ,Ĳ 13.9 (+2.9) Ĳ,Ĳ Oversampling 6M 11.9 (+1.3) Ĳ,-12.9 (+1.6) Ĳ,Ÿ 15.0 (+1.9) Ĳ,- 9.7 (+1.5) Ĳ,Ÿ 10.7 (+1.5) Ĳ,-12.6 (+1.6) Ĳ,- <ref type="table">Table 2</ref>: Translation performance (BLEU) on German-English and English-German WMT test sets <ref type="bibr">(newstest2014, 2015, and 2016</ref>) in a simulated low-resource setting. Back-translation refers to the work of <ref type="bibr" target="#b12">Sennrich et al. (2016a)</ref>. Statistically significant improvements are marked Ĳ at the p ă .01 and Ÿ at the p ă .05 level, with the first superscript referring to baseline and the second to back-translation 1:1 .</p><note type="other">De-En En-De</note><p>the importance of sizable training data for NMT. Next we observe that both back-translation and our proposed TDA method significantly improve translation quality. However TDA obtains the best results overall and significantly outperforms back- translation in all test sets. This is an important finding considering that our method involves only minor modifications to the original training sen- tences and does not involve any costly translation process. Improvements are consistent across both translation directions, regardless of whether rare word substitutions are first applied to the source or to the target side. We also observe that altering multiple words in a sentence performs slightly better than altering only one. This indicates that addressing more rare words is preferable even though the augmented sentences are likely to be noisier.</p><p>To verify that the gains are actually due to the rare word substitutions and not just to the repeti- tion of part of the training data, we perform a fi- nal experiment where each sentence pair selected for augmentation is added to the training data un- changed <ref type="table">(Oversampling in Table 2</ref>). Surprisingly, we find that this simple form of sampled data replication outperforms both baseline and back- translation systems, 2 while TDA rě1 remains the best performing system overall.</p><p>We also observe that the system trained on aug- mented data tends to generate longer translations. Averaging on all test sets, the length of translations generated by the baseline is 0.88 of the average reference length, while for TDA r"1 and TDA rě1 it is 0.95 and 0.94, respectively. We attribute this effect to the ability of the TDA-trained system to generate translations for rare words that were left 2 Note that this effect cannot be achieved by simply con- tinuing the baseline training for up to 50 epochs. untranslated by the baseline system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis of the Results</head><p>A desired effect of our method is to increase the number of correct rare words generated by the NMT system at test time.</p><p>To examine the impact of augmenting the train- ing data by creating contexts for rare words on the target side, <ref type="table" target="#tab_3">Table 3</ref> provides an example for GermanÑEnglish translation. We see that the baseline model is not able to generate the rare word centimetres as a correct translation of the German word zentimeter . However, this word is not rare in the training data of the TDA rě1 model after augmentation and is generated during trans- lation. <ref type="table" target="#tab_3">Table 3</ref> also provides several instances of augmented training sentences targeting the word centimetres. Note that even though some aug- mented sentences are nonsensical (e.g. the speed limit is five centimetres per hour), the NMT sys- tem still benefits from the new context for the rare word and is able to generate it during testing. <ref type="figure">Figure 2</ref> demonstrates that this is indeed the case for many words: the number of rare words occurring in the reference translation (V R X V ref ) is three times larger in the TDA system output than in the baseline output. One can also see that this increase is a direct effect of TDA as most of the rare words are not 'rare' anymore in the augmented data, i.e., they were augmented suffi- ciently many times to occur more than 100 times (see hatched pattern in <ref type="figure">Figure 2</ref>). Note that during the experiments we did not use any information from the evaluation sets.</p><p>To gauge the impact of augmenting the con- texts for rare words on the source side, we ex- amine normalized attention scores of these words before and after augmentation. When translating Source der tunnel hat einen querschnitt von 1,20 meter höhe und 90 zentimeter breite . Baseline translation the wine consists of about 1,20 m and 90 of the canal . TDA rě1 translation the tunnel has a UNK measuring meters 1.20 metres high and 90 centimetres wide . Reference the tunnel has a cross -section measuring 1.20 metres high and 90 centimetres across .  Words in V R X V ref affected by augmentation <ref type="figure">Figure 2</ref>: Effect of TDA on the number of unique rare words generated during DeÑEn translation. V R is the set of rare words targeted by TDA rě1 and V ref the reference translation vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples of</head><p>EnglishÑGerman with our TDA model, the at- tention scores for rare words on the source side are on average 8.8% higher than when translating with the baseline model. This suggests that hav- ing more accurate representations of rare words increases the model's confidence to attend to these words when encountered during test time.  Finally <ref type="table" target="#tab_5">Table 4</ref> provides examples of cases where augmentation results in incorrect sentences. In the first example, the sentence is ungrammati- cal after substitution (of / yearly), which can be the result of choosing substitutions with low probabil- ities from the English LM topK suggestions.</p><p>Errors can also occur during translation selec- tion, as in the second example where betraut is an acceptable translation of entrusted but would re- quire a rephrasing of the German sentence to be grammatically correct. Problems of this kind can be attributed to the German LM, but also to the lack of a more suitable translation in the lexicon extracted from the bitext. Interestingly, this noise seems to affect NMT only to a limited extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a simple but effective ap- proach to augment the training data of Neural Machine Translation for low-resource language pairs. By leveraging language models trained on large amounts of monolingual data, we gen- erate new sentence pairs containing rare words in new, synthetically created contexts. We show that this approach leads to generating more rare words during translation and, consequently, to higher translation quality. In particular we re- port substantial improvements in simulated low- resource EnglishÑGerman and GermanÑEnglish settings, outperforming another recently proposed data augmentation technique.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Top: flip and crop, two label-preserving data augmentation techniques in computer vision. Bottom: Altering one sentence in a parallel corpus requires changing its translation.</figDesc><graphic url="image-3.png" coords="1,306.29,222.05,220.24,71.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>: I had been told that you would [not / voluntarily] be speaking today. De: mir wurde signalisiert, sie würden heute [nicht / frei- willig] sprechen. En: the present situation is [indefensible / confusing] and completely unacceptable to the commission. De: die situation sei [unhaltbar / verwirrend] und für die kommission gänzlich unannehmbar. En: ... agree wholeheartedly with the institution of an ad hoc delegation of parliament on the turkish [prison / missile] system. De: ... ad-hoc delegation des parlaments für das regime in den türkischen [gefängnissen / flugwaffen] voll und ganz zustimmen.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Examples of augmented data with high-
lighted [original / substituted] and [original / 
translated] words. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>An example from newstest2014 illustrating the effect of augmenting rare words on generation 
during test time. The translation of the baseline does not include the rare word centimetres, however, the 
translation of our TDA model generates the rare word and produces a more fluent sentence. Instances of 
the augmentation of the word centimetres in training data are also provided. 

1,000 
2,000 
3,000 

baseline 

TDA 

baseline 

TDA 

baseline 

TDA 

Words in V R X V ref generated during translation 
Words in V R X V ref not generated during translation 

14 
test 

15 
test 

16 
test 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Examples of incorrectly augmented data 
with highlighted [original / substituted] and [orig-
inal / translated] words. 

</table></figure>

			<note place="foot" n="1"> We use fast-align (Dyer et al., 2013) to extract word alignments and a bilingual lexicon with lexical translation probabilities from the low-resource bitext.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213 and 639.021.646, and a Google Faculty Research Award. We also thank NVIDIA for their hardware support, Ke Tran for providing the neural machine translation baseline system, and the anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Proceedings of the</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno type="doi">10.5244/C.28.6</idno>
		<ptr target="https://doi.org/http://dx.doi.org/10.5244/C.28.6" />
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<publisher>BMVA Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST8)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural network model for lowresource universal dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1040" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N13-1073" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning. PMLR</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning. PMLR<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>F. Pereira</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" />
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved statistical machine translation using monolingually-derived paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D/D09/D09-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="381" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1009" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transfer learning for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-1163" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1568" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
