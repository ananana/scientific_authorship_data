<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Tweets to Help Sentence Compression for News Highlights Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">The University of Texas at Dallas Richardson</orgName>
								<address>
									<postCode>75080</postCode>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">Hamad Bin Khalifa University</orgName>
								<address>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using Tweets to Help Sentence Compression for News Highlights Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="50" to="56"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We explore using relevant tweets of a given news article to help sentence compression for generating compressive news highlights. We extend an unsupervised dependency-tree based sentence compression approach by incorporating tweet information to weight the tree edge in terms of informativeness and syntactic importance. The experimental results on a public corpus that contains both news articles and relevant tweets show that our proposed tweets guided sentence compression method can improve the summariza-tion performance significantly compared to the baseline generic sentence compression method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>"Story highlights" of news articles are provided by only a few news websites such as CNN.com. The highlights typically consist of three or four succinct itemized sentences for readers to quickly capture the gist of the document, and can dramat- ically reduce reader's information load. A high- light sentence is usually much shorter than its orig- inal corresponding news sentence; therefore ap- plying extractive summarization methods directly to sentences in a news article is not enough to gen- erate high quality highlights.</p><p>Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effective- ness of sentence compression for automatic doc- ument summarization <ref type="bibr" target="#b7">(Knight and Marcu, 2000;</ref><ref type="bibr" target="#b11">Lin, 2003;</ref><ref type="bibr" target="#b5">Galanis and Androutsopoulos, 2010;</ref><ref type="bibr" target="#b0">Chali and Hasan, 2012;</ref><ref type="bibr" target="#b17">Wang et al., 2013;</ref><ref type="bibr" target="#b9">Li et al., 2013;</ref><ref type="bibr" target="#b13">Qian and Liu, 2013;</ref><ref type="bibr" target="#b10">Li et al., 2014</ref>). The compressed summaries can be generated through a pipeline approach that combines a generic sen- tence compression model with a summary sen- tence pre-selection or post-selection step. Prior studies have mostly used the generic sentence compression approaches, however, a generic com- pression system may not be the best fit for the summarization purpose because it does not take into account the summarization task in the com- pression module. <ref type="bibr" target="#b9">Li et al. (2013)</ref> thus proposed a summary guided compression method to address this problem and showed the effectiveness of their method. But this approach relied heavily on the training data, thus has the limitation of domain generalization.</p><p>Instead of using a manually generated corpus, we investigate using existing external sources to guide sentence compression for the purpose of compressive news highlights generation. Nowa- days it becomes more and more common that users share interesting news content via Twitter to- gether with their comments. The availability of cross-media information provides new opportuni- ties for traditional tasks of Natural Language Pro- cessing ( <ref type="bibr" target="#b20">Zhao et al., 2011;</ref><ref type="bibr" target="#b16">Subaši´Subaši´c and Berendt, 2011;</ref><ref type="bibr" target="#b6">Gao et al., 2012;</ref><ref type="bibr" target="#b8">Kothari et al., 2013;</ref><ref type="bibr">ˇ Stajner et al., 2013)</ref>. In this paper, we propose to use relevant tweets of a news article to guide the sentence compression process in a pipeline frame- work for generating compressive news highlights. This is a pioneer study for using such parallel data to guide sentence compression for document sum- marization.</p><p>Our work shares some similar ideas with <ref type="bibr" target="#b18">(Wei and Gao, 2014;</ref><ref type="bibr" target="#b19">Wei and Gao, 2015)</ref>. They also attempted to use tweets to help news highlights generation. <ref type="bibr" target="#b18">Wei and Gao (2014)</ref> derived external features based on the relevant tweet collection to assist the ranking of the original sentences for ex- tractive summarization in a fashion of supervised machine learning. <ref type="bibr" target="#b19">Wei and Gao (2015)</ref> proposed a graph-based approach to simultaneously rank the original news sentences and relevant tweets in an unsupervised way. Both of them focused on using tweets to help sentence extraction while we lever- age tweet information to guide sentence compres- sion for compressive summary generation.</p><p>We extend an unsupervised dependency-tree based sentence compression approach to incorpo- rate tweet information from the aspects of both in- formativeness and syntactic importance to weight the tree edge. We evaluate our method on a public corpus that contains both news articles and rele- vant tweets. The result shows that generic com- pression hurts the performance of highlights gen- eration, while sentence compression guided by relevant tweets of the news article can improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework</head><p>We adopt a pipeline approach for compressive news highlights generation. The framework in- tegrates a sentence extraction component and a post-sentence compression component. Each is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tweets Involved Sentence Extraction</head><p>We use LexRank ( <ref type="bibr" target="#b2">Erkan and Radev, 2004</ref>) as the baseline to select the salient sentences in a news article. This baseline is an unsupervised extractive summarization approach and has been proved to be effective for the summarization task.</p><p>Besides LexRank, we also use Heterogeneous Graph Random Walk (HGRW) ( <ref type="bibr" target="#b19">Wei and Gao, 2015)</ref> to incorporate relevant tweet information to extract news sentences. In this model, an undirected similarity graph is created, similar to LexRank. However, the graph is heterogeneous, with two types of nodes for the news sentences and tweets respectively.</p><p>Suppose we have a sentence set S and a tweet set T . By considering the similarity between the same type of nodes and cross types, the score of a news sentence s is computed as follows:</p><formula xml:id="formula_0">p(s) = d N + M + (1 − d)   m∈T sim(s, m) v∈T sim(s, v) p(m)   +(1 − d)   (1 − ) n∈S\{s} sim(s, n) v∈S\{s} sim(s, v) p(n)   (1)</formula><p>where N and M are the size of S and T , respec- tively, d is a damping factor, sim(x, y) is the simi- larity function, and the parameter is used to con- trol the contribution of relevant tweets. For a tweet node t, its score can be computed similarly. Both d and sim(x, y) are computed following the setup of LexRank, where sim(x, y) is computed as co- sine similarity:</p><formula xml:id="formula_1">sim(x, y) = w∈x,y tfw,xtfw,y(idfw) 2 w i ∈x (tfw i ,x idfw i ) 2 × w i ∈y (tfw i ,y idfw i ) 2 (2)</formula><p>where tf w,x is the number of occurrences of word w in instance x, idf w is the inverse document fre- quency of word w in the dataset. In our task, each sentence or tweet is treated as a document to com- pute the IDF value.</p><p>Although both types of nodes can be ranked in this framework, we only output the top news sen- tences as the highlights, and the input to the sub- sequent compression component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dependency Tree Based Sentence Compression</head><p>We use an unsupervised dependency tree based compression framework <ref type="bibr" target="#b4">(Filippova and Strube, 2008)</ref> as our baseline. This method achieved a higher F-score ( <ref type="bibr" target="#b14">Riezler et al., 2003</ref>) than other sys- tems on the Edinburgh corpus <ref type="bibr" target="#b1">(Clarke and Lapata, 2006</ref>). We will introduce the baseline in this part and describe our extended model that lever- ages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w 1 , w 2 , ..., w m , identify a subset of the words of s, such that it is grammatical and preserves es- sential information of s. In the baseline frame- work, a dependency graph for an original sentence is first generated and then the compression is done by deleting edges of the dependency graph. The goal is to find a subtree with the highest score:</p><formula xml:id="formula_2">f (X) = e∈E x e × w inf o (e) × w syn (e)<label>(3)</label></formula><p>where x e is a binary variable, indicating whether a directed dependency edge e is kept (x e is 1) or removed (x e is 0), and E is the set of edges in the dependency graph. The weighting of edge e con- siders both its syntactic importance (w syn (e)) as well as the informativeness (w inf o (e)). Suppose edge e is pointed from head h to node n with de- pendency label l, both weights can be computed from a background news corpus as:</p><formula xml:id="formula_3">w inf o (e) = P summary (n) P article (n)<label>(4)</label></formula><p>w syn (e) = P (l|h)</p><p>where P summary (n) and P article (n) are the uni- gram probabilities of word n in the two language models trained on human generated summaries and the original articles respectively. P (l|h) is the conditional probability of label l given head h. Note that here we use the formula in (Filip- pova and Altun, 2013) for w inf o (e), which was shown to be more effective for sentence compres- sion than the original formula in <ref type="bibr" target="#b4">(Filippova and Strube, 2008)</ref>. The optimization problem can be solved under the tree structure and length constraints by integer linear programming 1 . Given that L is the maxi- mum number of words permitted for the compres- sion, the length constraint is simply represented as:</p><formula xml:id="formula_5">e∈E x e ≤ L<label>(6)</label></formula><p>The surface realizatdion is standard: the words in the compression subtree are put in the same or- der they are found in the source sentence. Due to space limit, we refer readers to <ref type="bibr" target="#b4">(Filippova and Strube, 2008</ref>) for a detailed description of the baseline method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Leverage Tweets for Edge Weighting</head><p>We then extend the dependency-tree based com- pression framework by incorporating tweet infor- mation for dependency edge weighting. We in- troduce two new factors, w T inf o (e) and w T syn (e), for informativeness and syntactic importance re- spectively, computed from relevant tweets of the news. These are combined with the weights ob- tained from the background news corpus defined in Section 2.2, as shown below: The new informative weight w T inf o (e) is calcu- lated as:</p><formula xml:id="formula_6">w inf o (e) = (1 − α) · w N inf o (e) + α · w T inf o (e) (7) w syn (e) = (1 − β) · w N syn (e) + β · w T syn (e)<label>(</label></formula><formula xml:id="formula_7">w T inf o (e) = P relevantT (n) P backgroundT (n)<label>(9)</label></formula><p>P relevantT (n) and P backgroundT (n) are the uni- gram probabilities of word n in two language mod- els trained on the relevant tweet dataset and a background tweet dataset respectively. The new syntactic importance score is:</p><formula xml:id="formula_8">w T syn (e) = N T (h, n) N T<label>(10)</label></formula><p>N T (h, n) is the number of tweets where n and head h appear together within a window frame of K, and N T is the total number of tweets in the relevant tweet collection. Since tweets are always noisy and informal, traditional parsers are not reli- able to extract dependency trees. Therefore, we use co-occurrence as pseudo syntactic informa- tion here. Note w N inf o (e), w T inf o (e), w N syn (e) and w T syn (e) are normalized before combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We evaluate our pipeline news highlights gen- eration framework on a public corpus based on CNN/USAToday news <ref type="bibr" target="#b18">(Wei and Gao, 2014</ref>). This corpus was constructed via an event-oriented strat- egy following four steps: 1) 17 salient news events taking place in 2013 and 2014 were manually identified. 2) For each event, relevant tweets were retrieved via Topsy 2 search API using a set of manually generated core queries. 3) News arti- cles explicitly linked by URLs embedded in the tweets were collected. 4) News articles from CNN/USAToday that have more than 100 explic- itly linked tweets were kept. The resulting cor- pus contains 121 documents, 455 highlights and 78,419 linking tweets. We used tweets explicitly linked to a news ar- ticle to help extract salience sentences in HGRW and to generate the language model for computing w T inf o (e). The co-occurrence information com- puted from the set of explicitly linked tweets is very sparse because the size of the tweet set is small. Therefore, we used all the tweets re- trieved for the event related to the target news arti- cle to compute the co-occurrence information for w T syn (e). Tweets retrieved for events were not pub- lished in <ref type="bibr" target="#b18">(Wei and Gao, 2014</ref>). We make it avail- able here <ref type="bibr">3</ref> . The statistics of the dataset can be found in <ref type="table">Table.</ref> 1. <ref type="table" target="#tab_0">Aurora shooting  14  54  12,463  588,140  African runner murder  8  29  9,461  303,535  Boston bombing  38  147  21,683  1,650,650  Syria chemical weapons use  1  4  331  11,850  Connecticut shooting  13  47  3,021  213,864  US military in Syria  2  7  719  619,22  Edward Snowden  5  17  1,955  379,349  DPRK Nuclear Test  2  8  3,329  103,964  Egypt balloon crash  3  12  836  36,261  Asiana Airlines Flight 214  11  42  8,353  351,412  Hurricane Sandy  4  15  607  189,082  Moore Tornado  5  19  1,259  1,154,656  Russian meteor  3  11  6,841  239,281  Chinese Computer Attacks  2  8  507  28,988  US Flu Season  7  23  6,304  1,042,169  Williams Olefins Explosion  1  4  268  14,196  Super Bowl blackout  2  8  482  214,775  Total  121  455</ref> 78,419 6,890,987  Following <ref type="bibr" target="#b18">(Wei and Gao, 2014</ref>), we output 4 sentences for each news article as the highlights and report the ROUGE-1 scores <ref type="bibr" target="#b12">(Lin, 2004</ref>) using human-generated highlights as the reference.</p><note type="other">Event Doc # HLight # Linked Retrieved Event Doc # HLight # Linked Retrieved Tweet # Tweet # Tweet # Tweet #</note><p>The sentence compression rates are set to 0.8 for short sentences containing fewer than 9 words, and 0.5 for long sentences with more than 9 words, fol- lowing ( <ref type="bibr" target="#b4">Filippova and Strube, 2008)</ref>. We empiri- cally use 0.8 for α, β and such that tweets have more impact for both sentence selection and com- pression. We leveraged The New York Times An- notated Corpus (LDC Catalog No: LDC2008T19) as the background news corpus. It has both the original news articles and human generated sum- maries. The Stanford Parser 4 is used to obtain de- pendency trees. The background tweet corpus is collected from Twitter public timeline via Twitter API, and contains more than 50 million tweets. <ref type="table" target="#tab_1">Table 2</ref> shows the overall performance <ref type="bibr">5</ref> . For sum- maries generated by both LexRank and HGRW, "+SC" means generic sentence compression base- 4 http://nlp.stanford.edu/software/ lex-parser.shtml <ref type="bibr">5</ref> The performance of HGRW reported here is different from <ref type="bibr" target="#b19">(Wei and Gao, 2015)</ref> because the setup is different. We use all the explicitly linked tweets in the ranking process here without considering redundancy while a redundancy filtering process was applied in ( <ref type="bibr" target="#b19">Wei and Gao, 2015</ref>) . line (Section. 2.2) is used, "+w T inf o " and "+w T syn " indicate tweets are used to help edge weighting for sentence compression in terms of informative- ness and syntactic importance respectively, and "+both" means both factors are used. We have several findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>• The tweets involved sentence extraction model HGRW can improve LexRank by 8.8% rela- tively in terms of ROUGE-1 F score, showing the effectiveness of relevant tweets for sentence selection.</p><p>• With generic sentence compression, the ROUGE-1 F scores for both LexRank and HGRW drop, mainly because of a much lower recall score. This indicates that generic sen- tence compression without certain guidance removes salient content of the original sentence that may be important for summarization and thus hurts the performance. This is consistent with the finding of (Chali and Hasan, 2012).</p><p>• By adding either w T inf o or w T syn , the perfor- mance of summarization increases, showing that relevant tweets can be used to help the scores of both informativeness and syntactic im- portance.</p><p>• +SC+both improves the summarization perfor- mance significantly 6 compared to the corre- sponding compressive summarization baseline +SC, and outperforms the corresponding origi- nal baseline, LexRank and HGRW. • The improvement obtained by LexRank+SC+both compared to LexRank is more promising than that obtained by HGRW+SC+both compared to HGRW. This may be because HGRW has used tweet in- formation already, and leaves limited room for improvement for the sentence compres- sion model when using the same source of information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LexRank+SC+both</head><p>Boston bombing suspect Tamerlan Tsarnaev, Three people were hospitalized in critical condition, killed in a shootout after the blast, has been according to information provided by hospitals. buried at an location police said.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Boston bombing suspect Tamerlan Tsarnaev Hospitals report three people in critical condition has been buried at an undisclosed location • By incorporating tweet information for both sentence selection and compression, the per- formance of HGRW+SC+both outperforms LexRank significantly. <ref type="table" target="#tab_3">Table 3</ref> shows some examples. As we can see in Example 1, with the help of tweet informa- tion, our compression model keeps the valuable part "Boston bombing" for summarization while the generic one abandons it.</p><p>We also investigate the influence of α and β. To study the impact of α, we fix β to 0.8, and vice versa. As shown in <ref type="figure">Figure 1</ref>, it is clear that larger α or β, i.e., giving higher weights to tweets related information, is generally helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we showed that the relevant tweet collection of a news article can guide the process of sentence compression to generate better story highlights. We extended a dependency-tree based sentence compression model to incorporate tweet information. The experiment results on a public corpus that contains both news articles and rele- vant tweets showed the effectiveness of our ap- proach. With the popularity of Twitter and increas- ing interaction between social media and news media, such parallel data containing news and re- lated tweets is easily available, making our ap- proach feasible to be used in a real system.</p><p>There are some interesting future directions. For example, we can explore more effective ways to incorporate tweets for sentence compression; we can study joint models to combine both sen- tence extraction and compression with the help of relevant tweets; it will also be interesting to use the parallel dataset of the news articles and the tweets for timeline generation for a specific event.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>8) where α and β are used to balance the contribution of the two sources, and w N inf o (e) and w N syn (e) are based on Equation 4 and 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Distribution of documents, highlights and tweets with respect to different events 

Method 
ROUGE-1 
Compr. 
F(%) 
P(%) 
R(%) 
Rate(%) 
LexRank 
26.1 
19.9 
39.1 
100 
LexRank + SC 
25.2 
22.4 
29.6 
63.0 
LexRank + SC+w T 

inf o 

25.7 
22.8 
30.1 
62.0 
LexRank + SC+w T 

syn 

26.2 
23.5 
30.4 
63.7 
LexRank + SC+both 
27.5 
25.0 
31.4 
61.5 
HGRW 
28.1 
22.6 
39.5 
100 
HGRW + SC 
26.4 
24.9 
29.5 
66.1 
HGRW + SC+w T 

inf o 

27.5 
25.7 
30.8 
65.4 
HGRW + SC+w T 

syn 

27.0 
25.3 
30.2 
66.7 
HGRW + SC+both 
28.4 
26.9 
31.2 
64.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Overall Performance. Bold: the best value in each group in terms of different metrics.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Example highlight sentences from different systems</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> In our implementation we use GNU Linear Programming Kit (GULP) (https://www.gnu.org/ software/glpk/)</note>

			<note place="foot" n="2"> http://topsy.com 3 http://www.hlt.utdallas.edu/ ˜ zywei/ data/CNNUSATodayEvent.zip</note>

			<note place="foot" n="6"> Significance throughout the paper is computed by two tailed t-test and reported when p &lt; 0.05.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their de-tailed and insightful comments on earlier drafts of this paper. The work is partially supported by NSF award IIS-0845484 and DARPA Contract No. FA8750-13-2-0041. Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily re-flect the views of the funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the effectiveness of using sentence compression models for query-focused multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics</title>
		<meeting>the 25th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="457" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Models for sentence compression: A comparison across domains, training requirements and evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1481" to="1491" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dependency tree based sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Natural Language Generation Conference</title>
		<meeting>the Fifth International Natural Language Generation Conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An extractive supervised two-stage method for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="885" to="893" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint topic modeling for event summarization across news and social media streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statisticsbased summarization-step one: Sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 7th National Conference on Artificial Intelligence</title>
		<meeting>The 7th National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="703" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting comments on news articles in microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 7th International AAAI Conference on Weblogs and Social Media</title>
		<meeting>The 7th International AAAI Conference on Weblogs and Social Media</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
	<note>Ahmed Mourad Kareem Darwish, and Ahmed Taei</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="490" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving multi-documents summarization by sentence compression based on expanded constituent parse trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="691" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving summarization performance by sentence compression: a pilot study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth international workshop on Information retrieval with Asian languages</title>
		<meeting>the sixth international workshop on Information retrieval with Asian languages</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast joint compression and summarization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1492" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Crouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="118" to="125" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic selection of social media responses to news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Tadejštajnertadejˇtadejštajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana-Maria</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 19th ACM International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="50" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Peddling or creating? investigating the role of twitter in news reporting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Subaši´subaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bettina</forename><surname>Berendt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="207" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A sentence compression based framework to query-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1384" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Utilizing microblog for automatic news highlights extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics</title>
		<meeting>the 25th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="872" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gibberish, assistant, or master? using tweets linking to news for extractive single-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Comparing twitter and traditional media using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
