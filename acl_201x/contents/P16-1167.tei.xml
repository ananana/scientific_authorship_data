<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Prototypical Event Structure from Photo Albums</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 7-12, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfu</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science &amp; Engineering</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Prototypical Event Structure from Photo Albums</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
						<meeting>the 54th Annual Meeting of the Association for Computational Linguistics <address><addrLine>Berlin, Germany</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1769" to="1779"/>
							<date type="published">August 7-12, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Activities and events in our lives are structural , be it a vacation, a camping trip, or a wedding. While individual details vary, there are characteristic patterns that are specific to each of these scenarios. For example , a wedding typically consists of a sequence of events such as walking down the aisle, exchanging vows, and dancing. In this paper, we present a data-driven approach to learning event knowledge from a large collection of photo albums. We formulate the task as constrained optimization to induce the prototypical temporal structure of an event, integrating both visual and textual cues. Comprehensive evaluation demonstrates that it is possible to learn multimodal knowledge of event structure from noisy web content.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many common scenarios in our lives, such as a wedding or a camping trip, show characteristic structural patterns. As illustrated in <ref type="figure">Figure 1</ref>, these patterns can be sequential, such as in a wedding, where exchanging vows generally happens before cutting the cake. In other scenarios, there may be a set of composing events, but no prominent tempo- ral relations. A camping trip, for example, might include events such as hiking, which can happen either before or after setting up a tent.</p><p>This observation on the prototypical patterns in everyday scenarios goes back to early artificial in- telligence research. Scripts ( <ref type="bibr" target="#b27">Schank and Abelson, 1975)</ref>, an early formalism, were developed to en- code the necessary background knowledge to sup- port an inference engine for common sense rea- soning in limited domains. However, early ap- Figure 1: We collect photo albums of common scenar- ios (e.g., weddings) and cluster their images and captions to learn the hierarchical events that make up these scenarios. We use constrained optimization to decode the temporal order of these events, and we extract the prototypical descriptions that define them.</p><p>proaches based on hand-coded symbolic represen- tations proved to be brittle and difficult to scale. An alternative direction in recent years has been statistical knowledge induction, i.e., learn- ing script or common sense knowledge bottom-up from large-scale data. While most prior work is based on text ( <ref type="bibr" target="#b22">Pichotta and Mooney, 2014;</ref><ref type="bibr" target="#b14">Jans et al., 2012;</ref><ref type="bibr" target="#b4">Chambers and Jurafsky, 2008;</ref><ref type="bibr" target="#b6">Chambers, 2013</ref>), recent work begins exploring the use of images as well ( <ref type="bibr" target="#b0">Bagherinezhad et al., 2016;</ref><ref type="bibr" target="#b32">Vedantam et al., 2015)</ref>.</p><p>In this paper, we present the first study for learn- ing knowledge about common life scenarios (e.g., weddings, camping trips) from a large collection of online photo albums with time-stamped images and their captions. The resulting dataset includes 34,818 time-stamped photo albums corresponding to 12 distinct event scenarios with 1.5 million im- ages and captions (see <ref type="table" target="#tab_1">Table 1</ref> for more details).</p><p>We cast unsupervised learning of event struc- ture as a sequential multimodal clustering prob-lem, which requires solving two subproblems con- currently: identifying the boundaries of events and assigning identities to each of these events. We formulate this process as constrained optimiza- tion, where constraints encode the temporal event patterns that are induced directly from the data. The outcome is a statistically induced prototypi- cal structure of events characterized by their visual and textual representations.</p><p>We evaluate the quality and utility of the learned knowledge in three tasks: temporal event ordering, segmentation prediction, and multimodal summa- rization. Our experimental results show the per- formance of our model in predicting the order of photos in albums, partitioning photo albums into event sequences, and summarizing albums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>The high-level goal of this work is unsupervised induction of the prototypical event structure of common scenarios from multimodal data. We assume a two-level structure: high-level events, which we refer to as scenarios (e.g., wedding, fu- neral), are given, and low-level events (e.g., dance, kiss, vows), which we refer to as events, are to be automatically induced. In this section, we provide the overview of the paper (Section 2.1), and intro- duce our new dataset (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approach</head><p>Given a large collection of photo albums corre- sponding to a scenario, we want to learn three as- pects of event knowledge by (1) identifying events common to the given scenario (Section 4.1), (2) learning temporal relations across events (Sec- tion 4.2), and (3) extracting prototypical captions for each event (Section 4.3).</p><p>To induce the prototypical event structure, an important subproblem we consider is individual photo album analysis, where the task is (1) par- titioning each photo album into a sequence of seg- ments, and (2) assigning the event identity to each segment. We present an inference model based on Integer Linear Programming (ILP) in Section 3 to perform both segmentation and event identi- fication simultaneously, in consideration of the learned knowledge that we describe in Section 4.</p><p>Finally, we evaluate the utility of the automat- ically induced knowledge in the context of three concrete tasks: temporal ordering of photos (Sec- tion 6.1), album segmentation <ref type="table" target="#tab_1">(Section 6.2), and   scenario  # of albums # of images  WEDDING  4689  192K  MARATHON  3961  158K  COOKING  1168  36K  FUNERAL  781  28K  BARBECUE  735  22K  BABY BIRTH  688  21K  PARIS TRIP  4603  306K  NEW YORK TRIP  4205  267K  CAMPING  4063  159K  THANKSGIVING  5928  153K  CHRISTMAS  3449  98K  INDEPENDENCE DAY  548  22K  TOTAL  34,818</ref> 1.5M photo album summarization (Section 6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset</head><p>For this study, we have compiled a new corpus of multimodal photo albums across 12 distinct sce- narios. It comprises of 34,818 albums containing 1.5 million pairs of online photographs and their textual descriptions. <ref type="table" target="#tab_1">Table 1</ref> shows the list of sce- narios and the corresponding data statistics. We choose six scenarios (the top half of <ref type="table" target="#tab_1">Table 1</ref>) that we expect have an inherent temporal event struc- ture that can be learned and six that we expect do not (the bottom half of <ref type="table" target="#tab_1">Table 1</ref>). The dataset is collected using the Flickr API 1,2 . We use the scenario names and variations of them (e.g., Paris Trip, Paris Vacation) as queries for images. We then form albums from these im- ages by grouping images by user, sorting them by timestamp, and extracting groups that are within a contained time frame (e.g., 24 hours for a wedding, 5 days for a trip). For all im- ages, we extract the first sentences of the cor- responding textual descriptions as captions and also store their timestamps. This data is publicly available at https://www.cs.washington.edu/ projects/nlp/protoevents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inference Model for Multimodal Event Segmentation and Identification</head><p>Given a photo album, the goal of the inference is to assign events to photos and to segment albums by event. More formally, given a sequence of M pho- tos P = {p 1 , . . . , p M }, and N learned events E = {e 1 , . . . , e N }, the task is to assign each photo to a  <ref type="figure">Figure 2</ref>: The events learned in Section 4 are assigned to photos based on textual (A c ) and visual (A v ) affinities, which encode how well a photo represents an event (φevent). Segmentation scores (φseg) between adjacent photos encourage similar photos to be assigned the same event. Local transition, PL, and global pairwise ordering, PG, probabilities encode the learned temporal knowledge between events. φ temporal encourages event assignments toward a learned temporal structure of the scenario.</p><p>single event. The event assignment can be viewed as a latent variable for each photo. We formulate a constrained optimization (depicted in <ref type="figure">Figure 2</ref>) that maximizes the objective function, F , which consists of three scoring components: (a) event as- signment scores φ event (Section 3.1), (b) segmen- tation scores φ seg (Section 3.2), and (c) temporal knowledge scores φ temporal (Section 3.3):</p><formula xml:id="formula_0">F = φ event + φ seg + φ temporal (1)</formula><p>Decision Variables. The binary decision variable X i,k indicates that photo p i is assigned to event e k . The binary decision variable Z i,j,k,l indicates that photos p i and p j are assigned to events e k and e l , respectively:</p><formula xml:id="formula_1">Z i,j,k,l := X i,k ∧ X j,l<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Event Assignment Scores</head><p>Event assignment scores quantify the textual and visual affinity between a photo p i and an event e k . Affinities are measures of representation sim- ilarity between photos and events. These scores push photos displaying a certain event to be as- signed to that event. For now we assume the tex-</p><formula xml:id="formula_2">tual affinity matrix A c ∈ [0, 1] M ×N and the vi- sual affinity matrix A v ∈ [0, 1] M ×N are given.</formula><p>We describe how we obtain these affinity matri- ces in Section 4.1. Event assignment scores are defined as the weighted sum of both textual and visual affinity:</p><formula xml:id="formula_3">φ event = M i=1 N k=1 γ ce A c i,k + γ ve A v i,k X i,k (3)</formula><p>where X i,k is a photo-event assignment decision variable, and γ ce and γ ve are hyperparameters that balance the contribution of both affinities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Segmentation Scores</head><p>Segmentation scores quantify textual and visual similarities between adjacent photos. These scores encourage similar adjacent photos to be assigned to the same event. We define a similarity score be- tween adjacent photos equal to the weighted sum of their textual (b c ) and visual (b v ) similarities:</p><formula xml:id="formula_4">φ seg = M −1 i=1 N k=1 γ cs b c i + γ vs b v i Z i,i+1,k,k (4)</formula><p>where</p><formula xml:id="formula_5">b c , b v ∈ [0, 1] (M −1)×1</formula><p>are vectors of tex- tual and visual similarity scores between adjacent photos whose i th element corresponds to the sim- ilarity score between photos p i and p i+1 , Z is a decision variable defined by Equation 2, and γ cs and γ vs are hyperparameters balancing the contri- bution of both types of similarity. The similarity scores in the b vectors are computed using cosine similarity of the feature representations of adja- cent images in both the textual and visual modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Temporal Knowledge Scores</head><p>Temporal knowledge scores quantify the compati- bilities across different event assignments in terms of their relative ordering. For now, we assume two types of temporal knowledge matrices are given: L ∈ [0, 1] N ×N which stores local transition prob- abilities for every pair of events, e k and e l , and G ∈ [0, 1] N ×N which stores global pairwise or- dering probabilities for every pair of events, e k and e l . We describe how we obtain these temporal knowledge matrices in Section 4.2. The temporal knowledge score, defined below, encourages the inference model to assign events that are compati- ble with the learned temporal knowledge:</p><formula xml:id="formula_6">φ temporal = γ lp M i=0 N k,l=1 L k,l Z i,i+1,k,l (5) + γ gp M i=1 M j=i N k,l=1 G k,l Z i,j,k,l</formula><p>where Z is a decision variable defined by Equa- tion 2, and γ lp and γ gp are hyperparameters that balance the contribution of local and global tem- poral knowledge in the objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Constraints</head><p>We include hard constraints that force each photo to be assigned to exactly one event:</p><formula xml:id="formula_7">N k=1 X i,k = 1 (6)</formula><p>The number of these constraints is linear in the number of photos in an album. We also include hard constraints to ensure consistencies among bi- nary decision variables X and Z:</p><formula xml:id="formula_8">1 2 (X i,k + X j,l ) − Z i,j,k,l ≥ 0<label>(7)</label></formula><p>which states that Z i,j,k,l can be 1 only if both X i,k and X j,l are 1. The number of constraints for seg- mentation scores and local transition probabilities is O(M N 2 ) because they model interactions be- tween adjacent photos for all event pairs. The number of these constraints for global pairwise ordering probabilities is O(M 2 N 2 ) because they model interactions between all pairs of photos in an album for all event pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learned Event Knowledge</head><p>We learn base events for each scenario by cluster- ing photos from training albums related to that sce- nario ( <ref type="figure">Figure 3</ref>). As described in Section 3, these base events and their temporal knowledge are in- corporated in a joint model for event induction in unseen albums.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learned Event Representation</head><p>We perform k-means clustering over captions to create a base event model. We perform text-only clustering at first since visual cues are significantly  noisier. Because not all photos have informative captions, it is expected that this base clustering will form meaningful clusters only over a subset of the data. For each scenario, the largest clus- ter corresponds to the "miscellaneous" cluster as the captions in it tend to be relatively uninforma- tive about specific events. This cluster is excluded when computing temporal knowledge probabili- ties (Section 4.2).</p><formula xml:id="formula_9">⇒ ⇒ ⇒ ⇒ ⇒ ⇒ → → → → → → Event Visual Center, e V Event</formula><p>The visual and textual representations of an event are computed using the average of the vi- sual and textual features, respectively, of photos assigned to that event. We compute each textual affinity A c i,k in the event assignment scores (Equa- tion 3) as the cosine similarity between the textual features of the caption for photo p i and the textual representation of event e k . For textual features, we extract noun and verb unigrams using Turbo- Tagger ( <ref type="bibr" target="#b19">Martins et al., 2013)</ref> and weigh them by their discriminativeness relative to their scenario, P (S|w). Given scenario S and word w, P (S|w) is defined as the number of albums for the scenario the word occurs in divided by the total number of albums in that scenario. The visual affinity A v i,k is the similarity between the visual features of photo p i and the visual representation of event e k . For visual features, we use the convolutional features from the final layer activations of the 16-layer VG- GNet model (Simonyan and Zisserman, 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Temporal Knowledge</head><p>Local transition probabilities. These probabili- ties, denoted as P L , encode an expected sequence of events using temporal patterns among adjacent   </p><formula xml:id="formula_10">P L (e k → e l ) = C(e k → e l ) N m=1 C(e k → e m ) (8)</formula><p>where C is the observed counts of that specific event transition. This is the likelihood that an event e k is immediately followed by event e l .</p><p>Global pairwise ordering probabilities. These probabilities, denoted as P G , encode global struc- tural patterns about events. We model P G for each pair of events as a binomial distribution by com- puting the likelihood that an event occurs before another at any point in an album,</p><formula xml:id="formula_11">P G (e k ⇒ e l ) = C(e k ⇒ e l ) C(e k ⇒ e l ) + C(e l ⇒ e k )<label>(9)</label></formula><p>where C(e k ⇒ e l ) is the observed counts of e k occurring anytime before e l in all photo albums. These global probabilities model relations among events assigned to all photos in the album, not just events assigned to photos that are adjacent to one another. This distinction is important because these probabilities can encode global patterns be- tween events and are not limited to modeling a se- quential event chain. We use these learned temporal probabilities, P L and P G , in matrices L and G from φ temporal (Equation 5). These matrices are used to index lo- cal transition probabilities and global pairwise or- dering probabilities for pairs of events when com- puting temporal knowledge scores in the inference model (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Prototypical Captions</head><p>After clustering the photos, the representative lan- guage of the captions in each cluster begins to tell a story about each scenario. The event names are automatically extracted using the most com- mon content words among captions in the clus- ter. For each cluster, we also compile prototypical captions by extracting captions whose lemmatized forms are frequently observed throughout multiple albums in the scenario. Sample events and their prototypical captions from three scenarios are dis- played in <ref type="table" target="#tab_5">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Data split. For scenarios with more than 1000 al- bums, we use 100 albums for each of the develop- ment and test sets and use the rest for training. For scenarios with less than 1000 albums, we use 50 albums for each of the development and test sets, and the rest for training.</p><p>Implementation details. We optimize our ob- jective function using integer linear programming ( <ref type="bibr" target="#b25">Roth and Yih, 2004</ref>) with the Gurobi solver <ref type="bibr">(Inc., 2015)</ref>. For computational efficiency, temporally close sets of consecutive photos are treated as one unit during the optimization. We use these units to reduce the number of variables and constraints in the model from a function of the number of photos to a function of the number of units. We form these units heuristically by merging images agglomera- tively when their timestamps are within a certain range of the closest image in a unit. When merg- ing photos, the textual affinity of each unit for a particular event is the maximum affinity for that event among photos in that unit. The visual affin- ity of each unit is the average of all affinities for that event among photos in that unit. The textual and visual similarities of consecutive units are de- fined in terms of the similarities between the two photos at the units' boundary. Temporal informa- tion for events not aligned well with a particu- lar unit should not influence the objective, so we include temporal scores only for unit-event pairs which have both textual and visual event assign- ment scores greater than 0.05.</p><p>Hyperparameters. We tune the hyperparameters using grid search on the development set. In mod- els where the corresponding objective components are included, we set γ ce = 1, γ ve = 1, γ cs = .5, γ vs = .15, γ lp = 1, and γ gp = 4 Q (where Q is the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We evaluate the performance of our model on three tasks. The first task evaluates the effect of learned temporal knowledge in predicting the correct order of photos in an unseen album (Sec- tion 6.1). The second task evaluates the model's ability to segment albums into logical groupings (Section 6.2). The third task evaluates the qual- ity of prototypical captions and their use in photo album summarization (Section 6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Temporal Ordering of Photos</head><p>We evaluate the model's ability to capture the tem- poral relationships between events in the scenario. Given two randomly selected photos p i and p j from an album, the task is to predict which of the photos appears earlier in the album using their event assignments. We compare the full model that assigns events to photos using ILP (Section 3) with two baselines: k-MEANS , which assigns events to photos using k-means clustering over captions (Section 4), and NO TEMPORAL: a variant of the full model that does not use temporal knowledge scores (φ temporal in Equation 1) for optimization. We run each method over a test photo album, in which the events e k and e l are assigned to the photos p i and p j , respectively. We then use the learned global pairwise ordering probabilities (Section 4.2) to predict which photo appears ear- lier in the album. We report the accuracy of each method in predicting the order of photos compared to the actual order of photos in the albums. We perform this experiment 50 times for each album and average the number of correct choices across every album and every trial.</p><p>Results. <ref type="table" target="#tab_7">Table 3</ref> reports the results of the full model compared to the baselines. The results show that temporal knowledge generally helps in predicting photo ordering. We observe that the full model achieves higher scores for scenar- ios for which we expect would have a sequential structure (e.g., WEDDING, BABY BIRTH, MARATHON). Conversely, the full model achieves lower over- all scores in non-sequential scenarios (e.g., PARIS TRIP, NEW YORK TRIP). Qualitatively, we notice in- teresting temporal patterns such as the fact that during a marathon, the starting line occurs before the medal awards with 92.3% probability, or that Parisian tourists have a 24% chance (∼10× higher than random chance) of visiting the Eiffel Tower immediately after the Arc de Triomphe (a high local transition probability that correctly implies their real world proximity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Album Segmentation</head><p>Our model partitions photos in albums into coher- ent events. The album segmentation evaluation tests if the model recovers the same sequences of photos that a human would identify in a photo al- bum as events.</p><p>Evaluation. We had an impartial annotator label where they thought events began and ended in 10 candidate albums of greater than 100 photos for three scenarios: WEDDING, FUNERAL, CAMPING. We evaluate how well our model can replicate these boundaries with two metrics. The first metric is the F 1 score of recovering the same boundaries annotated by humans. The second metric is d, the difference between the number of events seg- mented by the model compared to the annotated albums. We report results for exact event bound- aries as well as relaxed boundaries where the start of an event can be r photos away from the start of an annotated event, where r is the relaxation coefficient. For reference, we note that albums in the wedding scenario were dual annotated and the agreement between annotators is 56.9% for r = 0 and 77.5% for r = 2.</p><p>Results. <ref type="table" target="#tab_9">Table 4</ref> shows comparison of the the full ILP model with same baselines we described be- fore, k-MEANS and NO TEMPORAL. The table shows that the full model generally outperforms the k- MEANS baseline for all three scenarios.</p><p>In the WEDDING scenario, the F 1 score for the full   <ref type="table">Table 5</ref>: Ablation study of objective function components for the wedding scenario. P, R, and F1 are the precision, recall and F-measure of recovering the same boundaries an- notated by humans. d is the average difference between the number of events identified by our models and the annotators. model is consistently higher. The k-MEANS base- line oversamples the number of events in albums, which is indicated by an average d significantly greater than 0. For the FUNERAL scenario, the NO TEMPORAL baseline outperforms the full model. We attribute this difference to the smaller data sub- set (see <ref type="table" target="#tab_1">Table 1</ref>) making it harder to learn the temporal relations in the scenario, which makes the contributions of the local and global temporal probabilities unexpected. In the CAMPING scenario, the F 1 score for the k-MEANS baseline is higher than that of the full model when r = 0. At a high- level, CAMPING is a scenario we expect has less of a known structure compared to other scenarios and may be harder to segment into its events.</p><formula xml:id="formula_12">Model r WEDDING FUNERAL CAMPING F1 d F1 d F1 d k-MEANS 0</formula><p>Ablation Study. <ref type="table">Table 5</ref> depicts the performance of ablations of the full model for the wedding sce- nario. Results show that removing any component of the objective functions yields lower recall and F 1 scores than the full model for r = 0. The ex- ception is removing local ordering probabilities, which yields a higher d. These observations sup- port the hypothesis that all of the components of the objective function contribute to segmenting the album into subsequences of photos depicting the same event. Particularly, we note the degradation when removing the global ordering probabilities, indicating that approaches which model only local event transitions such as hidden Markov models would not be suitable for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Photo Album Summarization</head><p>The final experiment evaluates how our learned prototypical captions can improve downstream tasks such as summarization and captioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Summaries</head><p>The goal of a good summary is to select the most salient pictures of an album. In our setting, a good summary should have a high coverage of the events in an album and choose the photos that most appropriately depict these events. Given a photo budget b, we choose a subset of photos that aims for these goals. To summarize a test album, we run our model over the entire album. This will yield h unique events assigned to the photos in the album. For each of these h events, we choose the photo with the highest event assignment score for that event (Equation 3) to be in the summary. If h &gt; b, we count the number of photos in the train- ing set assigned to each of the h events and choose the photos corresponding to the b events with the largest membership of photos in the training set. If h &lt; b, we complete the summary with b − h pho- tos from the "miscellaneous" event that are spaced evenly throughout the album. Finally, we replace the caption of each selected photo with a prototyp- ical caption (Section 4.3) for the assigned event.</p><p>Baseline. We evaluate against two baselines. The first baseline, KTH, involves including a photo in the summary every k = M/b photos. The sec- ond baseline, k-MEANS, uses the events assigned to photos from k-means clustering and then picks b photos in the same manner as our main model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation.</head><p>We evaluate the summaries pro- duced by each method with a human evaluation using Amazon Mechanical Turk (AMT). We use albums from the test set that contain more than 40 photos for the wedding scenario. For each album, at random, we present two summaries generated by two algorithms. AMT workers are instructed to choose the better summary considering both the images and the captions. For each comparison of two summaries for an album, we aggregate an- swers from three workers by majority voting. We set b = 7. The number of assigned events in an album, h, varies by album.</p><p>Results. As seen in <ref type="table" target="#tab_11">Table 6</ref>, the summary from the full model is preferred 57.7% of the time com- pared to the KTH baseline. The summaries gener- ated using the full model perform slightly better than the summaries from k-MEANS. We attribute <ref type="figure">Figure 4</ref>: Example summaries from the wedding, Paris trip, and baby birth scenarios. In cases where the album had less events than b, the additionally chosen photos are outlined in red. These photos do not have their caption replaced by a prototypical captions and merely fill out the summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Selection  the superior performance of the full model to the fact that it redistributes photos with noisy captions throughout the events, allowing for a larger sample to estimate visual representations of events, yield- ing more accurate visual affinity measurements to choose the summarization photos. As can be seen from qualitative examples in <ref type="figure">Figure 4</ref>, the photos chosen and the captions assigned cover key events that would occur during the scenario and describe them in a coherent way. Additional examples are available at https://www.cs.washington.edu/ projects/nlp/protoevents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Prototypical Captions</head><p>We also evaluate the quality of the prototypical captions assigned to every photo in the summaries.</p><p>For each album, we use the same sets of b pho- tos from the full model in the summarization task and evaluate the quality of the prototypical cap- tions paired with that group of photos.</p><p>Evaluation. We evaluate the quality of captions assigned to every photo by asking AMT work- ers to rate the captions on three different metrics: grammaticality, relevance to the scenario to which the image belongs, and relevance to its paired im-  age. Five AMT workers rate each group of b pho- tos on a five point Likert scale for each metric. We compare the prototypical captions for every photo in the summary with captions generated by an LSTM model 3 trained on every photo-caption pair in the training set for a scenario. We also com- pare with the original raw captions for each image in the summary. Because we chose photos with the highest event assignment scores (Equation 3) to be in the summary, the raw captions for this evaluation are cleaner and more descriptive than most captions in the dataset. Results. Our model outperforms the LSTM- generated captions in the image relevance and grammaticality scores, but did worse in scenario relevance. We attribute this result to LSTM captions having little caption variation because the model learns frequency statistics without any knowledge of latent events. Almost all LSTM cap- tions mention the words bride, wedding, or groom, yielding a very high scenario score for the caption, even if that caption is grammatically incorrect or irrelevant to the image. As expected the raw cap- tions have high relevance to the original image, and they are grammatical, but can be less relevant to the corresponding scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Previous studies have explored unsupervised in- duction of salient content structure in newswire texts ( <ref type="bibr" target="#b1">Barzilay and Lee, 2004</ref>), temporal graph representations ( <ref type="bibr" target="#b2">Bramsen et al., 2006</ref>), and story- line extraction and event summarization ( <ref type="bibr" target="#b34">Xu et al., 2013)</ref>. Another line of research finds the common event structure from children's stories <ref type="bibr" target="#b20">(McIntyre and Lapata, 2009)</ref>, where the learned plot struc- ture is used to stochastically generate new stories ( <ref type="bibr" target="#b9">Goyal et al., 2010;</ref><ref type="bibr" target="#b10">Goyal et al., 2013)</ref>. Our work similarly aims to learn the typical temporal pat- terns and compositional elements that define com- mon scenarios, but with multimodal integration. Compared to studies that learn narrative schemas from natural language <ref type="bibr" target="#b22">(Pichotta and Mooney, 2014;</ref><ref type="bibr" target="#b14">Jans et al., 2012;</ref><ref type="bibr" target="#b5">Chambers and Jurafsky, 2009;</ref><ref type="bibr" target="#b6">Chambers, 2013;</ref><ref type="bibr" target="#b3">Cassidy et al., 2014</ref>), or compile script knowledge from crowd- sourcing ( <ref type="bibr" target="#b23">Regneri et al., 2010)</ref>, our work explores a new source of knowledge that allows grounded event learning with temporal dimensions, result- ing in a new dataset of scenario types that are not naturally accessible from newswire or literature.</p><p>While recent studies have explored videos and photo streams as a source of discovering com- plex events and learning their sequential patterns <ref type="bibr" target="#b17">(Kim and Xing, 2014;</ref><ref type="bibr" target="#b16">Kim and Xing, 2013;</ref><ref type="bibr" target="#b30">Tang et al., 2012;</ref><ref type="bibr" target="#b31">Tschiatschek et al., 2014</ref>), their fo- cus was mostly on the visual modality. <ref type="bibr" target="#b36">Zhang et al. (2015)</ref> explored multimodal information ex- traction focusing specifically on identifying video clips that referred to the same event in television news. This contrasts to the goal of our study that aims to learn the temporal structure by which com- mon scenarios unfold.</p><p>Integrating language and vision has attracted in- creasing attention in recent years across diverse tasks such as image captioning <ref type="bibr">(Karpathy and FeiFei, 2015;</ref><ref type="bibr" target="#b33">Vinyals et al., 2015;</ref><ref type="bibr" target="#b8">Fang et al., 2015;</ref><ref type="bibr" target="#b35">Xu et al., 2015;</ref><ref type="bibr" target="#b7">Chen et al., 2015)</ref>, cross modal semantic modeling ( <ref type="bibr" target="#b18">Lazaridou et al., 2015)</ref>, infor- mation extraction <ref type="bibr" target="#b21">(Morency et al., 2011;</ref><ref type="bibr" target="#b24">Rosas et al., 2013;</ref><ref type="bibr" target="#b36">Zhang et al., 2015;</ref><ref type="bibr" target="#b13">Izadinia et al., 2015)</ref>, common-sense knowledge <ref type="bibr" target="#b32">(Vedantam et al., 2015;</ref><ref type="bibr" target="#b0">Bagherinezhad et al., 2016)</ref>, and visual storytelling ( <ref type="bibr" target="#b11">Huang et al., 2016)</ref>. Our work is sim- ilar to both common sense knowledge learning and visual story completion. Our model learns com- monsense knowledge on the hierarchical and tem- poral event structure from scenario-specific multi- modal photo albums, which can be viewed as vi- sual stories about common life events.</p><p>Recent work focused on photo album summa- rization using visual ( ) and multimodal representations <ref type="bibr" target="#b29">(Sinha et al., 2011</ref>). Our work identifies the nature of common events in scenarios and learns their timelines and charac- teristic forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduce a novel exploration to learn script- like knowledge from photo albums. We model stochastic event structure to learn both the event representations (textual and visual) and the tempo- ral relations among those events. Our event induc- tion method incorporates learned knowledge about events, partitions photo albums into segments, and assigns events to those segments. We show the significance of our model in learning and using learned knowledge for photo ordering, album seg- mentation, and summarization. Finally, we pro- vide a dataset depicting 12 scenarios with ∼1.5 M images for future research. Future directions could include exploring nuances in the type of tempo- ral knowledge that can be learned across different scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Textual Center, e C Figure 3 :</head><label>C3</label><figDesc>Figure 3: Photos are clustered by their captions. We can compute the visual, e v k , and caption, e c k , centers for all the clusters, as well as the local transition, PL, and global pairwise ordering, PG, probabilities between these events based on the sequential patterns they exhibit in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Dataset Statistics: the number of albums and im-

ages compiled for each scenario. The middle horizontal line 
separates the scenarios we predict have a well-defined tem-
poral structure (top) from those we predict do not (bottom). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Sample learned events and prototypical captions 

photos. We model P L for each pair of events as a 
multinomial distribution, 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Model Wedding Baby Birth Marathon Cooking Funeral Barbecue Indep. Day Camping Thanksgiving Paris Trip NY</head><label></label><figDesc></figDesc><table>Trip Christmas 
k-MEANS 
52.7 
52.5 
53.8 
53.0 
50.5 
50.2 
53.2 
52.3 
51.4 
53.1 
50.3 
51.4 
NO TEMPORAL 
58.6 
66.3 
62.6 
56.5 
50.8 
51.7 
58.0 
52.6 
54.3 
51.7 
49.1 
50.9 
FULL MODEL 
60.0 
66.5 
64.5 
63.2 
53.1 
58.6 
56.0 
55.5 
56.1 
52.3 
48.5 
52.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 : Temporal ordering pairwise photo results. The metric reported is accuracy, the percentage of time the correct photo is picked as coming first based on the event assigned to it. Scenarios with an expected temporal structure are in the left half of the table.</head><label>3</label><figDesc></figDesc><table>number of event units). For k-means clustering, 
we use 10 random restarts and 40 cluster centers 
for the WEDDING, CAMPING, PARIS TRIP, and NY TRIP 
scenarios. For all other scenarios, we use 30 clus-
ter centers. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 4 : Segmentation results for our full model. F1 scores</head><label>4</label><figDesc></figDesc><table>how often our model recovers the same boundaries annotated 
by humans. d is the average difference between the number 
of events identified by the model in an album and marked by 
annotators. r is the relaxation coefficient. 

Feature Group Excluded 
P 
R 
F1 
d 
FULL MODEL 
36.7 42.8 37.8 
1.3 
-Visual Event Affinity 
37.7 37.0 35.3 -1.7 
-Textual Segmentation 
37.1 41.5 37.4 
.8 
-Visual Segmentation 
35.1 42.1 36.5 
1.7 
-Local Ordering Probs. 
36.9 40.3 36.9 
.2 
-Global Ordering Probs. 40.5 25.0 29.5 -5.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Summarization results. The selection rates indi-

cate the percentage of time the corresponding method in the 
left-most column was picked. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Captioning results. We evaluate the caption qual-

ity of the prototypical captions of the full model, those gen-
erated by an LSTM trained on the raw captions, and original 
captions. Captions were evaluated on 3 metrics: grammatical 
correctness, how relevant they were to the scenario, and how 
relevant they were to their assigned image. 

</table></figure>

			<note place="foot" n="1"> https://www.flickr.com/services/api/ 2 https://pypi.python.org/pypi/flickrapi/1.4.5</note>

			<note place="foot" n="3"> We use a single-layer encoder-decoder LSTM. The cell state and the input embedding dimensions are 256. Visual inputs are the final layer convolutional features of the VGG-16 model and are fine-tuned during training. We use RMSprop to train the network with a base learning rate of .0001 and 30% dropout. We train the model for 45 epochs on a single NVIDIA Titan X GPU with mini batch size 100. To decode, we use beam search with beam size 5.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for many in-sightful comments and members of UW NLP for feedback and support. The work is in part sup-ported by NSF grants IIS-1408287, IIS-1524371, IIS-1616112, DARPA under the CwC program through the ARO (W911NF-15-1-0543), the Allen Institute for AI (66-9175), the Allen Distinguished Investigator Award, and gifts by <ref type="bibr">Google and Facebook.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are elephants bigger than butterflies? reasoning about sizes of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hessam</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference in Artificial Intelligence (AAAI)</title>
		<meeting>the Conference in Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Catching the drift: Probabilistic content models, with applications to generation and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Yoong Keok Lee, and Regina Barzilay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bramsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Deshpande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
	<note>Inducing temporal graphs</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An annotation framework for dense event ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative schemas and their participants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Event schema induction with a probabilistic entity-driven model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Déja image-captions: A corpus of expressive descriptions in repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>David S Warren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACLHLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Hao Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">K</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatically producing plot unit representations for narrative text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A computational model for plot units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="466" to="488" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xiaodong He, Pushmeet Kohli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Hao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<editor>Dhruv Batra, C. Lawrence Zitnick, Devi Parikh, Lucy Vanderwende, Michel Galley, and Margaret Mitchell</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Visual storytelling</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gurobi Optimization Inc. 2015. Gurobi optimizer reference manual</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Segment-phrase table for semantic segmentation, visual entailment and paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fereshteh</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Santosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Skip n-grams and ranking functions for predicting script events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Jans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Jointly aligning and segmenting multiple web photo streams for the inference of collective photo storylines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="620" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reconstructing storyline graphs for image recommendation from web community photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3882" to="3889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nghia The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05" />
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order non-projective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to tell tales: A data-driven approach to story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: Harvesting opinions from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on multimodal interfaces</title>
		<meeting>the 13th international conference on multimodal interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statistical script learning with multi-argument events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning script knowledge with web experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis of spanish online videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Verónica Pérez Rosas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to select and order vacation photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fereshteh</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Tena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Scripts, plans, and knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert P Abelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
		<respStmt>
			<orgName>Yale University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Summarization of personal photologs using multidimensional content and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinaki</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharad</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM International Conference on Multimedia Retrieval</title>
		<meeting>the 1st ACM International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1250" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning mixtures of submodular functions for image collection summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Tschiatschek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rishabh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning common sense through visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference in Computer Vision (ICCV)</title>
		<meeting>the International Conference in Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Summarizing complex events: a cross-modal solution of storylines extraction and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shize</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1281" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cross-document event coreference resolution based on cross-media features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
