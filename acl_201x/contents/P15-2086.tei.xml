<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Hidden Markov Models with Distributed State Representations for Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information</orgName>
								<orgName type="institution">Sciences Temple University</orgName>
								<address>
									<postCode>19122</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information</orgName>
								<orgName type="institution">Sciences Temple University</orgName>
								<address>
									<postCode>19122</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Hidden Markov Models with Distributed State Representations for Domain Adaptation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="524" to="529"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, a variety of representation learning approaches have been developed in the literature to induce latent generalizable features across two domains. In this paper, we extend the standard hidden Markov models (HMMs) to learn distributed state representations to improve cross-domain prediction performance. We reformu-late the HMMs by mapping each discrete hidden state to a distributed representation vector and employ an expectation-maximization algorithm to jointly learn distributed state representations and model parameters. We empirically investigate the proposed model on cross-domain part-of-speech tagging and noun-phrase chunking tasks. The experimental results demonstrate the effectiveness of the distributed HMMs on facilitating domain adaptation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Domain adaptation aims to obtain an effective pre- diction model for a particular target domain where labeled training data is scarce by exploiting la- beled data from a related source domain. Domain adaptation is very important in the field of natu- ral language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain. Various NLP tasks have benefited from domain adaptation techniques, including part-of- speech tagging <ref type="bibr" target="#b0">(Blitzer et al., 2006;</ref><ref type="bibr" target="#b11">Huang and Yates, 2010a</ref>), chunking <ref type="bibr" target="#b5">(Daum√© III, 2007;</ref><ref type="bibr" target="#b10">Huang and Yates, 2009)</ref>, named entity recognition ( <ref type="bibr" target="#b8">Guo et al., 2009;</ref><ref type="bibr" target="#b28">Turian et al., 2010)</ref>, dependency pars- ing ( <ref type="bibr" target="#b7">Dredze et al., 2007;</ref><ref type="bibr" target="#b24">Sagae and Tsujii, 2007)</ref> and semantic role labeling <ref type="bibr" target="#b4">(Dahlmeier and Ng, 2010;</ref><ref type="bibr" target="#b12">Huang and Yates, 2010b)</ref>.</p><p>In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedi- cal ( <ref type="bibr" target="#b0">Blitzer et al., 2006)</ref>). Under such circum- stances, the original lexical features may not per- form well in cross-domain learning since differ- ent genres of text may use very different vocab- ularies and produce cross-domain feature distri- bution divergence and feature sparsity issue. A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, includ- ing clustering based word representation learn- ing methods <ref type="bibr" target="#b10">(Huang and Yates, 2009;</ref><ref type="bibr" target="#b2">Candito et al., 2011</ref>), word embedding based representation learning methods ( <ref type="bibr" target="#b28">Turian et al., 2010;</ref><ref type="bibr" target="#b9">Hovy et al., 2015)</ref> and some other representation learning methods <ref type="bibr" target="#b0">(Blitzer et al., 2006</ref>).</p><p>In this paper, we extend the standard hidden Markov models (HMMs) to perform distributed state representation learning and induce context- aware distributed word representations for domain adaptation. Instead of learning a single discrete latent state for each observation in a given sen- tence, we learn a distributed representation vec- tor. We define a state embedding matrix to map each latent state value to a low-dimensional dis- tributed vector and reformulate the three local dis- tributions of HMMs based on the distributed state representations. We then simultaneously learn the state embedding matrix and the model parame- ters using an expectation-maximization (EM) al- gorithm. The hidden states of each word in a sen- tence can be decoded using the standard Viterbi decoding procedure of HMMs, and its distributed representation can be obtained by a simple map- ping with the state embedding matrix. We then use the context-aware distributed representations of the words as their augmenting features to per- form cross-domain part-of-speech (POS) tagging and noun-phrase (NP) chunking.</p><p>The proposed approach is closely related to the clustering based method <ref type="bibr" target="#b10">(Huang and Yates, 2009</ref>) as we both use latent state representations as generalizable features. However, they use stan- dard HMMs to produce discrete hidden state fea- tures for each observation word, while we induce distributed state representation vectors. Our dis- tributed HMMs share similarities with the word embedding based method ( <ref type="bibr" target="#b9">Hovy et al., 2015)</ref>, and can be more space-efficient than the stan- dard HMMs. Moreover, our model can incor- porate context information into observation fea- ture vectors to perform representation learning in a context-aware manner. The distributed state representations induced by our model hence have larger representing capacities and generalizing ca- pabilities for cross-domain learning than standard HMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A variety of representation learning approaches have been developed in the literature to address NLP domain adaptation problems. The cluster- ing based word representation learning methods perform word clustering within the sentence struc- ture and use word cluster indicators as generaliz- able features to address domain adaptation prob- lems. For example, <ref type="bibr" target="#b10">Huang and Yates (2009)</ref> used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tag- ging and NP chunking. Brown clusters <ref type="bibr" target="#b1">(Brown et al., 1992)</ref>, which was used as latent features for simple in-domain dependency parsing ( <ref type="bibr" target="#b15">Koo et al., 2008)</ref>, has recently been exploited for out-of- domain statistical parsing <ref type="bibr" target="#b2">(Candito et al., 2011</ref>).</p><p>The word embedding based representation learning methods learn a dense real-valued repre- sentation vector for each word as latent features for domain adaptation. <ref type="bibr" target="#b28">Turian et al. (2010)</ref> em- pirically studied using word embeddings learned from hierarchical log-bilinear models <ref type="bibr" target="#b18">(Mnih and Geoffrey, 2008)</ref> and neural language models <ref type="bibr" target="#b3">(Collobert and Weston, 2008</ref>) for cross-domain NER tasks. <ref type="bibr" target="#b9">Hovy et al. (2015)</ref> used the word embed- dings learned from the Skip-gram Model (SGM) ( <ref type="bibr" target="#b17">Mikolov et al., 2013</ref>) to develop a POS tagger for Twitter data with labeled newswire training data.</p><p>Some other representation learning methods have been developed to tackle NLP cross-domain problems as well.</p><p>For example, <ref type="bibr" target="#b0">Blitzer et al. (2006)</ref> proposed a structural correspondence learning method for POS tagging, which first se- lects a set of pivot features (occurring frequently in In terms of performing distributed representa- tion learning for output variables, our proposed model shares similarity with the structured out- put representation learning approach developed by <ref type="bibr" target="#b25">Srikumar and Manning (2014)</ref>, which extends the structured support vector machines to simul- taneously learn the prediction model and the dis- tributed representations of the output labels. How- ever, the approach in ( <ref type="bibr" target="#b25">Srikumar and Manning, 2014</ref>) assumes the training labels (i.e., output val- ues) are given and performs learning in the stan- dard supervised in-domain setting, while our pro- posed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning. There are also a few works that extended standard HMMs in the liter- ature, including the observable operator models <ref type="bibr" target="#b14">(Jaeger, 1999)</ref>, and the spectral learning method ( <ref type="bibr" target="#b26">Stratos et al., 2013)</ref>. But none of them performs representation learning to address cross-domain adaptation problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>In this paper, we propose a novel distributed hid- den Markov model (dHMM) for representation learning over sequence data. This model ex- tends the hidden Markov models <ref type="bibr" target="#b21">(Rabiner and Juang, 1986)</ref> to learn distributed state representa- tions. Similar as HMMs, a dHMM (shown in <ref type="figure" target="#fig_0">Fig- ure 1)</ref> is a two-layer generative graphical model, which generates a sequence of observations from a sequence of latent state variables using Markov properties. Let O = {o 1 , o 2 , . . . , o T } be the se- quence of observations with length T , where each observation o t ‚àà R d is a d-dimensional feature vector. Let S = {s 1 , s 2 , . . . , s T } be the sequence of T hidden states, where each hidden state s t has a discrete state value from a total H hidden states H = {1, 2, . . . , H}. Besides, we assume that there is a low-dimensional distributed representa- tion vector associated with each hidden state. Let M ‚àà R H√óm be the state embedding matrix where the i-th row M i: denotes the m-dimensional repre- sentation vector for the i-th state. Previous works have demonstrated the usefulness of discrete hid- den states induced from a HMM on addressing feature sparsity in domain adaptation <ref type="bibr" target="#b10">(Huang and Yates, 2009)</ref>. However, expressing a semantic word by a single discrete state value is too re- strictive, as it has been shown in the literature that words have many different features in a multi- dimensional space where they could be separately characterized as number, POS tag, gender, tense, voice and other aspects <ref type="bibr" target="#b23">(Sag and Wasow, 1999;</ref><ref type="bibr" target="#b13">Huang et al., 2011</ref>). Our proposed model aims to overcome this inherent drawback of standard HMMs on learning word representations. Given a set of observation sequences in two domains, the dHMM induces a distributed representation vector with continuous real values for each observation word as generalizable features, which has the ca- pacity of capturing multi-aspect latent characteris- tics of the word clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Formulation</head><p>To build the dHMMs, we reformulate the standard HMMs by defining three main local distributions based on the distributed state representations, i.e., the initial state distribution, the state transition dis- tribution, and the observation emission distribu- tion. Below we introduce them by using Œò to de- note the set of parameters involved and using 1 to denote a column vector with all 1s.</p><p>First we use the following multinomial distribu- tion as the initial state distribution,</p><formula xml:id="formula_0">P (s 1 ; Œò) = œÜ(s 1 ) Œª,</formula><p>where œÜ(s t ) ‚àà {0, 1} H is a one-hot vector with a single 1 value at its s t -th entry, and Œª ‚àà R H is the parameter vector such that Œª ‚â• 0 and Œª 1 = 1.</p><p>We then define a multinomial logistic regression model for the state transition distribution,</p><formula xml:id="formula_1">P (s t+1 |s t ; Œò) = exp œÜ(s t+1 ) W M œÜ(s t ) Z(s t ; Œò)</formula><p>where W ‚àà R H√óm is the regression parameter matrix and Z(s t ; Œò) is the normalization term. Finally, we assume the observation vector is generated from a multivariate Gaussian distribu- tion, i.e., o t ‚àº N œÜ(s t ) M Q, œÉI d , and use the following model for the emission distribution,</p><formula xml:id="formula_2">P (o t |s t ; Œò) = exp ‚àí1 2œÉ Œ∫(s t , o t )Œ∫(s t , o t ) (2œÄ) d/2 œÉ d/2 , with Œ∫(s t , o t ) = œÜ(s t ) M Q ‚àí o t ,</formula><p>where Q ‚àà R m√ód and œÉ ‚àà R are the model parameters. Dif- ferent from the standard HMMs which have dis- crete hidden states and discrete observations, the multivariate Gaussian model here generates each observation o t as a d-dimensional continuous fea- ture vector. This type of emission distribution pro- vides us the flexibility to incorporate local context information or statistical global information for in- ducing distributed state representations. For ex- ample, we can use the concatenation of the one-hot word vectors within a sliding window around the target word as the observation vector. Moreover, we can also use the globally preprocessed continu- ous word vectors as the observation vectors, which we will describe later in our experiments.</p><p>The standard HMMs (Rabiner and Juang, 1986) use conditional probability tables for the state tran- sition distribution, which grows quadratically with respect to the number of hidden states, and the emission distribution, which grows linearly with respect to the observed vocabulary size that is usually very large in NLP tasks. Instead, the dHMMs can significantly reduce the sizes of these conditional probability tables by introducing the low-dimensional state embedding vectors, and the dHMM is much more efficient in terms of mem- ory storage. In fact, the complexity of dHMMs can be independent of the vocabulary size by us- ing flexible observation features. We represent the dHMM parameter set as Œò = {M ‚àà R H√óm , W ‚àà R H√óm , Q ‚àà R m√ód , œÉ ‚àà R, Œª ‚àà [0, 1] H }, where m is a small constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Training</head><p>Given a data set of N observed sequences {O 1 , . . . , O n , . . . , O N }, its regularized log- </p><formula xml:id="formula_3">L(Œò) = n log P (O n ; Œò)‚àí Œ∑ 2 R(W, Q, M ) (1)</formula><p>where the regularization function is defined with Frobenius norms such as</p><formula xml:id="formula_4">R(W, Q, M ) = W 2 F + Q 2 F + M 2 F .</formula><p>Moreover, each log- likelihood term has the following lower bound</p><formula xml:id="formula_5">logP (O n ; Œò) = log S n P (O n , S n ; Œò) ‚â• logP (O n ; Œò)‚àíKL(Q(S n )||P (S n |O n ; Œò)) (2)</formula><p>where Q(S n ) is any valid distribution over the hid- den state variables S n and KL(.||.) denotes the Kullback-Leibler divergence. Let F(Q, Œò) denote the regularized lower bound function obtained by plugging the lower bound (2) back into the ob- jective function (1). We then perform training by using an expectation-maximization (EM) algo- rithm <ref type="bibr" target="#b6">(Dempster et al., 1977</ref>) that iteratively max- imizes F(Q, Œò) to reach a local optimal solution. We first randomly initialize the model parame- ters while enforcing Œª to be in the feasible region (Œª ‚â• 0, Œª 1 = 1). In the (k+1)-th iteration, given {Q (k) , Œò (k) }, we then sequentially update Q with an E-step (3) and update Œò with a M-step (4).</p><formula xml:id="formula_6">Q (k+1) = arg max Q F(Q, Œò (k) )<label>(3)</label></formula><formula xml:id="formula_7">Œò (k+1) = arg max Œò F(Q (k+1) , Œò)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Domain Adaptation with Distributed State Representations</head><p>We use all training data from the two domains to train dHMMs for local optimal model pa-</p><formula xml:id="formula_8">rameters Œò * = {M * , W * , Q * , œÉ * , Œª * }.</formula><p>We then infer the latent state sequence S * = {s * 1 , s * 2 , . . . , s * T } using the standard Viterbi algo- rithm <ref type="bibr" target="#b21">(Rabiner and Juang, 1986)</ref> for each la- beled source training sentence and each target test sentence. The corresponding distributed state representation vectors can be obtained as</p><formula xml:id="formula_9">{M * * œÜ(s * 1 ), M * * œÜ(s * 2 ), . . . , M * * œÜ(s * T )}.</formula><p>We then train a supervised NLP system (e.g., POS tag- ging or NP chunking) on the labeled source train- ing sentences using the distributed state represen- tations as augmenting input features and perform prediction on the augmented test sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted experiments on cross-domain part- of-speech (POS) tagging and noun-phrase (NP) chunking tasks. We used the same experimen- tal datasets as in <ref type="bibr" target="#b10">(Huang and Yates, 2009</ref>) for cross-domain POS tagging from Wall Street Jour- nal (WSJ) domain <ref type="bibr" target="#b16">(Marcus et al., 1993)</ref> to MED- LINE domain <ref type="bibr" target="#b20">(PennBioIE, 2005</ref>) and for cross- domain NP chunking from CoNLL shared task dataset ( <ref type="bibr" target="#b27">Tjong et al., 2000</ref>) to Open American Na- tional Corpus (OANC) ( <ref type="bibr" target="#b22">Reppen et al., 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation Learning</head><p>We first built a unified vocabulary with all the data in the two domains. We then conducted latent semantic analysis (LSA) over the sentence-word frequency matrix to get a low-dimensional repre- sentation vector for each word. We used a sliding window with size 3 to construct the d-dimensional feature vector (d = 1500) for each observation in a given sentence. We used Œ∑ = 0.5, set the number of hidden states H to be 80 and the dimensionality m = 20. We used all the labeled and unlabeled training data in the two domains to train dHMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Test Results</head><p>We used the induced distributed state representa- tions of each observation as augmenting features to train conditional random fields (CRF) with the CRFSuite package <ref type="bibr" target="#b19">(Okazaki, 2007)</ref> on the labeled source sentences and perform prediction on the target test sentences. We compared with the fol- lowing systems: a Baseline system without repre- sentation learning, a SGM based word embedding system ( <ref type="bibr" target="#b9">Hovy et al., 2015)</ref>, and a discrete hidden state based clustering system <ref type="bibr" target="#b10">(Huang and Yates, 2009)</ref>. We used the word id and orthographic fea- tures as the baseline features for POS tagging and added POS tags for NP chunking. We reported the POS tagging accuracy for all words and out- of-vocabulary (OOV) words (which appear less than three times in the labeled source training sen- tences), and NP chunking F1 scores for all NPs and only OOV NPs (whose beginning word is an OOV word) in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>We can see that the Baseline method per- forms poorly on both tasks especially on the OOV words/NPs, which shows that the original lexical based features are not sufficient to develop a ro- bust POS tagger/NP chunker for the target domain with labeled source training sentences. By us- ing unlabeled training sentences from the two do- mains, all representation learning approaches in- crease the cross-domain test performance, espe- cially on the OOV words/NPs. These improve- ments over the Baseline method demonstrate that the induced latent features do alleviate feature sparsity issue across the two domains and help the trained NLP system generalize well in the target domain. Between these representation learning approaches, the proposed distributed state repre- sentation learning method outperforms both of the word embedding based and discrete HMM hidden state based systems. This suggests that by learn- ing distributed representations in a context-aware manner, dHMMs can effectively bridge domain divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sensitivity Analysis over the Dimensionality of State Embeddings</head><p>We also conducted experiments to investigate how does the dimensionality of the distributed state representations, m, in our proposed approach af- fect cross-domain test performance given a fixed state number H = 80. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we extended the standard HMMs to learn distributed state representations and fa- cilitate cross-domain sequence predictions. We mapped each state variable to a distributed rep- resentation vector and simultaneously learned the state embedding matrix and the model parameters with an EM algorithm. The experimental results on cross-domain POS tagging and NP chunking tasks demonstrated the effectiveness of the pro- posed approach for domain adaptation. In the future, we plan to apply this approach to other cross-domain prediction tasks such as named en- tity recognition or semantic role labeling. We also plan to extend our method to learn cross-lingual representations with auxiliary resources such as bilingual dictionaries or parallel sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hidden Markov models with distributed state representations (dHMM).</figDesc><graphic url="image-1.png" coords="2,308.41,62.81,216.00,158.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Cross-domain test performance with respect to different dimensionality values (m) of the hidden state representation vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Test performance for cross-domain POS tagging and NP chunking.</head><label>1</label><figDesc></figDesc><table>Systems 
POS Tagging (Accuracy (%)) NP Chunking (F1-score) 
All Words 
OOV Words 
All NPs 
OOV NPs 

Baseline 
88.3 
67.3 
0.86 
0.74 
SGM (Hovy et al., 2015) 
89.0 
71.4 
0.88 
0.78 
HMM (Huang and Yates, 2009) 
90.5 
75.2 
0.91 
0.85 
dHMM 
91.1 
76.0 
0.93 
0.88 

likelihood can be written as follows 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by NSF grant IIS-1065397.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Compututal Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A word clustering approach to domain adaptation: Effective parsing of biomedical texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Anguiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Inter. Conference on Parsing Technologies (IWPT)</title>
		<meeting>of the Inter. Conference on Parsing Technologies (IWPT)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Inter. Conference on Machine Learning (ICML)</title>
		<meeting>of the Inter. Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation for semantic role labeling in the biomedical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1098" to="1104" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daum√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association of Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Frustratingly hard domain adaptation for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gra√ßa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL Shared Task Session of EMNLP-CoNLL</title>
		<meeting>of CoNLL Shared Task Session of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation with latent semantic association for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Human Language Technologies: The Annual Conf. of North American Chapter of ACL (HLT-NAACL)</title>
		<meeting>of Human Language Technologies: The Annual Conf. of North American Chapter of ACL (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining for unambiguous instances to adapt pos taggers to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>S√∏gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference of the North American Chapter of ACL (NAACL)</title>
		<meeting>of the Conference of the North American Chapter of ACL (NAACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributional representations for handling sparsity in supervised sequencelabeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the ACL and the IJCNLP of the AFNLP (ACL-AFNLP)</title>
		<meeting>of the Annual Meeting of the ACL and the IJCNLP of the AFNLP (ACL-AFNLP)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploring representation-learning approaches to domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Domain Adaptation for Natural Language Processing</title>
		<meeting>of the Workshop on Domain Adaptation for Natural Language essing</meeting>
		<imprint>
			<publisher>DANLP</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open-domain semantic role labeling by modeling word spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of ACL (ACL)</title>
		<meeting>of the Annual Meeting of ACL (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language models as representations for weaklysupervised nlp tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Comput. Natural Language Learning (CoNLL)</title>
		<meeting>of the Conference on Comput. Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Observable operator models for discrete stochastic time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1371" to="1398" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">CRFsuite: a fast implementation of conditional random fields (CRFs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mining the bibliome project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennbioie</surname></persName>
		</author>
		<ptr target="http://bioie.ldc.upenn.edu" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An introduction to hidden Markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="16" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">American national corpus (anc) second release</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Linguistic Data Consortium</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Syntactic theory : a formal introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wasow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>CSLI publications</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dependency parsing and domain adaptation with lr models and parser ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL Shared Task Session of EMNLP-CoNLL</title>
		<meeting>of CoNLL Shared Task Session of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning distributed representations for structured output prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spectral learning of refinement HMMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>of the Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2000 shared task: Chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>of the Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Meeting of the Association for Comput. Linguistics (ACL)</title>
		<meeting>of the Annual Meeting of the Association for Comput. Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
