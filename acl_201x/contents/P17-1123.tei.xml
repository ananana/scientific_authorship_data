<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Ask: Neural Question Generation for Reading Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 30-August 4, 2017. July 30-August 4, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junru</forename><surname>Shao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
						</author>
						<title level="a" type="main">Learning to Ask: Neural Question Generation for Reading Comprehension</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</title>
						<meeting>the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) <address><addrLine>Vancouver, Canada; Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1342" to="1352"/>
							<date type="published">July 30-August 4, 2017. July 30-August 4, 2017</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.18653/v1/p17-1123</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence-vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammat-icality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question generation (QG) aims to create natu- ral questions from a given a sentence or para- graph. One key application of question generation is in the area of education -to generate ques- tions for reading comprehension materials <ref type="bibr" target="#b8">(Heilman and Smith, 2010)</ref>. <ref type="figure">Figure 1</ref>, for example, shows three manually generated questions that test a user's understanding of the associated text pas- sage. Question generation systems can also be de- ployed as chatbot components (e.g., asking ques- tions to start a conversation or to request feed- back ( <ref type="bibr" target="#b24">Mostafazadeh et al., 2016)</ref>) or, arguably, as a clinical tool for evaluating or improving mental health <ref type="bibr" target="#b37">(Weizenbaum, 1966;</ref><ref type="bibr" target="#b5">Colby et al., 1971)</ref>.</p><p>In addition to the above applications, question generation systems can aid in the development of <ref type="bibr">Sentence:</ref> Oxygen is used in cellular respiration and re- leased by photosynthesis, which uses the en- ergy of sunlight to produce oxygen from water. annotated data sets for natural language process- ing (NLP) research in reading comprehension and question answering. Indeed the creation of such datasets, e.g., <ref type="bibr">SQuAD (Rajpurkar et al., 2016)</ref> and MS MARCO ( <ref type="bibr" target="#b26">Nguyen et al., 2016)</ref>, has spurred research in these areas.</p><p>For the most part, question generation has been tackled in the past via rule-based approaches (e.g., <ref type="bibr" target="#b23">Mitkov and Ha (2003)</ref>; <ref type="bibr" target="#b32">Rus et al. (2010)</ref>. The success of these approaches hinges criti- cally on the existence of well-designed rules for declarative-to-interrogative sentence transforma- tion, typically based on deep linguistic knowledge.</p><p>To improve over a purely rule-based sys- tem, <ref type="bibr" target="#b8">Heilman and Smith (2010)</ref> introduced an overgenerate-and-rank approach that generates multiple questions from an input sentence using a rule-based approach and then ranks them us- ing a supervised learning-based ranker. Although the ranking algorithm helps to produce more ac-ceptable questions, it relies heavily on a manually crafted feature set, and the questions generated of- ten overlap word for word with the tokens in the input sentence, making them very easy to answer. <ref type="bibr" target="#b36">Vanderwende (2008)</ref> point out that learning to ask good questions is an important task in NLP research in its own right, and should consist of more than the syntactic transformation of a declar- ative sentence. In particular, a natural sounding question often compresses the sentence on which it is based (e.g., question 3 in <ref type="figure">Figure 1</ref>), uses syn- onyms for terms in the passage (e.g., "form" for "produce" in question 2 and "get" for "produce" in question 3), or refers to entities from preced- ing sentences or clauses (e.g., the use of "pho- tosynthesis" in question 2). Othertimes, world knowledge is employed to produce a good ques- tion (e.g., identifying "photosynthesis" as a "life process" in question 1). In short, constructing nat- ural questions of reasonable difficulty would seem to require an abstractive approach that can pro- duce fluent phrasings that do not exactly match the text from which they were drawn.</p><p>As a result, and in contrast to all previous work, we propose here to frame the task of question gen- eration as a sequence-to-sequence learning prob- lem that directly maps a sentence from a text pas- sage to a question. Importantly, our approach is fully data-driven in that it requires no manually generated rules. More specifically, inspired by the recent suc- cess in neural machine translation <ref type="bibr" target="#b35">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, summariza- tion ( <ref type="bibr" target="#b33">Rush et al., 2015;</ref><ref type="bibr" target="#b11">Iyer et al., 2016)</ref>, and im- age caption generation ( <ref type="bibr" target="#b40">Xu et al., 2015)</ref>, we tackle question generation using a conditional neural language model with a global attention mecha- nism ( <ref type="bibr" target="#b18">Luong et al., 2015a</ref>). We investigate several variations of this model, including one that takes into account paragraph-rather than sentence-level information from the reading passage as well as other variations that determine the importance of pre-trained vs. learned word embeddings.</p><p>In evaluations on the SQuAD dataset <ref type="bibr" target="#b29">(Rajpurkar et al., 2016</ref>) using three automatic eval- uation metrics, we find that our system signif- icantly outperforms a collection of strong base- lines, including an information retrieval-based system <ref type="bibr" target="#b31">(Robertson and Walker, 1994)</ref>, a statistical machine translation approach ( <ref type="bibr" target="#b13">Koehn et al., 2007)</ref>, and the overgenerate-and-rank approach of Heil- man and <ref type="bibr" target="#b8">Smith (2010)</ref>. Human evaluations also rated our generated questions as more grammati- cal, fluent, and challenging (in terms of syntactic divergence from the original reading passage and reasoning needed to answer) than the state-of-the- art <ref type="bibr" target="#b8">Heilman and Smith (2010)</ref> system.</p><p>In the sections below we discuss related work (Section 2), specify the task definition (Section 3) and describe our neural sequence learning based models (Section 4). We explain the experimental setup in Section 5. Lastly, we present the evalua- tion results as well as a detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Reading Comprehension is a challenging task for machines, requiring both understanding of nat- ural language and knowledge of the world <ref type="bibr" target="#b29">(Rajpurkar et al., 2016)</ref>. Recently many new datasets have been released and in most of these datasets, the questions are generated in a synthetic way. For example, bAbI <ref type="bibr" target="#b39">(Weston et al., 2016</ref>) is a fully synthetic dataset featuring 20 different tasks. <ref type="bibr" target="#b9">Hermann et al. (2015)</ref> released a corpus of cloze style questions by replacing entities with place- holders in abstractive summaries of CNN/Daily Mail news articles. <ref type="bibr" target="#b1">Chen et al. (2016)</ref> claim that the CNN/Daily Mail dataset is easier than previ- ously thought, and their system almost reaches the ceiling performance. <ref type="bibr" target="#b30">Richardson et al. (2013)</ref> cu- rated MCTest, in which crowdworker questions are paired with four answer choices. Although MCTest contains challenging natural questions, it is too small for training data-demanding question answering models.</p><p>Recently, <ref type="bibr" target="#b29">Rajpurkar et al. (2016)</ref> released the Stanford Question Answering Dataset 1 (SQuAD), which overcomes the aforementioned small size and (semi-)synthetic issues. The questions are posed by crowd workers and are of relatively high quality. We use SQuAD in our work, and simi- larly, we focus on the generation of natural ques- tions for reading comprehension materials, albeit via automatic means.</p><p>Question Generation has attracted the atten- tion of the natural language generation (NLG) community in recent years, since the work of <ref type="bibr" target="#b32">Rus et al. (2010)</ref>. Most work tackles the task with a rule-based ap- proach. Generally, they first transform the input sentence into its syntactic representation, which they then use to generate an interrogative sentence. A lot of research has focused on first manually constructing question templates, and then apply- ing them to generate questions ( <ref type="bibr" target="#b25">Mostow and Chen, 2009;</ref><ref type="bibr" target="#b17">Lindberg et al., 2013;</ref><ref type="bibr" target="#b22">Mazidi and Nielsen, 2014)</ref>. <ref type="bibr" target="#b14">Labutov et al. (2015)</ref> use crowdsourcing to collect a set of templates and then rank the rel- evant templates for the text of another domain. Generally, the rule-based approaches make use of the syntactic roles of words, but not their semantic roles. <ref type="bibr" target="#b8">Heilman and Smith (2010)</ref> introduce an overgenerate-and-rank approach: their system first overgenerates questions and then ranks them. Although they incorporate learning to rank, their system's performance still depends critically on the manually constructed generating rules. <ref type="bibr" target="#b24">Mostafazadeh et al. (2016)</ref> introduce visual question generation task, to explore the deep con- nection between language and vision. <ref type="bibr" target="#b34">Serban et al. (2016)</ref> propose generating simple factoid ques- tions from logic triple (subject, relation, object). Their task tackles mapping from structured repre- sentation to natural language text, and their gen- erated questions are consistent in terms of format and diverge much less than ours.</p><p>To our knowledge, none of the previous works has framed QG for reading comprehension in an end-to-end fashion, and nor have them used deep sequence-to-sequence learning approach to gener- ate questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>In this section, we define the question generation task. Given an input sentence x, our goal is to gen- erate a natural question y related to information in the sentence, y can be a sequence of an arbitrary length: [y 1 , ..., y |y| ]. Suppose the length of the in- put sentence is M , x could then be represented as a sequence of tokens [x 1 , ..., x M ]. The QG task is defined as finding y, such that:</p><formula xml:id="formula_0">y = arg max y P (y|x)<label>(1)</label></formula><p>where P (y|x) is the conditional log-likelihood of the predicted question sequence y, given the input x. In section 4.1, we will elaborate on the global attention mechanism for modeling P (y|x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>Our model is partially inspired by the way in which a human would solve the task. To ask a natural question, people usually pay attention to certain parts of the input sentence, as well as associating context information from the para- graph. We model the conditional probability us- ing RNN encoder-decoder architecture ( <ref type="bibr" target="#b0">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b3">Cho et al., 2014)</ref>, and adopt the global attention mechanism ( <ref type="bibr" target="#b18">Luong et al., 2015a</ref>) to make the model focus on certain elements of the input when generating each word during decoding.</p><p>Here, we investigate two variations of our mod- els: one that only encodes the sentence and an- other that encodes both sentence and paragraph- level information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Decoder</head><p>Similar to <ref type="bibr" target="#b35">Sutskever et al. (2014)</ref> and , we factorize the the conditional in equa- tion 1 into a product of word-level predictions:</p><formula xml:id="formula_1">P (y|x) = |y| t=1 P (y t |x, y &lt;t )</formula><p>where probability of each y t is predicted based on all the words that are generated previously (i.e., y &lt;t ), and input sentence x.</p><p>More specifically,</p><formula xml:id="formula_2">P (y t |x, y &lt;t ) = softmax (W s tanh (W t [h t ; c t ]))<label>(2)</label></formula><p>with h t being the recurrent neural networks state variable at time step t, and c t being the attention- based encoding of x at decoding time step t (Sec- tion 4.2). W s and W t are parameters to be learned.</p><formula xml:id="formula_3">h t = LSTM 1 (y t−1 , h t−1 )<label>(3)</label></formula><p>here, LSTM is the Long Short-Term Memory (LSTM) network <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref>. It generates the new state h t , given the representation of previously generated word y t−1 (obtained from a word look-up table), and the pre- vious state h t−1 . The initialization of the decoder's hidden state differentiates our basic model and the model that incorporates paragraph-level information.</p><p>For the basic model, it is initialized by the sen- tence's representation s obtained from the sen- tence encoder (Section 4.2). For our paragraph- level model, the concatenation of the sentence encoder's output s and the paragraph encoder's output s is used as the initialization of decoder hidden state. To be more specific, the architec- ture of our paragraph-level model is like a "Y"- shaped network which encodes both sentence- and paragraph-level information via two RNN branches and uses the concatenated representation for decoding the questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Encoder</head><p>The attention-based sentence encoder is used in both of our models, while the paragraph en- coder is only used in the model that incorporates paragraph-level information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-based sentence encoder:</head><p>We use a bidirectional LSTM to encode the sen- tence,</p><formula xml:id="formula_4">− → b t = −−−−→ LSTM 2 x t , −−→ b t−1 ← − b t = ←−−−− LSTM 2 x t , ←−− b t+1</formula><p>where − → b t is the hidden state at time step t for the forward pass LSTM, ← − b t for the backward pass. To get attention-based encoding of x at decod- ing time step t, namely, c t , we first get the context dependent token representation by</p><formula xml:id="formula_5">b t = [ − → b t ; ← − b t ]</formula><p>, then we take the weighted average over b t (t = 1, ..., |x|),</p><formula xml:id="formula_6">c t = i=1,..,|x| a i,t b i<label>(4)</label></formula><p>The attention weight are calculated by the bi- linear scoring function and softmax normalization,</p><formula xml:id="formula_7">a i,t = exp h T t W b b i j exp h T t W b b j<label>(5)</label></formula><p>To get the sentence encoder's output for initial- ization of decoder hidden state, we concatenate last hidden state of the forward and backward pass, namely,</p><formula xml:id="formula_8">s = [ − − → b |x| ; ← − b 1 ].</formula><p>Paragraph encoder:</p><p>Given sentence x, we want to encode the para- graph containing x. Since in practice the para- graph is very long, we set a length threshold L, and truncate the paragraph at the L th token. We call the truncated paragraph "paragraph" henceforth.</p><p>Denoting the paragraph as z, we use another bidirectional LSTM to encode z,</p><formula xml:id="formula_9">− → d t = −−−−→ LSTM 3 z t , −− → d t−1 ← − d t = ←−−−− LSTM 3 z t , ←− − d t+1</formula><p>With the last hidden state of the forward and backward pass, we use the concatenation</p><formula xml:id="formula_10">[ −→ d |z| ; ← − d 1 ]</formula><p>as the paragraph encoder's output s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Inference</head><p>Giving a training corpus of sentence-question pairs:</p><formula xml:id="formula_11">S = x (i) , y (i) S i=1</formula><p>, our models' train- ing objective is to minimize the negative log- likelihood of the training data with respect to all the parameters, as denoted by θ,</p><formula xml:id="formula_12">L = − S i=1 log P y (i) |x (i) ; θ = − S i=1 |y (i) | j=1 log P y (i) j |x (i) , y (i) &lt;j ; θ</formula><p>Once the model is trained, we do inference us- ing beam search. The beam search is parametrized by the possible paths number k.</p><p>As there could be many rare words in the input sentence that are not in the target side dictionary, during decoding many UNK tokens will be out- put. Thus, post-processing with the replacement of UNK is necessary. Unlike <ref type="bibr" target="#b19">Luong et al. (2015b)</ref>, we use a simpler replacing strategy for our task. For the decoded UNK token at time step t, we re- place it with the token in the input sentence with the highest attention score, the index of which is arg max i a i,t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We experiment with our neural question genera- tion model on the processed SQuAD dataset. In this section, we firstly describe the corpus of the task. We then give implementation details of our neural generation model, the baselines to compare, and their experimental settings. Lastly, we intro- duce the evaluation methods by automatic metrics and human raters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>With the SQuAD dataset ( <ref type="bibr" target="#b29">Rajpurkar et al., 2016)</ref>, we extract sentences and pair them with the ques-  Since there is a hidden part of the original SQuAD that we do not have access to, we treat the accessible parts (∼90%) as the entire dataset henceforth.</p><note type="other"># sentence-question pairs &lt; 10 (1 0, 20 ] (2 0, 30 ] (3 0, 40 ] (4 0, 50 ] (5 0, 60 ] (6 0, 70 ] (7 0, 80 ] (8 0, 90 ] (9 0, 10 0]</note><p>We first run Stanford CoreNLP ( ) for pre-processing: tokenization and sen- tence splitting. We then lower-case the entire dataset. With the offset of the answer to each ques- tion, we locate the sentence containing the answer and use it as the input sentence. In some cases (&lt; 0.17% in training set), the answer spans two or more sentences, and we then use the concatenation of the sentences as the input "sentence". <ref type="figure" target="#fig_2">Figure 2</ref> shows the distribution of the token overlap percentage of the sentence-question pairs. Although most of the pairs have over 50% over- lap rate, about 6.67% of the pairs have no non- stop-words in common, and this is mostly because of the answer offset error introduced during an- notation. Therefore, we prune the training set based on the constraint: the sentence-question pair must have at least one non-stop-word in common. Lastly we add &lt;SOS&gt; to the beginning of the sen-  tences, and &lt;EOS&gt; to the end of them. We randomly divide the dataset at the article- level into a training set (80%), a development set (10%), and a test set (10%). We report results on the 10% test set. <ref type="table">Table 1</ref> provides some statistics on the pro- cessed dataset: there are around 70k training sam- ples, the sentences are around 30 tokens, and the questions are around 10 tokens on average. For each sentence, there might be multiple corre- sponding questions, and, on average, there are 1.4 questions for each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We implement our models 2 in Torch7 3 on top of the newly released OpenNMT system ( <ref type="bibr" target="#b12">Klein et al., 2017)</ref>.</p><p>For the source side vocabulary V, we only keep the 45k most frequent tokens (including &lt;SOS&gt;, &lt;EOS&gt; and placeholders). For the target side vo- cabulary U, similarly, we keep the 28k most fre- quent tokens. All other tokens outside the vocab- ulary list are replaced by the UNK symbol. We choose word embedding of 300 dimensions and use the glove.840B.300d pre-trained embed- dings ( <ref type="bibr" target="#b28">Pennington et al., 2014</ref>) for initialization. We fix the word representations during training.</p><p>We set the LSTM hidden unit size to 600 and set the number of layers of LSTMs to 2 in both the en- coder and the decoder. Optimization is performed using stochastic gradient descent (SGD), with an initial learning rate of 1.0. We start halving the learning rate at epoch 8. The mini-batch size for the update is set at 64. Dropout with probability  For a detailed explanation of the baseline systems, please refer to Section 5.3. The best performing system for each column is highlighted in boldface. Our system which encodes only sentence with pre-trained word embeddings achieves the best performance across all the metrics.</p><note type="other">Model BLEU 1 BLEU 2 BLEU 3 BLEU 4 METEOR ROUGE</note><p>0.3 is applied between vertical LSTM stacks. We clip the gradient when the its norm exceeds 5. All our models are trained on a single GPU. We run the training for up to 15 epochs, which takes approximately 2 hours. We select the model that achieves the lowest perplexity on the dev set.</p><p>During decoding, we do beam search with a beam size of 3. Decoding stops when every beam in the stack generates the &lt;EOS&gt; token.</p><p>All hyperparameters of our model are tuned us- ing the development set. The results are reported on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines</head><p>To prove the effectiveness of our system, we com- pare it to several competitive systems. Next, we briefly introduce their approaches and the experi- mental setting to run them for our problem. Their results are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>IR stands for our information retrieval baselines. Similar to <ref type="bibr" target="#b33">Rush et al. (2015)</ref>, we implement the IR baselines to control memorizing questions from the training set. We use two metrics to calculate the distance between a question and the input sen- tence, i.e., BM-25 ( <ref type="bibr" target="#b31">Robertson and Walker, 1994)</ref> and edit distance <ref type="bibr" target="#b15">(Levenshtein, 1966)</ref>. According to the metric, the system retrieves the training set to find the question with the highest score.</p><p>MOSES+ ( <ref type="bibr" target="#b13">Koehn et al., 2007</ref>) is a widely used phrase-based statistical machine translation sys- tem. Here, we treat sentences as source language text, we treat questions as target language text, and we perform the translation from sentences to ques- tions. We train a tri-gram language model on tar- get side texts with <ref type="bibr">KenLM (Heafield et al., 2013)</ref>, and tune the system with MERT on dev set. Per- formance results are reported on the test set.</p><p>DirectIn is an intuitive yet meaningful baseline in which the longest sub-sentence of the sentence is directly taken as the predicted question. <ref type="bibr">4</ref> To split the sentence into sub-sentences, we use a set of splitters, i.e., {"?", "!", ",", ".", ";"}.</p><p>H&amp;S is the rule-based overgenerate-and-rank sys- tem that was mentioned in Section 2. When run- ning the system, we set the parameter just-wh true (to restrict the output of the system to being only wh-questions) and set max-length equal to the longest sentence in the training set. We also set downweight-pro true, to down weight questions with unresolved pronouns so that they appear towards the end of the ranked list. For com- parison with our systems, we take the top question in the ranked list. <ref type="bibr">Seq2seq (Sutskever et al., 2014</ref>) is a basic encoder-decoder sequence learning system for machine translation. We implement their model in Tensorflow. The input sequence is reversed be- fore training or translating. Hyperparameters are tuned with dev set. We select the model with the lowest perplexity on the dev set.</p><p>Naturalness Difficulty Best % Avg. rank   <ref type="table">Table 3</ref>: Human evaluation results for question generation. Naturalness and difficulty are rated on a 1-5 scale (5 for the best). Two-tailed t- test results are shown for our method compared to H&amp;S (statistical significance is indicated with * (p &lt; 0.005), * * (p &lt; 0.001)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Automatic Evaluation</head><p>We use the evaluation package released by <ref type="bibr" target="#b2">Chen et al. (2015)</ref>, which was originally used to score image captions. The package includes BLEU 1, BLEU 2, BLEU 3, BLEU 4 ( <ref type="bibr" target="#b27">Papineni et al., 2002</ref>), METEOR <ref type="bibr" target="#b6">(Denkowski and Lavie, 2014</ref>) and ROUGE L <ref type="bibr" target="#b16">(Lin, 2004</ref>) evaluation scripts. BLEU measures the average n-gram precision on a set of reference sentences, with a penalty for overly short sentences. BLEU-n is BLEU score that uses up to n-grams for counting co-occurrences. ME- TEOR is a recall-oriented metric, which calculates the similarity between generations and references by considering synonyms, stemming and para- phrases. ROUGE is commonly employed to eval- uate n-grams recall of the summaries with gold- standard sentences as references. ROUGE L (mea- sured based on longest common subsequence) re- sults are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Human Evaluation</head><p>We also perform human evaluation studies to mea- sure the quality of questions generated by our sys- tem and the H&amp;S system. We consider two modal- ities: naturalness, which indicates the grammati- cality and fluency; and difficulty, which measures the sentence-question syntactic divergence and the reasoning needed to answer the question. We ran- domly sampled 100 sentence-question pairs. We ask four professional English speakers to rate the pairs in terms of the modalities above on a 1-5 scale (5 for the best). We then ask the human raters to give a ranking of the questions according to the overall quality, with ties allowed. <ref type="table" target="#tab_2">Table 2</ref> shows automatic metric evaluation results for our models and baselines. Our model which only encodes sentence-level information achieves Sentence 1: the largest of these is the eldon square shop- ping centre , one of the largest city centre shopping com- plexes in the uk .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Analysis</head><p>Human: what is one of the largest city center shopping complexes in the uk ? H&amp;S: what is the eldon square shopping centre one of ? Ours: what is one of the largest city centers in the uk ?</p><p>Sentence 2: free oxygen first appeared in significant quantities during the paleoproterozoic eon -lrb-between 3.0 and 2.3 billion years ago -rrb-.</p><p>Human: during which eon did free oxygen begin ap- pearing in quantity ? H&amp;S: what first appeared in significant quantities dur- ing the paleoproterozoic eon ? Ours: how long ago did the paleoproterozoic exhibit ?</p><p>Sentence 3: inflammation is one of the first responses of the immune system to infection .</p><p>Human: what is one of the first responses the immune system has to infection ? H&amp;S: what is inflammation one of ? Ours: what is one of the first objections of the immune system to infection ?</p><p>Sentence 4: tea , coffee , sisal , pyrethrum , corn , and wheat are grown in the fertile highlands , one of the most successful agricultural production regions in Africa.</p><p>Human: (1) where is the most successful agricultural prodcution regions ? <ref type="formula" target="#formula_2">(2)</ref> what is grown in the fertile highlands ? H&amp;S: what are grown in the fertile highlands in africa ? Ours: what are the most successful agricultural produc- tion regions in africa ?</p><p>Sentence 5: as an example , income inequality did fall in the united states during its high school movement from 1910 to 1940 and thereafter .</p><p>Human: during what time period did income inequality decrease in the united states ? H&amp;S: where did income inequality do fall during its high school movement from 1910 to 1940 and thereafter as an example ? Ours: when did income inequality fall in the us ?</p><p>Sentence 6: however , the rainforest still managed to thrive during these glacial periods , allowing for the sur- vival and evolution of a broad diversity of species . Human: did the rainforest managed to thrive during the glacial periods ? H&amp;S: what are treaties establishing european union ? Ours: why do the birds still grow during glacial periods ?</p><p>Sentence 7: maududi founded the jamaat-e-islami party in 1941 and remained its leader until 1972. Human: when did maududi found the jamaat-e-islami party ? H&amp;S: who did maududi remain until 1972 ? Ours: when was the jamaat-e-islami party founded ? <ref type="figure">Figure 3</ref>: Sample output questions generated by human (ground truth questions), our system and the H&amp;S system.   <ref type="table">Table 4</ref>: An estimate of categories of questions of the processed dataset and per-category performance comparison of the systems. The estimate is based on our analysis of the 346 pairs from the dev set. Categories are decided by the information needed to generate the question. Bold numbers represent the best performing method for a given metric. * Here, we leave out performance results for "w/ article" category (2 samples, 0.58%) and "not askable" category (33 samples, 9.54%).</p><p>the best performance across all metrics. We note that IR performs poorly, indicating that memoriz- ing the training set is not enough for the task. The baseline DirectIn performs pretty well on BLEU and METEOR, which is reasonable given the over- lap statistics between the sentences and the ques- tions ( <ref type="figure" target="#fig_2">Figure 2</ref>). H&amp;S system's performance is on a par with DirectIn's, as it basically performs syn- tactic change without paraphrasing, and the over- lap rate is also high.</p><p>Looking at the performance of our three mod- els, it's clear that adding the pre-trained embed- dings generally helps. While encoding the para- graph causes the performance to drop a little, this makes sense because, apart from useful informa- tion, the paragraph also contains much noise. <ref type="table">Table 3</ref> shows the results of the human evalua- tion. We see that our system outperforms H&amp;S in all modalities. Our system is ranked best in 38.4% of the evaluations, with an average ranking of 1.94. An inter-rater agreement of Krippendorff's Alpha of 0.236 is achieved for the overall rank- ing. The results imply that our model can generate questions of better quality than the H&amp;S system.</p><p>For our qualitative analysis, we examine the sample outputs and the visualization of the align- ment between the input and the output. In <ref type="figure">Fig- ure 3</ref>, we present sample questions generated by H&amp;S and our best model. We see a large gap be- tween our results and H&amp;S's. For example, in the first sample, in which the focus should be put on "the largest." Our model successfully captures this information, while H&amp;S only performs some syntactic transformation over the input without paraphrasing. However, outputs from our system are not always "perfect", for example, in pair 6, our system generates a question about the reason why birds still grow, but the most related question would be why many species still grow. But from a different perspective, our question is more chal- lenging (readers need to understand that birds are one kind of species), which supports our system's performance listed in human evaluations (See <ref type="table">Ta- ble 3</ref>). It would be interesting to further investigate how to interpret why certain irrelavant words are generated in the question. <ref type="figure" target="#fig_6">Figure 4</ref> shows the at- tention weights (α i,t ) for the input sentence when generating each token in the question. We see that the key words in the output ("introduced", "tele- text", etc.) aligns well with those in the input sen- tence. Finally, we do a dataset analysis and fine- grained system performance analysis. We ran- domly sampled 346 sentence-question pairs from the dev set and label each pair with a category. <ref type="bibr">5</ref> The four categories are determined by how much information is needed to ask the question. To be specific, "w/ sentence" means it only requires the sentence to ask the question; "w/ paragraph" means it takes other information in the paragraph to ask the question; "w/ article" is similar to "w/ paragraph"; and "not askable" means that world knowledge is needed to ask the question or there is mismatch of sentence and question caused by an- notation error. <ref type="table">Table 4</ref> shows the per-category performance of the systems. Our model which encodes paragraph information achieves the best performance on the questions of "w/ paragraph" category. This veri- fies the effectiveness of our paragraph-level model on the questions concerning information outside the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We have presented a fully data-driven neural net- works approach to automatic question generation for reading comprehension. We use an attention- based neural networks approach for the task and investigate the effect of encoding sentence-vs. paragraph-level information. Our best model achieves state-of-the-art performance in both au- tomatic evaluations and human evaluations.</p><p>Here we point out several interesting future re- search directions. Currently, our paragraph-level model does not achieve best performance across all categories of questions. We would like to ex- plore how to better use the paragraph-level infor- mation to improve the performance of QG system regarding questions of all categories. Besides this, it would also be interesting to consider to incor- porate mechanisms for other language generation tasks (e.g., copy mechanism for dialogue genera- tion) in our model to further improve the quality of generated questions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>-</head><label></label><figDesc>Figure 1: Sample sentence from the second paragraph of the article Oxygen, along with the natural questions and their answers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overlap percentage of sentence-question pairs in training set. y-axis is # non-stop-words overlap with respect to the total # tokens in the question (a percentage); x-axis is # sentencequestion pairs for a given overlap percentage range. tions. We train our models with the sentencequestion pairs. The dataset contains 536 articles with over 100k questions posed about the articles. The authors employ Amazon Mechanical Turks crowd-workers to create questions based on the Wikipedia articles. Workers are encouraged to use their own words without any copying phrases from the paragraph. Later, other crowd-workers are employed to provide answers to the questions. The answers are spans of tokens in the passage. Since there is a hidden part of the original SQuAD that we do not have access to, we treat the accessible parts (∼90%) as the entire dataset henceforth. We first run Stanford CoreNLP (Manning et al., 2014) for pre-processing: tokenization and sentence splitting. We then lower-case the entire dataset. With the offset of the answer to each question, we locate the sentence containing the answer and use it as the input sentence. In some cases (&lt; 0.17% in training set), the answer spans two or more sentences, and we then use the concatenation of the sentences as the input "sentence". Figure 2 shows the distribution of the token overlap percentage of the sentence-question pairs. Although most of the pairs have over 50% overlap rate, about 6.67% of the pairs have no nonstop-words in common, and this is mostly because of the answer offset error introduced during annotation. Therefore, we prune the training set based on the constraint: the sentence-question pair must have at least one non-stop-word in common. Lastly we add &lt;SOS&gt; to the beginning of the sen</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>#</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>H&amp;S</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Category</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Heatmap of the attention weight matrix, which shows the soft alignment between the sentence (left) and the generated question (top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Automatic evaluation results of different systems by BLEU 1-4, METEOR and ROUGE L .</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://stanford-qa.com</note>

			<note place="foot" n="2"> The code is available at https://github.com/ xinyadu/nqg. 3 http://torch.ch/</note>

			<note place="foot" n="4"> We also tried using the entire input sentence as the prediction output, but the performance is worse than taking subsentence as the prediction, across all the automatic metrics except for METEOR.</note>

			<note place="foot" n="5"> The IDs of the questions examined will be made available at https://github.com/xinyadu/nqg/ blob/master/examined-question-ids.txt.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous ACL reviewers, Kai Sun and Yao Cheng for their helpful suggestions. We thank Victoria Litvinova for her careful proofread-ing. We also thank Xanda Schofield, Wil Thoma-son, Hubert Lin and Junxian He for doing the hu-man evaluations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A thorough examination of the cnn/daily mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1223" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2358" to="2367" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Artificial paranoia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth Mark</forename><surname>Colby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franklin</forename><forename type="middle">Dennis</forename><surname>Hilf</surname></persName>
		</author>
		<idno type="doi">10.1016/0004-3702</idno>
		<ptr target="https://doi.org/10.1016/0004-3702" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90002" to="90008" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W14-3348" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation. Association for Computational Linguistics</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation. Association for Computational Linguistics<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable modified kneser-ney language model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Pouzyrevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P13-2121" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="690" to="696" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Good question! statistical ranking for question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N10-1086" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="609" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Summarizing source code using a neural attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep questions without deep understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1086" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions and reversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">707</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W/W04/W04-1013.pdf" />
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. Association for Computational Linguistics</title>
		<editor>Stan Szpakowicz Marie-Francine Moens</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating natural language questions to support learning on-line</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Popowich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Winne</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W13-2114" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Workshop on Natural Language Generation. Association for Computational Linguistics</title>
		<meeting>the 14th European Workshop on Natural Language Generation. Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<ptr target="http://www.aclweb.org/anthology/P15-1002" />
		<title level="m">Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">F</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-5010" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Linguistic considerations in automatic question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Mazidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rodney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nielsen</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P14-2053" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Computeraided generation of multiple-choice tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Mitkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le An</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL 03 Workshop on Building Educational Applications Using Natural Language Processing</title>
		<editor>Jill Burstein and Claudia Leacock</editor>
		<meeting>the HLT-NAACL 03 Workshop on Building Educational Applications Using Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating natural questions about an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating instruction automatically for the reading strategy of selfquestioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Mostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Question Generation</title>
		<meeting>the 2nd Workshop on Question Generation</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Ms marco: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Philadelphia</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Philadelphia<address><addrLine>Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://aclweb.org/anthology/D16-" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MCTest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Renshaw</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D13-1020" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>New York, NY, USA, SIGIR &apos;94</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The first question generation shared task evaluation challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Vasile Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wyse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Piwek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lintean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Stoyanchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Natural Language Generation Conference. Association for Computational Linguistics</title>
		<meeting>the 6th International Natural Language Generation Conference. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="251" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/D15-1044" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1056" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="588" to="598" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The importance of being important: Question generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on the Question Generation Shared Task Evaluation Challenge</title>
		<meeting>the 1st Workshop on the Question Generation Shared Task Evaluation Challenge<address><addrLine>Arlington, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Eliza&amp;mdash;a computer program for the study of natural language communication between man and machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Weizenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="45" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<idno type="doi">10.1145/365153.365168</idno>
		<ptr target="https://doi.org/10.1145/365153.365168" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
