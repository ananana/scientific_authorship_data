<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Neural Autoencoder for Paragraphs and Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 26-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Neural Autoencoder for Paragraphs and Documents</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
						<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1106" to="1115"/>
							<date type="published">July 26-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models , our work has the potential to significantly impact natural language generation and summarization 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generating coherent text is a central task in natural language processing. A wide variety of theories exist for representing relationships between text units, such as Rhetorical Structure Theory ( <ref type="bibr" target="#b19">Mann and Thompson, 1988)</ref> or Discourse Representa- tion Theory ( <ref type="bibr" target="#b11">Lascarides and Asher, 1991)</ref>, for ex- tracting these relations from text units <ref type="bibr" target="#b20">(Marcu, 2000;</ref><ref type="bibr" target="#b12">LeThanh et al., 2004;</ref><ref type="bibr" target="#b7">Hernault et al., 2010;</ref><ref type="bibr" target="#b6">Feng and Hirst, 2012</ref>, inter alia), and for extract- ing other coherence properties characterizing the role each text unit plays with others in a discourse ( <ref type="bibr" target="#b1">Barzilay and Lapata, 2008</ref>; Barzilay and Lee, <ref type="bibr">1</ref> Code for models described in this paper are available at www.stanford.edu/ ˜ jiweil/.</p><p>2004; <ref type="bibr" target="#b4">Elsner and Charniak, 2008;</ref>, inter alia). However, applying these to text generation remains difficult. To understand how discourse units are connected, one has to under- stand the communicative function of each unit, and the role it plays within the context that en- capsulates it, recursively all the way up for the entire text. Identifying increasingly sophisticated human-developed features may be insufficient for capturing these patterns. But developing neural- based alternatives has also been difficult. Al- though neural representations for sentences can capture aspects of coherent sentence structure <ref type="bibr" target="#b9">(Ji and Eisenstein, 2014;</ref>), it's not clear how they could help in gener- ating more broadly coherent text.</p><p>Recent LSTM models <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>) have shown powerful results on gen- erating meaningful and grammatical sentences in sequence generation tasks like machine translation <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b18">Luong et al., 2015</ref>) or parsing ( ). This performance is at least partially attributable to the ability of these systems to capture local compositionally: the way neighboring words are combined semantically and syntactically to form meanings that they wish to express.</p><p>Could these models be extended to deal with generation of larger structures like paragraphs or even entire documents? In standard sequence- to-sequence generation tasks, an input sequence is mapped to a vector embedding that represents the sequence, and then to an output string of words. Multi-text generation tasks like summa- rization could work in a similar way: the sys- tem reads a collection of input sentences, and is then asked to generate meaningful texts with certain properties (such as-for summarization- being succinct and conclusive). Just as the local semantic and syntactic compositionally of words can be captured by LSTM models, can the com-positionally of discourse releations of higher-level text units (e.g., clauses, sentences, paragraphs, and documents) be captured in a similar way, with clues about how text units connect with each an- other stored in the neural compositional matrices?</p><p>In this paper we explore a first step toward this task of neural natural language generation. We fo- cus on the component task of training a paragraph (document)-to-paragraph (document) autoencoder to reconstruct the input text sequence from a com- pressed vector representation from a deep learn- ing model. We develop hierarchical LSTM mod- els that arranges tokens, sentences and paragraphs in a hierarchical structure, with different levels of LSTMs capturing compositionality at the token- token and sentence-to-sentence levels.</p><p>We offer in the following section to a brief de- scription of sequence-to-sequence LSTM models. The proposed hierarchical LSTM models are then described in Section 3, followed by experimental results in Section 4, and then a brief conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Long-Short Term Memory (LSTM)</head><p>In this section we give a quick overview of LSTM models. LSTM models <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>) are defined as follows: given a sequence of inputs X = {x 1 , x 2 , ..., x n X }, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as i t , f t and o t . For notations, we disambiguate e and h where e t denote the vector for individual text unite (e.g., word or sentence) at time step t while h t denotes the vector computed by LSTM model at time t by combining e t and h t−1 . σ denotes the sigmoid function. The vector representation h t for each time-step t is given by:</p><formula xml:id="formula_0">i t f t o t l t = σ σ σ tanh W · h t−1 e t<label>(1)</label></formula><formula xml:id="formula_1">c t = f t · c t−1 + i t · l t (2) h s t = o t · c t<label>(3)</label></formula><p>where W ∈ R 4K×2K In sequence-to-sequence generation tasks, each input X is paired with a sequence of outputs to predict: Y = {y 1 , y 2 , ..., y n Y }. An LSTM defines a distribution over outputs and sequentially predicts tokens us- ing a softmax function: <ref type="bibr">[1,ny]</ref> p(y t |x 1 , x 2 , ..., x t , y 1 , y 2 , ..., y t−1 ) = t∈ <ref type="bibr">[1,ny]</ref> exp(f (h t−1 , e yt )) y exp(f (h t−1 , e y )) (4) f (h t−1 , e yt ) denotes the activation function be- tween e h−1 and e yt , where h t−1 is the representa- tion outputted from the LSTM at time t − 1. Note that each sentence ends up with a special end-of- sentence symbol &lt;end&gt;. Commonly, the input and output use two different LSTMs with differ- ent sets of convolutional parameters for capturing different compositional patterns.</p><formula xml:id="formula_2">P (Y |X) = t∈</formula><p>In the decoding procedure, the algorithm termi- nates when an &lt;end&gt; token is predicted. At each timestep, either a greedy approach or beam search can be adopted for word prediction. Greedy search selects the token with the largest conditional prob- ability, the embedding of which is then combined with preceding output for next step token predic- tion. For beam search, ( ) dis- covered that a beam size of 2 suffices to provide most of benefits of beam search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Paragraph Autoencoder</head><p>In this section, we introduce our proposed hierar- chical LSTM model for the autoencoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>Let D denote a paragraph or a document, which is comprised of a sequence of N D sentences,</p><formula xml:id="formula_3">D = {s 1 , s 2 , ..., s N D , end D }. An additional "end D " token is appended to each document.</formula><p>Each sentence s is comprised of a sequence of tokens s = {w 1 , w 2 , ..., w Ns } where N s denotes the length of the sentence, each sentence end- ing with an "end s " token. The word w is as- sociated with a K-dimensional embedding e w , e w = {e 1 w , e 2 w , ..., e K w }. Let V denote vocabu- lary size. Each sentence s is associated with a K- dimensional representation e s .</p><p>An autoencoder is a neural model where output units are directly connected with or identical to in- put units. Typically, inputs are compressed into a representation using neural models (encoding), which is then used to reconstruct it back (decod- ing). For a paragraph autoencoder, both the input X and output Y are the same document D. The autoencoder first compresses D into a vector rep- resentation e D and then reconstructs D based on e D .</p><p>For simplicity, we define LST M (h t−1 , e t ) to be the LSTM operation on vectors h t−1 and e t to achieve h t as in Equ.1 and 2. For clarification, we first describe the following notations used in encoder and decoder:</p><p>• h w t and h s t denote hidden vectors from LSTM models, the subscripts of which indicate timestep t, the superscripts of which indi- cate operations at word level (w) or sequence level (s). h s t (enc) specifies encoding stage and h s t (dec) specifies decoding stage.</p><p>• e w t and e s t denotes word-level and sentence- level embedding for word and sentence at po- sition t in terms of its residing sentence or document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model 1: Standard LSTM</head><p>The whole input and output are treated as one sequence of tokens. Following  and <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref>, we trained an autoencoder that first maps input documents into vector representations from a LST M encode and then reconstructs inputs by predicting to- kens within the document sequentially from a LST M decode . Two separate LSTMs are imple- mented for encoding and decoding with no sen- tence structures considered. Illustration is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model 2: Hierarchical LSTM</head><p>The hierarchical model draws on the intuition that just as the juxtaposition of words creates a joint meaning of a sentence, the juxtaposition of sen- tences also creates a joint meaning of a paragraph or a document.</p><p>Encoder We first obtain representation vectors at the sentence level by putting one layer of LSTM (denoted as LST M word encode ) on top of its containing words:</p><formula xml:id="formula_4">h w t (enc) = LST M word encode (e w t , h w t−1 (enc)) (5)</formula><p>The vector output at the ending time-step is used to represent the entire sentence as</p><formula xml:id="formula_5">e s = h w ends</formula><p>To build representation e D for the current doc- ument/paragraph D, another layer of LSTM (de- noted as LST M sentence encode ) is placed on top of all sen- tences, computing representations sequentially for each timestep:</p><formula xml:id="formula_6">h s t (enc) = LST M sentence encode (e s t , h s t−1 (enc)) (6) Representation e s end D</formula><p>computed at the final time step is used to represent the entire document:</p><formula xml:id="formula_7">e D = h s end D .</formula><p>Thus one LSTM operates at the token level, leading to the acquisition of sentence-level rep- resentations that are then used as inputs into the second LSTM that acquires document-level repre- sentations, in a hierarchical structure.</p><p>Decoder As with encoding, the decoding algo- rithm operates on a hierarchical structure with two layers of LSTMs. LSTM outputs at sentence level for time step t are obtained by:</p><formula xml:id="formula_8">h s t (dec) = LST M sentence decode (e s t , h s t−1 (dec)) (7)</formula><p>The initial time step h s 0 (d) = e D , the end-to-end output from the encoding procedure. h s t (d) is used as the original input into LST M word decode for subse- quently predicting tokens within sentence t + 1. LST M word decode predicts tokens at each position se- quentially, the embedding of which is then com- bined with earlier hidden vectors for the next time- step prediction until the end s token is predicted. The procedure can be summarized as follows:</p><formula xml:id="formula_9">h w t (dec) = LST M sentence decode (e w t , h w t−1 (dec)) (8) p(w|·) = softmax(e w , h w t−1 (dec))<label>(9)</label></formula><p>During decoding, LST M word decode generates each word token w sequentially and combines it with earlier LSTM-outputted hidden vectors. The LSTM hidden vector computed at the final time step is used to represent the current sentence. This is passed to LST M sentence decode , combined with h s t for the acquisition of h t+1 , and outputted to the next time step in sentence decoding.</p><p>For each timestep t, LST M sentence decode has to first decide whether decoding should proceed or come to a full stop: we add an additional token end D to the vocabulary. Decoding terminates when token end D is predicted. Details are shown in <ref type="figure" target="#fig_1">Figure 2</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model 3: Hierarchical LSTM with Attention</head><p>Attention models adopt a look-back strategy by linking the current decoding stage with input sen- tences in an attempt to consider which part of the input is most responsible for the current decoding state. This attention version of hierarchical model is inspired by similar work in image caption gen- eration and machine translation ( <ref type="bibr" target="#b25">Xu et al., 2015;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>.</p><formula xml:id="formula_10">Let H = {h s 1 (e), h s 2 (e), ..., h s N (e)</formula><p>} be the collection of sentence-level hidden vectors for each sentence from the inputs, outputted from LST M Sentence encode . Each element in H contains in- formation about input sequences with a strong fo- cus on the parts surrounding each specific sentence (time-step). During decoding, suppose that e s t</p><note type="other">de- notes the sentence-level embedding at current step and that h s t−1 (dec) denotes the hidden vector out- putted from LST M sentence decode at previous time step t−1. Attention models would first link the current- step decoding information, i.e., h s t−1 (dec) which is outputted from LST M sentence dec with each of the input sentences i ∈ [1, N ], characterized by a strength indicator v i :</note><formula xml:id="formula_11">v i = U T f (W 1 · h s t−1 (dec) + W 2 · h s i (enc)) (10) W 1 , W 2 ∈ R K×K , U ∈ R K×1 . v i is then normal- ized:</formula><formula xml:id="formula_12">a i = exp(v i ) i exp(v i )<label>(11)</label></formula><p>The attention vector is then created by averaging weights over all input sentences:</p><formula xml:id="formula_13">m t = i∈[1,N D ] a i h s i (enc)<label>(12)</label></formula><p>LSTM hidden vectors for current step is then achieved by combining c t , e s t and h s t−1 (dec):</p><formula xml:id="formula_14">i t f t o t l t = σ σ σ tanh W · h s t−1 (dec) e s t m t<label>(13)</label></formula><formula xml:id="formula_15">c t = f t · c t−1 + i t · l t<label>(14)</label></formula><formula xml:id="formula_16">h s t = o t · c t<label>(15)</label></formula><p>where W ∈ R 4K×3K . h t is then used for word predicting as in the vanilla version of the hierar- chical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Testing</head><p>Parameters are estimated by maximizing likeli- hood of outputs given inputs, similar to standard sequence-to-sequence models. A softmax func- tion is adopted for predicting each token within output documents, the error of which is first back- propagated through For testing, we adopt a greedy strategy with no beam search. For a given document D, e D is first obtained given already learned LSTM encode parameters and word embeddings. Then in decod- ing, LST M sentence decode computes embeddings at each sentence-level time-step, which is first fed into the binary classifier to decide whether sentence de- coding terminates and then into LST M word decode for word decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We implement the proposed autoencoder on two datasets, a highly domain specific dataset consist- ing of hotel reviews and a general dataset extracted from Wkipedia.</p><p>Hotel Reviews We use a subset of hotel reviews crawled from TripAdvisor. We consider only re- views consisting sentences ranging from 50 to 250 words; the model has problems dealing with ex- tremely long sentences, as we will discuss later. We keep a vocabulary set consisting of the 25,000 most frequent words. A special "&lt;unk&gt;" token is used to denote all the remaining less frequent tokens. Reviews that consist of more than 2 per- cent of unknown words are discarded. Our train- ing dataset is comprised of roughly 340,000 re- views; the testing set is comprised of 40,000 re- views. Dataset details are shown in <ref type="table">Table 1</ref>.</p><p>Wikipedia We extracted paragraphs from Wikipedia corpus that meet the aforementioned length requirements. We keep a top frequent vocabulary list of 120,000 words. Paragraphs with larger than 4 percent of unknown words are discarded. The training dataset is comprised of roughly 500,000 paragraphs and testing contains roughly 50,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details and Implementation</head><p>Previous research has shown that deep LSTMs work better than shallow ones for sequence-to- sequence tasks ( ). We adopt a LSTM structure with four layer for encoding and four layer for decoding, each of which is comprised of a different set of pa- rameters. Each LSTM layer consists of 1,000 hid- den neurons and the dimensionality of word em- beddings is set to 1,000. Other training details are given below, some of which follow .</p><p>• Input documents are reversed.</p><p>• LSTM parameters and word embeddings are initialized from a uniform distribution be- tween [-0.08, 0.08].</p><p>• Stochastic gradient decent is implemented without momentum using a fixed learning rate of 0.1. We stated halving the learning rate every half epoch after 5 epochs. We trained our models for a total of 7 epochs.</p><p>• Batch size is set to 32 (32 documents).</p><p>• Decoding algorithm allows generating at most 1.5 times the number of words in inputs.</p><p>• 0.2 dropout rate.</p><p>• Gradient clipping is adopted by scaling gra- dients when the norm exceeded a threshold of 5. Our implementation on a single GPU 2 processes a speed of approximately 600-1,200 tokens per sec- ond. We trained our models for a total of 7 itera- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluations</head><p>We need to measure the closeness of the output (candidate) to the input (reference). We first adopt two standard evaluation metrics, ROUGE <ref type="bibr" target="#b17">(Lin, 2004;</ref><ref type="bibr" target="#b15">Lin and Hovy, 2003</ref>) and BLEU ( <ref type="bibr" target="#b21">Papineni et al., 2002</ref>).</p><p>ROUGE is a recall-oriented measure widely used in the summarization literature. It measures the n-gram recall between the candidate text and the reference text(s). In this work, we only have one reference document (the input document) and ROUGE score is therefore given by: where count match denotes the number of n-grams co-occurring in the input and output. We report ROUGE-1, 2 and W (based on weighted longest common subsequence).</p><formula xml:id="formula_17">ROUGE n = gram n ∈input count match (gram n ) gram n ∈input count(gram n )<label>(16)</label></formula><p>BLEU Purely measuring recall will inappropri- ately reward long outputs. BLEU is designed to address such an issue by emphasizing precision. n-gram precision scores for our situation are given by:</p><p>precision n = gram n ∈output count match (gram n ) gram n ∈output count(gram n ) (17) BLEU then combines the average logarithm of precision scores with exceeded length penaliza- tion. For details, see <ref type="bibr" target="#b21">Papineni et al. (2002)</ref>.</p><p>Coherence Evaluation Neither BLEU nor ROUGE attempts to evaluate true coherence. There is no generally accepted and readily avail- able coherence evaluation metric. <ref type="bibr">3</ref> Because of the difficulty of developing a universal coherence evaluation metric, we proposed here only a tailored metric specific to our case. Based on the assumption that human-generated texts (i.e., input documents in our tasks) are coherent ( <ref type="bibr" target="#b1">Barzilay and Lapata, 2008)</ref>, we compare generated outputs with input documents in terms of how much original text order is preserved.</p><p>We develop a grid evaluation metric similar to the entity transition algorithms in ( <ref type="bibr" target="#b2">Barzilay and Lee, 2004;</ref><ref type="bibr" target="#b10">Lapata and Barzilay, 2005</ref>). The key idea of Barzilay and Lapata's models is to first identify grammatical roles (i.e., object and sub- ject) that entities play and then model the transi- tion probability over entities and roles across sen- tences. We represent each sentence as a feature- vector consisting of verbs and nouns in the sen- tence. Next we align sentences from output doc- uments to input sentences based on sentence-to- sentence F1 scores (precision and recall are com- puted similarly to ROUGE and BLEU but at sen- tence level) using feature vectors. Note that multi- ple output sentences can be matched to one input Input-Wiki washington was unanimously elected President by the electors in both the 1788 -1789 and 1792 elections . he oversaw the creation of a strong, well-financed national government that maintained neutrality in the french revolutionary wars , suppressed the whiskey rebellion , and won acceptance among Americans of all types . washington established many forms in govern- ment still used today , such as the cabinet system and inaugural address . his retirement after two terms and the peaceful transition from his presidency to that of john adams established a tradition that continued up until franklin d . roosevelt was elected to a third term . washington has been widely hailed as the " father of his country " even during his lifetime. Output-Wiki washington was elected as president in 1792 and voters &lt;unk&gt; of these two elections until 1789 . he continued suppression &lt;unk&gt; whiskey rebellion of the french revolution war gov- ernment , strong , national well are involved in the establishment of the fin advanced operations , won acceptance . as in the government , such as the establishment of various forms of inau- guration speech washington , and are still in use . &lt;unk&gt; continued after the two terms of his quiet transition to retirement of &lt;unk&gt; &lt;unk&gt; of tradition to have been elected to the third paragraph . but , " the united nations of the father " and in washington in his life , has been widely praised . Input-Wiki apple inc . is an american multinational corporation headquartered in cupertino , california , that designs , develops , and sells consumer electronics , computer software , online services , and personal com -puters . its bestknown hardware products are the mac line of computers , the ipod media player , the iphone smartphone , and the ipad tablet computer . its online services include icloud , the itunes store , and the app store . apple's consumer software includes the os x and ios operating systems , the itunes media browser , the safari web browser , and the ilife and iwork creativity and productivity suites . Output-Wiki apple is a us company in california , &lt;unk&gt; , to develop electronics , softwares , and pc , sells . hardware include the mac series of computers , ipod , iphone . its online services , including icloud , itunes store and in app store . softwares , including os x and ios operating system , itunes , web browser , &lt; unk&gt; , including a productivity suite . Input-Wiki paris is the capital and most populous city of france . situated on the seine river , in the north of the country , it is in the centre of the le-de-france region . the city of paris has a population of 2273305 inhabitants . this makes it the fifth largest city in the european union measured by the population within the city limits . Output-Wiki paris is the capital and most populated city in france . located in the &lt;unk&gt; , in the north of the country , it is the center of &lt;unk&gt; . paris , the city has a population of &lt;num&gt; inhabitants . this makes the eu ' s population within the city limits of the fifth largest city in the measurement . Input-Review on every visit to nyc , the hotel beacon is the place we love to stay . so conveniently located to central park , lincoln center and great local restaurants . the rooms are lovely . beds so comfortable , a great little kitchen and new wizz bang coffee maker . the staff are so accommo- dating and just love walking across the street to the fairway supermarket with every imaginable goodies to eat . Output-Review every time in new york , lighthouse hotel is our favorite place to stay . very convenient , central park , lincoln center , and great restaurants . the room is wonderful , very comfortable bed , a kitchenette and a large explosion of coffee maker . the staff is so inclusive , just across the street to walk to the supermarket channel love with all kinds of what to eat . <ref type="table">Table 2</ref>: A few examples produced by the hierarchical LSTM alongside the inputs.</p><p>sentence. Assume that sentence s i output is aligned with sentence s i input , where i and i denote position index for a output sentence and its aligned input. The penalization score L is then given by:</p><formula xml:id="formula_18">L = 2 N output · (N output − 1) × i∈[1,Noutput−1] j∈[i+1,Noutput] |(j − i) − (j − i )| (18)</formula><p>Equ. 18 can be interpreted as follows: (j − i) denotes the distance in terms of position index be- tween two outputted sentences indexed by j and i, and (j − i ) denotes the distance between their mirrors in inputs. As we wish to penalize the degree of permutation in terms of text order, we penalize the absolute difference between the two computed distances. This metric is also relevant to the overall performance of prediction and re- call: an irrelevant output will be aligned to a ran- dom input, thus being heavily penalized. The de- ficiency of the proposed metric is that it concerns itself only with a semantic perspective on coher- ence, barely considering syntactical issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>A summary of our experimental results is given in  documents and sentences are written in a more fixed format and easy to predict for hotel reviews. The hierarchical model that considers sentence- level structure outperforms standard sequence- to-sequence models. Attention models at the sentence level introduce performance boost over vanilla hierarchical models.</p><p>With respect to the coherence evaluation, the original sentence order is mostly preserved: the hi- erarchical model with attention achieves L = 1.57 on the hotel-review dataset, equivalent to the fact that the relative position of two input sentences are permuted by an average degree of 1.57. Even for the Wikipedia dataset where more poor-quality sentences are observed, the original text order can still be adequately maintained with L = 2.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>In this paper, we extended recent sequence-to- sequence LSTM models to the task of multi- sentence generation. We trained an autoencoder to see how well LSTM models can reconstruct in- put documents of many sentences. We find that the proposed hierarchical LSTM models can par- tially preserve the semantic and syntactic integrity of multi-text units and generate meaningful and grammatical sentences in coherent order. Our model performs better than standard sequence-to- sequence models which do not consider the intrin- sic hierarchical discourse structure of texts.</p><p>While our work on auto-encoding for larger texts is only a preliminary effort toward allowing neural models to deal with discourse, it nonethe- less suggests that neural models are capable of en- coding complex clues about how coherent texts are connected .</p><p>The performance on this autoencoder task could certainly also benefit from more sophisticated neu- ral models. For example one extension might align the sentence currently being generated with the original input sentence (similar to sequence-to- sequence translation in ( <ref type="bibr" target="#b0">Bahdanau et al., 2014)</ref>), and later transform the original task to sentence- to-sentence generation. However our long-term goal here is not on perfecting this basic multi-text generation scenario of reconstructing input docu- ments, but rather on extending it to more important applications.</p><p>That is, the autoencoder described in this work, where input sequence X is identical to output Y , is only the most basic instance of the family of doc- ument (paragraph)-to-document (paragraph) gen- eration tasks. We hope the ideas proposed in this paper can play some role in enabling such more sophisticated generation tasks like summa- rization, where the inputs are original documents and outputs are summaries or question answering, where inputs are questions and outputs are the ac- tual wording of answers. Sophisticated genera- tion tasks like summarization or dialogue systems could extend this paradigm, and could themselves benefit from task-specific adaptations. In sum- marization, sentences to generate at each timestep might be pre-pointed to or pre-aligned to specific aspects, topics, or pieces of texts to be summa- rized. Dialogue systems could incorporate infor- mation about the user or the time course of the dialogue. In any case, we look forward to more sophi4d applications of neural models to the im- portant task of natural language generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Standard Sequence to Sequence Model.</figDesc><graphic url="image-2.png" coords="4,136.77,226.64,324.00,190.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchical Sequence to Sequence Model.</figDesc><graphic url="image-3.png" coords="4,136.77,454.71,324.00,183.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Hierarchical Sequence to Sequence Model with Attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>We observe better performances for 
the hotel-review dataset than the open domain 
Wikipedia dataset, for the intuitive reason that </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for three models on two datasets. As with coherence score L, smaller values signifies 
better performances. 

</table></figure>

			<note place="foot" n="3"> Wolf and Gibson (2005) and Lin et al. (2011) proposed metrics based on discourse relations, but these are hard to apply widely since identifying discourse relations is a difficult problem. Indeed sophisticated coherence evaluation metrics are seldom adopted in real-world applications, and summarization researchers tend to use simple approximations like number of overlapped tokens or topic distribution similarity (e.g., (Yan et al., 2011b; Yan et al., 2011a; Celikyilmaz and Hakkani-Tür, 2011)).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>The authors want to thank Gabor Angeli, Sam Bowman, Percy Liang and other members of the Stanford NLP group for insightful comments and suggestion. We also thank the three anonymous ACL reviewers for helpful comments. This work is supported by Enlight Foundation Graduate Fel-lowship, and a gift from Bloomberg L.P, which we gratefully acknowledge.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Catching the drift: Probabilistic content models, with applications to generation and summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno>cs/0405039</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovery of topically coherent sentences for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="491" to="499" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coreference-inspired coherence modeling</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Textlevel discourse parsing with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Vanessa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hilda: a discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Representation learning for text-level discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic evaluation of text coherence: Models and representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1085" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discourse relations and defeasible knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lascarides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Asher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 29th annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating discourse structures for written texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huong</forename><surname>Lethanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geetha</forename><surname>Abeysinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Huyck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">329</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recursive deep models for discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatically evaluating text coherence using discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The rhetorical parsing of unrestricted texts: A surface-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="448" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7449</idno>
		<title level="m">Grammar as a foreign language</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representing discourse coherence: A corpus-based study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="287" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<title level="m">Show, attend and tell: Neural image caption generation with visual attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Timeline generation through evolutionary trans-temporal summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congrui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="433" to="443" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evolutionary timeline summarization: a balanced optimization framework via iterative substitution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahna</forename><surname>Otterbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
