<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>July 15-20, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">⇤</forename><forename type="middle">Shikhar</forename><surname>Murty</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers)</title>
						<meeting>the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers) <address><addrLine>Melbourne, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="263" to="272"/>
							<date type="published">July 15-20, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE) (Vendrov et al., 2016), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE (Lai and Hockenmaier, 2017) have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anticorrelation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts , and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entail-ment graphs, and investigate the power of the model. * Equal contribution.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Structured embeddings based on regions, densi- ties, and orderings have gained popularity in re- cent years for their inductive bias towards the essential asymmetries inherent in problems such as image captioning <ref type="bibr">(Vendrov et al., 2016)</ref>, lexi- cal and textual entailment <ref type="bibr" target="#b5">(Erk, 2009;</ref><ref type="bibr">Vilnis and McCallum, 2015;</ref><ref type="bibr">Lai and Hockenmaier, 2017;</ref><ref type="bibr" target="#b0">Athiwaratkun and Wilson, 2018)</ref>, and knowledge graph completion and reasoning <ref type="bibr" target="#b9">(He et al., 2015;</ref><ref type="bibr">Nickel and Kiela, 2017;</ref><ref type="bibr">Li et al., 2017)</ref>.</p><p>Models that easily encode asymmetry, and re- lated properties such as transitivity (the two com- ponents of commonplace relations such as par- tially ordered sets and lattices), have great utility in these applications, leaving less to be learned from the data than arbitrary relational models. At their best, they resemble a hybrid between embed- ding models and structured prediction. As noted by <ref type="bibr">Vendrov et al. (2016)</ref> and <ref type="bibr">Li et al. (2017)</ref>, while the models learn sets of embeddings, these param- eters obey rich structural constraints. The entire set can be thought of as one, sometimes provably consistent, structured prediction, such as an ontol- ogy in the form of a single directed acyclic graph.</p><p>While the structured prediction analogy ap- plies best to Order Embeddings (OE), which em- beds consistent partial orders, other region-and density-based representations have been proposed for the express purpose of inducing a bias to- wards asymmetric relationships. For example, the Gaussian Embedding (GE) model <ref type="bibr">(Vilnis and McCallum, 2015</ref>) aims to represent the asymmetry and uncertainty in an object's relations and at- tributes by means of uncertainty in the represen- tation. However, while the space of representa- tions is a manifold of probability distributions, the model is not truly probabilistic in that it does not model asymmetries and relations in terms of prob-abilities, but in terms of asymmetric comparison functions such as the originally proposed KL di- vergence and the recently proposed thresholded divergences <ref type="bibr" target="#b0">(Athiwaratkun and Wilson, 2018</ref>).</p><p>Probabilistic models are especially compelling for modeling ontologies, entailment graphs, and knowledge graphs. Their desirable properties include an ability to remain consistent in the presence of noisy data, suitability towards semi- supervised training using the expectations and un- certain labels present in these large-scale applica- tions, the naturality of representing the inherent uncertainty of knowledge they store, and the abil- ity to answer complex queries involving more than 2 variables. Note that the final one requires a true joint probabilistic model with a tractable inference procedure, not something provided by e.g. matrix factorization.</p><p>We take the dual approach to density-based embeddings and model uncertainty about rela- tionships and attributes as explicitly probabilistic, while basing the probability on a latent space of geometric objects that obey natural structural bi- ases for modeling transitive, asymmetric relations. The most similar work are the probabilistic order embeddings (POE) of <ref type="bibr">Lai (Lai and Hockenmaier, 2017)</ref>, which apply a probability measure to each order embedding's forward cone (the set of points greater than the embedding in each dimension), assigning a finite and normalized volume to the unbounded space. However, POE suffers severe limitations as a probabilistic model, including an inability to model negative correlations between concepts, which motivates the construction of our box lattice model. Our model represents objects, concepts, and events as high-dimensional products-of-intervals (hyperrectangles or boxes), with an event's unary probability coming from the box volume and joint probabilities coming from overlaps. This contrasts with POE's approach of defining events as the for- ward cones of vectors, extending to infinity, in- tegrated under a probability measure that assigns them finite volume.</p><p>One desirable property of a structured represen- tation for ordered data, originally noted in <ref type="bibr">(Vendrov et al., 2016</ref>) is a "slackness" shared by OE, POE, and our model: when the model predicts an "edge" or lack thereof (i.e. P (a|b) = 0 or 1, or a zero constraint violation in the case of OE), being exposed to that fact again will not update the model. Moreover, there are large degrees of freedom in parameter space that exhibit this slack- ness, giving the model the ability to embed com- plex structure with 0 loss when compared to mod- els based on symmetric inner products or distances between embeddings, e.g. bilinear <ref type="bibr">GLMs (Collins et al., 2002</ref>), Trans-E ( <ref type="bibr" target="#b2">Bordes et al., 2013)</ref>, and other embedding models which must always be pushing and pulling parameters towards and away from each other.</p><p>Our experiments demonstrate the power of our approach to probabilistic ordering-biased rela- tional modeling. First, we investigate an instruc- tive 2-dimensional toy dataset that both demon- strates the way the model self organizes its box event space, and enables sensible answers to queries involving arbitrary numbers of variables, despite being trained on only pairwise data. We achieve a new state of the art in denotational prob- ability modeling on the Flickr entailment dataset <ref type="bibr">(Lai and Hockenmaier, 2017)</ref>, and a matching state-of-the-art on WordNet hypernymy ( <ref type="bibr">Vendrov et al., 2016;</ref><ref type="bibr">Miller, 1995)</ref> with the concurrent work on thresholded Gaussian embedding of <ref type="bibr" target="#b0">Athiwaratkun and Wilson (2018)</ref>, achieving our best results by training on additional co-occurrence ex- pectations aggregated from leaf types.</p><p>We find that the strong empirical performance of probabilistic ordering models, and our box lat- tice model in particular, and their endowment of new forms of training and querying, make them a promising avenue for future research in represent- ing structured knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In addition to the related work in structured em- beddings mentioned in the introduction, our focus on directed, transitive relational modeling and on- tology induction shares much with the rich field of directed graphical models and causal model- ing <ref type="bibr">(Pearl, 1988)</ref>, as well as learning the struc- ture of those models <ref type="bibr" target="#b10">(Heckerman et al., 1995)</ref>. Work in undirected structure learning such the Graphical Lasso ( <ref type="bibr" target="#b7">Friedman et al., 2008</ref>) is also relevant due to our desire to learn from pairwise joint/conditional probabilities and moment matri- ces, which are closely related in the setting of dis- crete variables.</p><p>Especially relevant research in Bayesian net- works are applications towards learning taxo- nomic structure of relational data ( <ref type="bibr" target="#b1">Bansal et al., 2014)</ref>, although this work is often restricted to- wards tree-shaped ontologies, which allow effi- cient inference by Chu-Liu-Edmonds' algorithm ( <ref type="bibr" target="#b3">Chu and Liu, 1995)</ref>, while we focus on arbitrary DAGs.</p><p>As our model is based on populating a latent "event space" into boxes (products of intervals), it is especially reminiscent of the Mondrian pro- cess ( <ref type="bibr">Roy and Teh, 2009)</ref>. However, the Mondrian process partitions the space as a high dimensional tree (a non-parametric kd-tree), while our model allows the arbitrary box placement required for DAG structure, and is much more tractable in high dimensions compared to the Mondrian's Bayesian non-parametric inference.</p><p>Embedding applications to relational learning constitute a huge field to which it is impossible to do justice, but one general difference between our approaches is that e.g. a matrix factorization model treats the embeddings as objects to score re- lation links with, as opposed to POE or our model in which embeddings represent subsets of proba- bilistic event space which are directly integrated. They are full probabilistic models of the joint set of variables, rather than embedding-based approx- imations of only low-order joint and conditional probabilities. That is, any set of our parame- ters can answer any arbitrary probabilistic ques- tion (possibly requiring intractable computation), rather than being fixed to modeling only certain subsets of the joint.</p><p>Embedding-based learning's large advantage over the combinatorial structure learning pre- sented by classical PGM approaches is its applica- bility to large-scale probability distributions con- taining hundreds of thousands of events or more, as in both our WordNet and Flickr experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Partial Orders and Lattices</head><p>A non-strict partial ordered set (poset) is a set P equipped with a binary relation such that for all a, b, c 2 P ,</p><p>• a a (reflexivity)</p><p>• a b a implies a = b (antisymmetry)</p><p>• a b c implies a c (transitivity) This is simply a generalization of a totally ordered set that allows some elements to be incomparable, and is a good model for the kind of acyclic directed graph data found in knowledge bases.</p><p>A lattice is a poset where any subset has a a unique least upper and greatest lower bound, which will be true of all posets (lattices) consid- ered in this paper. The least upper bound of two elements a, b 2 P is called the join, denoted a _ b, and the greatest lower bound is called the meet, denoted a ^ b.</p><p>Additionally, in a bounded lattice we have two extra elements, called top, denoted &gt; and bot- tom, denoted ?, which are respectively the least upper bound and greatest lower bound of the en- tire space. Using the extended real number line (adding points at infinity), all lattices considered in this paper are bounded lattices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Order Embeddings (OE)</head><p>Vendrov et al. <ref type="formula">(2016)</ref> introduced a method for em- bedding partially ordered sets and a task, partial order completion, an abstract term for things like hypernym or entailment prediction (learning tran- sitive relations). The goal is to learn a mapping from the partially-ordered data domain to some other partially-ordered space that will enable gen- eralization.</p><formula xml:id="formula_0">Definition 1. Vendrov et al. (2016) A function f : (X, X ) ! (Y, Y ) is an order- embedding if for all u, v 2 X u X v () f (u) Y f (v)</formula><p>They choose Y to be a vector space, and the order Y to be based on the reverse product order on R n + , which specifies</p><formula xml:id="formula_1">x y () 8i 2 {1..n}, x i y i</formula><p>so an embedding is below another in the hierarchy if all of the coordinates are larger, and 0 provides a top element. Although <ref type="bibr">Vendrov et al. (2016)</ref> do not explic- itly discuss it, their model does not just capture partial orderings, but is a standard construction of a vector (Hilbert) lattice, in which the opera- tions of meet and join can be defined as taking the pointwise maximum and minimum of two vectors, respectively <ref type="bibr">(Zaanen, 1997)</ref>. This observation is also used in ( <ref type="bibr">Li et al., 2017</ref>) to generate extra con- straints for training order embeddings.</p><p>As noted in the original work, these single point embeddings can be thought of as regions, i.e. the cone extending out from the vector towards infin- ity. All concepts "entailed" by a given concept must lie in this cone.</p><p>This ordering is optimized from examples of or- dered elements and negative samples via a max- margin loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Probabilistic Order Embeddings (POE)</head><p>Lai and Hockenmaier (2017) built on the "region" idea to derive a probabilistic formulation (which we will refer to as POE) to model entailment prob- abilities in a consistent, hierarchical way.</p><p>Noting that all of OE's regions obviously have the same infinite area under the standard (Lebesgue) measure of R n + , they propose a prob- abilistic interpretation where the Bernoulli prob- ability of each concept a or joint set of concepts {a, b} with corresponding vectors {x, y} is given by its volume under the exponential measure:</p><formula xml:id="formula_2">p(a) = exp( X i x i ) = Z zx exp(kzk 1 )dz p(a, b) = p(x ^ y) = exp(k max(x i , y i )k 1 )</formula><p>since the meet of two vectors is simply the in- tersection of their area cones, and replacing sums with`1with`with`1 norms for brevity since all coordinates are positive. While having the intuition of measuring the areas of cones, this also automatically gives a valid probability distribution over concepts since this is just the product likelihood under a coordi- natewise exponential distribution. However, they note a deficiency of their model -it can only model positive (Pearson) correla- tions between concepts (Bernoulli variables).</p><p>Consider two Bernoulli variables a and b, whose probabilities correspond to the areas of cones x and y. Recall the Bernoulli covariance formula (we will deal with covariances instead of correlations when convenient, since they always have the same sign):</p><formula xml:id="formula_3">cov(a, b) = p(a, b) p(a)p(b) = exp(k max(x i , y i )k 1 ) exp(kx i + y i k 1 )</formula><p>Since the sum of two positive vectors can only be greater than the sum of their pointwise max- imum, this quantity will always be nonnegative. This has real consequences for probabilistic mod- eling in KBs: conditioning on more concepts will only make probabilities higher (or unchanged), e.g. p(dog|plant) p(dog).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Probabilistic Asymmetric Transitive Relations</head><p>Probabilistic models have pleasing consistency properties for modeling asymmetric transitive re- lations, in particular compared to density-based embeddings -a pairwise conditional probability table can almost always (in the technical sense) be asymmetrized to produce a DAG by simply taking an edge if P (a|b) &gt; P(b|a). A matrix of pair- wise Gaussian KL divergences cannot be consis- tently asymmetrized in this manner. These claims are proven in Appendix C. While a high P (a|b) does not always indicate an edge in an ontology due to confounding variables, existing graphical model structure learning methods can be used to further prune on the base graph without adding a cycle, such as Graphical Lasso or simple thresh- olding <ref type="bibr" target="#b6">(Fattahi and Sojoudi, 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>We develop a probabilistic model for lattices based on hypercube embeddings that can model both positive and negative correlations. Before describ- ing this, we first motivate our choice to abandon OE/POE type cone-based models for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Correlations from Cone Measures</head><p>Claim. For a pair of Bernoulli variables p(a) and p(b), cov(a, b) 0 if the Bernoulli probabili- ties come from the volume of a cone as measured under any product (coordinate-wise) probability measure p(x) = Q n i p i (x i ) on R n , where F i , the associated CDF for p i , is monotone increasing.</p><p>Proof. For any product measure we have</p><formula xml:id="formula_4">Z zx p(z)dz = n Y i Z x i z i p i (z i )dz i = n Y i 1 F i (x i )</formula><p>This is just the area of the unique box correspond- ing to</p><formula xml:id="formula_5">Q n i [F i (x i ), 1] 2 [0, 1] n ,</formula><p>under the uniform measure. This box is unique as a monotone in- creasing univariate CDF is bijective with (0, 1) - cones in R n can be invertibly mapped to boxes of equivalent measure inside the unit hypercube <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> n . These boxes have only half their degrees of freedom, as they have the form [F i (x i ), 1] per dimension, (intuitively, they have one end "stuck at infinity" since the cone integrates to infinity.</p><p>So W.L.O.G. we can consider two transformed cones x and y corresponding to our Bernoulli variables a and b, and letting F i (x i ) = u i and F i (y i ) = v i , their intersection in the unit hyper- cube is Q n i [max(u i , v i ), 1]. Pairing terms in the right-hand product, we have</p><formula xml:id="formula_6">p(a, b) p(a)p(b) = n Y i (1 max(u i , v i )) n Y i (1 u i )(1 v i ) 0</formula><p>since the right contains all the terms of the left and can only grow smaller. This argument is easily modified to the case of the nonnegative orthant, mutatis mutandis.</p><p>An open question for future work is what non- product measures this claim also applies to. Note that some non-product measures, such as multi- variate Gaussian, can be transformed into product measures easily (whitening) and the above proof would still apply. It seems probable that some measures, nonlinearly entangled across dimen- sions, could encode negative correlations in cone volumes. However, it is not generally tractable to integrate high-dimensional cones under arbitrary non-product measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Box Lattices</head><p>The above proof gives us intuition about the pos- sible form of a better representation. Cones can be mapped into boxes within the unit hypercube while preserving their measure, and the lack of negative correlation seems to come from the fact that they always have an overly-large intersection due to "pinning" the maximum in each dimension to 1. To remedy this, we propose to learn repre- sentations in the space of all boxes (axis-aligned hyperrectangles), gaining back an extra degree of freedom. These representations can be learned with a suitable probability measure in R n , the non- negative orthant R n + , or directly in the unit hyper- cube with the uniform measure, which we elect.</p><p>We associate each concept with 2 vectors, the minimum and maximum value of the box at each dimension. Practically for numerical reasons these are stored as a minimum, a positive offset plus an ✏ term to prevent boxes from becoming too small and underflowing.</p><p>Let us define our box embeddings as a pair of vectors in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> n , (x m , x M ), representing the maximum and minimum at each coordinate.</p><p>Then we can define a partial ordering by inclu- sion of boxes, and a lattice structure as x ^ y = ? if x and y disjoint, else</p><formula xml:id="formula_7">x ^ y = Y i [max(x m,i , y m,i ), min(x M,i , y M,i )] x _ y = Y i [min(x m,i , y m,i ), max(x M,i , y M,i )]</formula><p>where the meet is the intersecting box, or bottom (the empty set) where no intersection exists, and join is the smallest enclosing box. This lattice, considered on its own terms as a non-probabilistic object, is strictly more general than the order em- bedding lattice in any dimension, which is proven in Appendix B. However, the finite sizes of all the lattice el- ements lead to a natural probabilistic interpre- tation under the uniform measure. Joint and marginal probabilities are given by the volume of the (intersection) box. For concept a with associ- ated box (x m , x M ), probability is simply p(a) = Q n i (x M,i x m,i ) (under the uniform measure). p(?) is of course zero since no probability mass is assigned to the empty set.</p><p>It remains to show that this representation can represent both positive and negative correlations.</p><p>Claim. For a pair of Bernoulli variables p(a) and p(b), corr(a, b) can take on any value in <ref type="bibr">[1,</ref><ref type="bibr">1]</ref> if the probabilities come from the volume of associ- ated boxes in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> n .</p><p>Proof. Boxes can clearly model disjointness (ex- actly 1 correlation if the total volume of the boxes equals 1). Two identical boxes give their concepts exactly correlation 1. The area of the meet is continuous with respect to translations of intersecting boxes, and all other terms in correla- tion stay constant, so by continuity of the corre- lation function our model can achieve all possible correlations for a pair of variables. This proof can be extended to boxes in R n with product measures by the previous reduction.</p><p>Limitations: Note that this model cannot per- fectly describe all possible probability distribu- tions or concepts as embedded objects. For exam- ple, the complement of a box is not a box. How- ever, queries about complemented variables can be calculated by the Inclusion-Exclusion princi- ple, made more efficient by the fact that all non- negated terms can be grouped and calculated ex- actly. We show some toy exact calculations with negated variables in Appendix A. Also, note that in a knowledge graph often true complements are not required -for example mortal and immortal are not actually complements, because the concept color is neither.</p><p>Additionally, requiring the total probability mass covered by boxes to equal 1, or exactly matching marginal box probabilities while model- ing all correlations is a difficult box-packing-type problem and not generally possible. Modeling limitations aside, the union of boxes having mass &lt; 1 can be seen as an open-world assumption on our KB (not all points in space have corresponding concepts, yet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning</head><p>While inference (calculation of pairwise joint, unary marginal, and pairwise conditional proba- bilities) is quite straightforward by taking inter- sections of boxes and computing volumes (and their ratios), learning does not appear easy at first glance. While the (sub)gradient of the joint prob- ability is well defined when boxes intersect, it is non-differentiable otherwise. Instead we optimize a lower bound.</p><p>Clearly</p><formula xml:id="formula_8">p(a _ b) p(a [ b)</formula><p>, with equality only when a = b, so this can give us a lower bound:</p><formula xml:id="formula_9">p(a ^ b) = p(a) + p(b) p(a [ b) p(a) + p(b) p(a _ b)</formula><p>Where probabilities are always given by the vol- ume of the associated box. This lower bound al- ways exists and is differentiable, even when the joint is not. It is guaranteed to be nonpositive ex- cept when a and b intersect, in which case the true joint likelihood should be used. While a negative bound on a probability is odd, inspecting the bound we see that its gradient will push the enclosing box to be smaller, while in- creasing areas of the individual boxes, until they intersect, which is a sensible learning strategy.</p><p>Since we are working with small probabilities it is advisable to negate this term and maximize the negative logarithm:</p><formula xml:id="formula_10">log(p(a _ b) p(a) p(b))</formula><p>This still has an unbounded gradient as the lower bound approaches 0, so it is also useful to add a constant within the logarithm function to avoid nu- merical problems.</p><p>Since the likelihood of the full data is usu- ally intractable to compute as a conjunction of many negations, we optimize binary conditional and unary marginal terms separately by maximum likelihood.</p><p>In this work, we parametrize the boxes as (min, = max min), with Euclidean pro- jections after gradient steps to keep our parame- ters in the unit hypercube and maintain the mini- mum/delta constraints. Now that we have the ability to compute prob- abilities and (surrogate) gradients for arbitrary marginals in the model, and by extension condi- tionals, we will see specific examples in the ex- periments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Warmup: 2D Embedding of a Toy Lattice</head><p>We begin by investigating properties of our model in modeling a small toy problem, consisting of a small hand constructed ontology over 19 con- cepts, aggregated from atomic synthetic examples first into a probabilistic lattice (e.g. some rabbits are brown, some are white), and then a full CPD. We model it using only 2 dimensions to enable visualization of the way the model self-organizes its "event space", training the model by mini- mize weighted cross-entropy with both the unary marginals and pairwise conditional probabilities. We also conduct a parallel experiment with POE as embedded in the unit cube, where each repre- sentation is constrained to touch the faces x = 1, y = 1. In <ref type="figure">Figure 2</ref>, we show the represen- tation of lattice structures by POE and the box lattice model as compared to the abstract proba- bilistic lattice used to construct the data, shown in <ref type="figure" target="#fig_0">Figure 1</ref>, and compare the conditional probabili- ties produced by our model to the ground truth, demonstrating the richer capacity of the box model in capturing strong positive and negative correla- tions. In <ref type="table">Table 1</ref>, we perform a series of multivari- able conditional queries and demonstrate intuitive results on high-order queries containing up to 4 variables, despite the model being trained on only 2-way information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">WordNet</head><p>We experiment on WordNet hypernym prediction, using the same train, development and test split as <ref type="bibr">Vendrov et al. (2016)</ref>, created by randomly taking 4,000 hypernym pairs from the 837,888-  P(grizzly bear | ... ) P(cactus | ... ) P(plant | ... ) P(grizzly bear) 0.12 P(cactus) 0.10 P(plant) 0.20 omnivore 0.29 green 0.16 green 0.37 white 0.00 plant 0.39 snake 0.00 brown 0.30 american, green 0.19 carnivore 0.00 omnivore, white 0.00 plant, green, american 0.40 cactus 0.78 omnivore, brown 0.38 american, carnivore 0.00 american, cactus 0.85 <ref type="table">Table 1</ref>: Multi-way queries: conditional probabilities adjust when adding additional evidence or contra- diction. In constrast, POE can only raise or preserve probability when conditioning. term1 term2 craftsman.n.02</p><p>shark.n.03 homogenized milk.n.01 apple juice.n.01 tongue depresser.n.01 paintbrush.n.01 deerstalker.n.01 bathing cap.n.01 skywriting.n.01 transcript.n.01  Since our model is probabilistic, we would like a sensible value for P (n), where n is a node. We assign these marginal probabilities by looking at the number of descendants in the hierarchy un- der a node, and normalizing over all nodes, taking</p><formula xml:id="formula_11">P (n) = | descendants(n) | | nodes | .</formula><p>Furthermore, we use the graph structure (only of the subset of edges in the training set to avoid leaking data) to augment the data with ap- proximate conditional probabilities P (x|y). For each leaf, we consider all of its ancestors as pairwise co-occurences, then aggregate and di- vide by the number of leaves to get an approx- imate joint probability distribution, P (x, y) = | x, y co-occur in ancestor set | | leaves | . With this and the unary marginals, we can create a conditional probabil- ity table, which we prune based on the difference of P (x|y) and P (y|x) and add cross entropy with these conditional "soft edges" to the training data. We refer to experiments using this additional data as Box + CPD in <ref type="table" target="#tab_1">Table 3</ref>.</p><p>We use 50 dimensions in our experiments. Since our model has 2 parameters per dimension, we also perform an apples-to-apples comparison with a 100D POE model. As seen in <ref type="table" target="#tab_1">Table 3</ref>, we outperform POE significantly even with this added representational power. We also observe sensible negatively correlated examples, shown in 2, in the trained box model, while POE cannot represent such relationships. We tune our mod- els on the development set, with parameters docu- mented in Appendix D.1. We observe that not only does our model outperform POE, it beats all previ- ous results on WordNet, aside from the concurrent work of Athiwaratkun and Wilson (2018) (using different train/dev negative examples), the base- line POE model does as well. This indicates that probabilistic embeddings for transitive relations are a promising avenue for future work. Addition- ally, the ability of the model to learn from the ex- pected "soft edges" improves it to state-of-the-art level. We expect that co-occurrence counts gath- ered from real textual corpora, rather than merely aggregating up the WordNet lattice, would further strengthen this effect.   We conduct experiments on the large-scale Flickr entailment dataset of 45 million image cap- tion pairs. We use the exactly same train/dev/test from <ref type="bibr">Lai and Hockenmaier (2017)</ref>. We use a slightly different unseen word pairs and unseen words test data, obtained from the author. We include their published results and also use their published code, marked ⇤, for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Flickr Entailment Graph</head><p>For these experiments, we relax our boxes from the unit hypercube to the nonnegative or- thant and obtain probabilities under the exponen- tial measure, p(x) = exp(x). We enforce the nonnegativity constraints by clipping the LSTM- generated embedding (Hochreiter and Schmidhu- ber, 1997) for the box minimum with a ReLU, and parametrize our embeddings using a soft- plus activation to prevent dead units. As in <ref type="bibr">Lai and Hockenmaier (2017)</ref>, we use 512 hidden units in our LSTM to compose sentence vectors. We then apply two single-layer feed-forward networks with 512 units applied to the final LSTM state to produce the embeddings.</p><p>As we can see from <ref type="table" target="#tab_3">Table 4</ref>, we note large im- provements in KL and Pearson correlation to the ground truth entailment probabilities. In further analysis, <ref type="figure" target="#fig_2">Figure 3</ref> demonstrates that while the box model outperforms POE in nearly every regime, the highest gains come from the comparatively dif- ficult to calibrate small entailment probabilities, indicating the greater capability of our model to produce fine-grained distinctions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We have only scratched the surface of possible applications. An exciting direction is the in- corporation of multi-relational data for general knowledge representation and inference. Sec- ondly, more complex representations, such as 2n-dimensional products of 2-dimensional con- vex polyhedra, would offer greater flexibility in tiling event space. Improved inference of the la- tent boxes, either through better optimization or through Bayesian approaches is another natural extension. Our greatest interest is in the applica- tion of this powerful new tool to the many areas where other structured embeddings have shown promise.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Representation of the toy probabilistic lattice used in Section 5.1. Darker color corresponds to more unary marginal probability. The associated CPD is obtained by a weighted aggregation of leaf elements.</figDesc><graphic url="image-3.png" coords="7,99.80,318.80,174.54,171.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>.</head><label></label><figDesc>Figure 2: Lattice representations and conditional probabilities from POE vs. box lattice. Note how the box lattice model's lack of "anchoring" to a corner allows it vastly more expressivity in matching the ground truth CPD seen in Figure 1.</figDesc><graphic url="image-6.png" coords="7,319.02,506.61,196.36,188.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: R between model and gold probabilities.</figDesc><graphic url="image-7.png" coords="9,72.00,134.53,226.78,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Negatively correlated variables produced 
by the model. 

Method 
Test Accuracy % 
transitive 
88.2 
word2gauss 
86.6 
OE 
90.6 
Li et al. (2017) 
91.3 
DOE (KL) 
92.3 
POE 
91.6 
POE (100 dim) 
91.7 
Box 
92.2 
Box + CPD 
92.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Classification accuracy on WordNet test 
set. 

edge transitive closure of the WordNet hypernym 
hierarchy as positive training examples for the 
development set, 4,000 for the test set, and us-
ing the rest as training data. Negative training 
examples are created by randomly corrupting a 
train/development/test edge (u, v) by replacing ei-
ther u or v with a randomly chosen negative node. 
We use their specific train/dev/test split, while 
Athiwaratkun and Wilson (2018) use a different 
train/dev split with the same test set (personal 
communication) to examine the effect of different 
negative sampling techniques. We cite their best 
performing model, called DOE (KL). 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>KL and Pearson correlation between 
model and gold probability. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We thank Alice Lai for making the code from her original paper public, and for providing the additional unseen pairs and unseen words data. We also thank Haw-Shiuan Chang, Laurent Dinh, and Ben Poole for helpful discussions. We also thank the anonymous reviewers for their construc-tive feedback.</p><p>This work was supported in part by the Center for Intelligent Information Retrieval and the Cen-ter for Data Science, in part by the Chan Zucker-berg Initiative under the project Scientific Knowl-edge Base Construction., and in part by the Na-tional Science Foundation under Grant No. IIS-1514053. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On modeling hierarchical data via probabilistic order embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Gordon</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structured learning for taxonomy induction with belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">De</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1041" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Sinica</title>
		<imprint>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A generalization of principal components analysis to the exponential family</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representing words as regions in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL &apos;09</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning, CoNLL &apos;09<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="57" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Graphical lasso and thresholding: Equivalence and closedform solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salar</forename><surname>Fattahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somayeh</forename><surname>Sojoudi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09479</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse inverse covariance estimation with the graphical lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="432" to="441" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to represent knowledge graphs with gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</title>
		<meeting>the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="623" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning bayesian networks: The combination of knowledge and statistical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="243" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
